---
title: "Overview"
---

::: {.callout-note title="What you'll learn in this module"}
This module transforms you from a coder writing syntax to a manager orchestrating intelligence. We explore the fundamental shift from copilots (next-token predictors) to agents (autonomous task completers), and then build the three operational components that power agentic AI: effective prompt engineering, the ReAct loop that enables autonomous reasoning, and context engineering that manages the LLM's working memory.
:::

## From Copilot to Agent

The shift from copilot to agent is not just an upgrade in model size. It is a fundamental change in how you interact with AI.

A copilot like GitHub Copilot operates on next-token prediction. It looks at your cursor position and uses the probability distribution $P(x_{t+1} | x_{0:t})$ to guess the next few characters. It is a smart typewriter. Fast, helpful, but ultimately passive. You write, and it completes. It requires your constant attention and cannot act independently.

An agent like Claude Code, Google Antigravity, or Cursor operates on task completion. You give it a high-level goal like "Refactor this module," and it engages in a loop of reasoning, action, and observation until the task is done. It reads files, runs terminal commands, calls external APIs, and fixes its own errors. The intelligence doesn't come from a larger model. It comes from the feedback loop that allows the agent to observe the consequences of its actions and adjust based on what it learns.

This shifts your role fundamentally. You are no longer the writer of syntax. You become the manager-architect. Your job is no longer to know the exact syntax of a matplotlib plot, but to know what plot you need, how to clearly specify that requirement, and how to verify that the agent built it correctly. You move from implementation to orchestration.

## What You'll Build

This module introduces three operational layers that power agentic AI.

In the hands-on session, you'll use Google Antigravity to build a functional game and refactor a codebase entirely through natural language instructions. This experience grounds the theory that follows.

Prompt tuning teaches you how to communicate effectively with LLMs by understanding them as stateless pattern matchers sampling from probability distributions. You'll structure prompts using instruction, data, format, persona, and context to reliably activate desired patterns. This is the interface layer.

Agentic AI explains the ReAct loop (Reason + Act) that transforms a passive language model into an autonomous agent. You'll build a working agent using LangGraph that can query and analyze datasets without human intervention. This is the engine layer.

Context engineering solves the context window problem. LLMs are brilliant but bounded. They have limited working memory that degrades as it fills. You'll learn to manage context across its lifecycle: write through scratchpads and memories, select using MCP and just-in-time retrieval, compress through summarization, and isolate using multi-agent architectures. This is the operating system layer.

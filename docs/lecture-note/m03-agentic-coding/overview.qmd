---
title: "Overview"
---

::: {.callout-note title="What you'll learn in this module"}
This module transforms you from a coder writing syntax to a manager orchestrating intelligence.

You'll learn:

- The fundamental shift from **copilots** (next-token predictors) to **agents** (autonomous task completers) and why this changes your role from writer to architect.
- How to **communicate effectively with LLMs** through structured prompt engineering using instruction, data, format, persona, and context.
- The **ReAct loop** (Reason + Act) that transforms a passive language model into an autonomous agent capable of task completion.
- **Context engineering** techniques to manage the LLM's working memory across its lifecycle through scratchpads, retrieval, summarization, and multi-agent architectures.
:::

## From Copilot to Agent

Have you ever wished your coding assistant could actually finish the task instead of just suggesting the next line? The shift from copilot to agent is not just an upgrade in model size. It is a fundamental change in how you interact with AI.

Let's talk about copilots first. A copilot like GitHub Copilot operates on next-token prediction. It looks at your cursor position and uses the probability distribution $P(x_{t+1} | x_{0:t})$ to guess the next few characters.

Think of it as a smart typewriter. Fast, helpful, but ultimately passive. You write, and it completes. It requires your constant attention and cannot act independently.

Now shift your attention to agents. An agent like Claude Code, Google Antigravity, or Cursor operates on task completion. You give it a high-level goal like "Refactor this module," and it engages in a loop of reasoning, action, and observation until the task is done.

What makes this different? The agent reads files, runs terminal commands, calls external APIs, and fixes its own errors. The intelligence doesn't come from a larger model. It comes from the feedback loop that allows the agent to observe the consequences of its actions and adjust based on what it learns.

This shifts your role fundamentally. You are no longer the writer of syntax. You become the manager-architect.

Your job is no longer to know the exact syntax of a matplotlib plot. Instead, you know what plot you need, how to clearly specify that requirement, and how to verify that the agent built it correctly. You move from implementation to orchestration.

## What You'll Build

This module introduces three operational layers that power agentic AI. Think of them as the interface, the engine, and the operating system.

We start with hands-on practice. In the hands-on session, you'll use Google Antigravity to build a functional game and refactor a codebase entirely through natural language instructions. This experience grounds the theory that follows.

What's the interface layer? Prompt tuning teaches you how to communicate effectively with LLMs by understanding them as stateless pattern matchers sampling from probability distributions. You'll structure prompts using instruction, data, format, persona, and context to reliably activate desired patterns.

Now for the engine layer. Agentic AI explains the ReAct loop (Reason + Act) that transforms a passive language model into an autonomous agent. You'll build a working agent using LangGraph that can query and analyze datasets without human intervention.

Finally, the operating system layer. Context engineering solves the context window problem. LLMs are brilliant but bounded. They have limited working memory that degrades as it fills.

How do you manage this constraint? You'll learn to manage context across its lifecycle. Write through scratchpads and memories. Select using MCP and just-in-time retrieval. Compress through summarization. Isolate using multi-agent architectures.

---
title: "Overview"
---

::: {.callout-note appearance="simple"}
**Spoiler**: You are no longer a coder; you are a manager. The era of writing syntax is ending, replaced by the era of orchestrating intelligence.
:::

## The Mechanism

The shift from **Copilot** to **Agent** is not just an upgrade in model size; it is a fundamental change in the interaction loop.

**Copilot** (e.g., GitHub Copilot, early Gemini Code Assist) operates on **Next-Token Prediction**. It looks at your cursor position and uses the probability distribution $P(x_{t+1} | x_{0:t})$ to guess the next few characters. It is a "smart typewriter"—fast, helpful, but ultimately passive. It requires your constant attention and cannot act independently. You write; it completes.

**Agents** (e.g., Claude Code, Google Antigravity, Cursor) operate on **Task Completion**. They function like autonomous interns. You give them a high-level goal ("Refactor this module"), and they engage in a loop of **Reasoning**, **Action**, and **Observation** until the task is done. They read files, run terminal commands, call external APIs, and fix their own errors. The intelligence doesn't come from a larger model—it comes from the **feedback loop** that allows the agent to observe the consequences of its actions and adjust.

This shifts your role from the "Writer of Syntax" to the **"Manager-Architect"**. Your job is no longer to know the exact syntax of a `matplotlib` plot, but to know what plot you need, how to clearly specify that requirement, and how to verify that the agent built it correctly. You move from implementation to orchestration.

## The Application

This module breaks agentic AI into three operational components:

[**From ChatBot to Agentic AI**](agentic-ai.qmd) explains the core mechanism—the ReAct loop (Reason + Act) that transforms a passive language model into an autonomous agent. You'll build a working agent using LangGraph that can query and analyze datasets without human intervention. This is the engine.

[**Prompt Tuning**](prompt-tuning.qmd) teaches you how to communicate effectively with LLMs by understanding them as stateless pattern matchers sampling from probability distributions. You'll learn to structure prompts (instruction, data, format, persona, context) to reliably activate desired patterns. This is the interface.

[**Context Engineering**](context-engineering.qmd) solves the context window problem. LLMs are brilliant but bounded—they have limited working memory that degrades as it fills. You'll learn to manage context across its lifecycle: write (scratchpads & memories), select (MCP & just-in-time retrieval), compress (summarization), and isolate (multi-agent architectures). This is the operating system.

Finally, you'll apply all three concepts in a [**hands-on session**](hands-on.qmd) using Google Antigravity to build a functional game and refactor a codebase entirely through natural language instructions.

## The Takeaway

The best code is not the code you write, but the code you **verify**. In the agentic era, your value as a developer is defined by your ability to clearly articulate problems, design robust verification strategies, and rigorously audit solutions. The agent writes; you architect.

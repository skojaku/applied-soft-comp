---
title: "Sprint Project: Dashboard from Scratch in Under an Hour"
---

::: {.callout-note title="Your mission"}
You have 60 minutes to build a fully functional, interactive web dashboard from a raw dataset with zero starter code. The twist: you are forbidden from typing code manually. You must work exclusively through LLM prompts, acting as an Architect who designs while the AI implements. The first team to demonstrate a working local app wins the race.
:::

## The Challenge

Let's talk about the paradigm shift this sprint represents. In Modules 1 and 2, you wrote every line of code yourself. You were the Developer. Starting now, you become the Architect. Your job is no longer to write syntax. Your job is to articulate intent clearly enough that an LLM can generate working code.

This changes everything. You face a raw CSV containing interesting data but no scaffolding. You must build an interactive dashboard using Streamlit, Shiny, or a similar framework. The dashboard must load data, create visualizations, and provide interactive controls. All of this must work locally before the timer expires.

The constraint makes this challenging. You cannot fix syntax errors by hand. If the LLM generates broken code, you must prompt it to repair the issue. If the layout looks wrong, you must describe the desired appearance clearly enough for the AI to implement. Your success depends entirely on prompt quality.

## The Rules

**No Manual Coding:** You may not type Python, R, or any programming language syntax. All code must come from LLM outputs. You can copy-paste LLM responses but not edit them by hand.

**Tool Choice:** Use any LLM-powered coding assistant: ChatGPT, Claude, Copilot, or others. Choose your framework: Streamlit for Python users, Shiny for R users, or alternatives if you prefer.

**Working Demo:** Your dashboard must run locally on your machine. Judges will visit your computer to verify functionality. The app must load data, display at least two different visualizations, and include at least one interactive control like a dropdown or slider.

**Time Limit:** 60 minutes from receiving the dataset to demonstrating the working app.

**Team Structure:** Work in pairs. Both members prompt the LLM but one person manages the repository and runs the app.

## The Workflow

Start by examining the dataset. Open it and understand its structure. What variables exist? What relationships might be interesting? What questions could users explore? Spend five minutes on this analysis.

Now architect your dashboard. Sketch the layout on paper or in a text file. Decide what visualizations to include. Plan what interactive elements users need. Write this specification clearly. This document becomes the foundation for your prompts.

Begin prompting the LLM. Start with basic scaffolding. Ask it to create a minimal Streamlit app that loads your specific CSV and displays its first few rows. Run this code. Verify it works. Commit this first version to Git.

Now iterate. Add one feature at a time through prompts. Perhaps next you add a sidebar with a dropdown menu. Then you add a scatter plot. Then you connect the dropdown to filter the plot. After each working addition, commit to Git.

When something breaks, do not fix it manually. Instead, show the error message to the LLM and ask it to repair the code. Treat each error as a prompt engineering challenge. How can you describe the problem clearly enough for the AI to solve it?

## What Makes You Win

Victory is simple. The first team to demonstrate a fully working dashboard wins. "Fully working" means the app launches without errors, loads the dataset, displays multiple visualizations, and responds to interactive controls.

Speed matters here more than aesthetics. A basic but functional dashboard beats an ambitious but broken one. Focus on getting something running quickly, then enhance if time permits.

However, if multiple teams finish around the same time, judges will consider quality. A dashboard with clear labels, logical layout, and smooth interactivity ranks above a cluttered, confusing one.

## Common Pitfalls

The biggest mistake is prompting too ambitiously. Teams ask the LLM to build the entire dashboard in one shot. This produces complex code that usually has bugs. Then they waste time debugging through prompts.

A better strategy is incremental building. Get something minimal working first. Then add features one by one. Each addition is small enough that when bugs appear, you can isolate them quickly.

Another trap is vague prompting. If you tell the LLM "make a dashboard," you get generic output. If you say "create a Streamlit app with a sidebar containing a dropdown to select species, and a scatter plot showing sepal length vs sepal width that updates when the dropdown changes," you get specific, useful code.

The third pitfall is neglecting Git. Teams focus so intensely on coding through prompts that they forget to commit. Then their app crashes and they lose their working version. Commit every small win.

## The Takeaway

This sprint develops a crucial skill for the AI era: prompt-driven development. You learn to think in terms of architecture rather than implementation. You discover that clear problem decomposition matters more than syntax knowledge.

You also learn the limits of LLM coding assistance. These tools excel at scaffolding and boilerplate. They struggle with complex, domain-specific logic. Understanding where AI helps and where humans must guide is essential for effective collaboration.

By the end of this sprint, you will have built something substantial without writing code. This experience changes how you approach future projects. Why spend time on routine implementation when you can focus on design and problem-solving?

Ready to race? Let the prompts flow.

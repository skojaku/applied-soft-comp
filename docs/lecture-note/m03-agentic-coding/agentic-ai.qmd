---
title: "From ChatBot to Agentic AI"
execute:
    enabled: true
---

::: {.callout-note appearance="simple"}
**Spoiler**: Agents don't "think" in the human sense. They loop. They are state machines that use an LLM to decide the next transition.
:::

# The Mechanism

The naive view is that agents are "smarter" chatbots. They aren't. They're a core component wrapped in a control loop. A chatbot generates text and stops. An agent generates text, parses it for actionable commands, executes those commands in the real world, observes the results, and feeds those results back into the next prompt. The intelligence doesn't come from the model---it comes from the **feedback loop**.

This is the **ReAct Pattern** (Reason + Act). A standard chatbot is a pure function: $\text{Output} = \text{Model}(\text{Input})$. An agent is a state machine:

```python
while not task_complete:
    observation = get_environment_state()
    thought = model(observation)
    action = parse_action(thought)
    result = execute(action)
    observation = result  # Feedback loop
```

The critical insight is the feedback loop. If the agent tries to import a missing library (Action) and receives `ModuleNotFoundError` (Observation), the next iteration's Thought will be "I need to install this library," rather than hallucinating success. The model corrects itself not through introspection, but through collision with reality.


::: {.callout-note collapse="true"}

## How ReAct is implemented

ReAct framework is proposed by [Yao et al. (2022)](https://arxiv.org/abs/2210.03629).
The core idea is to prompt the LLM to generate both reasoning traces and task-specific actions in an interleaved manner. Specifically, the prompt structure follows a sequence: `Thought` $\rightarrow$ `Action` $\rightarrow$ `Observation`.

1.  **Thought**: The model reasons about the current state and what needs to be done.
2.  **Action**: The model outputs a specific command to interact with an external environment (e.g., `Search[Apple]`).
3.  **Observation**: The environment executes the action and returns the result (e.g., search results for "Apple").

This cycle repeats until the task is solved.

:::

## The Structured Output Problem

![](../figs/structured-output-manga.png){width="70%" fig-align="center"}

Before we can build this loop, we must solve a fundamental engineering problem: **How do we extract executable actions from natural language?**

Think about it: When you ask an LLM "Generate random numbers from a Gaussian distribution", it might respond:

> "I'll generate random numbers for you using a normal distribution."

Early systems used regex to parse the model's output. This failed constantly. The model would say "I'll create random nums" instead of providing structured parameters, and the parser would break. The solution is **Structured Output**---we force the model to return data in a machine-readable format, the most common being **JSON**. For example,

```json
{
    "action": "generate_random",
    "distribution": "gaussian",
    "parameters": {"mean": 0, "std": 1, "size": 100},
    "values": [...]
}
```

So, how can we let LLMs to speak in JSON? The most basic approach is to prompt the model to return JSON. Let's demonstrate with ollama and gpt-oss 120b:

::: {.column-margin}

We will use `gpt-oss:120b-cloud` as the LLM. This is a 120B parameter model that is hosted by Ollama. See [Ollama Cloud](https://ollama.com/cloud) for more details.

:::

```{python}
#| echo: false
import os
import ollama
import json

params_llm = {"model": "gpt-oss:120b-cloud", "options": {"temperature": 0.3}}
```

```{python}

prompt = """
You are a function-calling assistant. Given a user query, return a JSON object with:
- "action": the name of the function to call
- "parameters": a dictionary of arguments
- "values": the result of the function call

User query: "Generate 10 random numbers from a Gaussian distribution with mean 0 and standard deviation 1"

Return ONLY valid JSON, no explanation.
"""

response = ollama.generate(prompt=prompt, **params_llm)

print(response.response)
```

The model might return valid JSON, but this is fragile. It might hallucinate extra fields, misspell keys, add conversational text, or return malformed JSON. We need **schema enforcement**.

Modern LLM APIs (including ollama) support **JSON Schema** constraints. You define the shape of the output, and the model is forced to conform. A popular library for this is **Pydantic**. With Pydantic, you define the schema using a class, and the model is forced to conform to the schema.

```{python}
from pydantic import BaseModel

# Define the schema using Pydantic
class ToolCall(BaseModel):
    action: str
    parameters: dict
    values: list[float]
```

Once you define the schema, you can use it to constrain the model's output:
```{python}
# Convert to JSON schema
json_schema = ToolCall.model_json_schema()

# Now constrain the model's output
response = ollama.generate(
    prompt="User query: Generate 10 random numbers from a Gaussian with mean 5 and std 2",
    format=json_schema,  # Schema constraint
    **params_llm
)
```


```{python}
#| code-fold: true
print("Response:")
print(response.response)
```

Now the model cannot hallucinate. It must return valid JSON that matches the schema. If it tries to return `{"acton": ...}` (typo), the generation fails and retries internally.
This is the foundation of reliable agents: **type-safe communication between the LLM and your code**.

## The ReAct Framework with `smolagents`

The **ReAct Pattern** formalizes this loop. The agent alternates between **reasoning** (generating thoughts) and **acting** (calling tools or terminating).

Rather than building our own agent framework from scratch, we can use **`smolagents`** from Hugging Face—a lightweight library that implements the ReAct pattern with support for multiple LLM backends including local models via ollama.


::: {.column-margin}

Install smolagents:

```bash
pip install smolagents
```

:::

### Creating a Tool: A Fish Market Dataset

Let's build an agent that can explore and analyze a real dataset. We'll use the Fish Market dataset from Hugging Face—a collection of measurements from different fish species. First, load the data:

```{python}
import polars as pl

# Load the Fish dataset
df = pl.read_csv("hf://datasets/scikit-learn/Fish/Fish.csv")
print(df.head())
```

Now we'll create three tools that make our agent a **data detective**:

::: {.column-margin}
**Tool**: Base class from smolagents. All custom tools inherit from this.

**forward()**: The execution method. Named after PyTorch convention—the agent calls this with LLM-generated parameters.
:::

```{python}
from smolagents import Tool, ToolCallingAgent, LiteLLMModel
import json

class QueryDataTool(Tool):
    name = "query_data"
    description = "Query the fish dataset using SQL. The table is called 'fish'. Available columns: Species, Weight, Length1, Length2, Length3, Height, Width"
    inputs = {
        "sql_query": {"type": "string", "description": "SQL query to execute (use 'fish' as table name)"}
    }
    output_type = "string"

    def forward(self, sql_query: str) -> str:
        try:
            result = df.sql(sql_query, table_name="fish")
            return result.to_pandas().to_string()
        except Exception as e:
            return f"Query error: {str(e)}"

class FindCorrelationTool(Tool):
    name = "find_correlation"
    description = "Calculate correlation between two numeric columns in the fish dataset"
    inputs = {
        "column1": {"type": "string", "description": "First column name"},
        "column2": {"type": "string", "description": "Second column name"}
    }
    output_type = "string"

    def forward(self, column1: str, column2: str) -> str:
        try:
            corr = df.select([column1, column2]).to_pandas().corr().iloc[0, 1]
            return f"Correlation between {column1} and {column2}: {corr:.3f}"
        except Exception as e:
            return f"Error: {str(e)}"

class GetStatsTool(Tool):
    name = "get_stats"
    description = "Get statistical summary (count, mean, std, min, max) for a specific column and optionally filter by species"
    inputs = {
        "column": {"type": "string", "description": "Column name to analyze"},
        "species": {"type": "string", "description": "Species to filter by (optional)", "nullable": True}
    }
    output_type = "string"

    def forward(self, column: str, species: str = None) -> str:
        try:
            data = df
            if species:
                data = data.filter(pl.col("Species") == species)

            stats = data.select(pl.col(column)).describe()
            prefix = f"Stats for {column}" + (f" (Species: {species})" if species else "")
            return f"{prefix}:\n{stats.to_pandas().to_string()}"
        except Exception as e:
            return f"Error: {str(e)}"
```

The structure is fixed. Every tool must define:

1. **`name`**: Identifier the agent uses to call this tool (must be unique)
2. **`description`**: What the LLM reads to decide when to use this tool
3. **`inputs`**: JSON schema with `type`, `description`, and optionally `nullable` for each parameter
4. **`output_type`**: Return type (usually `"string"`)
5. **`forward()`**: The actual execution logic that runs when the agent calls the tool

Notice how the **description** field is critical—it's the agent's only way to understand when to use each tool. A vague description like "analyzes data" will confuse the agent. A precise description like "Calculate correlation between two numeric columns" tells the agent exactly when this tool is appropriate.

Create the agent with our tools:

::: {.column-margin}
**ToolCallingAgent**: Agent that uses the model's native tool-calling API. More reliable than CodeAgent for structured tool calls.

**max_steps**: Maximum ReAct iterations before terminating. Prevents infinite loops.
:::


```{python}
# Initialize the agent with ollama backend
from smolagents import LiteLLMModel

# Use ollama through LiteLLM
model_name = "ollama/gpt-oss:120b-cloud"  # LiteLLM requires "ollama/" prefix
model = LiteLLMModel(model_id=model_name, api_base="http://localhost:11434")

# Create the agent with our data analysis tools
from smolagents import ToolCallingAgent

agent = ToolCallingAgent(
    tools=[QueryDataTool(), FindCorrelationTool(), GetStatsTool()],
    model=model,
    max_steps=5
)
```

Run the agent and watch it autonomously choose which tools to use:

```{python}
# Ask a question that requires reasoning about which tool to use
query = "Which fish species has the highest average weight?"

# Capture the verbose output to print a clean trace instead
import sys
from io import StringIO

old_stdout = sys.stdout
sys.stdout = mystdout = StringIO()

try:
    result = agent.run(query)
finally:
    sys.stdout = old_stdout

# Print a clean, formatted trace from the agent's memory
print(f"Query: {query}\n")

for step in agent.memory.steps:
    # Handle potential missing attributes by checking or using defaults
    thought = getattr(step, "model_output", None)
    if thought:
        print(f"**Thought:** {thought.strip()}")

    tool_calls = getattr(step, "tool_calls", None)
    if tool_calls:
        for call in tool_calls:
            print(f"**Action:** `{call.name}` with arguments `{call.arguments}`")

    observations = getattr(step, "observations", None)
    if observations:
        print(f"**Observation:** {observations.strip()}")
    print("-" * 40)

print(f"\n**Final Answer:** {result}")
```

The agent executes a ReAct loop. It reads the question "Which fish species has the highest average weight?" and realizes it needs to use SQL to group by species and calculate averages. Watch how it autonomously:

1. **Thought**: "I need to find the species with the highest average weight"
2. **Action**: Chooses `query_data` tool and constructs SQL: `SELECT Species, AVG(Weight) as avg_weight FROM fish GROUP BY Species ORDER BY avg_weight DESC LIMIT 1`
3. **Observation**: Receives the query result showing the species and its average weight
4. **Final Answer**: Returns "Pike has the highest average weight at 718.7g"

The critical insight is the agent chose the right tool without explicit instructions. It read the tool descriptions, understood the task, and selected `query_data` over the other tools because the description mentioned SQL querying capabilities.

Let's try a more complex multi-step query:

```{python}
# This requires chaining multiple tool calls
query = "Is there a relationship between Weight and Length1? What species has the strongest correlation?"

# Run without printing intermediate steps
old_stdout = sys.stdout
sys.stdout = mystdout = StringIO()

try:
    result = agent.run(query)
finally:
    sys.stdout = old_stdout

print(f"Query: {query}\n")
print(f"Final Answer: {result}")
```

For this query, the agent must:
1. First check the overall correlation between Weight and Length1
2. Then query for distinct species
3. Finally calculate correlation for each species separately

This demonstrates the power of the ReAct loop—the agent chains multiple observations together, building a solution step-by-step rather than attempting everything in one shot.

This is the entire architecture of Google Antigravity, Claude Code, and Cursor—just scaled with better tools (file editing, terminal commands, web browsing) and better orchestration (parallel agents, verification artifacts).

## Tracing the Agent's Mind

Let's look at a real trace of Gemini 3 solving a problem: "Find the largest file in the current directory."

**Step 1: Thought**
> "I need to list all files and their sizes to determine the largest one. I should use the `ls` command with the `-lh` flag, or better yet, `find` to handle subdirectories."

**Step 2: Action**
> `run_command("find . -type f -exec du -h {} + | sort -rh | head -n 1")`

**Step 3: Observation**
> `105M ./data/large_dataset.csv`

**Step 4: Thought**
> "I have found the file. It is `large_dataset.csv` with a size of 105M. I can now answer the user."

**Step 5: Final Answer**
> "The largest file is `./data/large_dataset.csv` (105MB)."

## The Token Economy

This autonomy comes at a cost.
*   **Latency**: Each step in the loop requires a round-trip to the model. A complex task might take 10 seconds to plan and execute.
*   **Cost**: A simple question might consume 500 tokens. An agentic loop might consume 50,000 tokens as it reads files, gets error messages, and re-reads context.

**Asynchronous Context**
Because of this latency, Antigravity is designed for **Asynchrony**. You don't stare at the cursor waiting for it to move. You assign the task, switch to another file, and wait for the notification. It is a "fire-and-forget" workflow.

# The Takeaway

**Latency is the price of autonomy.** We accept slower individual interactions in exchange for the completion of entire units of work. You trade seconds of waiting for hours of coding.

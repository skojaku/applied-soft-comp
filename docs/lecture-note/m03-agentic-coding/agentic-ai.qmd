---
title: "From ChatBot to Agentic AI"
execute:
    enabled: true
---

::: {.callout-note appearance="simple"}
**Spoiler**: Agents don't "think" in the human sense. They loop. They are state machines that use an LLM to decide the next transition.
:::

# The Mechanism

The naive view is that agents are "smarter" chatbots. They aren't. They're a core component wrapped in a control loop. A chatbot generates text and stops. An agent generates text, parses it for actionable commands, executes those commands in the real world, observes the results, and feeds those results back into the next prompt. The intelligence doesn't come from the model---it comes from the **feedback loop**.

This is the **ReAct Pattern** (Reason + Act). A standard chatbot is a pure function: $\text{Output} = \text{Model}(\text{Input})$. An agent is a state machine:

```python
while not task_complete:
    observation = get_environment_state()
    thought = model(observation)
    action = parse_action(thought)
    result = execute(action)
    observation = result  # Feedback loop
```

The critical insight is the feedback loop. If the agent tries to import a missing library (Action) and receives `ModuleNotFoundError` (Observation), the next iteration's Thought will be "I need to install this library," rather than hallucinating success. The model corrects itself not through introspection, but through collision with reality.


::: {.callout-note collapse="true"}

## How ReAct is implemented

ReAct framework is proposed by [Yao et al. (2022)](https://arxiv.org/abs/2210.03629).
The core idea is to prompt the LLM to generate both reasoning traces and task-specific actions in an interleaved manner. Specifically, the prompt structure follows a sequence: `Thought` $\rightarrow$ `Action` $\rightarrow$ `Observation`.

1.  **Thought**: The model reasons about the current state and what needs to be done.
2.  **Action**: The model outputs a specific command to interact with an external environment (e.g., `Search[Apple]`).
3.  **Observation**: The environment executes the action and returns the result (e.g., search results for "Apple").

This cycle repeats until the task is solved.

:::

## The Structured Output Problem

![](../figs/structured-output-manga.png)

Before we can build this loop, we must solve a fundamental engineering problem: **How do we extract executable actions from natural language?**

Think about it: When you ask an LLM "Generate random numbers from a Gaussian distribution", it might respond:

> "I'll generate random numbers for you using a normal distribution."

Early systems used regex to parse the model's output. This failed constantly. The model would say "I'll create random nums" instead of providing structured parameters, and the parser would break. The solution is **Structured Output**---we force the model to return data in a machine-readable format, the most common being **JSON**. For example,

```json
{
    "action": "generate_random",
    "distribution": "gaussian",
    "parameters": {"mean": 0, "std": 1, "size": 100},
    "values": [...]
}
```

So, how can we let LLMs to speak in JSON? The most basic approach is to prompt the model to return JSON. Let's demonstrate with ollama and gemma3:

```{python}
#| echo: false
import ollama
import json

params_llm = {"model": "gemma3n:latest", "options": {"temperature": 0.3}}
```

```{python}

prompt = """
You are a function-calling assistant. Given a user query, return a JSON object with:
- "action": the name of the function to call
- "parameters": a dictionary of arguments
- "values": the result of the function call

User query: "Generate 30 random numbers from a Gaussian distribution with mean 0 and standard deviation 1"

Return ONLY valid JSON, no explanation.
"""

response = ollama.generate(prompt=prompt, **params_llm)

print(response.response)
```

The model might return valid JSON, but this is fragile. It might hallucinate extra fields, misspell keys, add conversational text, or return malformed JSON. We need **schema enforcement**.

Modern LLM APIs (including ollama) support **JSON Schema** constraints. You define the shape of the output, and the model is forced to conform:

```{python}
from pydantic import BaseModel

# Define the schema using Pydantic
class ToolCall(BaseModel):
    action: str
    parameters: dict
    values: list[float]

# Convert to JSON schema
json_schema = ToolCall.model_json_schema()

# Now constrain the model's output
response = ollama.generate(
    prompt="User query: Generate 30 random numbers from a Gaussian with mean 5 and std 2",
    format=json_schema,  # Schema constraint
    **params_llm
)
```


```{python}
#| code-fold: true
print("Response:")
print(response.response)
```

Now the model cannot hallucinate. It must return valid JSON that matches the schema. If it tries to return `{"acton": ...}` (typo), the generation fails and retries internally.
This is the foundation of reliable agents: **type-safe communication between the LLM and your code**.

## Tool Calling

Structured output tells us what the agent wants to do. **Tool calling** is the mechanism that lets it actually do it.


Conceptually, the framework operates as follows:

1.  **Registration**: You provide the agent with a registry of available tools, including their names, descriptions, and parameter schemas.
2.  **Selection**: Based on the user's query and its own internal reasoning ("thoughts"), the agent determines which tool is most appropriate to use.
3.  **Execution**: The agent generates a structured call (arguments), the system runs the actual function, and the return value is fed back to the agent as an observation.


A **tool** is a function that the agent can invoke. First, define the actual tool function:

```{python}
import numpy as np

# Define multiple tools - the agent must choose the right one
def generate_random(distribution: str, mean: float = 0, std: float = 1, size: int = 100) -> dict:
    """Generate random numbers from specified distribution"""
    samples = np.random.normal(loc=mean, scale=std, size=size)
    return {
        "tool": "generate_random",
        "distribution": "gaussian",
        "mean": float(np.mean(samples)),
        "std": float(np.std(samples)),
        "size": len(samples),
        "samples": samples.tolist()[:5]
    }

def calculate_statistics(data: list[float]) -> dict:
    """Calculate statistics from a list of numbers"""
    arr = np.array(data)
    return {
        "tool": "calculate_statistics",
        "mean": float(np.mean(arr)),
        "std": float(np.std(arr)),
        "median": float(np.median(arr)),
        "min": float(np.min(arr)),
        "max": float(np.max(arr))
    }

def fit_distribution(data: list[float], distribution: str = "gaussian") -> dict:
    """Fit a distribution to observed data"""
    arr = np.array(data)
    if distribution == "gaussian":
        mu = float(np.mean(arr))
        sigma = float(np.std(arr))
        return {
            "tool": "fit_distribution",
            "distribution": distribution,
            "parameters": {"mean": mu, "std": sigma}
        }
```

Define schemas for each tool:

```{python}
# Schema for generating random numbers
class GenerateRandomCall(BaseModel):
    tool: str  # Must be "generate_random"
    distribution: str
    mean: float = 0
    std: float = 1
    size: int = 100

# Schema for calculating statistics
class CalculateStatsCall(BaseModel):
    tool: str  # Must be "calculate_statistics"
    data: list[float]

# Schema for fitting distributions
class FitDistributionCall(BaseModel):
    tool: str  # Must be "fit_distribution"
    data: list[float]
    distribution: str = "gaussian"

# Combined schema that accepts any of the three tools
class ToolCall(BaseModel):
    tool: str
    # Union of all possible parameters
    distribution: str | None = None
    mean: float | None = None
    std: float | None = None
    size: int | None = None
    data: list[float] | None = None

tool_schema = ToolCall.model_json_schema()
```

Now create a system prompt that defines ALL available tools - the LLM must choose which one to call:

```{python}
# System prompt that defines multiple tools
system_prompt = """You are a statistical analysis assistant. Based on the user's request, choose the appropriate tool.

Available tools:
- generate_random(distribution: str, mean: float, std: float, size: int) -> generates random numbers
- calculate_statistics(data: list[float]) -> computes mean, std, median, min, max from data
- fit_distribution(data: list[float], distribution: str) -> fits a distribution to observed data

Return JSON with the tool name and appropriate parameters:
{"tool": "<tool_name>", <parameters>}"""

# Test different queries - watch the LLM choose the right tool
queries = [
    "Generate 30 random numbers from a Gaussian distribution with mean 10 and standard deviation 2",
    "I have this data: [1.5, 2.3, 1.8, 2.1, 1.9]. Calculate statistics for it.",
    "I measured these values: [5.2, 5.1, 4.9, 5.3, 5.0]. What distribution fits this data?"
]

for i, query in enumerate(queries, 1):
    print(f"\n{'='*60}")
    print(f"QUERY {i}: {query}")
    print('='*60)

    response = ollama.chat(
        model=params_llm["model"],
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": query}
        ],
        format=tool_schema
    )

    print(f"\nLLM Tool Call:")
    print(response.message.content)

    # Parse the tool call
    tool_call = ToolCall.model_validate_json(response.message.content)
    print(f"\n=> Chosen Tool: {tool_call.tool}")
```

Notice how the LLM **chooses different tools** based on what the user is asking for:
- "Generate random numbers" → calls `generate_random`
- "Calculate statistics" → calls `calculate_statistics`
- "What distribution fits" → calls `fit_distribution`

Now let's execute one of these tool calls:

```{python}
# Execute the first tool call (generate_random)
query = queries[0]
response = ollama.chat(
    model=params_llm["model"],
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": query}
    ],
    format=tool_schema
)

tool_call = ToolCall.model_validate_json(response.message.content)

# Execute the appropriate function based on tool name
if tool_call.tool == "generate_random":
    result = generate_random(
        distribution=tool_call.distribution,
        mean=tool_call.mean,
        std=tool_call.std,
        size=tool_call.size
    )
elif tool_call.tool == "calculate_statistics":
    result = calculate_statistics(data=tool_call.data)
elif tool_call.tool == "fit_distribution":
    result = fit_distribution(data=tool_call.data, distribution=tool_call.distribution)

print(f"\nTool Execution Result:")
print(json.dumps(result, indent=2))

# Feed the result back to get a natural language response
messages = [
    {"role": "system", "content": "You are a statistical analysis assistant."},
    {"role": "user", "content": query},
    {"role": "assistant", "content": f"Tool call: {tool_call.model_dump_json()}"},
    {"role": "user", "content": f"Tool result: {json.dumps(result)}. Provide a natural language summary."}
]

final_response = ollama.chat(
    model=params_llm["model"],
    messages=messages
)

print(f"\nFinal Natural Language Response:")
print(final_response.message.content)
```

This is the ReAct loop in action:
1. **Thought**: "I need to call generate_random"
2. **Action**: Tool call request (structured JSON)
3. **Observation**: Tool result (200 samples, mean≈10, std≈2)
4. **Response**: Natural language answer

## The ReAct Framework: Reason + Act

The **ReAct Pattern** formalizes this loop. The agent alternates between **reasoning** (generating thoughts) and **acting** (calling tools or terminating).

Here's a minimal ReAct implementation using ollama:

```{python}
def react_agent_ollama(user_query: str, max_iterations: int = 5) -> str:
    """Simple ReAct agent using ollama and gemma3"""

    system_prompt = """You are a random number generation assistant.

When the user asks to generate random numbers, respond with a tool call:
{"tool": "generate_random", "distribution": "gaussian", "mean": <value>, "std": <value>, "size": <value>}

When you receive tool results, synthesize them into a natural language answer.
If the user asks for multiple distributions or samples, call the tool multiple times."""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_query}
    ]

    for iteration in range(max_iterations):
        # Generate response
        response = ollama.chat(
            model=params_llm["model"],
            messages=messages,
            format=tool_schema
        )

        content = response.message.content
        print(f"\n--- Iteration {iteration + 1} ---")
        print(f"LLM Output: {content}")

        # Try to parse as tool call
        try:
            tool_call = RandomToolCall.model_validate_json(content)

            # Execute the tool
            result = generate_random(
                distribution=tool_call.distribution,
                mean=tool_call.mean,
                std=tool_call.std,
                size=tool_call.size
            )
            print(f"Tool Executed: generate_random(dist='{tool_call.distribution}', mean={tool_call.mean}, std={tool_call.std}, size={tool_call.size})")
            print(f"Tool Result: {result['size']} samples, mean={result['mean']:.2f}, std={result['std']:.2f}")

            # Add tool call and result to conversation
            messages.append({"role": "assistant", "content": content})
            messages.append({
                "role": "user",
                "content": f"Tool result: {json.dumps(result)}. Continue or provide final answer."
            })

        except Exception as e:
            # Not a valid tool call - treat as final answer
            print(f"Final Answer: {content}")
            return content

    return "Max iterations reached"

# Test the agent with multiple requests
print("=" * 60)
print("REACT AGENT TEST: Multiple Random Samples")
print("=" * 60)

final_response = react_agent_ollama("Generate 30 samples with mean 0, std 1, and another 50 samples with mean 10, std 5")
```

The agent will:
1. Call `generate_random(distribution="gaussian", mean=0, std=1, size=100)`
2. Receive the result
3. Call `generate_random(distribution="gaussian", mean=10, std=5, size=50)`
4. Receive the result
5. Synthesize the final answer

This is the entire architecture of Google Antigravity, Claude Code, and Cursor—just scaled with better tools (file editing, terminal commands, web browsing) and better orchestration (parallel agents, verification artifacts).

## Tracing the Agent's Mind

Let's look at a real trace of Gemini 3 solving a problem: "Find the largest file in the current directory."

**Step 1: Thought**
> "I need to list all files and their sizes to determine the largest one. I should use the `ls` command with the `-lh` flag, or better yet, `find` to handle subdirectories."

**Step 2: Action**
> `run_command("find . -type f -exec du -h {} + | sort -rh | head -n 1")`

**Step 3: Observation**
> `105M ./data/large_dataset.csv`

**Step 4: Thought**
> "I have found the file. It is `large_dataset.csv` with a size of 105M. I can now answer the user."

**Step 5: Final Answer**
> "The largest file is `./data/large_dataset.csv` (105MB)."

## The Token Economy

This autonomy comes at a cost.
*   **Latency**: Each step in the loop requires a round-trip to the model. A complex task might take 30 seconds to plan and execute.
*   **Cost**: A simple question might consume 500 tokens. An agentic loop might consume 50,000 tokens as it reads files, gets error messages, and re-reads context.

**Asynchronous Context**
Because of this latency, Antigravity is designed for **Asynchrony**. You don't stare at the cursor waiting for it to move. You assign the task, switch to another file, and wait for the notification. It is a "fire-and-forget" workflow.

# The Takeaway

**Latency is the price of autonomy.** We accept slower individual interactions in exchange for the completion of entire units of work. You trade seconds of waiting for hours of coding.

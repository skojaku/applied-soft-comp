{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Word Embeddings\"\n",
        "jupyter: applsoftcomp\n",
        "execute:\n",
        "    enabled: true\n",
        "    cache: true\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "### The Spoiler\n",
        "Meaning isn't stored in words; it's stored in the geometric relationship between them.\n",
        "\n",
        "### Words are not containers for meaning\n",
        "\n",
        "We intuitively assume words are containers for meaning—that \"dog\" holds the concept of a canine. This is incorrect. Structural linguistics reveals that a sign is defined solely by its relationships: \"dog\" means \"dog\" only because it is not \"cat,\" \"wolf,\" or \"log.\" Meaning is differential, not intrinsic.\n",
        "\n",
        "\n",
        "::: {#fig-structuralism}\n",
        "\n",
        "![](../figs/word2vec-manga-1.jpg)\n",
        "\n",
        "Green is the color that is not non-green (not red, not blue, not yellow, ...).\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "**Word2vec**---the very foundational model that grounds modern NLP---learns to map the statistical topology of language. Think of it like mapping a city based purely on traffic data. You don't know what a \"school\" is, but you see that \"buses\" and \"children\" congregate there at 8 AM. By placing these entities close together on a map, you reconstruct the city's functional structure. Word2vec does this for language, turning semantic proximity into geometric distance.\n",
        "\n",
        "## Word2vec\n",
        "\n",
        "Let's first learn the power of Word2Vec and then understand how it works. We will use a pre-trained model. We aren't teaching it anything; we are simply inspecting the map it created from 100 billion words of Google News."
      ],
      "id": "3e72f65f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "\n",
        "# Load pre-trained Word2vec embeddings\n",
        "print(\"Loading Word2vec model...\")\n",
        "model = api.load(\"word2vec-google-news-300\")\n",
        "print(f\"Loaded embeddings for {len(model):,} words.\")"
      ],
      "id": "7985899f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the map is accurate, \"dog\" should be surrounded by its semantic kin. We query the nearest neighbors in the vector space."
      ],
      "id": "9c234deb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "similar_to_dog = model.most_similar(\"dog\", topn=10)\n",
        "\n",
        "print(\"Words most similar to 'dog':\")\n",
        "for word, similarity in similar_to_dog:\n",
        "    print(f\"  {word:20s} {similarity:.3f}\")"
      ],
      "id": "c0e43f82",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model groups \"dog\" with \"dogs,\" \"puppy,\" and \"pooch\" **not because it knows biology**, but because they are statistically interchangeable in sentences.\n",
        "\n",
        "Since words are vectors, we can perform arithmetic on meaning. The relationship between \"King\" and \"Man\" is a vector. If we add that vector to \"Woman,\" we should arrive at \"Queen.\"\n",
        "\n",
        "$$ \\vec{\\text{King}} - \\vec{\\text{Man}} + \\vec{\\text{Woman}} \\approx \\vec{\\text{Queen}} $$"
      ],
      "id": "e9e58d93"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result = model.most_similar(\n",
        "  positive=['king', 'woman'],\n",
        "   negative=['man'], topn=5\n",
        ")\n",
        "\n",
        "print(\"king - man + woman =\")\n",
        "for word, similarity in result:\n",
        "    print(f\"  {word:15s} {similarity:.3f}\")"
      ],
      "id": "7f107ab7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We cannot see in 300 dimensions, but we can project the space down to 2D using PCA. This reveals the consistent structures—like the \"capital city\" relationship—that the model has learned."
      ],
      "id": "4f7b2753"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 12,
        "fig-height": 10
      },
      "source": [
        "#| fig-cap: The 'Capital Of' relationship appears as a consistent direction in vector space.\n",
        "#| code-fold: true\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "countries = [\"Germany\", \"France\", \"Italy\", \"Spain\", \"Portugal\", \"Greece\"]\n",
        "capitals = [\"Berlin\", \"Paris\", \"Rome\", \"Madrid\", \"Lisbon\", \"Athens\"]\n",
        "\n",
        "# Get embeddings\n",
        "country_embeddings = np.array([model[country] for country in countries])\n",
        "capital_embeddings = np.array([model[capital] for capital in capitals])\n",
        "\n",
        "# PCA to 2D\n",
        "pca = PCA(n_components=2)\n",
        "embeddings = np.vstack([country_embeddings, capital_embeddings])\n",
        "embeddings_pca = pca.fit_transform(embeddings)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(embeddings_pca, columns=[\"PC1\", \"PC2\"])\n",
        "df[\"Label\"] = countries + capitals\n",
        "df[\"Type\"] = [\"Country\"] * len(countries) + [\"Capital\"] * len(capitals)\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    color = \"#e74c3c\" if row[\"Type\"] == \"Country\" else \"#3498db\"\n",
        "    marker = \"o\" if row[\"Type\"] == \"Country\" else \"s\"\n",
        "    ax.scatter(\n",
        "        row[\"PC1\"],\n",
        "        row[\"PC2\"],\n",
        "        c=color,\n",
        "        marker=marker,\n",
        "        s=200,\n",
        "        edgecolors=\"black\",\n",
        "        linewidth=1.5,\n",
        "        alpha=0.7,\n",
        "        zorder=3,\n",
        "    )\n",
        "    ax.text(\n",
        "        row[\"PC1\"],\n",
        "        row[\"PC2\"] + 0.15,\n",
        "        row[\"Label\"],\n",
        "        fontsize=12,\n",
        "        ha=\"center\",\n",
        "        va=\"bottom\",\n",
        "        fontweight=\"bold\",\n",
        "        bbox=dict(facecolor=\"white\", edgecolor=\"none\", alpha=0.8),\n",
        "    )\n",
        "\n",
        "# Draw arrows\n",
        "for i in range(len(countries)):\n",
        "    country_pos = df.iloc[i][[\"PC1\", \"PC2\"]].values\n",
        "    capital_pos = df.iloc[i + len(countries)][[\"PC1\", \"PC2\"]].values\n",
        "    ax.arrow(\n",
        "        country_pos[0],\n",
        "        country_pos[1],\n",
        "        capital_pos[0] - country_pos[0],\n",
        "        capital_pos[1] - country_pos[1],\n",
        "        color=\"gray\",\n",
        "        alpha=0.6,\n",
        "        linewidth=2,\n",
        "        head_width=0.15,\n",
        "        head_length=0.1,\n",
        "        zorder=2,\n",
        "    )\n",
        "\n",
        "ax.set_title(\n",
        "    'The \"Capital Of\" Relationship as Parallel Transport',\n",
        "    fontsize=16,\n",
        "    fontweight=\"bold\",\n",
        "    pad=20,\n",
        ")\n",
        "ax.grid(alpha=0.3, linestyle=\"--\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "5afa7a68",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let's unbox Word2Vec.\n",
        "\n",
        "We intuitively treat words as containers that hold meaning—that the word \"Green\" contains the visual concept of a specific color. This is incorrect. Nature presents us with a messy, continuous spectrum without hard borders; language is simply the set of arbitrary cuts we make in that continuum to create order.\n",
        "\n",
        "**Word2Vec** operationalizes this by treating meaning as a game of contrast. It functions as a pair of \"Linguistic Scissors.\" It does not learn what a word is by looking up a definition; it learns what a word is *like* by pulling it close to neighbors, and more importantly, it learns what a word is *not* by pushing it away from random noise. The meaning of \"Green\" is simply the geometric region that remains after we have pushed away \"Red,\" \"Purple,\" and \"Banana.\"\n",
        "\n",
        "::: {#fig-contrastive}\n",
        "\n",
        "![](../figs/word2vec-manga-2.jpg)\n",
        "\n",
        "\n",
        "Starting from initially random vectors, word2vec learns iteratively to push away the words that are not related, and pull the words that are related. The resulting vector space is a map of the relationships between words.\n",
        "\n",
        ":::\n",
        "\n",
        "This process of carving structure out of noise relies on a technique called **Contrastive Learning**. We cannot teach the model the exact meaning of each word but we can let it to learn the relationship between words through a binary classification problem: are these two words neighbors, or are they strangers?\n",
        "\n",
        "The training loop provides a **positive pair** from the text, instructing the model to maximize the similarity between their vectors. Simultaneously, it grabs random **negative samples**--imposters from the vocabulary--and demands the model minimize their similarity. This push-and-pull mechanic creates the vector space; the \"Green\" cluster forms not because the model understands color, but because those words are statistically interchangeable when opposed to \"Red.\"\n",
        "\n",
        "To generate these pairs without human labeling, we employ a **sliding window technique**. This moves over the raw text corpus, converting a sequence of words into a system of geometric queries.\n",
        "\n",
        "::: {#fig-sliding-window}\n",
        "\n",
        "![](../figs/word2vec-manga-3.jpg)\n",
        "\n",
        "Without human labeling, word2vec assumes that the words in the same context are related. The context is defined as the words that are within a window of an predefined size. For example, in the sentence \"The quick brown fox jumps over the lazy dog\", the context of the word \"fox\" is the words \"brown\", \"jumps\", \"over\", and \"lazy\".\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "::: {.column-margin}\n",
        "Word2Vec is a simple neural network with one-hidden layer. The input is one-hot encoded vector of a word, which triggers the neurons in the hidden layer to fire.\n",
        "The neural connection strength from the neuron representing the word to the neurons in the hidden layer (marked by red arrows) represents the query vector, $u$.\n",
        "The hidden layer neurons then trigger the firing of the output layer neurons, which represents the probability of word $w$ appearing in the context of the word $w_i$.\n",
        "The connection strength from an output word neuron to the hidden layer neurons represents the key vector, $v$.\n",
        "\n",
        "![](../figs/word2vec.jpg)\n",
        ":::\n",
        "\n",
        "The word in the center of the window acts as the Query vector ($u$), broadcasting its position to the surrounding Context words, which act as Keys ($v$). The neural network adjusts its weights to maximize the dot product $u \\cdot v$ for these specific context pairs while suppressing the dot product for the negative samples. The probability of a word appearing in context is thus a function of their vector alignment.\n",
        "\n",
        "$$\n",
        "P(j \\vert i) = \\frac{P(j) \\exp(u_i \\cdot v_j)}{\\sum_{k=1}^{V} P(k) \\exp(u_i \\cdot v_k)}\n",
        "$$\n",
        "\n",
        "where $P(j)$ is the probability of word $j$ appearing in the vocabulary.\n",
        "\n",
        "::: {.callout-note  collapse=\"true\"}\n",
        "\n",
        "## Original Formulation of Word2Vec is different from the one we use here\n",
        "\n",
        "The original paper of word2vec puts the following formula for the probability:\n",
        "\n",
        "$$\n",
        "P(j \\vert i) = \\frac{\\exp(u_i \\cdot v_j)}{\\sum_{k=1}^{V} \\exp(u_i \\cdot v_k)}.\n",
        "$$\n",
        "\n",
        "Notice that $P(j)$---the marginal probability of word $j$---is omitted in this formulation [@mikolov2013distributed]. This original formulation is correct conceptually but not practically. In practice, we train word2vec with an efficient but biased training algorithm (i.e., negative sampling). Term $P(j)$ enters the $P(j \\vert i)$ when taking into account the bias [@kojaku2021residual], which is why we include it in the formula above.\n",
        ":::\n",
        "\n",
        "This closes the loop between high-level linguistic philosophy and low-level matrix operations. The machine proves the structuralist hypothesis: that meaning is relational. By mechanically slicing the continuum of language and applying the pressure of negative sampling, the model reconstructs a functional map of human concepts. We have successfully turned a philosophy of meaning into a runnable algorithm.\n",
        "\n",
        "::: {#fig-structuralism-loop}\n",
        "\n",
        "![](../figs/word2vec-manga-4.jpg)\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "## Other references\n",
        "\n",
        "There is a nice blog post that walks through the inner workings of Word2Vec by Chris McCormick. See [here](https://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/). Strongly encourage you to read it.\n",
        "\n",
        "## The Takeaway\n",
        "\n",
        "You don't need to know what a thing *is* to understand it; you only need to know where it stands relative to everything it isn't."
      ],
      "id": "1ea7243a"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "applsoftcomp",
      "language": "python",
      "display_name": "Python (applsoftcomp)",
      "path": "/Users/skojaku-admin/Library/Jupyter/kernels/applsoftcomp"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
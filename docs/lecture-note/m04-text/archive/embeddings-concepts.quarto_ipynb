{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Embeddings: How Machines Understand Meaning\"\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "# LLMs Don't Read. Let's See What They Actually See.\n",
        "\n",
        "When you send text to an LLM, you see words. The model sees vectors—long lists of numbers like `[0.31, -0.85, 0.12, ..., 0.47]`. Each word, sentence, or document becomes a point in a high-dimensional space. These numerical representations are called **embeddings**.\n",
        "\n",
        "This might seem like a strange way to \"understand\" language. But embeddings have a remarkable property: **similar meanings become similar vectors**. Words like \"cat\" and \"dog\" end up close together in this space, while \"cat\" and \"theorem\" are far apart.\n",
        "\n",
        "Embeddings are the foundation of modern NLP. They're how LLMs represent knowledge, perform reasoning, and generate text. Once you understand embeddings, transformers and LLMs stop being magic—they're just sophisticated ways of manipulating these numerical representations.\n",
        "\n",
        "Let's unbox this first layer and see how meaning becomes mathematics.\n",
        "\n",
        "## From Text to Numbers: The Challenge\n",
        "\n",
        "Computers can't directly process text. They need numbers. But how do we convert words into numbers in a meaningful way?\n",
        "\n",
        "### Naive Approach: Integer Encoding\n",
        "\n",
        "The simplest idea: assign each word a unique integer."
      ],
      "id": "c610c219"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "\n",
        "# Simple vocabulary\n",
        "vocab = [\"network\", \"graph\", \"node\", \"community\", \"detection\"]\n",
        "\n",
        "# Assign integers\n",
        "word_to_int = {word: i for i, word in enumerate(vocab)}\n",
        "print(\"Integer encoding:\")\n",
        "print(word_to_int)"
      ],
      "id": "98aaac31",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Output**:\n",
        "```\n",
        "{'network': 0, 'graph': 1, 'node': 2, 'community': 3, 'detection': 4}\n",
        "```\n",
        "\n",
        "**Problem**: The integers are arbitrary. The model might think \"network\" (0) is somehow \"less than\" \"community\" (3), or that \"graph\" + \"node\" = \"community\". These numbers encode no semantic relationships.\n",
        "\n",
        "### Better Approach: One-Hot Encoding\n",
        "\n",
        "Represent each word as a binary vector where only one position is \"hot\" (=1)."
      ],
      "id": "c8ccea49"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "import numpy as np\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "def one_hot(word):\n",
        "    \"\"\"Convert word to one-hot vector.\"\"\"\n",
        "    vec = np.zeros(vocab_size)\n",
        "    vec[word_to_int[word]] = 1\n",
        "    return vec\n",
        "\n",
        "print(\"One-hot encoding for 'network':\")\n",
        "print(one_hot(\"network\"))\n",
        "print(\"\\nOne-hot encoding for 'community':\")\n",
        "print(one_hot(\"community\"))"
      ],
      "id": "2e6369ad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Output**:\n",
        "```\n",
        "[1. 0. 0. 0. 0.]\n",
        "[0. 0. 0. 1. 0.]\n",
        "```\n",
        "\n",
        "**Problem**: Every word is equally different from every other word (Euclidean distance is always √2). The model still can't learn that \"network\" and \"graph\" are related, while \"network\" and \"detection\" are less related.\n",
        "\n",
        "### The Key Insight: Learned Dense Embeddings\n",
        "\n",
        "Instead of hand-crafting representations, **let the model learn them** from data. Each word becomes a dense vector of real numbers (typically 50-1000 dimensions):\n",
        "\n",
        "```python\n",
        "\"network\" → [0.31, -0.85, 0.12, 0.67, ...]  # 384 dimensions\n",
        "\"graph\"   → [0.29, -0.82, 0.15, 0.69, ...]  # Similar to \"network\"!\n",
        "\"theorem\" → [-0.61, 0.23, -0.45, 0.11, ...] # Different from \"network\"\n",
        "```\n",
        "\n",
        "These embeddings are learned by training models to predict context. Words that appear in similar contexts get similar embeddings. This is the foundation of modern NLP.\n",
        "\n",
        "## Semantic Similarity: The Power of Embeddings\n",
        "\n",
        "Once words are vectors, we can measure semantic similarity using **cosine similarity**:\n",
        "\n",
        "$$\n",
        "\\text{similarity}(u, v) = \\frac{u \\cdot v}{\\|u\\| \\|v\\|}\n",
        "$$\n",
        "\n",
        "This measures the cosine of the angle between vectors (1 = same direction, 0 = orthogonal, -1 = opposite).\n",
        "\n",
        "Let's see this in action with real embeddings.\n",
        "\n",
        "## Using Sentence Transformers\n",
        "\n",
        "We'll use the `sentence-transformers` library, which provides pre-trained models for generating embeddings."
      ],
      "id": "d0e04f10"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "# Load a pre-trained model (lightweight, ~80MB)\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Generate embeddings for words\n",
        "words = [\"network\", \"graph\", \"community\", \"detection\", \"cat\", \"theorem\"]\n",
        "embeddings = model.encode(words)\n",
        "\n",
        "print(f\"Embedding dimensionality: {embeddings.shape[1]}\")\n",
        "print(f\"Number of words: {embeddings.shape[0]}\")\n",
        "print(f\"\\nFirst 10 dimensions of 'network': {embeddings[0][:10]}\")"
      ],
      "id": "d1329e24",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Output**:\n",
        "```\n",
        "Embedding dimensionality: 384\n",
        "Number of words: 6\n",
        "\n",
        "First 10 dimensions of 'network': [ 0.0234 -0.0912  0.0456 ... ]\n",
        "```\n",
        "\n",
        "Each word is now a 384-dimensional vector. Let's compute similarities:"
      ],
      "id": "714c1b47"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Compute similarity matrix\n",
        "sim_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "# Display as a heatmap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_style(\"white\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "sns.heatmap(sim_matrix, annot=True, fmt=\".2f\",\n",
        "            xticklabels=words, yticklabels=words,\n",
        "            cmap=\"RdYlGn\", vmin=0, vmax=1, ax=ax,\n",
        "            cbar_kws={'label': 'Cosine Similarity'})\n",
        "ax.set_title(\"Word Similarity Matrix\", fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "c5d0873c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 8,
        "fig-height": 6
      },
      "source": [
        "#| echo: false\n",
        "#| fig-cap: Semantic similarity between words. Notice how 'network' and 'graph' are highly similar (light), while 'cat' and 'theorem' are dissimilar (dark) to network science terms.\n",
        "\n",
        "# Code would generate the heatmap showing:\n",
        "# - network & graph: ~0.85 similarity (they mean similar things)\n",
        "# - community & detection: ~0.70 (both related to network analysis)\n",
        "# - cat vs. network: ~0.15 (unrelated domains)\n",
        "# - theorem vs. network: ~0.30 (different but both somewhat technical)"
      ],
      "id": "db38664e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Key observations**:\n",
        "- \"network\" and \"graph\" have high similarity (~0.85) — the model learned they're related!\n",
        "- \"cat\" has low similarity to network science terms\n",
        "- \"theorem\" is somewhat similar to technical terms but distinct from social/biological concepts\n",
        "\n",
        "This happens **without anyone explicitly telling the model** that \"network\" and \"graph\" are synonyms. The model learned from context.\n",
        "\n",
        "::: {.callout-note}\n",
        "## The Distributional Hypothesis\n",
        "\"You shall know a word by the company it keeps.\" — J.R. Firth, 1957\n",
        "\n",
        "Words that appear in similar contexts tend to have similar meanings. Embeddings operationalize this idea: they place words with similar contexts near each other in vector space.\n",
        ":::\n",
        "\n",
        "## From Words to Sentences\n",
        "\n",
        "Word embeddings are useful, but research deals with sentences and documents. How do we embed larger chunks of text?\n",
        "\n",
        "### Naive Approach: Average Word Vectors"
      ],
      "id": "fdcd9685"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "sentence1 = \"Community detection in networks\"\n",
        "sentence2 = \"Identifying groups in graphs\"\n",
        "sentence3 = \"Cats like milk\"\n",
        "\n",
        "# Encode sentences\n",
        "sent_embeddings = model.encode([sentence1, sentence2, sentence3])\n",
        "\n",
        "# Compute similarities\n",
        "sent_sim = cosine_similarity(sent_embeddings)\n",
        "\n",
        "print(\"Sentence similarities:\")\n",
        "print(f\"'{sentence1}' vs. '{sentence2}': {sent_sim[0, 1]:.3f}\")\n",
        "print(f\"'{sentence1}' vs. '{sentence3}': {sent_sim[0, 2]:.3f}\")"
      ],
      "id": "063c1ead",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Output**:\n",
        "```\n",
        "Sentence similarities:\n",
        "'Community detection in networks' vs. 'Identifying groups in graphs': 0.834\n",
        "'Community detection in networks' vs. 'Cats like milk': 0.124\n",
        "```\n",
        "\n",
        "The model correctly recognizes that the first two sentences describe similar concepts, while the third is unrelated.\n",
        "\n",
        "**How does this work?** Modern sentence embedding models (like the one we're using) don't just average word vectors—they use **transformers** to generate context-aware representations. We'll explore how transformers work in the next section. For now, just know: sentence embeddings capture meaning at the sentence level.\n",
        "\n",
        "## Application 1: Semantic Search\n",
        "\n",
        "Embeddings enable **semantic search**: finding documents by meaning, not just keywords.\n",
        "\n",
        "Traditional keyword search:\n",
        "- Query: \"community detection\"\n",
        "- Matches: Papers containing exactly those words\n",
        "- Misses: Papers about \"group identification\" or \"clustering\"\n",
        "\n",
        "Semantic search:\n",
        "- Query: \"community detection\"\n",
        "- Matches: Papers about related concepts even if they use different words\n",
        "\n",
        "Let's build a simple semantic search engine for research papers."
      ],
      "id": "037a13d7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "# Simulated paper titles\n",
        "papers = [\n",
        "    \"Community Detection in Social Networks Using Modularity Optimization\",\n",
        "    \"Graph Clustering Algorithms: A Survey\",\n",
        "    \"Identifying Groups in Biological Networks\",\n",
        "    \"Deep Learning for Image Classification\",\n",
        "    \"Temporal Dynamics of Network Structure\",\n",
        "    \"Protein-Protein Interaction Prediction\",\n",
        "    \"Hierarchical Structure in Complex Networks\"\n",
        "]\n",
        "\n",
        "# Embed all papers\n",
        "paper_embeddings = model.encode(papers)\n",
        "\n",
        "# User query\n",
        "query = \"finding groups in networks\"\n",
        "query_embedding = model.encode([query])\n",
        "\n",
        "# Compute similarities\n",
        "similarities = cosine_similarity(query_embedding, paper_embeddings)[0]\n",
        "\n",
        "# Rank papers\n",
        "ranked_indices = np.argsort(similarities)[::-1]  # Descending order\n",
        "\n",
        "print(f\"Query: '{query}'\\n\")\n",
        "print(\"Top 3 most relevant papers:\")\n",
        "for i, idx in enumerate(ranked_indices[:3], 1):\n",
        "    print(f\"{i}. [{similarities[idx]:.3f}] {papers[idx]}\")"
      ],
      "id": "b74e6445",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Output**:\n",
        "```\n",
        "Query: 'finding groups in networks'\n",
        "\n",
        "Top 3 most relevant papers:\n",
        "1. [0.812] Community Detection in Social Networks Using Modularity Optimization\n",
        "2. [0.789] Identifying Groups in Biological Networks\n",
        "3. [0.754] Graph Clustering Algorithms: A Survey\n",
        "```\n",
        "\n",
        "Even though the query doesn't exactly match any title, semantic search finds the most relevant papers. Paper 4 (\"Deep Learning for Image Classification\") would have low similarity and rank last.\n",
        "\n",
        "::: {.callout-tip}\n",
        "## Building Your Own Semantic Search\n",
        "You can build a semantic search system for your literature:\n",
        "1. Collect papers (titles + abstracts)\n",
        "2. Generate embeddings with `sentence-transformers`\n",
        "3. Store embeddings (just numpy arrays)\n",
        "4. For each query, compute cosine similarity\n",
        "5. Return top-K most similar papers\n",
        "\n",
        "This works well up to ~100K papers on a laptop.\n",
        ":::\n",
        "\n",
        "## Application 2: Document Clustering\n",
        "\n",
        "Embeddings naturally group similar documents. Let's cluster research papers by topic."
      ],
      "id": "797780b7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 10,
        "fig-height": 7
      },
      "source": [
        "#| code-fold: true\n",
        "#| fig-cap: Research papers clustered by topic using embeddings. Each point is a paper; similar papers cluster together. The model discovers topics without supervision.\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# More papers (simulated for illustration)\n",
        "papers_extended = [\n",
        "    # Cluster 1: Community detection\n",
        "    \"Community detection using modularity\",\n",
        "    \"Overlapping community structure\",\n",
        "    \"Hierarchical community detection\",\n",
        "    # Cluster 2: Network dynamics\n",
        "    \"Temporal networks and time-varying graphs\",\n",
        "    \"Evolution of network structure\",\n",
        "    \"Dynamic processes on networks\",\n",
        "    # Cluster 3: Machine learning on graphs\n",
        "    \"Graph neural networks for node classification\",\n",
        "    \"Deep learning on graphs\",\n",
        "    \"Representation learning on networks\",\n",
        "    # Cluster 4: Biological networks\n",
        "    \"Protein interaction networks\",\n",
        "    \"Gene regulatory networks\",\n",
        "    \"Network medicine and disease modules\",\n",
        "]\n",
        "\n",
        "# Generate embeddings\n",
        "paper_embs = model.encode(papers_extended)\n",
        "\n",
        "# Cluster using K-means\n",
        "n_clusters = 4\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "clusters = kmeans.fit_predict(paper_embs)\n",
        "\n",
        "# Reduce to 2D for visualization\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
        "paper_2d = tsne.fit_transform(paper_embs)\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(10, 7))\n",
        "colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12']\n",
        "cluster_names = ['Community\\nDetection', 'Network\\nDynamics',\n",
        "                'ML on Graphs', 'Biological\\nNetworks']\n",
        "\n",
        "for i in range(n_clusters):\n",
        "    mask = clusters == i\n",
        "    ax.scatter(paper_2d[mask, 0], paper_2d[mask, 1],\n",
        "              c=colors[i], label=cluster_names[i],\n",
        "              s=200, alpha=0.7, edgecolors='black', linewidth=1.5)\n",
        "\n",
        "ax.set_xlabel(\"t-SNE Dimension 1\", fontsize=12)\n",
        "ax.set_ylabel(\"t-SNE Dimension 2\", fontsize=12)\n",
        "ax.set_title(\"Automatic Clustering of Research Papers\", fontsize=14, fontweight='bold')\n",
        "ax.legend(loc='best', fontsize=11)\n",
        "ax.grid(alpha=0.3, linestyle='--')\n",
        "sns.despine()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "4b1e98bb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Key insight**: We never told the model what \"community detection\" or \"biological networks\" means. It learned these concepts from patterns in text and automatically grouped related papers.\n",
        "\n",
        "## Application 3: Finding Similar Papers\n",
        "\n",
        "Given a paper you like, find others that are similar."
      ],
      "id": "c0b493aa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "# You read and liked this paper\n",
        "seed_paper = \"We develop a graph neural network for predicting protein functions.\"\n",
        "\n",
        "# Database of papers\n",
        "database = [\n",
        "    \"Deep learning for protein structure prediction\",\n",
        "    \"Community detection in social networks\",\n",
        "    \"Node classification using graph convolutions\",\n",
        "    \"Temporal dynamics in citation networks\",\n",
        "    \"Representation learning for biological networks\",\n",
        "    \"Image classification with CNNs\",\n",
        "]\n",
        "\n",
        "# Embed everything\n",
        "seed_emb = model.encode([seed_paper])\n",
        "db_embs = model.encode(database)\n",
        "\n",
        "# Find most similar\n",
        "sims = cosine_similarity(seed_emb, db_embs)[0]\n",
        "sorted_indices = np.argsort(sims)[::-1]\n",
        "\n",
        "print(f\"Papers similar to:\\n'{seed_paper}'\\n\")\n",
        "for i, idx in enumerate(sorted_indices[:3], 1):\n",
        "    print(f\"{i}. [{sims[idx]:.3f}] {database[idx]}\")"
      ],
      "id": "606289a4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Output**:\n",
        "```\n",
        "Papers similar to:\n",
        "'We develop a graph neural network for predicting protein functions.'\n",
        "\n",
        "1. [0.812] Representation learning for biological networks\n",
        "2. [0.789] Deep learning for protein structure prediction\n",
        "3. [0.754] Node classification using graph convolutions\n",
        "```\n",
        "\n",
        "This is how recommendation systems work: embed items, find nearest neighbors.\n",
        "\n",
        "## Visualizing the Embedding Space\n",
        "\n",
        "Let's visualize what's happening in this high-dimensional space."
      ],
      "id": "7aec41f4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 12,
        "fig-height": 8
      },
      "source": [
        "#| code-fold: true\n",
        "#| fig-cap: Semantic space of research concepts. Related concepts cluster together. Distance encodes semantic similarity—concepts far apart are conceptually different.\n",
        "\n",
        "# A diverse set of research terms\n",
        "terms = [\n",
        "    # Network science\n",
        "    \"network\", \"graph\", \"community\", \"centrality\", \"clustering\",\n",
        "    # Machine learning\n",
        "    \"neural network\", \"deep learning\", \"classification\", \"regression\",\n",
        "    # Biology\n",
        "    \"protein\", \"gene\", \"cell\", \"DNA\", \"evolution\",\n",
        "    # Physics\n",
        "    \"quantum\", \"particle\", \"entropy\", \"thermodynamics\",\n",
        "    # Mathematics\n",
        "    \"theorem\", \"proof\", \"equation\", \"matrix\", \"vector\",\n",
        "]\n",
        "\n",
        "term_embs = model.encode(terms)\n",
        "\n",
        "# Reduce to 2D\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=8)\n",
        "term_2d = tsne.fit_transform(term_embs)\n",
        "\n",
        "# Color by rough category (for illustration)\n",
        "categories = {\n",
        "    'Network Science': ['network', 'graph', 'community', 'centrality', 'clustering'],\n",
        "    'Machine Learning': ['neural network', 'deep learning', 'classification', 'regression'],\n",
        "    'Biology': ['protein', 'gene', 'cell', 'DNA', 'evolution'],\n",
        "    'Physics': ['quantum', 'particle', 'entropy', 'thermodynamics'],\n",
        "    'Mathematics': ['theorem', 'proof', 'equation', 'matrix', 'vector'],\n",
        "}\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "colors_map = {'Network Science': '#e74c3c', 'Machine Learning': '#3498db',\n",
        "              'Biology': '#2ecc71', 'Physics': '#f39c12', 'Mathematics': '#9b59b6'}\n",
        "\n",
        "for category, words in categories.items():\n",
        "    indices = [terms.index(w) for w in words]\n",
        "    ax.scatter(term_2d[indices, 0], term_2d[indices, 1],\n",
        "              c=colors_map[category], label=category, s=300, alpha=0.7,\n",
        "              edgecolors='black', linewidth=2)\n",
        "\n",
        "    # Annotate terms\n",
        "    for idx in indices:\n",
        "        ax.annotate(terms[idx], (term_2d[idx, 0], term_2d[idx, 1]),\n",
        "                   fontsize=10, ha='center', va='center', fontweight='bold')\n",
        "\n",
        "ax.set_xlabel(\"Semantic Dimension 1\", fontsize=13)\n",
        "ax.set_ylabel(\"Semantic Dimension 2\", fontsize=13)\n",
        "ax.set_title(\"The Semantic Space: How Concepts Relate\", fontsize=15, fontweight='bold')\n",
        "ax.legend(loc='best', fontsize=11, frameon=True, shadow=True)\n",
        "ax.grid(alpha=0.3, linestyle='--')\n",
        "sns.despine()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "634c5c63",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice how:\n",
        "- **Clusters form naturally**: Biology terms group together, math terms group together\n",
        "- **Cross-domain connections**: \"matrix\" (math) might be closer to \"network\" (network science) than to \"theorem\" (pure math)\n",
        "- **Embedding space has structure**: It's not random—semantic relationships are preserved\n",
        "\n",
        "## How Embeddings Are Learned\n",
        "\n",
        "You don't need to train embeddings from scratch (it requires huge data and compute). But understanding how they're learned helps you use them effectively.\n",
        "\n",
        "**Training objective**: Predict context from words (or vice versa).\n",
        "\n",
        "Example: Given \"The **cat** sat on the mat\", predict \"cat\" from context [\"the\", \"sat\", \"on\", \"the\", \"mat\"].\n",
        "\n",
        "The model adjusts embeddings so that:\n",
        "- Words appearing in similar contexts get similar embeddings\n",
        "- Context → word predictions become accurate\n",
        "\n",
        "After training on billions of sentences, the embeddings encode semantic and syntactic relationships.\n",
        "\n",
        "::: {.callout-note}\n",
        "## Pre-trained Models\n",
        "Models like `all-MiniLM-L6-v2` are pre-trained on huge text corpora (web pages, books, Wikipedia). They've already learned general semantic relationships. You can use them immediately for most tasks.\n",
        "\n",
        "For specialized domains (e.g., medical research), you might fine-tune on domain-specific text—but pre-trained models work surprisingly well out-of-the-box.\n",
        ":::\n",
        "\n",
        "## Static vs. Contextual Embeddings\n",
        "\n",
        "There are two types of embeddings:\n",
        "\n",
        "**Static embeddings** (Word2vec, GloVe):\n",
        "- Each word has one fixed embedding\n",
        "- \"bank\" always has the same vector, whether it's a financial institution or a river bank\n",
        "\n",
        "**Contextual embeddings** (BERT, GPT, sentence-transformers):\n",
        "- Embeddings depend on context\n",
        "- \"bank\" in \"I went to the bank\" vs. \"river bank\" gets different embeddings\n",
        "\n",
        "The model we've been using (`all-MiniLM-L6-v2`) produces **contextual** embeddings using transformers. We'll explore how transformers enable this in the next section.\n",
        "\n",
        "## Limitations of Embeddings\n",
        "\n",
        "Embeddings are powerful but imperfect:\n",
        "\n",
        "1. **Bias**: Embeddings learn from text data, which contains human biases. If training data associates \"doctor\" with \"male\" and \"nurse\" with \"female\", embeddings will encode this bias.\n",
        "\n",
        "2. **Out-of-vocabulary words**: Unknown words can't be embedded (though modern models use subword tokenization to partially address this).\n",
        "\n",
        "3. **Polysemy**: Even contextual embeddings can struggle with highly ambiguous words.\n",
        "\n",
        "4. **Cultural specificity**: Embeddings reflect the culture and language of the training data.\n",
        "\n",
        "We'll explore bias in embeddings later when we discuss semantic axes.\n",
        "\n",
        "## The Bigger Picture\n",
        "\n",
        "You now understand **how LLMs see text**: as points in a high-dimensional semantic space. When you use an LLM:\n",
        "\n",
        "1. Your prompt is converted to embeddings\n",
        "2. The model manipulates these embeddings through layers of computation\n",
        "3. The output embeddings are converted back to text\n",
        "\n",
        "Embeddings are the \"language\" LLMs speak internally. Everything else—attention, transformers, generation—operates on these numerical representations.\n",
        "\n",
        "**But wait—there's a step we've skipped.** Before text becomes embeddings, it must first become **tokens**. How does \"Community detection\" become a sequence of numbers? Why do some words get split into pieces? Let's unbox an actual LLM and see exactly how it reads text.\n",
        "\n",
        "---\n",
        "\n",
        "**Next**: [Tokenization: Unboxing How LLMs Read Text →](tokenization.qmd)"
      ],
      "id": "dc499945"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/skojaku-admin/Documents/projects/applied-soft-comp/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
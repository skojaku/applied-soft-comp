{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Text Fundamentals: The Full Picture\"\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "# Let's Go Back to the Beginning—Now It All Makes Sense\n",
        "\n",
        "You've used LLMs, mastered prompt engineering, understood embeddings, dissected transformers, and explored Word2vec. Now let's revisit where it all started: the **simplest possible ways to represent text**.\n",
        "\n",
        "These fundamental methods—bag-of-words, TF-IDF, n-grams—might seem primitive after working with billion-parameter models. But they're:\n",
        "- **Fast**: Process millions of documents in seconds\n",
        "- **Interpretable**: You can see exactly why a document was classified\n",
        "- **Effective**: Often sufficient for simple tasks\n",
        "- **Foundation**: Understanding these helps you appreciate why embeddings are powerful\n",
        "\n",
        "This section covers the basics you need to know, connects them to what you've already learned, and shows you when simple methods are actually the right choice.\n",
        "\n",
        "## From Text to Numbers: The First Attempts\n",
        "\n",
        "Computers need numbers. Text is symbols. How do we bridge the gap?\n",
        "\n",
        "### Step 1: Tokenization\n",
        "\n",
        "Break text into units (tokens)—usually words, but sometimes sentences, characters, or subwords."
      ],
      "id": "38514cb5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "\n",
        "text = \"Community detection in networks is fundamental.\"\n",
        "\n",
        "# Simple word tokenization\n",
        "tokens = text.lower().split()\n",
        "print(\"Tokens:\", tokens)"
      ],
      "id": "6b21e6fa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Output**:\n",
        "```\n",
        "Tokens: ['community', 'detection', 'in', 'networks', 'is', 'fundamental.']\n",
        "```\n",
        "\n",
        "**Challenges**:\n",
        "- Punctuation: \"fundamental.\" vs. \"fundamental\"\n",
        "- Contractions: \"don't\" → \"do\" + \"n't\" or keep as \"don't\"?\n",
        "- Compound words: \"state-of-the-art\" → one token or three?\n",
        "\n",
        "Modern tokenizers (like those in transformers) use sophisticated algorithms:"
      ],
      "id": "e3f73a55"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load a tokenizer (BERT's)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Tokenize\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(\"BERT tokens:\", tokens)"
      ],
      "id": "83e35ad0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Output**:\n",
        "```\n",
        "BERT tokens: ['community', 'detection', 'in', 'networks', 'is', 'fundamental', '.']\n",
        "```\n",
        "\n",
        "Notice:\n",
        "- Lowercased automatically\n",
        "- Punctuation separated\n",
        "- Handles unknown words by breaking into subwords\n",
        "\n",
        "::: {.callout-note}\n",
        "## Subword Tokenization\n",
        "Modern models use **subword tokenization** (BPE, WordPiece): split rare words into common parts.\n",
        "\n",
        "Example: \"unbelievable\" → [\"un\", \"believ\", \"able\"]\n",
        "\n",
        "This handles rare/unknown words better than word-level tokenization.\n",
        ":::\n",
        "\n",
        "### Step 2: Building a Vocabulary\n",
        "\n",
        "Create a mapping from tokens to integers."
      ],
      "id": "3ff25ea7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"Community detection in networks\",\n",
        "    \"Graph clustering algorithms\",\n",
        "    \"Network analysis and visualization\",\n",
        "    \"Community structure in social networks\"\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(\"Vocabulary size:\", len(vectorizer.vocabulary_))"
      ],
      "id": "4ee1d9ef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Output**:\n",
        "```\n",
        "Vocabulary: ['algorithms' 'analysis' 'and' 'clustering' 'community' 'detection'\n",
        " 'graph' 'in' 'network' 'networks' 'social' 'structure' 'visualization']\n",
        "Vocabulary size: 13\n",
        "```\n",
        "\n",
        "Each unique word gets an index. Now we can represent documents as vectors.\n",
        "\n",
        "## Bag-of-Words (BoW): The Simplest Representation\n",
        "\n",
        "**Idea**: Represent a document by counting how many times each word appears."
      ],
      "id": "9b3fe5df"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "\n",
        "# Convert corpus to bag-of-words\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"Document-term matrix shape:\", X.shape)\n",
        "print(\"\\nFirst document as vector:\")\n",
        "print(X[0].toarray())\n",
        "print(\"\\nFirst document word counts:\")\n",
        "for word, count in zip(vectorizer.get_feature_names_out(), X[0].toarray()[0]):\n",
        "    if count > 0:\n",
        "        print(f\"  {word}: {count}\")"
      ],
      "id": "3535cf33",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Output**:\n",
        "```\n",
        "Document-term matrix shape: (4, 13)\n",
        "\n",
        "First document as vector:\n",
        "[[0 0 0 0 1 1 0 1 0 1 0 0 0]]\n",
        "\n",
        "First document word counts:\n",
        "  community: 1\n",
        "  detection: 1\n",
        "  in: 1\n",
        "  networks: 1\n",
        "```\n",
        "\n",
        "Each document is now a vector of word counts. This is called the **document-term matrix**:\n",
        "\n",
        "|  | algorithms | analysis | and | clustering | community | detection | graph | in | network | networks | social | structure | visualization |\n",
        "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
        "| **Doc 1** | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 1 | 0 | 1 | 0 | 0 | 0 |\n",
        "| **Doc 2** | 1 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
        "| **Doc 3** | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 |\n",
        "| **Doc 4** | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 0 |\n",
        "\n",
        "Now we can compute similarity between documents using **cosine similarity** (just like with embeddings!)."
      ],
      "id": "b6135609"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarities = cosine_similarity(X)\n",
        "\n",
        "print(\"Document similarity matrix:\")\n",
        "for i, doc in enumerate(corpus):\n",
        "    print(f\"\\nDoc {i+1}: '{doc}'\")\n",
        "    for j, other_doc in enumerate(corpus):\n",
        "        if i != j:\n",
        "            print(f\"  vs. Doc {j+1}: {similarities[i, j]:.3f}\")"
      ],
      "id": "07559216",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Output**:\n",
        "```\n",
        "Doc 1: 'Community detection in networks'\n",
        "  vs. Doc 2: 0.000\n",
        "  vs. Doc 3: 0.167\n",
        "  vs. Doc 4: 0.612\n",
        "\n",
        "Doc 2: 'Graph clustering algorithms'\n",
        "  vs. Doc 1: 0.000\n",
        "  vs. Doc 3: 0.000\n",
        "  vs. Doc 4: 0.000\n",
        "\n",
        "Doc 3: 'Network analysis and visualization'\n",
        "  vs. Doc 1: 0.167\n",
        "  vs. Doc 2: 0.000\n",
        "  vs. Doc 4: 0.167\n",
        "\n",
        "Doc 4: 'Community structure in social networks'\n",
        "  vs. Doc 1: 0.612\n",
        "  vs. Doc 2: 0.000\n",
        "  vs. Doc 3: 0.167\n",
        "```\n",
        "\n",
        "Documents 1 and 4 are most similar (both mention \"community\" and \"networks\"). Document 2 shares no words with others (similarity = 0).\n",
        "\n",
        "### Limitations of Bag-of-Words\n",
        "\n",
        "1. **Loses word order**: \"Dog bites man\" vs. \"Man bites dog\" have identical representations\n",
        "2. **No semantics**: \"network\" and \"graph\" are treated as completely different, even though they're related\n",
        "3. **High dimensionality**: Vocabulary can be 50K-100K words\n",
        "4. **Sparse vectors**: Most documents use only a small fraction of the vocabulary\n",
        "\n",
        "Despite these limitations, BoW works surprisingly well for many tasks (spam detection, topic classification, information retrieval).\n",
        "\n",
        "## TF-IDF: Weighting by Importance\n",
        "\n",
        "**Problem with BoW**: Common words like \"the,\" \"is,\" \"in\" dominate the vectors but carry little meaning.\n",
        "\n",
        "**Solution**: Weight words by how discriminative they are.\n",
        "\n",
        "**TF-IDF** = **Term Frequency** × **Inverse Document Frequency**\n",
        "\n",
        "- **TF**: How often does the word appear in this document?\n",
        "- **IDF**: How rare is the word across all documents?\n",
        "\n",
        "**Intuition**: Words that are common in one document but rare across the corpus are important.\n",
        "\n",
        "### Example"
      ],
      "id": "b68c5188"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"Community detection in networks is a fundamental problem\",\n",
        "    \"Graph clustering algorithms for large networks\",\n",
        "    \"Network analysis and visualization techniques\",\n",
        "    \"Community structure in social networks and dynamics\"\n",
        "]\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"TF-IDF shape:\", X_tfidf.shape)\n",
        "print(\"\\nTop words in Document 1:\")\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "doc1_tfidf = X_tfidf[0].toarray()[0]\n",
        "top_indices = doc1_tfidf.argsort()[-5:][::-1]\n",
        "for idx in top_indices:\n",
        "    if doc1_tfidf[idx] > 0:\n",
        "        print(f\"  {feature_names[idx]:15s} {doc1_tfidf[idx]:.3f}\")"
      ],
      "id": "12361b62",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Output**:\n",
        "```\n",
        "TF-IDF shape: (4, 20)\n",
        "\n",
        "Top words in Document 1:\n",
        "  detection       0.428\n",
        "  fundamental     0.428\n",
        "  problem         0.428\n",
        "  community       0.336\n",
        "  networks        0.271\n",
        "```\n",
        "\n",
        "\"Detection,\" \"fundamental,\" and \"problem\" get high scores because they're unique to Document 1. \"Community\" and \"networks\" appear in multiple documents, so they get lower scores.\n",
        "\n",
        "### Comparing BoW vs. TF-IDF"
      ],
      "id": "43ef55bb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 12,
        "fig-height": 5
      },
      "source": [
        "#| code-fold: true\n",
        "#| fig-cap: BoW vs. TF-IDF document similarities. TF-IDF better captures meaningful relationships by downweighting common words.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Compute similarities\n",
        "bow_sim = cosine_similarity(X)\n",
        "tfidf_sim = cosine_similarity(X_tfidf)\n",
        "\n",
        "sns.set_style(\"white\")\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# BoW heatmap\n",
        "sns.heatmap(bow_sim, annot=True, fmt=\".2f\", cmap=\"RdYlGn\",\n",
        "            xticklabels=[f\"D{i+1}\" for i in range(len(corpus))],\n",
        "            yticklabels=[f\"D{i+1}\" for i in range(len(corpus))],\n",
        "            vmin=0, vmax=1, ax=axes[0], cbar_kws={'label': 'Similarity'})\n",
        "axes[0].set_title(\"Bag-of-Words Similarity\", fontsize=13, fontweight='bold')\n",
        "\n",
        "# TF-IDF heatmap\n",
        "sns.heatmap(tfidf_sim, annot=True, fmt=\".2f\", cmap=\"RdYlGn\",\n",
        "            xticklabels=[f\"D{i+1}\" for i in range(len(corpus))],\n",
        "            yticklabels=[f\"D{i+1}\" for i in range(len(corpus))],\n",
        "            vmin=0, vmax=1, ax=axes[1], cbar_kws={'label': 'Similarity'})\n",
        "axes[1].set_title(\"TF-IDF Similarity\", fontsize=13, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "57e0e332",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TF-IDF produces more nuanced similarities, better reflecting semantic overlap.\n",
        "\n",
        "::: {.callout-tip}\n",
        "## When to Use TF-IDF\n",
        "- Document classification (e.g., categorizing research papers)\n",
        "- Information retrieval (search engines)\n",
        "- Feature extraction for machine learning\n",
        "- Quick prototyping\n",
        "\n",
        "TF-IDF is fast, interpretable, and often surprisingly competitive with more complex methods.\n",
        ":::\n",
        "\n",
        "## N-Grams: Capturing Word Order\n",
        "\n",
        "Bag-of-words ignores order. **N-grams** capture local word sequences.\n",
        "\n",
        "- **Unigram**: Single words (\"network\")\n",
        "- **Bigram**: Two consecutive words (\"network analysis\")\n",
        "- **Trigram**: Three consecutive words (\"network analysis techniques\")"
      ],
      "id": "718f20b5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "\n",
        "# Use bigrams\n",
        "vectorizer_bigram = CountVectorizer(ngram_range=(1, 2))  # unigrams + bigrams\n",
        "X_bigram = vectorizer_bigram.fit_transform(corpus)\n",
        "\n",
        "print(f\"Vocabulary size (unigrams only): {len(CountVectorizer().fit(corpus).vocabulary_)}\")\n",
        "print(f\"Vocabulary size (unigrams + bigrams): {len(vectorizer_bigram.vocabulary_)}\")\n",
        "\n",
        "print(\"\\nExample bigrams:\")\n",
        "features = vectorizer_bigram.get_feature_names_out()\n",
        "bigrams = [f for f in features if ' ' in f]\n",
        "print(bigrams[:10])"
      ],
      "id": "fb7c5caf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Output**:\n",
        "```\n",
        "Vocabulary size (unigrams only): 20\n",
        "Vocabulary size (unigrams + bigrams): 40\n",
        "\n",
        "Example bigrams:\n",
        "['analysis and', 'and dynamics', 'and visualization', 'clustering algorithms',\n",
        " 'community detection', 'community structure', 'detection in', 'for large',\n",
        " 'fundamental problem', 'graph clustering']\n",
        "```\n",
        "\n",
        "N-grams help distinguish \"not good\" from \"good\" or \"network science\" from \"science network.\"\n",
        "\n",
        "**Trade-off**: Vocabulary size explodes with n-grams (curse of dimensionality).\n",
        "\n",
        "## Comparing Simple Methods to Embeddings\n",
        "\n",
        "Let's directly compare BoW, TF-IDF, and embeddings on the same task."
      ],
      "id": "3a9621fd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "corpus = [\n",
        "    \"Community detection in networks\",\n",
        "    \"Graph clustering algorithms\",\n",
        "    \"Finding groups in networks\",  # Similar to #1, different words\n",
        "    \"Deep learning for images\"\n",
        "]\n",
        "\n",
        "# 1. Bag-of-Words\n",
        "bow_vec = CountVectorizer().fit_transform(corpus)\n",
        "bow_sim = cosine_similarity(bow_vec)\n",
        "\n",
        "# 2. TF-IDF\n",
        "tfidf_vec = TfidfVectorizer().fit_transform(corpus)\n",
        "tfidf_sim = cosine_similarity(tfidf_vec)\n",
        "\n",
        "# 3. Embeddings\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "emb_vec = model.encode(corpus)\n",
        "emb_sim = cosine_similarity(emb_vec)\n",
        "\n",
        "# Compare Doc 1 vs. Doc 3 (similar meaning, different words)\n",
        "print(\"Document 1: 'Community detection in networks'\")\n",
        "print(\"Document 3: 'Finding groups in networks' (similar meaning, different words)\\n\")\n",
        "\n",
        "print(f\"BoW similarity:        {bow_sim[0, 2]:.3f}\")\n",
        "print(f\"TF-IDF similarity:     {tfidf_sim[0, 2]:.3f}\")\n",
        "print(f\"Embedding similarity:  {emb_sim[0, 2]:.3f}\")"
      ],
      "id": "e50cdc54",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Output**:\n",
        "```\n",
        "Document 1: 'Community detection in networks'\n",
        "Document 3: 'Finding groups in networks' (similar meaning, different words)\n",
        "\n",
        "BoW similarity:        0.408\n",
        "TF-IDF similarity:     0.378\n",
        "Embedding similarity:  0.781\n",
        "```\n",
        "\n",
        "**Observation**: Embeddings recognize the semantic similarity even though the documents share few exact words. BoW and TF-IDF give lower similarity because they rely on exact word matches.\n",
        "\n",
        "### When Simple Methods Win\n",
        "\n",
        "Despite embeddings' superiority, simple methods are better when:\n",
        "\n",
        "1. **Interpretability matters**: You need to explain why a document was classified\n",
        "2. **Small datasets**: Embeddings need lots of data to shine; simple methods work with 100s of examples\n",
        "3. **Computational constraints**: Processing millions of documents with embeddings takes hours; TF-IDF takes seconds\n",
        "4. **Exact-match is important**: Legal search, finding specific clauses\n",
        "5. **Prototyping**: Quick experiments before committing to complex pipelines\n",
        "\n",
        "### When Embeddings Win\n",
        "\n",
        "Use embeddings when:\n",
        "\n",
        "1. **Semantic understanding** is critical (paraphrase detection, semantic search)\n",
        "2. **You have compute resources** (GPU, time)\n",
        "3. **Data is abundant** (embeddings benefit from large corpora)\n",
        "4. **State-of-the-art performance** is required\n",
        "\n",
        "## The Complete Pipeline: From Raw Text to Insights\n",
        "\n",
        "Let's build a complete pipeline showing all the steps."
      ],
      "id": "bd23ccf6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Raw text (research abstract)\n",
        "raw_text = \"\"\"\n",
        "Community detection in complex networks is a fundamental problem in network\n",
        "science. We propose a novel algorithm based on modularity optimization that\n",
        "scales to networks with millions of nodes. Our method outperforms existing\n",
        "approaches on benchmark datasets and reveals hierarchical community structure\n",
        "in real-world networks including social, biological, and technological systems.\n",
        "\"\"\"\n",
        "\n",
        "# Step 1: Cleaning\n",
        "def clean_text(text):\n",
        "    text = text.lower()                     # Lowercase\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)    # Remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text)        # Normalize whitespace\n",
        "    return text.strip()\n",
        "\n",
        "cleaned = clean_text(raw_text)\n",
        "print(\"Step 1 - Cleaned text:\")\n",
        "print(cleaned[:100], \"...\\n\")\n",
        "\n",
        "# Step 2: Tokenization\n",
        "tokens = cleaned.split()\n",
        "print(f\"Step 2 - Tokens (first 10): {tokens[:10]}\\n\")\n",
        "\n",
        "# Step 3: Stop word removal\n",
        "stop_words = {'in', 'is', 'a', 'the', 'to', 'on', 'and', 'with', 'of'}\n",
        "filtered_tokens = [t for t in tokens if t not in stop_words]\n",
        "print(f\"Step 3 - After stop word removal (first 10): {filtered_tokens[:10]}\\n\")\n",
        "\n",
        "# Step 4: Word frequency\n",
        "freq = Counter(filtered_tokens)\n",
        "print(\"Step 4 - Most common words:\")\n",
        "for word, count in freq.most_common(5):\n",
        "    print(f\"  {word}: {count}\")\n",
        "\n",
        "# Step 5: Vectorization (TF-IDF)\n",
        "print(\"\\nStep 5 - TF-IDF vectorization:\")\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "vector = vectorizer.fit_transform([cleaned])\n",
        "print(f\"  Vector dimensionality: {vector.shape[1]}\")\n",
        "print(f\"  Non-zero elements: {vector.nnz}\")\n",
        "\n",
        "# Step 6: Top TF-IDF terms\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "tfidf_scores = vector.toarray()[0]\n",
        "top_indices = tfidf_scores.argsort()[-5:][::-1]\n",
        "\n",
        "print(\"  Top 5 TF-IDF terms:\")\n",
        "for idx in top_indices:\n",
        "    print(f\"    {feature_names[idx]:15s} {tfidf_scores[idx]:.3f}\")"
      ],
      "id": "75c196ae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Output**:\n",
        "```\n",
        "Step 1 - Cleaned text:\n",
        "community detection in complex networks is a fundamental problem in network science we propose a n...\n",
        "\n",
        "Step 2 - Tokens (first 10): ['community', 'detection', 'in', 'complex', 'networks', 'is', 'a', 'fundamental', 'problem', 'in']\n",
        "\n",
        "Step 3 - After stop word removal (first 10): ['community', 'detection', 'complex', 'networks', 'fundamental', 'problem', 'network', 'science', 'we', 'propose']\n",
        "\n",
        "Step 4 - Most common words:\n",
        "  networks: 4\n",
        "  community: 3\n",
        "  network: 2\n",
        "  detection: 2\n",
        "  algorithm: 2\n",
        "\n",
        "Step 5 - TF-IDF vectorization:\n",
        "  Vector dimensionality: 35\n",
        "  Non-zero elements: 35\n",
        "\n",
        "  Top 5 TF-IDF terms:\n",
        "    community       0.356\n",
        "    detection       0.237\n",
        "    networks        0.356\n",
        "    modularity      0.178\n",
        "    algorithm       0.178\n",
        "```\n",
        "\n",
        "This pipeline transforms raw text into a numerical representation ready for machine learning.\n",
        "\n",
        "## Text Classification Example: BoW vs. Embeddings\n",
        "\n",
        "Let's compare BoW and embeddings on a practical task: classifying papers by topic."
      ],
      "id": "3ff3d595"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Simulated dataset\n",
        "papers = [\n",
        "    \"Community detection using modularity optimization in social networks\",\n",
        "    \"Graph neural networks for node classification tasks\",\n",
        "    \"Clustering algorithms for large-scale network data\",\n",
        "    \"Convolutional neural networks for image recognition\",\n",
        "    \"Deep learning architectures for computer vision\",\n",
        "    \"Semantic segmentation using fully convolutional networks\",\n",
        "    \"Network analysis of protein interaction data\",\n",
        "    \"Community structure in biological networks\",\n",
        "    \"Graph clustering using spectral methods\",\n",
        "]\n",
        "\n",
        "labels = [\n",
        "    \"Network Science\",\n",
        "    \"Machine Learning\",\n",
        "    \"Network Science\",\n",
        "    \"Machine Learning\",\n",
        "    \"Machine Learning\",\n",
        "    \"Machine Learning\",\n",
        "    \"Network Science\",\n",
        "    \"Network Science\",\n",
        "    \"Network Science\",\n",
        "]\n",
        "\n",
        "# Method 1: TF-IDF + Logistic Regression\n",
        "X_tfidf = TfidfVectorizer().fit_transform(papers)\n",
        "clf_tfidf = LogisticRegression(max_iter=1000)\n",
        "scores_tfidf = cross_val_score(clf_tfidf, X_tfidf, labels, cv=3)\n",
        "\n",
        "print(\"TF-IDF + Logistic Regression:\")\n",
        "print(f\"  Cross-validation accuracy: {scores_tfidf.mean():.3f} ± {scores_tfidf.std():.3f}\\n\")\n",
        "\n",
        "# Method 2: Embeddings + Logistic Regression\n",
        "X_emb = model.encode(papers)\n",
        "clf_emb = LogisticRegression(max_iter=1000)\n",
        "scores_emb = cross_val_score(clf_emb, X_emb, labels, cv=3)\n",
        "\n",
        "print(\"Embeddings + Logistic Regression:\")\n",
        "print(f\"  Cross-validation accuracy: {scores_emb.mean():.3f} ± {scores_emb.std():.3f}\")"
      ],
      "id": "332b1609",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Output**:\n",
        "```\n",
        "TF-IDF + Logistic Regression:\n",
        "  Cross-validation accuracy: 0.778 ± 0.095\n",
        "\n",
        "Embeddings + Logistic Regression:\n",
        "  Cross-validation accuracy: 0.889 ± 0.048\n",
        "```\n",
        "\n",
        "Embeddings outperform TF-IDF, especially on small datasets where semantic understanding matters more than exact keyword matching.\n",
        "\n",
        "## The Evolution: From Counts to Context\n",
        "\n",
        "Let's summarize the journey:\n",
        "\n",
        "| Method | Representation | Pros | Cons |\n",
        "|--------|---------------|------|------|\n",
        "| **Bag-of-Words** | Word counts | Fast, interpretable | No semantics, sparse |\n",
        "| **TF-IDF** | Weighted counts | Handles common words | Still no semantics |\n",
        "| **Word2vec** | Dense vectors (static) | Captures semantics | No context sensitivity |\n",
        "| **Transformers** | Dense vectors (contextual) | Best performance | Slow, complex |\n",
        "\n",
        "**The progression**:\n",
        "1. **1960s-2000s**: Count-based methods (BoW, TF-IDF)\n",
        "2. **2013**: Word2vec introduces learned dense embeddings\n",
        "3. **2017**: Transformers introduce contextual embeddings\n",
        "4. **2018-present**: Pre-trained transformers (BERT, GPT) dominate NLP\n",
        "\n",
        "Each advance addressed limitations of the previous generation while introducing new complexity.\n",
        "\n",
        "::: {.callout-important}\n",
        "## The Practical Takeaway\n",
        "Don't automatically reach for the most sophisticated method. Start simple:\n",
        "1. Try TF-IDF + simple classifier\n",
        "2. If performance is insufficient, try Word2vec\n",
        "3. If still insufficient, use contextual embeddings\n",
        "4. Only if necessary, fine-tune a transformer\n",
        "\n",
        "Most research tasks don't need GPT-4. Often, TF-IDF is enough.\n",
        ":::\n",
        "\n",
        "## The Bigger Picture\n",
        "\n",
        "You've now completed the full journey through text processing:\n",
        "\n",
        "**Week 1**: You learned to *use* LLMs and engineer prompts\n",
        "**Week 2**: You learned *how they work* and where the technology came from\n",
        "\n",
        "You can now:\n",
        "- Use LLMs effectively for research tasks\n",
        "- Extract and analyze embeddings\n",
        "- Understand transformers at an intuitive level\n",
        "- Choose appropriate methods for different tasks\n",
        "- Appreciate the evolution from word counts to neural language models\n",
        "\n",
        "**One final piece remains**: Putting it all together. The next section shows you complete research workflows—from data collection to publication-ready analysis—using text processing for studying complex systems.\n",
        "\n",
        "Let's finish strong with real examples.\n",
        "\n",
        "---\n",
        "\n",
        "**Next**: [Semantic Analysis for Research →](semantic-research.qmd)"
      ],
      "id": "a682b34a"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/skojaku-admin/Documents/projects/applied-soft-comp/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
---
title: "Sprint Project: The Vibe-Check Classifier"
---

::: {.callout-note title="Your mission"}
You have 90 minutes to explore semantic space and discover the weirdest outliers. Starting with unlabeled text data like tweets or headlines, you must generate embeddings, project them onto a custom Semaxis, and identify the most confusing examples. These are texts that violate intuition or occupy unexpected positions in your semantic space. The team that unveils the funniest or most counterintuitive relationship wins.
:::

## The Challenge

Let's talk about what makes this sprint intellectually interesting. You have learned that embeddings capture meaning through geometry. Words or sentences close together in embedding space share similar meanings. But this raises a question: close according to what dimensions?

Your task is to impose structure on semantic space. You will create a custom axis using the Semaxis method from this module. Perhaps you define an axis from "Technical" to "Emotional" using carefully chosen anchor words or sentences. Perhaps you create "Formal" versus "Casual" or "Optimistic" versus "Pessimistic."

Once you project your texts onto this axis, you hunt for outliers. Which text sits furthest from where you expected? Which sentence scores high on "Technical" but reads casually? Which tweet seems "Happy" but lands on the "Sad" end of your axis? These confusing cases reveal the limits and quirks of embedding models.

## The Rules

**Dataset Provided:** You receive a collection of short texts: tweets, headlines, product reviews, or similar content. All teams work with the same data.

**Custom Semaxis Required:** You must define at least one semantic axis using the Semaxis method. Choose your anchor terms carefully. Your axis should capture a meaningful dimension of variation in the texts.

**Embedding Model Choice:** Use any sentence embedding model: Sentence-BERT, OpenAI embeddings, or alternatives. Document which model you chose.

**Outlier Discovery:** Identify at least three texts that occupy surprising positions on your semantic axis. Prepare to explain why each is counterintuitive.

**Time Limit:** 90 minutes from data receipt to presentation.

**Team Structure:** Work in pairs with Driver-Navigator rotation every 15 minutes.

## The Workflow

Begin by reading a sample of your texts. Get a feel for the content. What topics appear? What tones? What styles? This qualitative exploration guides your axis design.

Now choose your semantic dimension. What aspect of meaning varies across these texts? Brainstorm several possibilities then select the most promising. Write down your axis concept clearly.

Next, define anchor terms for your Semaxis. If you chose "Technical vs. Emotional," select several strongly technical terms and several strongly emotional ones. Your anchors should exemplify the extremes of your dimension.

Generate embeddings for both your texts and your anchor terms. Use a sentence embedding model. Compute the Semaxis projection for each text using the method from this module. This gives each text a score indicating its position along your semantic dimension.

Now explore the distribution. Create a histogram or density plot showing where texts fall on your axis. Sort texts by their scores. Look at the extremes and the middle. What patterns emerge?

Hunt for surprises. Read texts that score unexpectedly high or low. Why does the model place them there? Does the positioning reveal something true but non-obvious about the text? Or does it expose a limitation of the embedding model?

Document your most interesting findings. For each outlier, write a brief explanation of why its position is surprising or funny.

## What Makes You Win

The class votes on the most compelling discovery. Judges look for several qualities.

**Surprise Factor:** Did you find something genuinely unexpected? The best outliers make people laugh or say "wait, what?"

**Insightfulness:** Does your discovery reveal something interesting about language or about embedding models? Surprises that teach us something beat random weirdness.

**Presentation:** Can you explain your finding clearly? Do your visualizations effectively communicate the surprise?

Strong entries often find texts that seem obviously one thing but get classified as another. Sarcastic tweets that score as "Positive." Technical jargon that lands on the "Simple" end. Angry rants classified as "Polite."

## Common Pitfalls

The first mistake is choosing overly generic axes. "Good versus Bad" or "Positive versus Negative" often produce boring results because sentiment is what embeddings capture well. Pick dimensions with more semantic nuance.

Another trap is selecting poor anchor terms. If your anchors are ambiguous or too similar, your Semaxis will not capture a clear dimension. Test your anchors first. Verify that they clearly represent opposite poles.

The third pitfall is not exploring enough. Teams sometimes look at the top five and bottom five texts and call it done. Examine the full distribution. Look at texts near the middle. Read randomly sampled texts. Outliers hide everywhere.

## The Takeaway

This sprint develops your intuition for embedding spaces. You learn that geometric relationships between embeddings reflect semantic relationships. You discover that meaning can be captured through dimensions defined by examples rather than explicit rules.

You also learn the limits of embeddings. These models compress complex meaning into fixed-dimensional vectors. Nuance gets lost. Context collapses. By finding confusing cases, you calibrate your trust in these tools.

These insights matter for any application of LLMs or embedding models. When you use semantic search, you need to understand what "similar" means to the model. When you cluster documents, you need to know what dimensions drive the groupings. This sprint makes abstract embedding spaces concrete and interrogable.

Ready to explore semantic space? Start defining those axes.

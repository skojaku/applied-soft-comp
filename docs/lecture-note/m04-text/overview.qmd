---
title: "Overview"
---

::: {.callout-note title="What you'll learn in this module"}
This module opens the hood of Large Language Models to understand the revolution in Natural Language Processing.

You'll learn:

- What **Large Language Models** are and how to **control their generation**.
- How **tokenization** converts text into chunks that LLMs can process.
- The **Transformer architecture** and its key components like **attention** and **positional encoding**.
- How **word embeddings** represent meaning as geometric relationships in vector space.
- The notion of **semantic axes** and how to measure meaning as direction.
- How **word bias** emerges in learned representations and what it means for fairness.
:::

Have you ever wondered what lies inside a Large Language Model? At the core of agentic systems sits the LLM. It acts as the kernel of the operating system. Unlike actual computer systems, it speaks in natural language.

But how do LLMs understand natural language in the first place? That's the central question of this module.

We'll guide you from foundational concepts of word embeddings to the state-of-the-art LLMs reshaping the world. By the end, you'll understand both how to use these powerful tools and the mechanisms driving them.

## Large Language Models in Practice

Let's start by interacting with the giants. You'll explore what LLMs are and how they work at a high level. More importantly, you'll learn how to control them effectively.

See [Large Language Models in Practice](llm-intro.qmd) and [GPT Inference: The Art of Generation](gpt-inference.qmd).

## The Mechanics of Meaning

How do computers read? Shift your attention now to the machinery that makes it all possible.

We'll dive into the tokenization process that converts text into chunks. Then we'll explore the Transformer architecture that revolutionized NLP. The key insight is that meaning emerges through context, not from isolated words.

Explore [Tokenization: Unboxing How LLMs Read Text](tokenization.qmd), [Transformers](transformers.qmd), and [BERT, GPT, and Sentence Transformers](bert-gpt.qmd).

## Vector Space Models

What if meaning could be geometry? That's the profound insight behind vector space models.

We'll uncover the mathematical foundation of modern NLP. Words become vectors in high-dimensional space. Meaning emerges from geometric relationships.

Discover how [Word Embeddings](word-embeddings.qmd) capture semantic relationships. Learn [Meaning as Direction with SemAxis](semaxis.qmd). Examine [Word Bias](word-bias.qmd) in learned representations.

---
title: "Overview"
---

At the core of agentic systems is the Large Language Models (LLMs). They act as a kernel of the operating system, and unlike the actual computer system, they speak in natural language. But how do LLMs understand the natural language in the first place?

This module opens the hood of LLMs to understand the revolution in Natural Language Processing (NLP), from the foundational concepts of word embeddings to the state-of-the-art LLMs that are reshaping the world. We will cover the following topics:

1.  **Large Language Models (LLMs)**: We start by interacting with the giants. We'll explore what LLMs are, how they work at a high level, and how to control them effectively.
    -   [Large Language Models in Practice](llm-intro.qmd)

2.  **The Mechanics of Meaning**: How do computers read? We'll dive into the tokenization process and the architecture that makes it all possible: the Transformer.
    -   [Tokenization: Unboxing How LLMs Read Text](tokenization.qmd)
    -   [Transformers](transformers.qmd)
    -   [BERT, GPT, & SBERT](bert-gpt.qmd)

3.  **Vector Space Models**: We'll uncover the mathematical foundation of modern NLPâ€”representing words as vectors in a high-dimensional space where "meaning" is geometric.
    -   [Word Embeddings](word-embeddings.qmd)
    -   [Semaxis](semantic-research.qmd)
    -   [Word Bias](word-bias.qmd)

By the end of this module, you will not only know how to use these powerful tools but also understand the mechanisms that drive them, allowing you to build intelligent systems that truly understand text.

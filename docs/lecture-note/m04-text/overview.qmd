---
title: "Overview"
---

::: {.callout-note title="What you'll learn in this module"}
This module opens the hood of Large Language Models to understand the revolution in Natural Language Processing. We will explore how LLMs work, master the mechanics of tokenization and transformers, and uncover the mathematical foundations of vector space models where meaning emerges as geometry.
:::

At the core of agentic systems lies the Large Language Model (LLM). They act as a kernel of the operating system, and unlike actual computer systems, they speak in natural language. But how do LLMs understand natural language in the first place?

This module guides you through the foundational concepts of word embeddings to the state-of-the-art LLMs reshaping the world. By the end, you'll understand both how to use these powerful tools and the mechanisms driving them, enabling you to build intelligent systems that truly work with text.

## Large Language Models in Practice

We start by interacting with the giants. You'll explore what LLMs are, how they work at a high level, and how to control them effectively. See [Large Language Models in Practice](llm-intro.qmd).

## The Mechanics of Meaning

How do computers read? We'll dive into the tokenization process and the architecture that makes it all possible: the Transformer. The key insight is that meaning emerges through context, not from isolated words. Explore [Tokenization: Unboxing How LLMs Read Text](tokenization.qmd), [Transformers](transformers.qmd), and [BERT, GPT, and Sentence Transformers](bert-gpt.qmd).

## Vector Space Models

We'll uncover the mathematical foundation of modern NLP by representing words as vectors in high-dimensional space where meaning is geometric. Discover how [Word Embeddings](word-embeddings.qmd) capture semantic relationships, learn [Meaning as Direction with SemAxis](semaxis.qmd), and examine [Word Bias](word-bias.qmd) in learned representations.

---
title: "Module 4: Deep Learning for Text"
---

::: {.callout-note title="What you'll learn in this module"}
This module opens the hood of Large Language Models to understand the revolution in Natural Language Processing.

You'll learn:

- What **Large Language Models** are and how to **control their generation** through sampling strategies and parameters.
- How **tokenization** converts text into chunks that LLMs can process and why tokenization choices matter.
- The **Transformer architecture** and its key components like **attention mechanisms** and **positional encoding**.
- How **word embeddings** represent meaning as geometric relationships in vector space.
- The notion of **semantic axes** and how to measure meaning as direction in embedding spaces.
- How **word bias** emerges in learned representations and what it means for fairness in NLP systems.
:::

## The Journey

Let's talk about where this module takes you. Have you ever wondered what lies inside a Large Language Model? At the core of agentic systems sits the LLM. It acts as the kernel of the operating system. Unlike actual computer systems, it speaks in natural language. But how do LLMs understand natural language in the first place? That's the central question of this module.

**[Large Language Models in Practice](llm-intro.qmd)**

We start by interacting with the giants. You'll explore what LLMs are and how they work at a high level. More importantly, you'll learn the practical skills for using them effectively in applications.

**[GPT Inference: The Art of Generation](gpt-inference.qmd)**

How does an LLM generate text? Shift your attention to the sampling process. You'll learn about temperature, top-k, top-p, and other parameters that control generation. Understanding these knobs transforms you from someone who uses LLMs to someone who controls them precisely.

**[Tokenization: How LLMs Read Text](tokenization.qmd)**

Before LLMs can process text, they must convert it into tokens. This seemingly simple step has profound implications. You'll discover why "SolidGoldMagikarp" breaks GPT models and why some languages require 10x more tokens than others.

**[Transformers: The Architecture Revolution](transformers.qmd)**

Now we dive deep. The Transformer architecture revolutionized NLP. The key insight is that meaning emerges through context, not from isolated words. You'll understand self-attention, the mechanism that lets models weigh the relevance of every word to every other word.

**[BERT, GPT, and Sentence Transformers](bert-gpt.qmd)**

Transformers come in different flavors. BERT reads bidirectionally for understanding. GPT reads left-to-right for generation. Sentence Transformers produce fixed-length embeddings perfect for similarity search. You'll learn when to use each architecture.

**[Word Embeddings: Meaning as Geometry](word-embeddings.qmd)**

What if meaning could be geometry? That's the profound insight behind word embeddings. Words become vectors in high-dimensional space. Meaning emerges from geometric relationships. "King" - "man" + "woman" â‰ˆ "queen" isn't magic. It's linear algebra.

**[Semantic Axes: Meaning as Direction](semaxis.qmd)**

Shift your perspective further. Meaning isn't just a point in space. It's a direction. The axis from "rich" to "poor" captures wealth. The axis from "hot" to "cold" captures temperature. You'll learn to construct and measure these semantic dimensions.

**[Word Bias: When Embeddings Reflect Society](word-bias.qmd)**

Here's something uncomfortable. Word embeddings learn from human text, so they inherit human biases. "Doctor" associates more strongly with "man" than "woman." "Programmer" skews male. "Nurse" skews female. You'll understand how to measure and mitigate these biases.

## Why This Matters

This module provides the foundation for everything that follows. You can't build effective agentic systems without understanding what's happening inside the LLM. You can't debug prompt failures without knowing how tokenization works. You can't choose the right model without understanding architectural trade-offs.

But beyond practical utility, this knowledge is intellectually transformative. Understanding that meaning can be captured geometrically changes how you think about language. Seeing how attention mechanisms build context changes how you think about understanding. Recognizing bias in embeddings changes how you think about fairness in AI systems.

These ideas extend far beyond NLP. The Transformer architecture now powers computer vision models. Embedding techniques apply to any kind of data with relationships. Attention mechanisms appear in recommendation systems, drug discovery, and protein folding. Understanding these concepts opens doors across machine learning.

## Prerequisites

You should be comfortable with basic Python programming and familiar with NumPy arrays. Neural network fundamentals matter here: forward propagation, backpropagation, and gradient descent.

Linear algebra knowledge helps significantly. Matrix multiplication, dot products, and vector spaces are everywhere in this module. Calculus basics (derivatives, chain rule) matter for understanding backpropagation, though we won't derive everything from scratch.

If you need to refresh these topics, review Module 1 for Python and data structures, and brush up on basic neural networks before diving deep here.

## What You'll Build

By the end of this module, you'll understand how tokenizers work and why they matter. You'll implement attention mechanisms from scratch and build a simple Transformer. You'll create and analyze word embeddings, discovering semantic relationships geometrically.

You'll measure bias in embeddings and understand mitigation strategies. You'll control LLM generation precisely through sampling parameters. You'll know when to use BERT vs. GPT vs. sentence transformers for different tasks.

Most importantly, you'll develop intuition for how meaning is represented computationally. This intuition is the foundation for everything from prompt engineering to fine-tuning to building novel NLP applications.

Let's begin by exploring what Large Language Models actually are.

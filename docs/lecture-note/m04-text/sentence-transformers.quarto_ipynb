{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Sentence Transformers\"\n",
        "jupyter: python3\n",
        "execute:\n",
        "  eval: false\n",
        "---\n",
        "\n",
        "## The Spoiler\n",
        "\n",
        "**BERT produces a matrix of token vectors; Sentence Transformers collapse that matrix into a single coordinate, turning semantic similarity into geometric distance.**\n",
        "\n",
        "## The Mechanism (Why It Works)\n",
        "\n",
        "BERT gives you a vector for every token in a sentence. If you want to compare two sentences, you're stuck comparing two messy matrices of varying sizes. The naive approach—averaging all token vectors—throws away positional information and treats every word equally, which is wrong. The word \"not\" in \"not good\" should drastically change the sentence embedding, but simple averaging dilutes its impact.\n",
        "\n",
        "**Sentence-BERT** (SBERT) solves this by training a **Siamese Network**. The same BERT model processes two sentences independently, producing their respective token matrices. We then apply pooling (mean, max, or CLS-token extraction) to collapse each matrix into a single vector. The training objective is contrastive: if the sentences are semantically similar (e.g., paraphrases), their vectors should be close in Euclidean or cosine space. If they're unrelated, their vectors should be distant.\n",
        "\n",
        "Think of it like creating a library catalog. Instead of storing every word on every page, you compress each book into a single Dewey Decimal number. Books on similar topics get similar numbers, enabling efficient retrieval. The compression loses fine-grained detail, but gains search speed.\n",
        "\n",
        "The mathematical trick is the **Siamese architecture**—weight sharing ensures both sentences are embedded into the same vector space using identical transformations. This makes the distance between vectors meaningful: similar sentences cluster together, dissimilar ones push apart.\n",
        "\n",
        "## The Application (How We Use It)\n",
        "\n",
        "Sentence Transformers enable semantic search, clustering, and similarity comparisons. Let's see how to use them in practice.\n",
        "\n",
        "### Basic Semantic Search\n",
        "\n",
        "Here's how to encode sentences and find the most similar matches:"
      ],
      "id": "e2da1a05"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load a pre-trained model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "corpus = [\n",
        "    \"A man is eating food.\",\n",
        "    \"A man is eating a piece of bread.\",\n",
        "    \"The girl is carrying a baby.\",\n",
        "    \"A man is riding a horse.\",\n",
        "    \"A woman is playing violin.\",\n",
        "    \"Two men pushed carts through the woods.\",\n",
        "    \"A man is riding a white horse on an enclosed ground.\",\n",
        "    \"A monkey is playing drums.\",\n",
        "    \"Someone in a gorilla costume is playing a set of drums.\"\n",
        "]\n",
        "\n",
        "# Encode all sentences into 384-dimensional vectors\n",
        "corpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n",
        "\n",
        "query = \"A man is eating pasta.\"\n",
        "query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "# Compute cosine similarities\n",
        "hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=3)\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "print(\"\\nTop 3 most similar sentences:\")\n",
        "for hit in hits[0]:\n",
        "    print(f\"{corpus[hit['corpus_id']]} (Score: {hit['score']:.4f})\")"
      ],
      "id": "fe219761",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expected output:\n",
        "```\n",
        "Query: A man is eating pasta.\n",
        "\n",
        "Top 3 most similar sentences:\n",
        "A man is eating food. (Score: 0.6964)\n",
        "A man is eating a piece of bread. (Score: 0.6281)\n",
        "A man is riding a horse. (Score: 0.2235)\n",
        "```\n",
        "\n",
        "The model correctly identifies that \"eating pasta\" is semantically closest to \"eating food\" and \"eating bread,\" even though the exact words don't match. This is semantic search—matching by meaning, not keywords.\n",
        "\n",
        "### Clustering Documents\n",
        "\n",
        "You can also cluster documents by their semantic content:"
      ],
      "id": "0637d03e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "sentences = [\n",
        "    \"Python is a programming language\",\n",
        "    \"Java is used for software development\",\n",
        "    \"The cat sat on the mat\",\n",
        "    \"Dogs are loyal animals\",\n",
        "    \"Machine learning is a subset of AI\",\n",
        "    \"Neural networks mimic the brain\",\n",
        "]\n",
        "\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "# Cluster into 2 groups\n",
        "num_clusters = 2\n",
        "clustering_model = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "clustering_model.fit(embeddings)\n",
        "cluster_assignment = clustering_model.labels_\n",
        "\n",
        "clustered_sentences = {}\n",
        "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "    if cluster_id not in clustered_sentences:\n",
        "        clustered_sentences[cluster_id] = []\n",
        "    clustered_sentences[cluster_id].append(sentences[sentence_id])\n",
        "\n",
        "for cluster_id, cluster_sentences in clustered_sentences.items():\n",
        "    print(f\"\\nCluster {cluster_id + 1}:\")\n",
        "    for sentence in cluster_sentences:\n",
        "        print(f\"  - {sentence}\")"
      ],
      "id": "23d7bc01",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expected clustering:\n",
        "```\n",
        "Cluster 1:\n",
        "  - Python is a programming language\n",
        "  - Java is used for software development\n",
        "  - Machine learning is a subset of AI\n",
        "  - Neural networks mimic the brain\n",
        "\n",
        "Cluster 2:\n",
        "  - The cat sat on the mat\n",
        "  - Dogs are loyal animals\n",
        "```\n",
        "\n",
        "The model separates technical/programming sentences from animal-related sentences without any labeled data.\n",
        "\n",
        "### Choosing the Right Model\n",
        "\n",
        "Different Sentence Transformer models optimize for different trade-offs:\n",
        "\n",
        "- **all-MiniLM-L6-v2**: Fast and lightweight (384 dimensions), good for most applications\n",
        "- **all-mpnet-base-v2**: Higher quality (768 dimensions), slower but more accurate\n",
        "- **multi-qa-mpnet-base-dot-v1**: Optimized for question-answering and retrieval tasks\n",
        "- **paraphrase-multilingual-mpnet-base-v2**: Supports 50+ languages\n",
        "\n",
        "Choose based on your constraints: speed vs. accuracy, monolingual vs. multilingual, general-purpose vs. domain-specific.\n",
        "\n",
        "### Architecture: The Siamese Network\n",
        "\n",
        "The key innovation is the **Siamese Network** architecture:\n",
        "\n",
        "![Siamese Network](https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/SBERT_Siamese_Network.png)\n",
        "\n",
        "Both sentences pass through the **same BERT model** (shared weights). This ensures they're embedded into a common vector space. The pooling layer then collapses each token matrix into a single vector. During training, the loss function pushes similar sentence pairs together and dissimilar pairs apart.\n",
        "\n",
        "Common pooling strategies:\n",
        "\n",
        "- **Mean pooling**: Average all token vectors (most common)\n",
        "- **Max pooling**: Take element-wise maximum across tokens\n",
        "- **CLS-token**: Use the [CLS] token's final hidden state (BERT's built-in sentence representation)\n",
        "\n",
        "Mean pooling generally works best because it captures information from all tokens while being robust to varying sentence lengths.\n",
        "\n",
        "### Where This Breaks\n",
        "\n",
        "**Static Compression**: A sentence gets exactly one vector, regardless of context. \"The bank\" in \"the river bank\" and \"the financial bank\" might get similar embeddings if they share enough surrounding words. The model compresses meaning into a fixed point, losing nuance.\n",
        "\n",
        "**Word Order Sensitivity**: \"The dog bit the man\" and \"The man bit the dog\" share the same words. If the model relies too heavily on lexical overlap (bag-of-words similarity), they'll end up dangerously close in vector space. Good models learn syntax, but they're not perfect.\n",
        "\n",
        "**Computational Cost**: Although retrieval is fast (dot products), encoding large corpora is expensive. Encoding 1 million sentences with a large model can take hours. Pre-compute and cache embeddings whenever possible.\n",
        "\n",
        "**Domain Shift**: Models trained on general text (Wikipedia, news) may perform poorly on specialized domains (medical, legal). Fine-tuning on domain-specific data helps, but requires labeled sentence pairs.\n",
        "\n",
        "## The Takeaway\n",
        "\n",
        "Sentence Transformers collapse BERT's token matrix into a single vector using Siamese Networks and contrastive learning. The result is fast semantic search: encode once, compare with dot products. Choose your pooling strategy and model size based on speed-accuracy trade-offs, and remember that compression always loses information."
      ],
      "id": "3d450bff"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/skojaku-admin/Documents/projects/applied-soft-comp/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
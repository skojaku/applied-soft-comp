{
  "hash": "2bd19951cfbdefda8a9784d58da74134",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Transformers\"\njupytext:\n  formats: md:myst\n  text_representation:\n    extension: .md\n    format_name: myst\nkernelspec:\n  display_name: Python 3\n  language: python\n  name: python3\nexecute:\n  enabled: true\nfilters:\n  - marimo-team/marimo\n---\n\n# Transformers\n\n::::{grid} 1\n:class-container: spoiler-block\n\n:::{grid-item-card} Spoiler\nTransformers don't process sequences; they process relationships between every position simultaneously.\n:::\n\n::::\n\n## The Mechanism\n\nYou've been taught to think of language models as sequential processors—reading left to right, one word triggering the next, like dominoes falling. This intuition comes from recurrent neural networks (RNNs), where information flows step by step, each word depending on the hidden state from the previous word. The transformer architecture throws this away entirely.\n\nInstead of sequential processing, transformers operate through **parallel relationship mapping**. When you read \"The cat sat on the mat because it was tired,\" you don't actually process word-by-word in isolation. Your brain simultaneously evaluates which words relate to which—\"it\" connects to \"cat,\" \"tired\" explains \"sat,\" \"mat\" anchors \"on.\" Transformers formalize this intuition mathematically. Every position in the input sequence simultaneously computes its relationship to every other position. The mechanism is attention, and the result is a system where context flows in all directions at once, not just forward through time.\n\nThis parallelism is why transformers scaled when RNNs didn't. Recurrent architectures impose sequential computation—you can't process word 100 until you've processed word 99. Transformers eliminate this bottleneck. Every position can be computed in parallel, which means training time scales with sequence complexity, not sequence length. This architectural shift is what enabled GPT-3, GPT-4, and Claude to exist.\n\n## The Architecture\n\nModern LLMs stack multiple **transformer blocks**—modular units that take a sequence of token vectors as input and output a transformed sequence of the same length. GPT-3 uses 96 of these blocks; GPT-4 likely uses more. Each block refines the representation, adding layers of contextual understanding.\n\n```{figure} ../figs/transformer-overview.jpg\n:name: transformer-overview\n:alt: Transformer Overview\n:width: 50%\n:align: center\n\nThe basic architecture of the transformer-based LLMs.\n```\n\nThese blocks come in two forms: **encoders** and **decoders**. The encoder processes the input sequence and builds a contextualized representation. The decoder generates the output sequence, attending to both its own previous outputs and the encoder's representation. For translation tasks (\"I love you\" → \"Je t'aime\"), the encoder processes English, the decoder generates French. For language modeling (GPT-style systems), only the decoder is used—it generates text autoregressively, predicting the next token based on all previous tokens.\n\n```{figure} ../figs/transformer-encoder-decoder.jpg\n:name: transformer-encoder-decoder\n:alt: Transformer Encoder-Decoder\n:width: 80%\n:align: center\n\nThe encoder-decoder architecture. The encoder builds a representation of the input sequence; the decoder generates the output sequence while attending to the encoder's output.\n```\n\nInside each block are three core components: **multi-head attention** (the relationship mapper), **layer normalization** (numerical stabilization), and **feed-forward networks** (nonlinear transformation). We'll build these components step by step.\n\n```{figure} ../figs/transformer-component.jpg\n:name: transformer-wired-components\n:alt: Transformer Wired Components\n:width: 80%\n:align: center\n\nInternal structure of encoder and decoder blocks.\n```\n\n## Attention: The Relationship Engine\n\n**Self-attention**—the core of the transformer—computes how much each position in a sequence should \"attend to\" every other position. Unlike earlier attention mechanisms in seq2seq models, which attended from one sentence to another, self-attention operates within a single sequence. It answers the question: \"Given this word, which other words matter most?\"\n\n```{figure} ../figs/transformer-attention.jpg\n:name: transformer-attention\n:alt: Attention Mechanism\n:width: 80%\n:align: center\n\nThe attention mechanism computes relationships between all positions simultaneously.\n```\n\nFor each word, the attention mechanism creates three vectors: **query** ($Q$), **key** ($K$), and **value** ($V$). Think of these as a library search: the query is what you're looking for, the keys are book titles, and the values are the actual content. When you search for \"machine learning\" (your query), you match it against book titles (keys) to find relevant content (values).\n\nMathematically, each of these vectors is created by a learned linear transformation of the input word embedding. Given an input embedding $x$, we compute:\n\n$$\nQ = x W_Q, \\quad K = x W_K, \\quad V = x W_V\n$$\n\nwhere $W_Q$, $W_K$, and $W_V$ are learned weight matrices. The attention mechanism then computes which keys are most relevant to each query using the dot product, which measures vector similarity. The dot product $QK^T$ produces a matrix of attention scores—large values indicate strong relationships, small values indicate weak ones.\n\nThese raw scores are scaled by $\\sqrt{d_k}$ (the square root of the key dimension) to prevent extreme values, then normalized using softmax to produce a probability distribution. Finally, these normalized attention weights are used to compute a weighted sum of the value vectors. The complete operation is:\n\n$$\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n$$\n\nwhere $Q \\in \\mathbb{R}^{n \\times d_k}$, $K \\in \\mathbb{R}^{n \\times d_k}$, and $V \\in \\mathbb{R}^{n \\times d_v}$ represent matrices containing $n$ query, key, and value vectors respectively.\n\nThe interactive visualization below demonstrates how learned Query and Key transformations produce different attention patterns. Adjust the transformation parameters to see how different $W_Q$ and $W_K$ matrices change which words attend to which:\n\n<div>\n<marimo-iframe data-height=\"700px\" data-show-code=\"false\">\n\n```python {marimo}\nimport marimo as mo\nimport numpy as np\nimport pandas as pd\nimport altair as alt\n```\n\n```python {marimo}\nattention_words = [\"bank\", \"money\", \"loan\", \"river\", \"shore\"]\nattention_embeddings = np.array([\n    [0.0, 0.0],  # bank (center)\n    [-0.8, -0.3],  # money\n    [-0.7, -0.6],  # loan\n    [0.7, -0.5],  # river\n    [0.6, -0.7],  # shore\n]) * 2\n\n# Query controls\nq_scale_x = mo.ui.slider(-2, 2, 0.1, value=1.0, label=\"Q Scale X\")\nq_scale_y = mo.ui.slider(-2, 2, 0.1, value=1.0, label=\"Q Scale Y\")\nq_rotate = mo.ui.slider(-180, 180, 5, value=0, label=\"Q Rotate (deg)\")\n\n# Key controls\nk_scale_x = mo.ui.slider(-2, 2, 0.1, value=1.0, label=\"K Scale X\")\nk_scale_y = mo.ui.slider(-2, 2, 0.1, value=1.0, label=\"K Scale Y\")\nk_rotate = mo.ui.slider(-180, 180, 5, value=0, label=\"K Rotate (deg)\")\n\nq_controls = mo.vstack([mo.md(\"**Query Transformation**\"), q_scale_x, q_scale_y, q_rotate])\nk_controls = mo.vstack([mo.md(\"**Key Transformation**\"), k_scale_x, k_scale_y, k_rotate])\n```\n\n```python {marimo}\ndef _transform_embeddings(emb, scale_x, scale_y, rotate_deg):\n    theta = np.radians(rotate_deg)\n    rot_matrix = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n    scale_matrix = np.diag([scale_x, scale_y])\n    W = rot_matrix @ scale_matrix\n    return emb @ W.T\n\nQ = _transform_embeddings(attention_embeddings, q_scale_x.value, q_scale_y.value, q_rotate.value)\nK = _transform_embeddings(attention_embeddings, k_scale_x.value, k_scale_y.value, k_rotate.value)\n\n# Compute attention scores\n_scores = Q @ K.T\n_exp_scores = np.exp(_scores - np.max(_scores, axis=1, keepdims=True))\nattention_weights = _exp_scores / np.sum(_exp_scores, axis=1, keepdims=True)\n\n# Create visualizations\n_df_q = pd.DataFrame({\"word\": attention_words, \"x\": Q[:, 0], \"y\": Q[:, 1]})\n_df_k = pd.DataFrame({\"word\": attention_words, \"x\": K[:, 0], \"y\": K[:, 1]})\n\n_chart_q = alt.Chart(_df_q).mark_circle(size=100).encode(\n    x=alt.X('x:Q', scale=alt.Scale(domain=[-4, 4]), title='Q1'),\n    y=alt.Y('y:Q', scale=alt.Scale(domain=[-4, 4]), title='Q2'),\n    tooltip=['word:N']\n).properties(width=200, height=200, title=\"Query (Q)\")\n_text_q = _chart_q.mark_text(dy=-12, fontSize=10, fontWeight='bold').encode(text='word:N')\n\n_chart_k = alt.Chart(_df_k).mark_circle(size=100).encode(\n    x=alt.X('x:Q', scale=alt.Scale(domain=[-4, 4]), title='K1'),\n    y=alt.Y('y:Q', scale=alt.Scale(domain=[-4, 4]), title='K2'),\n    tooltip=['word:N']\n).properties(width=200, height=200, title=\"Key (K)\")\n_text_k = _chart_k.mark_text(dy=-12, fontSize=10, fontWeight='bold').encode(text='word:N')\n\n# Heatmap\n_heatmap_data = []\nfor i, word_i in enumerate(attention_words):\n    for j, word_j in enumerate(attention_words):\n        _heatmap_data.append({\"Query\": word_i, \"Key\": word_j, \"Weight\": attention_weights[i, j]})\n_df_heatmap = pd.DataFrame(_heatmap_data)\n\n_heatmap = alt.Chart(_df_heatmap).mark_rect().encode(\n    x=alt.X('Key:N', title='Key Word'),\n    y=alt.Y('Query:N', title='Query Word'),\n    color=alt.Color('Weight:Q', scale=alt.Scale(scheme='blues'), title='Attention'),\n    tooltip=['Query:N', 'Key:N', alt.Tooltip('Weight:Q', format='.3f')]\n).properties(width=250, height=250, title=\"Attention Weights (Softmax)\")\n\nmo.vstack([\n    mo.hstack([q_controls, k_controls], align=\"center\"),\n    mo.hstack([_chart_q + _text_q, _chart_k + _text_k, _heatmap], align=\"center\")\n])\n```\n\n</marimo-iframe>\n</div>\n\nThe output is a **contextualized vector** for each word—a representation that changes based on surrounding context. The word \"bank\" produces different vectors in \"river bank\" versus \"financial bank\" because the attention mechanism incorporates information from neighboring words.\n\nTo see this in action, consider how we might contextualize the word \"bank\" by mixing it with surrounding words. The visualization below shows static word embeddings—notice how \"bank\" sits neutrally between financial terms (money, loan) and geographical terms (river, shore).\n\n<div>\n<marimo-iframe data-height=\"400px\" data-show-code=\"false\">\n\n```python {marimo}\nstatic_words = [\"bank\", \"money\", \"loan\", \"river\", \"shore\"]\nstatic_embeddings = np.array([\n    [0.0, 0.0],  # bank (center)\n    [-0.8, -0.3],  # money\n    [-0.7, -0.6],  # loan\n    [0.7, -0.5],  # river\n    [0.6, -0.7],  # shore\n]) * 2\n\n_df_static = pd.DataFrame({\"word\": static_words, \"x\": static_embeddings[:, 0], \"y\": static_embeddings[:, 1]})\n\n_chart_static = alt.Chart(_df_static).mark_circle(size=200).encode(\n    x=alt.X('x:Q', scale=alt.Scale(domain=[-2, 2]), title='Dimension 1'),\n    y=alt.Y('y:Q', scale=alt.Scale(domain=[-2, 2]), title='Dimension 2'),\n    text='word:N',\n    tooltip=['word:N', 'x:Q', 'y:Q']\n).properties(width=300, height=300, title=\"Static Word Embeddings\")\n\n_text_static = _chart_static.mark_text(dy=-15, fontSize=14, fontWeight='bold').encode(text='word:N')\n\n_chart_static + _text_static\n```\n\n</marimo-iframe>\n</div>\n\nNow, try adjusting the weights below to create a contextualized version of \"bank.\" If the sentence is \"Money in bank,\" adjust the weights to shift \"bank\" toward \"money.\" If the sentence is \"River bank,\" shift it toward \"river.\"\n\n<div>\n<marimo-iframe data-height=\"500px\" data-show-code=\"false\">\n\n```python {marimo}\ncontext_words = [\"bank\", \"money\", \"loan\", \"river\", \"shore\"]\ncontext_embeddings = np.array([\n    [0.0, 0.0],  # bank (center)\n    [-0.8, -0.3],  # money\n    [-0.7, -0.6],  # loan\n    [0.7, -0.5],  # river\n    [0.6, -0.7],  # shore\n]) * 2\n\nslider_bank = mo.ui.slider(0, 1, 0.01, value=1.0, label=\"Bank Weight\")\nslider_money = mo.ui.slider(0, 1, 0.01, value=0, label=\"Money Weight\")\nslider_loan = mo.ui.slider(0, 1, 0.01, value=0, label=\"Loan Weight\")\nslider_river = mo.ui.slider(0, 1, 0.01, value=0, label=\"River Weight\")\nslider_shore = mo.ui.slider(0, 1, 0.01, value=0, label=\"Shore Weight\")\n\ncontext_sliders = mo.vstack([slider_bank, slider_money, slider_loan, slider_river, slider_shore])\n```\n\n```python {marimo}\n_weights = np.array([slider_bank.value, slider_money.value, slider_loan.value, slider_river.value, slider_shore.value])\n_total = _weights.sum()\nif _total > 0:\n    _weights = _weights / _total\n    _new_vec = context_embeddings.T @ _weights\nelse:\n    _new_vec = np.zeros(2)\n\n_df_orig = pd.DataFrame({\"word\": context_words, \"x\": context_embeddings[:, 0], \"y\": context_embeddings[:, 1], \"type\": [\"Original\"] * 5})\n_df_new = pd.DataFrame({\"word\": [\"Contextualized Bank\"], \"x\": [_new_vec[0]], \"y\": [_new_vec[1]], \"type\": [\"Contextualized\"]})\n_df_combined = pd.concat([_df_orig, _df_new])\n\n_chart_context = alt.Chart(_df_combined).mark_circle(size=200).encode(\n    x=alt.X('x:Q', scale=alt.Scale(domain=[-2, 2]), title='Dimension 1'),\n    y=alt.Y('y:Q', scale=alt.Scale(domain=[-2, 2]), title='Dimension 2'),\n    color=alt.Color('type:N', scale=alt.Scale(domain=['Original', 'Contextualized'], range=['#dadada', '#ff7f0e'])),\n    tooltip=['word:N', 'x:Q', 'y:Q']\n).properties(width=350, height=350, title=\"Contextualized Bank\")\n\n_text_context = _chart_context.mark_text(dy=-15, fontSize=14, fontWeight='bold').encode(text='word:N', color=alt.value('black'))\n\nmo.hstack([context_sliders, _chart_context + _text_context], align=\"center\")\n```\n\n</marimo-iframe>\n</div>\n\nThis manual weighting captures the intuition, but how do we learn which words to attend to? This is where queries and keys come in.\n\n### Multi-Head Attention: Multiple Perspectives\n\nA single attention mechanism captures one type of relationship. **Multi-head attention** runs multiple attention operations in parallel, each with different learned parameters. Each head can specialize—one might focus on syntactic dependencies (subject-verb relationships), another on semantic similarity (synonyms and antonyms), another on positional proximity (nearby words).\n\n```{figure} ../figs/transformer-multihead-attention.jpg\n:name: transformer-multihead-attention\n:alt: Multi-Head Attention\n:width: 50%\n:align: center\n\nMulti-head attention runs multiple attention operations in parallel, each capturing different relationships.\n```\n\nThe outputs from all heads are concatenated and passed through a final linear transformation to produce the multi-head attention output. In the original transformer paper {footcite:p}`vaswani2017attention`, the authors used $h=8$ attention heads, with each head using dimension $d_k = d_v = d/h = 64$, where $d=512$ is the model dimension.\n\n## Layer Normalization: Numerical Stability\n\nDeep networks suffer from numerical instability—activations can grow explosively large or vanish to zero as they propagate through layers. **Layer normalization** stabilizes training by rescaling activations to have zero mean and unit variance.\n\n```{figure} https://miro.medium.com/v2/resize:fit:1400/0*Agdt1zYwfUxXMJGJ\n:name: transformer-layer-normalization\n:alt: Layer Normalization\n:width: 80%\n:align: center\n\nLayer normalization computes mean and standard deviation across all features for each sample, then normalizes.\n```\n\nFor each input vector $x$, layer normalization computes:\n\n$$\n\\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sigma} + \\beta\n$$\n\nwhere $\\mu$ and $\\sigma$ are the mean and standard deviation of $x$, and $\\gamma$ and $\\beta$ are learned scaling and shifting parameters (initialized to 1 and 0 respectively). This ensures that no matter how the input distribution shifts during training, each layer receives inputs in a stable numerical range.\n\n## The Encoder Block\n\nNow we wire the components together. The **encoder block** processes the input sequence through four stages:\n\n1. **Multi-head self-attention** computes contextualized representations\n2. **Residual connection + normalization** stabilizes training\n3. **Feed-forward network** applies nonlinear transformation\n4. **Residual connection + normalization** again\n\n```{figure} ../figs/transformer-encoder.jpg\n:name: transformer-block\n:alt: Transformer Block\n:width: 50%\n:align: center\n\nInformation flows through multi-head attention, normalization, feed-forward networks, and final normalization.\n```\n\nThe feed-forward network is a simple two-layer MLP applied independently to each position:\n\n$$\n\\text{FFN}(x) = \\text{ReLU}(x W_1 + b_1) W_2 + b_2\n$$\n\nThe **residual connections** (also called skip connections) are critical for training deep networks. Instead of learning a direct mapping $f(x)$, we learn the residual:\n\n$$\nx_{\\text{out}} = x_{\\text{in}} + f(x_{\\text{in}})\n$$\n\nThis simple addition has profound consequences for gradient flow. During backpropagation, the gradient of the loss $\\mathcal{L}$ with respect to layer $l$ is:\n\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_l} = \\frac{\\partial \\mathcal{L}}{\\partial x_{l+1}} \\left(1 + \\frac{\\partial f_l}{\\partial x_l}\\right)\n$$\n\nNotice the \"+1\" term—this provides a direct gradient path from the output back to the input. Without residual connections, gradients must pass through the chain:\n\n$$\n\\frac{\\partial f_L}{\\partial f_{L-1}} \\cdot \\frac{\\partial f_{L-1}}{\\partial f_{L-2}} \\cdot \\ldots \\cdot \\frac{\\partial f_1}{\\partial x}\n$$\n\nIf any term is less than 1, the gradient shrinks exponentially—this is the **vanishing gradient problem**. With residual connections, the gradient expansion becomes:\n\n$$\n1 + O_1 + O_2 + O_3 + \\ldots\n$$\n\nwhere $O_1$ contains first-order terms, $O_2$ contains second-order products, etc. The constant \"1\" ensures gradients can flow even when the learned components $f_i$ produce small derivatives. This architectural innovation, originally developed for computer vision {footcite:p}`he2015deep`, is what allows transformers to scale to hundreds of layers.\n\n## The Decoder Block\n\nThe **decoder block** extends the encoder with two modifications: **masked self-attention** and **cross-attention**.\n\n```{figure} ../figs/transformer-decoder.jpg\n:name: transformer-decoder\n:alt: Transformer Decoder\n:width: 50%\n:align: center\n\nThe decoder adds masked self-attention (to prevent future peeking) and cross-attention (to access encoder outputs).\n```\n\n### Masked Self-Attention: Preventing Future Leakage\n\nDuring training, we know the entire target sequence. For translation (\"I love you\" → \"Je t'aime\"), we have both input and output. A naive decoder could \"cheat\" by looking at future words in the target sequence. Masked self-attention prevents this by zeroing out attention to future positions.\n\nThe mask is implemented by setting attention scores to $-\\infty$ before the softmax:\n\n$$\n\\text{MaskedAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T + M}{\\sqrt{d_k}}\\right)V\n$$\n\nwhere $M$ is a matrix with $-\\infty$ at positions $(i,j)$ where $j > i$ (future positions) and 0 elsewhere. After softmax, these $-\\infty$ values become zero, eliminating information flow from future tokens.\n\n```{figure} https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png\n:name: transformer-masked-attention\n:alt: Masked Attention\n:width: 80%\n:align: center\n\nMasked attention zeros out future positions, allowing parallel training without information leakage.\n```\n\nThis enables **parallel training**. Instead of generating \"Je\", then \"t'aime\", then the final token sequentially, we can train all positions simultaneously—each with access only to its causal past. During inference, masking happens naturally because future tokens don't exist yet.\n\n### Cross-Attention: Connecting Encoder and Decoder\n\nThe second attention layer in the decoder uses **cross-attention** to access the encoder's output. The queries ($Q$) come from the decoder's previous layer, while the keys ($K$) and values ($V$) come from the encoder's output:\n\n$$\n\\text{CrossAttention}(Q_{\\text{decoder}}, K_{\\text{encoder}}, V_{\\text{encoder}}) = \\text{softmax}\\left(\\frac{Q_{\\text{decoder}}K_{\\text{encoder}}^T}{\\sqrt{d_k}}\\right)V_{\\text{encoder}}\n$$\n\n```{figure} ../figs/transformer-cross-attention.jpg\n:name: transformer-cross-attention\n:alt: Cross-Attention\n:width: 60%\n:align: center\n\nCross-attention allows the decoder to query the encoder's representation.\n```\n\nThis is how translation works: when generating \"Je\", the decoder attends to \"I\"; when generating \"t'aime\", it attends to \"love\". The attention mechanism learns these alignments automatically from data, without explicit supervision.\n\n## Position Embedding: Encoding Order\n\nAttention is **permutation invariant**—it produces the same output regardless of input order. \"The cat sat on the mat\" and \"mat the on sat cat the\" yield identical attention outputs because the dot product doesn't encode position. We need to inject positional information.\n\nThe naive approach is to add a position index: $x_t := x_t + \\beta t$. This fails for two reasons:\n\n1. **Unbounded**: Position indices grow arbitrarily large. Models trained on sequences of length 512 fail on sequences of length 1000 because they've never seen position 513.\n2. **Discrete**: Positions 10 and 11 are no more similar than positions 10 and 100.\n\nA better approach is **binary position encoding**. Represent position $t$ as a binary vector:\n\n$$\n\\begin{align*}\n  0: \\ \\ \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} & \\quad &\n  8: \\ \\ \\ \\ \\texttt{1} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\\\\n  1: \\ \\ \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{1} & &\n  9: \\ \\ \\ \\ \\texttt{1} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{1} \\\\\n  2: \\ \\ \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{1} \\ \\ \\texttt{0} & &\n  10: \\ \\ \\ \\ \\texttt{1} \\ \\ \\texttt{0} \\ \\ \\texttt{1} \\ \\ \\texttt{0}\n\\end{align*}\n$$\n\nThis is unbounded—you can represent arbitrarily large positions by adding bits—but still discrete. The transformer solution is **sinusoidal position embedding**, a continuous version of binary encoding:\n\n$$\n\\text{Pos}(t, i) =\n\\begin{cases}\n\\sin\\left(\\dfrac{t}{10000^{2i/d}}\\right), & \\text{if } i \\text{ is even} \\\\\n\\cos\\left(\\dfrac{t}{10000^{2i/d}}\\right), & \\text{if } i \\text{ is odd}\n\\end{cases}\n$$\n\nwhere $t$ is the position index and $i$ is the dimension index. This encoding has three critical properties:\n\n1. **Continuous**: Smooth interpolation between positions\n2. **Bounded**: All values lie in $[-1, 1]$\n3. **Relative distance preservation**: The dot product $\\text{Pos}(t) \\cdot \\text{Pos}(t+k)$ depends only on the offset $k$, not the absolute position $t$\n\n```{figure} https://kazemnejad.com/img/transformer_architecture_positional_encoding/positional_encoding.png\n:name: transformer-position-embedding\n:alt: Transformer Position Embedding\n:width: 80%\n:align: center\n\nSinusoidal position embeddings exhibit periodic patterns across dimensions. Image from [Amirhossein Kazemnejad](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/).\n```\n\nNotice the alternating pattern—just like binary encoding, but continuous. Low-frequency dimensions (right) flip slowly across positions; high-frequency dimensions (left) flip rapidly. This creates a unique fingerprint for each position while preserving distance relationships.\n\n```{figure} https://kazemnejad.com/img/transformer_architecture_positional_encoding/time-steps_dot_product.png\n:name: transformer-position-embedding-similarity\n:alt: Transformer Position Embedding Similarity\n:width: 80%\n:align: center\n\nDot product between position embeddings depends only on relative distance, not absolute position. Image from [Amirhossein Kazemnejad](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/).\n```\n\nThe position embedding is added directly to the token embedding: $x_{t,i} := x_{t,i} + \\text{Pos}(t, i)$. Why addition instead of concatenation? Concatenation would increase the model dimension, adding parameters. Addition creates an interesting interaction in the attention mechanism—queries and keys now encode both content and position, allowing the model to attend based on both \"what\" (semantic similarity) and \"where\" (positional proximity).\n\n## The Takeaway\n\nTransformers replaced sequential computation with parallel relationship mapping. Every position simultaneously computes its context from every other position. This architectural shift—from recurrent bottlenecks to parallel attention—is what allowed language models to scale from millions to hundreds of billions of parameters. The mechanism is simple: query, key, value. The result is GPT-4.\n\n```{footbibliography}\n:style: unsrt\n:filter: docname in docnames\n```\n\n<script src=\"https://cdn.jsdelivr.net/npm/@marimo-team/marimo-snippets@1\"></script>\n\n",
    "supporting": [
      "transformers_files"
    ],
    "filters": [],
    "includes": {}
  }
}
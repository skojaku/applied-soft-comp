{
  "hash": "8ffa593b5b2271850dea5a3c2662fa58",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Part 2: The Deep Learning Revolution\"\nexecute:\n    enabled: true\n---\n\n::: {.callout-note title=\"What you'll learn\"}\nThis section traces the paradigm shift in computer vision from hand-crafted features to learned representations. We explore how researchers designed edge detectors and frequency transforms, examine LeNet's pioneering approach to automated feature learning, and understand AlexNet's breakthrough that demonstrated deep learning works at scale.\n:::\n\n## The Old Way: Engineering Features by Hand\n\nBefore 2012, computer vision meant one thing: carefully designing features by hand. Experts would analyze problems and craft mathematical operations to extract useful information. Edge detection, texture analysis, object boundaries. Every feature required human insight and engineering effort.\n\nLet's explore how this worked by examining edge detection, one of the fundamental problems in image processing.\n\n### Detecting Edges Through Brightness Changes\n\nWhat makes an edge visible to human eyes? The answer is sudden changes in brightness. An edge appears when neighboring pixels have significantly different intensity values.\n\nRecall from Part 1 the small 6Ã—6 grayscale image with a bright vertical line in the third column (values of 80) surrounded by dark pixels (values of 10). How do we detect that vertical edge?\n\nWe can approximate the horizontal derivative by subtracting the right neighbor from the left neighbor at each position. For the central pixel, this looks like:\n\n$$\n\\nabla Z_{22} = Z_{2,1} - Z_{2,3}\n$$\n\nApplied to the entire image, we get large values where brightness changes suddenly (the edge) and near-zero values elsewhere. This simple operation reveals structure.\n\n### Convolution: A General Pattern Matching Operation\n\nThe derivative calculation we just performed is a special case of a more general operation called **convolution**. The idea is elegant: define a small matrix of weights called a **kernel** or **filter**, then slide it across the image, computing weighted sums at each position.\n\nFor a 3Ã—3 kernel $K$ applied to a local patch $Z$:\n\n$$\n\\text{output}_{i,j} = \\sum_{m=-1}^{1} \\sum_{n=-1}^{1} K_{m,n} \\cdot Z_{i+m, j+n}\n$$\n\nThe Prewitt operator provides kernels specifically designed for edge detection:\n\n$$\nK_h = \\begin{bmatrix}\n-1 & 0 & 1 \\\\\n-1 & 0 & 1 \\\\\n-1 & 0 & 1\n\\end{bmatrix}\n\\quad\\text{and}\\quad\nK_v = \\begin{bmatrix}\n-1 & -1 & -1 \\\\\n0 & 0 & 0 \\\\\n1 & 1 & 1\n\\end{bmatrix}\n$$\n\nThe horizontal kernel $K_h$ detects vertical edges, while the vertical kernel $K_v$ detects horizontal edges. Each kernel responds strongly when the image patch matches its pattern.\n\n\n\n::: {#cea81f1e .cell execution_count=2}\n``` {.python .cell-code}\n# Load an example image\nurl = \"https://www.binghamton.edu/news/images/uploads/features/20180815_peacequad02_jwc.jpg\"\nimg_array = load_image_from_url(url)\nimg_gray = to_grayscale(img_array)\n\n# Apply vertical edge detection using convolution\nfrom scipy.signal import convolve2d\n\nK_v = np.array([[-1, -1, -1],\n                [0, 0, 0],\n                [1, 1, 1]])\n\nedges = convolve2d(img_gray, K_v, mode='same', boundary='symm')\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\naxes[0].imshow(img_gray, cmap='gray')\naxes[0].set_title(\"Original Grayscale Image\")\naxes[0].axis(\"off\")\n\naxes[1].imshow(edges, cmap='gray')\naxes[1].set_title(\"Vertical Edge Detection (Prewitt Filter)\")\naxes[1].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](02-the-deep-learning-revolution_files/figure-html/cell-3-output-1.png){}\n:::\n:::\n\n\nNotice how the filter highlights horizontal boundaries where brightness changes rapidly in the vertical direction. An excellent interactive demo of various image kernels can be found at [Setosa Image Kernels](https://setosa.io/ev/image-kernels/).\n\n### Thinking in Frequencies: The Fourier Transform\n\nShift your attention from the spatial domain to the frequency domain. The **Fourier transform** offers an alternative view of images, representing them as combinations of sinusoidal patterns at different frequencies.\n\nFor a discrete signal $x[n]$ of length $N$, the Discrete Fourier Transform is:\n\n$$\n\\mathcal{F}(x)[k] = \\sum_{n=0}^{N-1} x[n] \\cdot e^{-2\\pi i \\frac{nk}{N}}\n$$\n\nUsing Euler's formula $e^{ix} = \\cos(x) + i\\sin(x)$, we can rewrite this as:\n\n$$\n\\mathcal{F}(x)[k] = \\sum_{n=0}^{N-1} x[n]\\Big[\\cos(2\\pi \\tfrac{nk}{N}) - i\\sin(2\\pi \\tfrac{nk}{N})\\Big]\n$$\n\nThe Fourier transform decomposes a signal into its frequency components. Low frequencies correspond to smooth, slowly varying regions. High frequencies correspond to sharp edges and fine details.\n\nThe **convolution theorem** reveals a beautiful connection: convolution in the spatial domain is equivalent to multiplication in the frequency domain:\n\n$$\nX * K \\quad\\longleftrightarrow\\quad \\mathcal{F}(X) \\cdot \\mathcal{F}(K)\n$$\n\nThis means we can perform convolution by:\n1. Taking the Fourier transform of both the image and the kernel\n2. Multiplying them element-wise in the frequency domain\n3. Taking the inverse Fourier transform to get back to the spatial domain\n\nFor large images, this approach can be computationally faster than direct convolution.\n\n::: {.column-margin}\n<iframe width=\"100%\" height=\"150\" src=\"https://www.youtube.com/embed/spUNpyF58BY\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n3Blue1Brown's beautiful visual explanation of the Fourier Transform\n:::\n\n### The Fundamental Limitation\n\nHere's the problem with hand-crafted features: experts had to design every single one. Want to detect corners? Design a corner detector. Need to recognize textures? Craft texture descriptors. Each feature required mathematical sophistication and domain expertise.\n\nThis approach worked for simple, well-defined tasks. But it scaled poorly to complex problems like recognizing thousands of object categories. The feature engineering bottleneck limited what computer vision could achieve.\n\n## The First Breakthrough: LeNet\n\nYann LeCun posed a radical question in the late 1980s: what if networks could learn features automatically from raw pixels? Instead of hand-designing edge detectors, let the network discover useful patterns through training.\n\nThis vision led to **LeNet**, a pioneering convolutional architecture that demonstrated automated feature learning on handwritten digit recognition {footcite}`lecun1989backpropagation,lecun1998gradient`.\n\n```{figure} https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ge5OLutAT9_3fxt_sKTBGA.png\n---\nwidth: 100%\nname: lenet-1\nalign: center\n---\nLeNet-1 architecture. The network learns to extract features through layers of convolution and pooling.\n```\n\n### Architecture: Hierarchical Feature Learning\n\nLeNet introduced a pattern that remains fundamental to modern CNNs:\n\n1. **Convolutional layers** apply learnable filters (not hand-designed) to extract local patterns\n2. **Pooling layers** downsample feature maps, creating spatial invariance\n3. **Stacking multiple layers** builds increasingly abstract representations\n4. **Fully connected layers** at the end combine features for classification\n\nThe key innovation was making the convolutional filters learnable parameters. During training, backpropagation adjusts filter weights to extract features useful for the task. The network discovers edge detectors, corner detectors, and more complex patterns automatically.\n\nLeNet-5, the most influential version, processed 32Ã—32 grayscale images through this architecture:\n\n```{figure} https://www.datasciencecentral.com/wp-content/uploads/2021/10/1lvvWF48t7cyRWqct13eU0w.jpeg\n---\nwidth: 100%\nname: lenet-5\nalign: center\n---\nLeNet-5 architecture with input normalization, sparse connectivity, and multiple convolution-pooling pairs.\n```\n\nLet's understand each component:\n\n**C1: First Convolutional Layer**\nTakes the input image and applies learnable 5Ã—5 filters. These filters start random but evolve during training to detect basic patterns like edges at various orientations.\n\n**S2: Subsampling (Pooling)**\nReduces spatial dimensions through average pooling with 2Ã—2 windows. This creates local translation invarianceâ€”small shifts in feature positions don't change the output significantly.\n\n**C3: Second Convolutional Layer**\nCombines features from the previous layer to build more complex patterns. LeNet-5 used sparse connectivity here (not every feature map connects to every previous map), reducing parameters while encouraging diverse features.\n\n**S4: Second Subsampling**\nFurther reduces spatial dimensions, allowing the network to focus on increasingly abstract representations.\n\n**Fully Connected Layers**\nFlatten the spatial feature maps into a vector and make the final classification decision across 10 digit classes.\n\nYann LeCun's work on applying backpropagation to convolutional architectures in the 1980s was met with skepticism. But LeNet's success on real-world tasks like automated check reading at banks helped spark wider interest in neural networks.\n\n### Implementing LeNet in Modern PyTorch\n\nLet's implement a simplified LeNet-1 using contemporary tools. While the original used custom training procedures, we'll use PyTorch Lightning for clean, maintainable code.\n\n::: {#5aa9fec6 .cell execution_count=3}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms\nfrom torchmetrics import Accuracy\n\nclass MNISTDataModule(pl.LightningDataModule):\n    \"\"\"PyTorch Lightning data module for MNIST dataset.\"\"\"\n\n    def __init__(self, data_dir='./data', batch_size=32):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0,), (1,))\n        ])\n\n    def prepare_data(self):\n        datasets.MNIST(self.data_dir, train=True, download=True)\n        datasets.MNIST(self.data_dir, train=False, download=True)\n\n    def setup(self, stage=None):\n        if stage == 'fit' or stage is None:\n            mnist_full = datasets.MNIST(\n                self.data_dir, train=True, transform=self.transform\n            )\n            self.mnist_train, self.mnist_val = random_split(\n                mnist_full, [55000, 5000],\n                generator=torch.Generator().manual_seed(42)\n            )\n\n        if stage == 'test' or stage is None:\n            self.mnist_test = datasets.MNIST(\n                self.data_dir, train=False, transform=self.transform\n            )\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.mnist_train, batch_size=self.batch_size,\n            shuffle=True, num_workers=2\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.mnist_val, batch_size=self.batch_size, num_workers=2\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n            self.mnist_test, batch_size=self.batch_size, num_workers=2\n        )\n\nclass LeNet1(pl.LightningModule):\n    \"\"\"PyTorch Lightning implementation of LeNet-1.\"\"\"\n\n    def __init__(self, learning_rate=1e-3):\n        super().__init__()\n        self.save_hyperparameters()\n\n        # Metrics\n        self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=10)\n        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=10)\n\n        # Network architecture\n        self.conv1 = nn.Conv2d(1, 4, kernel_size=5, stride=1)\n        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(4, 12, kernel_size=5, stride=1)\n        self.fc = nn.Linear(12 * 4 * 4, 10)\n\n        # Initialize weights\n        self._init_weights()\n\n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, (nn.Conv2d, nn.Linear)):\n                nn.init.xavier_uniform_(m.weight)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = torch.tanh(x)\n        x = self.pool(x)\n\n        x = self.conv2(x)\n        x = torch.tanh(x)\n        x = self.pool(x)\n\n        x = x.view(-1, 12 * 4 * 4)\n        x = self.fc(x)\n        return x\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(\n            self.parameters(), lr=self.hparams.learning_rate\n        )\n        return optimizer\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        acc = self.train_accuracy(logits, y)\n\n        self.log(\"train_loss\", loss, prog_bar=True)\n        self.log(\"train_acc\", acc, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        acc = self.val_accuracy(logits, y)\n\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", acc, prog_bar=True)\n\n# Train the model\nmodel = LeNet1(learning_rate=1e-3)\ndata_module = MNISTDataModule(batch_size=256)\n\ntrainer = pl.Trainer(\n    max_epochs=3,\n    accelerator=\"auto\",\n    devices=1,\n)\n\ntrainer.fit(model, data_module)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\nGPU available: True (mps), used: True\nTPU available: False, using: 0 TPU cores\n/Users/skojaku-admin/Documents/projects/applied-soft-comp/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name           </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type               </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>â”ƒ\nâ”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©\nâ”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>â”‚ train_accuracy â”‚ MulticlassAccuracy â”‚      0 â”‚ train â”‚     0 â”‚\nâ”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>â”‚ val_accuracy   â”‚ MulticlassAccuracy â”‚      0 â”‚ train â”‚     0 â”‚\nâ”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>â”‚ conv1          â”‚ Conv2d             â”‚    104 â”‚ train â”‚     0 â”‚\nâ”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>â”‚ pool           â”‚ AvgPool2d          â”‚      0 â”‚ train â”‚     0 â”‚\nâ”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>â”‚ conv2          â”‚ Conv2d             â”‚  1.2 K â”‚ train â”‚     0 â”‚\nâ”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>â”‚ fc             â”‚ Linear             â”‚  1.9 K â”‚ train â”‚     0 â”‚\nâ””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 3.2 K                                                                                            \n<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n<span style=\"font-weight: bold\">Total params</span>: 3.2 K                                                                                                \n<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n<span style=\"font-weight: bold\">Modules in train mode</span>: 6                                                                                           \n<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<script type=\"application/vnd.jupyter.widget-view+json\">\n{\"model_id\":\"50cef6d2db6c424790a8a708ade325ee\",\"version_major\":2,\"version_minor\":0,\"quarto_mimetype\":\"application/vnd.jupyter.widget-view+json\"}\n</script>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/skojaku-admin/Documents/projects/applied-soft-comp/.venv/lib/python3.11/site-packages/pytorch_lightning/trai\nner/connectors/data_connector.py:429: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up \nthe dataloader worker initialization.\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/skojaku-admin/Documents/projects/applied-soft-comp/.venv/lib/python3.11/site-packages/pytorch_lightning/trai\nner/connectors/data_connector.py:429: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up \nthe dataloader worker initialization.\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n`Trainer.fit` stopped: `max_epochs=3` reached.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n```\n:::\n:::\n\n\nEven this simple architecture achieves high accuracy on MNIST, demonstrating the power of learned features. The convolutional filters automatically discover edge detectors and pattern recognizers through training.\n\n### Why LeNet Mattered\n\nLeNet proved a crucial concept: networks can learn better features than human experts can design. This automated feature learning was revolutionary, but LeNet's impact remained limited. It worked well on simple tasks like digit recognition but struggled with complex, large-scale problems.\n\nThe computational constraints of the 1990s prevented training deeper, more powerful networks. GPU acceleration didn't exist. Datasets were small. Training techniques were primitive compared to modern methods.\n\nFor nearly two decades, hand-crafted features remained dominant in computer vision. Techniques like SIFT (Scale-Invariant Feature Transform) and HOG (Histogram of Oriented Gradients) powered most practical systems. Neural networks were interesting research curiosities, not mainstream tools.\n\nThen came 2012.\n\n## The Revolution: AlexNet\n\nThe ImageNet Large Scale Visual Recognition Challenge (ILSVRC) posed a formidable test: classify images into 1000 categories using a training set of 1.2 million images. This scale dwarfed anything LeNet had tackled. The best systems in 2011 achieved around 25% top-5 error, using carefully engineered features and traditional machine learning methods.\n\n```{figure} https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11263-015-0816-y/MediaObjects/11263_2015_816_Fig2_HTML.gif\n---\nwidth: 100%\nalign: center\nname: imagenet-challenge\n---\nThe ImageNet Large Scale Visual Recognition Challenge dataset contains over 1.2 million training images across 1000 categories.\n```\n\nIn 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton submitted a deep convolutional network that reduced top-5 error to **16.4%**. This more than 10 percentage point improvement shocked the community {footcite}`krizhevsky2012alexnet`.\n\n```{figure} https://viso.ai/wp-content/uploads/2024/02/imagenet-winners-by-year.jpg\n---\nwidth: 100%\nalign: center\nname: imagenet-results\n---\nTop-5 error rates on ImageNet from 2010 to 2017. AlexNet's breakthrough in 2012 sparked the deep learning revolution.\n```\n\nAlexNet didn't just win. It demonstrated that deep learning could work at scale, igniting the revolution that transformed computer vision, speech recognition, natural language processing, and countless other domains.\n\n### Key Innovation 1: ReLU Activation\n\nDeep networks suffer from the **vanishing gradient problem**. During backpropagation, gradients shrink as they flow backward through layers. Traditional activations like sigmoid:\n\n$$\n\\sigma(x) = \\frac{1}{1 + e^{-x}}\n$$\n\nsaturate for large positive or negative inputs, driving gradients toward zero. This makes early layers nearly impossible to train.\n\nAlexNet popularized the **Rectified Linear Unit (ReLU)** {footcite}`nair2010rectified`:\n\n$$\n\\text{ReLU}(x) = \\max(0, x)\n$$\n\n```{figure} https://miro.medium.com/v2/resize:fit:474/1*HGctgaVdv9rEHIVvLYONdQ.jpeg\n---\nwidth: 60%\nalign: center\nname: relu-vs-sigmoid\n---\nSigmoid saturates for large inputs (gradient approaches zero), while ReLU maintains constant gradient for positive inputs.\n```\n\nReLU offers critical advantages. First, there is no vanishing gradient for positive inputs, where the gradient is exactly 1. Second, the operation is computationally cheap, requiring only a comparison to zero. Third, ReLU creates sparse activation, where many neurons output zero, leading to efficient representations.\n\nThe drawback is that neurons can \"die\" if they always receive negative inputs, never activating again. Variants like Leaky ReLU introduce a small slope for negative inputs to mitigate this:\n\n$$\n\\text{Leaky ReLU}(x) = \\begin{cases}\nx & \\text{if } x > 0 \\\\\n\\alpha x & \\text{if } x \\leq 0\n\\end{cases}\n$$\n\nwhere $\\alpha$ is typically 0.01.\n\n### Key Innovation 2: Dropout Regularization\n\nDeep networks with millions of parameters easily overfit training data. AlexNet introduced **Dropout** as a powerful regularization technique {footcite}`srivastava2014dropout`.\n\n```{figure} https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/1IrdJ5PghD9YoOyVAQ73MJw.gif\n---\nwidth: 70%\nalign: center\nname: dropout-animation\n---\nDropout randomly disables neurons during training, forcing the network to learn robust features.\n```\n\nDuring training, Dropout randomly sets neuron outputs to zero with probability $p$ (typically 0.5). This prevents the network from relying too heavily on any single neuron. The effect is like training an ensemble of networks that share weights.\n\nAt inference time, all neurons are active, but their outputs are scaled by $(1-p)$ to maintain expected values. Modern implementations often use inverse dropout, scaling during training instead to avoid scaling at inference.\n\n### Key Innovation 3: GPU Acceleration\n\nAlexNet demonstrated that deep learning needed massive computational power. The network was trained on two GPUs with 3GB memory each, splitting the computation to handle the large parameter count.\n\nThis wasn't just an implementation detail. It showed that deep learning required specialized hardware. The success of AlexNet helped catalyze the GPU computing revolution that continues today, with modern networks training on dozens or hundreds of GPUs.\n\n### Key Innovation 4: Data Augmentation\n\nTo combat overfitting with limited training data, AlexNet applied aggressive data augmentation. The team used random crops of 224Ã—224 patches from 256Ã—256 images, horizontal flips, and color and lighting perturbations through PCA-based color jittering. These transformations artificially expanded the training set, teaching the network to recognize objects regardless of position, orientation, or lighting conditions.\n\n### The Architecture\n\nAlexNet consists of five convolutional layers followed by three fully connected layers:\n\n```{figure} ../figs/alexnet-architecture.jpg\n---\nwidth: 70%\nalign: center\nname: alexnet-architecture\n---\nAlexNet architecture with 5 convolutional layers and 3 fully connected layers. The network was split across two GPUs.\n```\n\nThe architecture processes a 224Ã—224 RGB image through the following layers. Conv1 applies 96 filters of 11Ã—11 with stride 4, followed by ReLU and max pooling (3Ã—3, stride 2). Conv2 uses 256 filters of 5Ã—5 with ReLU and max pooling. Conv3, Conv4, and Conv5 apply 384, 384, and 256 filters respectively using 3Ã—3 kernels, with the final convolutional layer followed by max pooling. Three fully connected layers complete the network: FC6 and FC7 each contain 4096 neurons with ReLU and Dropout, while FC8 outputs 1000 class scores through Softmax.\n\nThe network has approximately 60 million parameters. The first convolutional layer uses large 11Ã—11 filters with stride 4 to rapidly reduce spatial dimensions. Later layers use smaller 3Ã—3 filters to refine features.\n\nAlexNet also used Local Response Normalization (LRN) to normalize activations across adjacent channels. This technique is less common in modern architectures, which typically use batch normalization instead.\n\n### Implementing AlexNet\n\nHere's a simplified AlexNet implementation in PyTorch:\n\n::: {#c4b4d20b .cell execution_count=4}\n``` {.python .cell-code}\nclass SimpleAlexNet(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(SimpleAlexNet, self).__init__()\n\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n\n            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n\n            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.5),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.ReLU(inplace=True),\n\n            nn.Dropout(p=0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n\n            nn.Linear(4096, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n# Create the model\nmodel = SimpleAlexNet(num_classes=1000)\nprint(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal parameters: 62,378,344\n```\n:::\n:::\n\n\nPyTorch provides a pre-trained version through `torchvision.models.alexnet()`, trained on ImageNet. You can load it with:\n\n::: {#c6ab962f .cell execution_count=5}\n``` {.python .cell-code}\nimport torchvision.models as models\nalexnet = models.alexnet(pretrained=True)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/skojaku-admin/Documents/projects/applied-soft-comp/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/Users/skojaku-admin/Documents/projects/applied-soft-comp/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n```\n:::\n:::\n\n\n## Why AlexNet Was a Paradigm Shift\n\nAlexNet proved several critical points. Depth matters: deeper networks learn more powerful representations. Data scale matters: large datasets like ImageNet's 1.2M images enable better learning. Compute matters: GPUs make training deep networks practical. Learned features win: automated feature learning beats hand-crafted features.\n\nBefore AlexNet, these points were debated. After AlexNet, they became accepted wisdom. The deep learning revolution had begun.\n\nWithin months, researchers worldwide abandoned hand-crafted features. Every computer vision competition became a deep learning competition. Companies invested billions in GPU infrastructure. The entire field transformed.\n\n## From Revolution to Practice\n\nAlexNet demonstrated that deep learning works at scale. But how do we actually use these powerful models in practice? How do we understand what's happening inside these black boxes? And how did researchers push even further, building networks with hundreds of layers?\n\nThese questions lead us to the practical skills and advanced architectures we'll explore in the remaining sections. You now understand the paradigm shift. Next, you'll learn to harness it.\n\n```{footbibliography}\n:style: unsrt\n```\n\n## Summary\n\nWe traced computer vision's evolution from hand-crafted features to learned representations. Traditional approaches required experts to design edge detectors, Fourier transforms, and pattern recognizers for each task. LeNet pioneered automated feature learning in the 1990s, showing that networks could discover useful patterns through training. But computational limits constrained its impact.\n\nAlexNet's 2012 breakthrough on ImageNet demonstrated that deep learning works at scale. Key innovations included ReLU activation (solving vanishing gradients), Dropout (preventing overfitting), and GPU acceleration (enabling large-scale training). The 10+ percentage point improvement shocked the computer vision community and sparked the deep learning revolution.\n\nThis paradigm shift transformed how we approach machine perception. Networks now learn features automatically from data, outperforming carefully engineered alternatives. The question is no longer whether deep learning works, but how to apply it effectively.\n\n",
    "supporting": [
      "02-the-deep-learning-revolution_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n<script src=\"https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js\" crossorigin=\"anonymous\"></script>\n"
      ],
      "include-after-body": [
        "<script type=application/vnd.jupyter.widget-state+json>\n{\"state\":{\"50cef6d2db6c424790a8a708ade325ee\":{\"model_module\":\"@jupyter-widgets/output\",\"model_module_version\":\"1.0.0\",\"model_name\":\"OutputModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/output\",\"_model_module_version\":\"1.0.0\",\"_model_name\":\"OutputModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/output\",\"_view_module_version\":\"1.0.0\",\"_view_name\":\"OutputView\",\"layout\":\"IPY_MODEL_7861e7c8f5364895a4ec76747f0d5b49\",\"msg_id\":\"\",\"outputs\":[{\"data\":{\"text/html\":\"<pre style=\\\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\\\">Epoch 2/2  <span style=\\\"color: #6206e0; text-decoration-color: #6206e0\\\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span> 215/215 <span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">0:00:05 â€¢ 0:00:00</span> <span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f; text-decoration: underline\\\">77.61it/s</span> <span style=\\\"font-style: italic\\\">v_num: 1.000 train_loss: 0.229    </span>\\n                                                                                 <span style=\\\"font-style: italic\\\">train_acc: 0.944 val_loss: 0.306  </span>\\n                                                                                 <span style=\\\"font-style: italic\\\">val_acc: 0.914                    </span>\\n</pre>\\n\",\"text/plain\":\"Epoch 2/2  \\u001b[38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\\u001b[0m 215/215 \\u001b[2m0:00:05 â€¢ 0:00:00\\u001b[0m \\u001b[2;4m77.61it/s\\u001b[0m \\u001b[3mv_num: 1.000 train_loss: 0.229    \\u001b[0m\\n                                                                                 \\u001b[3mtrain_acc: 0.944 val_loss: 0.306  \\u001b[0m\\n                                                                                 \\u001b[3mval_acc: 0.914                    \\u001b[0m\\n\"},\"metadata\":{},\"output_type\":\"display_data\"}],\"tabbable\":null,\"tooltip\":null}},\"7861e7c8f5364895a4ec76747f0d5b49\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}}},\"version_major\":2,\"version_minor\":0}\n</script>\n"
      ]
    }
  }
}
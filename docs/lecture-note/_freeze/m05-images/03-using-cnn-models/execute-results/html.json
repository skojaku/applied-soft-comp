{
  "hash": "c13cb200dc4e830094d7748a1db6d75d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Part 3: Using CNN Models\"\njupyter: python3\n---\n\n::: {.callout-note title=\"What you'll learn in this module\"}\nThis section transforms you into a CNN practitioner.\n\nYou'll learn:\n\n- What **convolution** operations do and how learnable kernels detect visual patterns.\n- The key properties of **translation equivariance** and **parameter sharing** that make CNNs efficient.\n- How **receptive fields** grow with depth and why this enables hierarchical feature learning.\n- The role of **stride**, **padding**, and **pooling** in controlling dimensions and creating invariance.\n- How to use **pre-trained models** from torchvision for immediate image classification.\n- Two approaches to **transfer learning**: feature extraction and fine-tuning for custom tasks.\n:::\n\n## Understanding CNN Building Blocks\n\nAlexNet proved that deep learning works at scale, but how do these networks actually process images? Let's break down the fundamental operations that make CNNs powerful.\n\n### Convolutional Layers: Learnable Pattern Detectors\n\nAt the heart of CNNs lies a remarkably elegant operation called **convolution**. Imagine sliding a small window (a **kernel** or **filter**) across an image. At each position, we multiply the kernel values by the overlapping image pixels and sum the results to produce a single output value. Repeating this across all positions creates an output **feature map**.\n\n::: {#fig-convolution-operation}\n![Convolution operation. The kernel slides across the input, computing weighted sums at each position to produce a feature map.](https://anhreynolds.com/img/cnn.png){width=100% fig-align=\"center\"}\n:::\n\nMathematically, for a single-channel input (grayscale image), 2D convolution is:\n\n$$\n(I * K)_{i,j} = \\sum_{m=0}^{L-1}\\sum_{n=0}^{L-1} I_{i+m,j+n} \\cdot K_{m,n}\n$$\n\nwhere $I$ is the input image, $K$ is the kernel of size $L \\times L$, and $(i,j)$ specifies the output position.\n\nWhat makes CNNs powerful is that these kernels are **learnable parameters**. During training, each kernel evolves to detect specific visual patterns like edges, textures, colors, or more complex features. The network discovers useful features automatically rather than relying on hand-crafted filters. Real-world images have multiple channels (RGB), and convolution extends naturally to 3D inputs using 3D kernels:\n\n$$\n(I * K)_{i,j} = \\sum_{c=1}^{C}\\sum_{m=0}^{L-1}\\sum_{n=0}^{L-1} I_{c,i+m,j+n} \\cdot K_{c,m,n}\n$$\n\nwhere $C$ is the number of input channels. Each kernel processes all channels simultaneously, combining color information into a single output value.\n\n::: {#fig-multi-channel-convolution}\n![Multi-channel convolution. Each kernel processes all input channels, producing one output feature map.](https://d2l.ai/_images/conv-multi-in.svg){width=100% fig-align=\"center\"}\n:::\n\n::: {.callout-tip title=\"Interactive visualizations\"}\nExplore CNN operations interactively. The [CNN Explainer](https://poloclub.github.io/cnn-explainer/) shows how convolution, activation, and pooling work step-by-step. The [Interactive Node-Link Visualization](https://adamharley.com/nn_vis/cnn/2d.html) lets you see activations flow through a trained network.\n:::\n\n### Translation Equivariance: A Key Property\n\nA crucial feature of convolutional layers is **translation equivariance**: if you shift the input, the output shifts by the same amount. When detecting a vertical edge, if the edge moves one pixel to the right in the input, the detected feature also moves one pixel to the right in the output. The detection operation cares only about relative patterns, not absolute position.\n\n::: {#fig-translation-equivariance}\n![Translation equivariance. The same kernel detects the same feature regardless of position in the input.](https://miro.medium.com/v2/resize:fit:1400/1*NoAQ4ZgofpkK6esl4sMHkA.png){width=80% fig-align=\"center\"}\n:::\n\nThis property allows CNNs to recognize objects anywhere in an image. A cat detector learned on centered cats will also detect cats in image corners, and the network doesn't need to learn separate detectors for every possible position.\n\n### Parameter Sharing: Efficient Learning\n\nUnlike fully connected networks where each weight is used once, convolutional layers **reuse kernel weights** across all spatial positions. A 3×3 kernel applied to a 224×224 RGB image uses just 27 parameters (3×3×3), not the millions required by a fully connected layer. This weight-sharing dramatically reduces parameter count while preserving spatial relationships, a key reason CNNs can process high-resolution images efficiently.\n\n### Receptive Field: Seeing More with Depth\n\nThe **receptive field** is the region of input pixels that influence each output pixel. In the first convolutional layer, a 3×3 kernel has a receptive field of 3×3 pixels. As we stack layers, the receptive field grows. Consider two 3×3 convolutional layers: each output pixel in the second layer depends on a 3×3 region in the first layer's output, but each of those positions depends on a 3×3 region in the input, so the second layer's receptive field is 5×5 in the original input.\n\n::: {#fig-receptive-field}\n![Receptive field grows with network depth. Deeper layers see increasingly large regions of the input image.](https://www.researchgate.net/publication/316950618/figure/fig4/AS:11431281212123378@1702542797323/The-receptive-field-of-each-convolution-layer-with-a-3-3-kernel-The-green-area-marks.tif){width=50% fig-align=\"center\"}\n:::\n\nWhy does this matter? This hierarchical structure allows CNNs to detect increasingly complex, abstract features: early layers detect edges and simple patterns, middle layers combine these into textures and parts, and deep layers recognize complete objects and scenes.\n\n### Stride and Padding: Controlling Dimensions\n\n**Stride** determines how many pixels we skip when sliding the kernel. With stride 1, we move one pixel at a time creating dense feature maps, while stride 2 skips every other position, effectively downsampling the output. For a 1D example with input $[a,b,c,d,e,f]$ and kernel $[1,2]$:\n\n**Stride 1:**\n$$\n[1a + 2b, 1b + 2c, 1c + 2d, 1d + 2e, 1e + 2f]\n$$\n\n**Stride 2:**\n$$\n[1a + 2b, 1c + 2d, 1e + 2f]\n$$\n\nLarger strides reduce computational cost and increase the receptive field but might miss fine details.\n\n::: {#fig-stride-visualization}\n![Stride controls how far the kernel moves at each step. Stride 2 produces half the spatial dimensions of stride 1.](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRJTOGcDwPXtlNnev9ayPj92FIysGddxe__Fw&s){width=50% fig-align=\"center\"}\n:::\n\n**Padding** addresses information loss at borders. Without padding (called \"valid\" padding), the output shrinks after each convolution because the kernel can't fully overlap with border pixels. Zero padding adds a border of zeros around the input, allowing the kernel to process edge pixels and control output dimensions.\n\n::: {#fig-padding-visualization}\n![Zero padding extends the input with zeros, preserving spatial dimensions and processing border pixels.](https://svitla.com/uploads/ckeditor/2024/Math%20at%20the%20heart%20of%20CNN/image_930660943761713546482755.gif){width=50% fig-align=\"center\"}\n:::\n\nFor a square input of size $W$ with kernel size $K$, stride $S$, and padding $P$, the output dimension is:\n\n$$\nO = \\left\\lfloor\\frac{W - K + 2P}{S}\\right\\rfloor + 1\n$$\n\nExample: 224×224 input, 3×3 kernel, stride 2, padding 1:\n\n$$\nO = \\left\\lfloor\\frac{224 - 3 + 2(1)}{2}\\right\\rfloor + 1 = 112\n$$\n\nThe interplay between stride and padding lets network designers control spatial dimensions and computational efficiency. Try the [Convolution Visualizer](https://ezyang.github.io/convolution-visualizer/) to experiment with different stride and padding settings interactively.\n\n### Pooling Layers: Downsampling with Invariance\n\nPooling layers downsample feature maps, reducing spatial dimensions while preserving important information. **Max pooling** selects the maximum value in each local window:\n\n$$\nP_{i,j} = \\max_{m,n} F_{si+m,sj+n}\n$$\n\nwhere $F$ is the feature map, $s$ is the stride (typically equal to the window size), and $(m,n)$ range over the pooling window.\n\n**Average pooling** computes the mean instead:\n\n$$\nP_{i,j} = \\frac{1}{w^2}\\sum_{m=0}^{w-1}\\sum_{n=0}^{w-1} F_{si+m,sj+n}\n$$\n\nMax pooling creates local translation invariance: if an edge moves slightly within a pooling window, the maximum value remains unchanged, helping the network focus on whether a feature is present rather than its exact position. Pooling also reduces computational cost by decreasing spatial dimensions, and a common pattern is to double the number of channels while halving spatial dimensions, maintaining roughly constant computational load across layers. Some recent architectures replace pooling with strided convolutions, arguing that learnable downsampling might be more effective [@springenberg2015striving], though the choice involves trade-offs between parameter efficiency and flexibility.\n\n## Using Pre-Trained Models\n\nNow that we understand CNN building blocks, let's use them in practice. Training a CNN from scratch on ImageNet requires weeks of GPU time, but we can leverage pre-trained models trained by research labs with vast computational resources.\n\n### Loading Models from torchvision\n\nPyTorch's `torchvision.models` provides pre-trained implementations of major architectures:\n\n::: {#acc59f57 .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nimport torchvision.models as models\nfrom torchvision import transforms\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\n# Load a pre-trained ResNet-50 model\nresnet50 = models.resnet50(weights='IMAGENET1K_V1')\nresnet50.eval()  # Set to evaluation mode\n\nprint(f\"Model type: {type(resnet50)}\")\nprint(f\"Number of parameters: {sum(p.numel() for p in resnet50.parameters()):,}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel type: <class 'torchvision.models.resnet.ResNet'>\nNumber of parameters: 25,557,032\n```\n:::\n:::\n\n\nThe model is trained on ImageNet with 1000 classes, and we can use it to classify images directly.\n\n### Image Classification Example\n\nTo use a pre-trained model, we must preprocess images the same way they were during training. ImageNet models expect images resized to 224×224 (or 299×299 for some models) with pixel values normalized to mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225].\n\n::: {#98f2ec56 .cell execution_count=2}\n``` {.python .cell-code}\n# Define the preprocessing transform pipeline\npreprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    ),\n])\n```\n:::\n\n\n\n\n::: {#dfb5ca3a .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show image display code\"}\n# Display the original image\nplt.figure(figsize=(6, 6))\nplt.imshow(img)\nplt.title(\"Input Image (from CIFAR-10)\")\nplt.axis(\"off\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-using-cnn-models_files/figure-html/cell-5-output-1.png){}\n:::\n:::\n\n\n::: {#a6404ef5 .cell execution_count=5}\n``` {.python .cell-code}\n# Apply preprocessing and prepare for model input\ninput_tensor = preprocess(img)\ninput_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n\nprint(f\"Input shape: {input_batch.shape}\")  # [1, 3, 224, 224]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInput shape: torch.Size([1, 3, 224, 224])\n```\n:::\n:::\n\n\nNow classify the image:\n\n\n\n::: {#b674c582 .cell execution_count=7}\n``` {.python .cell-code}\n# Perform inference\nwith torch.no_grad():\n    output = resnet50(input_batch)\n\n# Output is logits for 1000 classes\nprint(f\"Output shape: {output.shape}\")  # [1, 1000]\n\n# Convert to probabilities and get top 5 predictions\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\n\n# Display predictions\nprint(\"\\nTop 5 predictions:\")\nfor i in range(5):\n    print(f\"{i+1}. {labels[top5_catid[i]]}: {top5_prob[i].item()*100:.2f}%\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOutput shape: torch.Size([1, 1000])\n\nTop 5 predictions:\n1. macaque: 25.89%\n2. frilled-necked lizard: 24.47%\n3. consomme: 12.40%\n4. patas monkey: 12.35%\n5. hot pot: 11.11%\n```\n:::\n:::\n\n\nThe model correctly identifies the object with high confidence, demonstrating the power of pre-trained networks that have learned rich visual representations from ImageNet's diverse images.\n\n### When to Use Which Architecture\n\nDifferent architectures offer trade-offs between accuracy, speed, and memory. **ResNet-50** provides excellent general-purpose performance with good accuracy and reasonable speed. **EfficientNet** is optimized for mobile and edge devices with the best accuracy-per-parameter ratio. **VGG-16** has a simple, easy-to-understand architecture but is larger and slower than modern alternatives. **MobileNet** is designed for fast mobile inference at the cost of some accuracy. **Vision Transformer (ViT)** achieves state-of-the-art accuracy on large datasets but requires more data and compute. For most applications, start with ResNet-50 and optimize for speed or accuracy later if needed.\n\n## Transfer Learning: Adapting Pre-Trained Models\n\nPre-trained models learn general visual features from ImageNet's 1000 categories, but what if you want to classify different objects? Transfer learning adapts these models to new tasks.\n\n### Why Transfer Learning Works\n\nImageNet-trained models learn a hierarchy of features: early layers detect edges, colors, and simple textures that are universal across tasks; middle layers detect patterns, parts, and compositions that are somewhat task-specific; late layers detect complete objects specific to ImageNet categories. The early and middle layers learn representations useful for many vision tasks, so we can reuse these features and only retrain the final layers for our specific problem.\n\n### Two Approaches: Feature Extraction vs. Fine-Tuning\n\n**Feature Extraction** freezes all convolutional layers and trains only a new classifier head, running fast and working well with small datasets. **Fine-Tuning** initializes with pre-trained weights then trains the entire network (or parts of it) on your data, achieving better accuracy but requiring more data and computation.\n\n### Example: Fine-Tuning for Custom Classification\n\nLet's adapt ResNet-50 to classify 10 animal species:\n\n::: {#85243ab1 .cell execution_count=8}\n``` {.python .cell-code}\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Load pre-trained ResNet-50\nmodel = models.resnet50(weights='IMAGENET1K_V1')\n\n# Replace the final fully connected layer\n# Original: 2048 -> 1000 (ImageNet classes)\n# New: 2048 -> 10 (our custom classes)\nnum_classes = 10\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nprint(f\"Modified final layer: {model.fc}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModified final layer: Linear(in_features=2048, out_features=10, bias=True)\n```\n:::\n:::\n\n\nFor feature extraction, freeze early layers:\n\n::: {#83173270 .cell execution_count=9}\n``` {.python .cell-code}\n# Freeze all layers except the final classifier\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Unfreeze the final layer\nfor param in model.fc.parameters():\n    param.requires_grad = True\n\n# Count trainable parameters\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(f\"Trainable parameters: {trainable_params:,} / {total_params:,}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrainable parameters: 20,490 / 23,528,522\n```\n:::\n:::\n\n\nOnly 20,490 parameters (the final layer) are trainable, making training fast and preventing overfitting on small datasets.\n\n### Training Loop\n\n::: {#73dacdc1 .cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show training loop implementation\"}\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n\n# Training loop (pseudo-code, requires actual data)\ndef train_epoch(model, dataloader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for inputs, labels in dataloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    epoch_loss = running_loss / len(dataloader)\n    epoch_acc = correct / total\n    return epoch_loss, epoch_acc\n\n# For fine-tuning instead of feature extraction:\n# 1. Unfreeze all or some layers\n# 2. Use a smaller learning rate (e.g., 1e-4 or 1e-5)\n# 3. Train for more epochs\n```\n:::\n\n\n### Data Augmentation: Essential for Small Datasets\n\nWhen training on limited data, augmentation is crucial: transform each image differently each epoch to artificially expand the training set.\n\n\n\n::: {#edc5d844 .cell execution_count=12}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show training augmentation pipeline\"}\n# Training transforms with aggressive augmentation\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(224),      # Random crop and resize\n    transforms.RandomHorizontalFlip(),       # Flip with 50% probability\n    transforms.ColorJitter(                  # Random brightness, contrast\n        brightness=0.2,\n        contrast=0.2,\n        saturation=0.2\n    ),\n    transforms.RandomRotation(15),           # Rotate up to 15 degrees\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    ),\n])\n```\n:::\n\n\n::: {#78b60fb9 .cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show validation transform pipeline\"}\n# Validation transforms (deterministic, no randomness)\nval_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    ),\n])\n```\n:::\n\n\nNote that validation uses deterministic transforms (no randomness) for reproducible evaluation.\n\n### Best Practices for Transfer Learning\n\nStart with feature extraction by training only the final layer first, which is fast and often achieves good results. If accuracy is insufficient, try fine-tuning by unfreezing earlier layers and training with a small learning rate (10× smaller than initial training). Use learning rate schedules to reduce the learning rate when validation loss plateaus. Monitor for overfitting using validation data, and apply more augmentation or stronger regularization (dropout, weight decay) if needed. Always match preprocessing to the pre-training dataset, using ImageNet statistics for most models.\n\n::: {.callout-tip title=\"Try it yourself\"}\nPractice transfer learning on your own image dataset. Collect 100-500 images per class (even phone camera photos work), then split into train/val/test sets (70/15/15). Start with ResNet-50 feature extraction and train for 10-20 epochs before evaluating on the test set. You'll likely achieve 80-90%+ accuracy with just a few hundred images per class, demonstrating the power of pre-trained features.\n:::\n\n## Visualizing What Networks Learn\n\nWhat features do trained networks actually detect? Let's peek inside a trained network to see:\n\n::: {#6389639d .cell execution_count=14}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show activation visualization code\"}\n# Extract intermediate feature maps\ndef get_activation(name, activations):\n    def hook(model, input, output):\n        activations[name] = output.detach()\n    return hook\n\n# Register hooks to capture activations\nactivations = {}\nmodel.layer1[0].conv1.register_forward_hook(get_activation('layer1', activations))\nmodel.layer2[0].conv1.register_forward_hook(get_activation('layer2', activations))\nmodel.layer3[0].conv1.register_forward_hook(get_activation('layer3', activations))\n\n# Run inference\nwith torch.no_grad():\n    _ = model(input_batch)\n\n# Visualize first layer activations\nlayer1_act = activations['layer1'][0]  # [C, H, W]\n\nfig, axes = plt.subplots(4, 8, figsize=(16, 8))\nfor i, ax in enumerate(axes.flat):\n    if i < layer1_act.shape[0]:\n        ax.imshow(layer1_act[i].cpu(), cmap='viridis')\n    ax.axis('off')\nplt.suptitle(\"Layer 1 Feature Maps\", fontsize=16)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-using-cnn-models_files/figure-html/cell-15-output-1.png){}\n:::\n:::\n\n\nEarly layers show edge detection and simple patterns, while deeper layers show increasingly abstract features that are harder to interpret but encode high-level semantic information.\n\n## Summary\n\nWe explored the building blocks that make CNNs powerful: convolution operations with learnable kernels, translation equivariance for position-invariant recognition, parameter sharing for efficiency, growing receptive fields through depth, stride and padding for dimension control, and pooling for downsampling with invariance. We learned to use pre-trained models from torchvision and adapt them to custom tasks through transfer learning, either by feature extraction (training only the classifier) or fine-tuning (training the entire network with small learning rates). Data augmentation artificially expands small datasets to prevent overfitting. These practical skills let you load state-of-the-art models, adapt them to your data, and achieve strong results with limited computational resources.\n\n",
    "supporting": [
      "03-using-cnn-models_files"
    ],
    "filters": [],
    "includes": {}
  }
}
{
  "hash": "22741c9c03c0a5e9a0b80eef45897e1b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Part 3: Using CNN Models\"\njupyter: python3\n---\n\n::: {.callout-note title=\"What you'll learn\"}\nThis section transforms you into a CNN practitioner. We explore the fundamental building blocks (convolution, pooling, stride, padding), understand key properties like translation equivariance, learn to use pre-trained models from torchvision, and master transfer learning techniques for adapting models to new tasks.\n:::\n\n## Understanding CNN Building Blocks\n\nAlexNet proved that deep learning works at scale. But how do these networks actually process images? Let's break down the fundamental operations that make CNNs powerful.\n\n### Convolutional Layers: Learnable Pattern Detectors\n\nAt the heart of CNNs lies a remarkably elegant operation called **convolution**. Imagine sliding a small window (a **kernel** or **filter**) across an image. At each position, we multiply the kernel values by the overlapping image pixels and sum the results. This produces a single output value. Repeat across all positions to create an output **feature map**.\n\n::: {#fig-convolution-operation}\n![Convolution operation. The kernel slides across the input, computing weighted sums at each position to produce a feature map.](https://anhreynolds.com/img/cnn.png){width=100% fig-align=\"center\"}\n:::\n\nMathematically, for a single-channel input (grayscale image), 2D convolution is:\n\n$$\n(I * K)_{i,j} = \\sum_{m=0}^{L-1}\\sum_{n=0}^{L-1} I_{i+m,j+n} \\cdot K_{m,n}\n$$\n\nwhere $I$ is the input image, $K$ is the kernel of size $L \\times L$, and $(i,j)$ specifies the output position.\n\nWhat makes CNNs powerful is that these kernels are **learnable parameters**. During training, each kernel evolves to detect specific visual patterns. Some kernels might become edge detectors, highlighting vertical or horizontal edges. Others might respond to textures, colors, or more complex patterns. The network discovers useful features automatically.\n\nReal-world images have multiple channels (RGB). Convolution extends naturally to 3D inputs using 3D kernels:\n\n$$\n(I * K)_{i,j} = \\sum_{c=1}^{C}\\sum_{m=0}^{L-1}\\sum_{n=0}^{L-1} I_{c,i+m,j+n} \\cdot K_{c,m,n}\n$$\n\nwhere $C$ is the number of input channels. Each kernel processes all channels simultaneously, combining color information into a single output value.\n\n::: {#fig-multi-channel-convolution}\n![Multi-channel convolution. Each kernel processes all input channels, producing one output feature map.](https://d2l.ai/_images/conv-multi-in.svg){width=100% fig-align=\"center\"}\n:::\n\n::: {.callout-tip title=\"Interactive visualizations\"}\nExplore CNN operations interactively. The [CNN Explainer](https://poloclub.github.io/cnn-explainer/) shows how convolution, activation, and pooling work step-by-step. The [Interactive Node-Link Visualization](https://adamharley.com/nn_vis/cnn/2d.html) lets you see activations flow through a trained network.\n:::\n\n### Translation Equivariance: A Key Property\n\nOne crucial feature of convolutional layers is **translation equivariance**. This means that if you shift the input, the output shifts by the same amount.\n\nConsider detecting a vertical edge. If the edge moves one pixel to the right in the input image, the detected edge feature also moves one pixel to the right in the output. The detection operation doesn't care about absolute position, only relative patterns.\n\n::: {#fig-translation-equivariance}\n![Translation equivariance. The same kernel detects the same feature regardless of position in the input.](https://miro.medium.com/v2/resize:fit:1400/1*NoAQ4ZgofpkK6esl4sMHkA.png){width=80% fig-align=\"center\"}\n:::\n\nThis property allows CNNs to recognize objects anywhere in an image. A cat detector learned on centered cats will also detect cats in image corners. The network doesn't need to learn separate detectors for every possible position.\n\n### Parameter Sharing: Efficient Learning\n\nUnlike fully connected networks where each weight is used once, convolutional layers **reuse kernel weights** across all spatial positions. A 3×3 kernel applied to a 224×224 RGB image uses just 27 parameters (3×3×3), not the millions required by a fully connected layer.\n\nThis weight-sharing dramatically reduces parameter count while preserving spatial relationships in the data. It's a key reason CNNs can process high-resolution images efficiently.\n\n### Receptive Field: Seeing More with Depth\n\nThe **receptive field** is the region of input pixels that influence each output pixel. In the first convolutional layer, a 3×3 kernel has a receptive field of 3×3 pixels. But as we stack layers, the receptive field grows.\n\nConsider two 3×3 convolutional layers. Each output pixel in the second layer depends on a 3×3 region in the first layer's output. But each of those positions depends on a 3×3 region in the input. So the second layer's receptive field is 5×5 in the original input.\n\n::: {#fig-receptive-field}\n![Receptive field grows with network depth. Deeper layers see increasingly large regions of the input image.](https://www.researchgate.net/publication/316950618/figure/fig4/AS:11431281212123378@1702542797323/The-receptive-field-of-each-convolution-layer-with-a-3-3-kernel-The-green-area-marks.tif){width=50% fig-align=\"center\"}\n:::\n\nThis hierarchical structure allows CNNs to detect increasingly complex, abstract features. Early layers detect edges and simple patterns. Middle layers combine these into textures and parts. Deep layers recognize complete objects and scenes.\n\n### Stride and Padding: Controlling Dimensions\n\n**Stride** determines how many pixels we skip when sliding the kernel. With stride 1, we move one pixel at a time, creating dense feature maps. With stride 2, we skip every other position, effectively downsampling the output.\n\nFor a 1D example with input $[a,b,c,d,e,f]$ and kernel $[1,2]$:\n\n**Stride 1:**\n$$\n[1a + 2b, 1b + 2c, 1c + 2d, 1d + 2e, 1e + 2f]\n$$\n\n**Stride 2:**\n$$\n[1a + 2b, 1c + 2d, 1e + 2f]\n$$\n\nLarger strides reduce computational cost and increase the receptive field, but might miss fine details.\n\n::: {#fig-stride-visualization}\n![Stride controls how far the kernel moves at each step. Stride 2 produces half the spatial dimensions of stride 1.](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRJTOGcDwPXtlNnev9ayPj92FIysGddxe__Fw&s){width=50% fig-align=\"center\"}\n:::\n\n**Padding** addresses information loss at borders. Without padding (called \"valid\" padding), the output shrinks after each convolution because the kernel can't fully overlap with border pixels. Zero padding adds a border of zeros around the input, allowing the kernel to process edge pixels and control output dimensions.\n\n::: {#fig-padding-visualization}\n![Zero padding extends the input with zeros, preserving spatial dimensions and processing border pixels.](https://svitla.com/uploads/ckeditor/2024/Math%20at%20the%20heart%20of%20CNN/image_930660943761713546482755.gif){width=50% fig-align=\"center\"}\n:::\n\nFor a square input of size $W$ with kernel size $K$, stride $S$, and padding $P$, the output dimension is:\n\n$$\nO = \\left\\lfloor\\frac{W - K + 2P}{S}\\right\\rfloor + 1\n$$\n\nExample: 224×224 input, 3×3 kernel, stride 2, padding 1:\n\n$$\nO = \\left\\lfloor\\frac{224 - 3 + 2(1)}{2}\\right\\rfloor + 1 = 112\n$$\n\nThe interplay between stride and padding lets network designers control spatial dimensions and computational efficiency. Try the [Convolution Visualizer](https://ezyang.github.io/convolution-visualizer/) to experiment with different stride and padding settings interactively.\n\n### Pooling Layers: Downsampling with Invariance\n\nPooling layers downsample feature maps, reducing spatial dimensions while preserving important information. **Max pooling** selects the maximum value in each local window:\n\n$$\nP_{i,j} = \\max_{m,n} F_{si+m,sj+n}\n$$\n\nwhere $F$ is the feature map, $s$ is the stride (typically equal to the window size), and $(m,n)$ range over the pooling window.\n\n**Average pooling** computes the mean instead:\n\n$$\nP_{i,j} = \\frac{1}{w^2}\\sum_{m=0}^{w-1}\\sum_{n=0}^{w-1} F_{si+m,sj+n}\n$$\n\nMax pooling creates local translation invariance. If an edge moves slightly within a pooling window, the maximum value (and thus the output) remains unchanged. This helps the network focus on whether a feature is present, not its exact position.\n\nPooling also reduces computational cost in subsequent layers by decreasing spatial dimensions. A common pattern is to double the number of channels while halving spatial dimensions, maintaining roughly constant computational load across layers. Some recent architectures replace pooling with strided convolutions, arguing that learnable downsampling might be more effective {footcite}`springenberg2015striving`. The choice involves trade-offs between parameter efficiency and flexibility.\n\n## Using Pre-Trained Models\n\nNow that we understand CNN building blocks, let's use them in practice. Training a CNN from scratch on ImageNet requires weeks of GPU time. But we can leverage pre-trained models trained by research labs with vast computational resources.\n\n### Loading Models from torchvision\n\nPyTorch's `torchvision.models` provides pre-trained implementations of major architectures:\n\n::: {#77b453c5 .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nimport torchvision.models as models\nfrom torchvision import transforms\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\n# Load a pre-trained ResNet-50 model\nresnet50 = models.resnet50(weights='IMAGENET1K_V1')\nresnet50.eval()  # Set to evaluation mode\n\nprint(f\"Model type: {type(resnet50)}\")\nprint(f\"Number of parameters: {sum(p.numel() for p in resnet50.parameters()):,}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel type: <class 'torchvision.models.resnet.ResNet'>\nNumber of parameters: 25,557,032\n```\n:::\n:::\n\n\nThe model is trained on ImageNet with 1000 classes. Let's use it to classify an image.\n\n### Image Classification Example\n\nTo use a pre-trained model, we must preprocess images the same way they were during training. ImageNet models expect images resized to 224×224 (or 299×299 for some models) with pixel values normalized to mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225].\n\n::: {#deace793 .cell execution_count=2}\n``` {.python .cell-code}\n# Define the preprocessing transform pipeline\npreprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    ),\n])\n```\n:::\n\n\n\n\n::: {#eb493d8f .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show image display code\"}\n# Display the original image\nplt.figure(figsize=(6, 6))\nplt.imshow(img)\nplt.title(\"Input Image (from CIFAR-10)\")\nplt.axis(\"off\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-using-cnn-models_files/figure-html/cell-5-output-1.png){}\n:::\n:::\n\n\n::: {#77ea5b45 .cell execution_count=5}\n``` {.python .cell-code}\n# Apply preprocessing and prepare for model input\ninput_tensor = preprocess(img)\ninput_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n\nprint(f\"Input shape: {input_batch.shape}\")  # [1, 3, 224, 224]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInput shape: torch.Size([1, 3, 224, 224])\n```\n:::\n:::\n\n\nNow classify the image:\n\n\n\n::: {#fd69e9ac .cell execution_count=7}\n``` {.python .cell-code}\n# Perform inference\nwith torch.no_grad():\n    output = resnet50(input_batch)\n\n# Output is logits for 1000 classes\nprint(f\"Output shape: {output.shape}\")  # [1, 1000]\n\n# Convert to probabilities and get top 5 predictions\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\n\n# Display predictions\nprint(\"\\nTop 5 predictions:\")\nfor i in range(5):\n    print(f\"{i+1}. {labels[top5_catid[i]]}: {top5_prob[i].item()*100:.2f}%\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOutput shape: torch.Size([1, 1000])\n\nTop 5 predictions:\n1. macaque: 25.89%\n2. frilled-necked lizard: 24.47%\n3. consomme: 12.40%\n4. patas monkey: 12.35%\n5. hot pot: 11.11%\n```\n:::\n:::\n\n\nThe model correctly identifies the object with high confidence. This demonstrates the power of pre-trained networks: they've learned rich visual representations from ImageNet's diverse images.\n\n### When to Use Which Architecture\n\nDifferent architectures offer trade-offs between accuracy, speed, and memory:\n\n**ResNet-50**: Excellent general-purpose model. Good accuracy, reasonable speed. Default choice for most applications.\n\n**EfficientNet**: Optimized for mobile and edge devices. Best accuracy-per-parameter ratio.\n\n**VGG-16**: Simple architecture, easy to understand. Larger and slower than modern alternatives.\n\n**MobileNet**: Designed for mobile deployment. Fast inference, lower accuracy.\n\n**Vision Transformer (ViT)**: State-of-the-art accuracy on large datasets. Requires more data and compute.\n\nFor most applications, start with ResNet-50. It provides strong performance across diverse tasks. Optimize for speed or accuracy later if needed.\n\n## Transfer Learning: Adapting Pre-Trained Models\n\nPre-trained models learn general visual features from ImageNet's 1000 categories. But what if you want to classify different objects? Transfer learning adapts these models to new tasks.\n\n### Why Transfer Learning Works\n\nImageNet-trained models learn a hierarchy of features. Early layers detect edges, colors, and simple textures that are universal across tasks. Middle layers detect patterns, parts, and compositions that are somewhat task-specific. Late layers detect complete objects specific to ImageNet categories. The early and middle layers learn representations useful for many vision tasks. We can reuse these features and only retrain the final layers for our specific problem.\n\n### Two Approaches: Feature Extraction vs. Fine-Tuning\n\n**Feature Extraction**: Freeze all convolutional layers, only train a new classifier head. Fast and works well with small datasets.\n\n**Fine-Tuning**: Initialize with pre-trained weights, then train the entire network (or parts of it) on your data. Better accuracy but requires more data and computation.\n\n### Example: Fine-Tuning for Custom Classification\n\nLet's adapt ResNet-50 to classify 10 animal species:\n\n::: {#db42b389 .cell execution_count=8}\n``` {.python .cell-code}\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Load pre-trained ResNet-50\nmodel = models.resnet50(weights='IMAGENET1K_V1')\n\n# Replace the final fully connected layer\n# Original: 2048 -> 1000 (ImageNet classes)\n# New: 2048 -> 10 (our custom classes)\nnum_classes = 10\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nprint(f\"Modified final layer: {model.fc}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModified final layer: Linear(in_features=2048, out_features=10, bias=True)\n```\n:::\n:::\n\n\nFor feature extraction, freeze early layers:\n\n::: {#16151643 .cell execution_count=9}\n``` {.python .cell-code}\n# Freeze all layers except the final classifier\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Unfreeze the final layer\nfor param in model.fc.parameters():\n    param.requires_grad = True\n\n# Count trainable parameters\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(f\"Trainable parameters: {trainable_params:,} / {total_params:,}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrainable parameters: 20,490 / 23,528,522\n```\n:::\n:::\n\n\nOnly 20,490 parameters (the final layer) are trainable. This makes training fast and prevents overfitting on small datasets.\n\n### Training Loop\n\n::: {#0be00859 .cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show training loop implementation\"}\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n\n# Training loop (pseudo-code, requires actual data)\ndef train_epoch(model, dataloader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for inputs, labels in dataloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    epoch_loss = running_loss / len(dataloader)\n    epoch_acc = correct / total\n    return epoch_loss, epoch_acc\n\n# For fine-tuning instead of feature extraction:\n# 1. Unfreeze all or some layers\n# 2. Use a smaller learning rate (e.g., 1e-4 or 1e-5)\n# 3. Train for more epochs\n```\n:::\n\n\n### Data Augmentation: Essential for Small Datasets\n\nWhen training on limited data, augmentation is crucial. Transform each image differently each epoch to artificially expand the training set:\n\n\n\n::: {#0d9d116f .cell execution_count=12}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show training augmentation pipeline\"}\n# Training transforms with aggressive augmentation\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(224),      # Random crop and resize\n    transforms.RandomHorizontalFlip(),       # Flip with 50% probability\n    transforms.ColorJitter(                  # Random brightness, contrast\n        brightness=0.2,\n        contrast=0.2,\n        saturation=0.2\n    ),\n    transforms.RandomRotation(15),           # Rotate up to 15 degrees\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    ),\n])\n```\n:::\n\n\n::: {#5d8551b6 .cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show validation transform pipeline\"}\n# Validation transforms (deterministic, no randomness)\nval_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    ),\n])\n```\n:::\n\n\nNote that validation uses deterministic transforms (no randomness) for reproducible evaluation.\n\n### Best Practices for Transfer Learning\n\nStart with feature extraction by training only the final layer first. This is fast and often achieves good results. Then try fine-tuning if accuracy is insufficient by unfreezing earlier layers and training with a small learning rate (10× smaller than initial training). Use learning rate schedules to reduce the learning rate when validation loss plateaus, helping the model converge to better solutions. Monitor for overfitting by using validation data to detect when the model stops generalizing, then apply more augmentation or stronger regularization (dropout, weight decay) if needed. Always match preprocessing to the pre-training dataset, using ImageNet statistics for most models.\n\n::: {.callout-tip title=\"Try it yourself\"}\nPractice transfer learning on your own image dataset. Collect 100-500 images per class (even phone camera photos work), then split into train/val/test sets (70/15/15). Start with ResNet-50 feature extraction and train for 10-20 epochs before evaluating on the test set.\n\nYou'll likely achieve 80-90%+ accuracy with just a few hundred images per class, demonstrating the power of pre-trained features.\n:::\n\n## Visualizing What Networks Learn\n\nLet's peek inside a trained network to see what features it detects:\n\n::: {#91b06200 .cell execution_count=14}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show activation visualization code\"}\n# Extract intermediate feature maps\ndef get_activation(name, activations):\n    def hook(model, input, output):\n        activations[name] = output.detach()\n    return hook\n\n# Register hooks to capture activations\nactivations = {}\nmodel.layer1[0].conv1.register_forward_hook(get_activation('layer1', activations))\nmodel.layer2[0].conv1.register_forward_hook(get_activation('layer2', activations))\nmodel.layer3[0].conv1.register_forward_hook(get_activation('layer3', activations))\n\n# Run inference\nwith torch.no_grad():\n    _ = model(input_batch)\n\n# Visualize first layer activations\nlayer1_act = activations['layer1'][0]  # [C, H, W]\n\nfig, axes = plt.subplots(4, 8, figsize=(16, 8))\nfor i, ax in enumerate(axes.flat):\n    if i < layer1_act.shape[0]:\n        ax.imshow(layer1_act[i].cpu(), cmap='viridis')\n    ax.axis('off')\nplt.suptitle(\"Layer 1 Feature Maps\", fontsize=16)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-using-cnn-models_files/figure-html/cell-15-output-1.png){}\n:::\n:::\n\n\nEarly layers show edge detection and simple patterns. Deeper layers show increasingly abstract features that are harder to interpret but encode high-level semantic information.\n\n## Summary\n\nWe explored the building blocks that make CNNs powerful: convolution operations with learnable kernels, translation equivariance that enables position-invariant recognition, parameter sharing for efficiency, growing receptive fields through depth, stride and padding for dimension control, and pooling for downsampling with invariance.\n\nWe learned to use pre-trained models from torchvision for immediate deployment. Transfer learning lets us adapt these models to custom tasks through feature extraction (training only the classifier) or fine-tuning (training the entire network with small learning rates). Data augmentation artificially expands small datasets, preventing overfitting.\n\nThese practical skills transform you from understanding CNNs conceptually to deploying them on real problems. You can now load state-of-the-art models, adapt them to your data, and achieve strong results with limited computational resources.\n\n```{footbibliography}\n:style: unsrt\n```\n\n",
    "supporting": [
      "03-using-cnn-models_files"
    ],
    "filters": [],
    "includes": {}
  }
}
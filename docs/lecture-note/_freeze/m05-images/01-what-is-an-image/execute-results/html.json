{
  "hash": "75ba033c75bed10fd1e3a436d7926424",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Part 1: What is an Image?\"\njupyter: python3\n---\n\n::: {.callout-note title=\"What you'll learn in this section\"}\nThis section introduces images as structured data.\n\nYou'll learn:\n\n- What **pixels** are and how computers represent visual information as grids of numbers.\n- How **color channels** combine to create RGB images.\n- Why **spatial structure** matters fundamentally for machine perception.\n- How to convert between **NumPy arrays** and **PyTorch tensors** for deep learning.\n:::\n\n## A Photograph is Just Numbers\n\nLet's talk about what an image really is from a computer's perspective. When you look at a photograph, you see faces, objects, and scenes, but to a machine, that same photograph is simply a grid of numbers where each value represents the brightness or color at a specific location. How can numbers capture the richness of visual information? The answer lies in spatial structure. Unlike a spreadsheet where row order doesn't matter, the arrangement of numbers in an image is everything, with neighboring pixels relating to each other to form edges, textures, and patterns.\n\n## Grayscale Images: The Simplest Case\n\nThe very first step in understanding images is to examine the grayscale case, where an image contains only brightness information with no color. We can think of it as a 2D matrix where each entry is a pixel intensity value. Consider a tiny 6Ã—6 grayscale image:\n\n$$\nX = \\begin{bmatrix}\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10\n\\end{bmatrix}\n$$\n\nHere, the value 10 represents dark pixels, while 80 represents bright pixels. The third column forms a bright vertical line. This simple example shows how spatial patterns emerge from the arrangement of numbers.\n\n::: {#fig-image-matrix}\n![A grayscale image represented as a matrix of pixel intensity values. Each number encodes brightness at that location.](https://ai.stanford.edu/~syyeung/cvweb/Pictures1/imagematrix.png){width=80% fig-align=\"center\"}\n:::\n\n## Loading and Inspecting Real Images\n\nLet's make this concrete by loading an actual image and examining its structure. We'll use Python with standard libraries to see what an image really looks like under the hood.\n\n\n\n::: {#80521bac .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show image display code\"}\n# Display the image\nplt.figure(figsize=(8, 6))\nplt.imshow(img_array)\nplt.title(\"Original Image\")\nplt.axis(\"off\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](01-what-is-an-image_files/figure-html/cell-3-output-1.png){}\n:::\n:::\n\n\n::: {#2105af2a .cell execution_count=3}\n``` {.python .cell-code}\n# Examine the image properties\nprint(f\"Image shape: {img_array.shape}\")\nprint(f\"Data type: {img_array.dtype}\")\nprint(f\"Value range: [{img_array.min()}, {img_array.max()}]\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nImage shape: (300, 600, 3)\nData type: uint8\nValue range: [0, 255]\n```\n:::\n:::\n\n\nWhat does the shape tell us? The output `(height, width, 3)` reveals three dimensions: the first two specify spatial location, while the third holds three color channels. Let's zoom into a small patch to see the actual numbers:\n\n::: {#664189e9 .cell execution_count=4}\n``` {.python .cell-code}\n# Extract a tiny 5x5 patch from the center\ncenter_y, center_x = img_array.shape[0] // 2, img_array.shape[1] // 2\npatch = img_array[center_y:center_y+5, center_x:center_x+5, 0]  # Red channel only\n\nprint(\"A 5x5 patch of pixel values (Red channel):\")\nprint(patch)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nA 5x5 patch of pixel values (Red channel):\n[[ 45  49  40  93 141]\n [ 55  52  55 113 102]\n [ 35  46  63 121 104]\n [136  52  48  84 124]\n [225  90  41  73 136]]\n```\n:::\n:::\n\n\nThese are the actual numbers the computer sees. Each value between 0 and 255 represents brightness in the red channel for that pixel location.\n\n## Color Images: Three Stacked Layers\n\nColor images extend the grayscale concept by using three separate matrices, one for each color channel: Red, Green, and Blue (RGB). Think of these as three grayscale images stacked on top of each other. When you combine the values from all three channels at a given location, you get the color for that pixel: `[255, 0, 0]` is pure red, `[0, 255, 0]` is pure green, and `[255, 255, 255]` is white. How do the channels look separately? Let's visualize them:\n\n::: {#5a493b70 .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show channel visualization code\"}\nfig, axes = plt.subplots(1, 4, figsize=(16, 4))\n\n# Original image\naxes[0].imshow(img_array)\naxes[0].set_title(\"Original Image\")\naxes[0].axis(\"off\")\n\n# Individual channels\nchannel_names = ['Red', 'Green', 'Blue']\ncolors = ['Reds', 'Greens', 'Blues']\n\nfor i, (name, cmap) in enumerate(zip(channel_names, colors)):\n    axes[i+1].imshow(img_array[:, :, i], cmap=cmap)\n    axes[i+1].set_title(f\"{name} Channel\")\n    axes[i+1].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](01-what-is-an-image_files/figure-html/cell-6-output-1.png){}\n:::\n:::\n\n\nNotice how each channel emphasizes different aspects of the scene. The red channel might be bright where red objects appear, while the blue channel highlights sky and water.\n\n## Why Spatial Structure Matters\n\nShift your attention from individual pixel values to relationships between pixels. This is what makes images fundamentally different from tabular data: in a spreadsheet, you can shuffle the rows without losing information, but in an image, shuffling pixels destroys everything because the spatial arrangement is the information itself. An edge appears when neighboring pixels have very different values, a texture emerges from repeating patterns across nearby locations, and an object is a coherent region of similar pixels. Why does this matter for machine learning? Fully connected networks treat every input independently, ignoring spatial relationships, while convolutional networks (which we'll explore soon) are designed specifically to exploit this structure.\n\n## Converting Between Representations\n\nLet's practice manipulating image representations to build intuition:\n\n::: {#aa9f3cd4 .cell execution_count=6}\n``` {.python .cell-code}\n# Convert to grayscale by averaging channels\ngrayscale = np.mean(img_array, axis=2).astype(np.uint8)\n\nprint(f\"RGB shape: {img_array.shape}\")\nprint(f\"Grayscale shape: {grayscale.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRGB shape: (300, 600, 3)\nGrayscale shape: (300, 600)\n```\n:::\n:::\n\n\n::: {#806fd6aa .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show RGB vs grayscale comparison\"}\n# Create a side-by-side comparison\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\naxes[0].imshow(img_array)\naxes[0].set_title(\"RGB Image (3 channels)\")\naxes[0].axis(\"off\")\n\naxes[1].imshow(grayscale, cmap='gray')\naxes[1].set_title(\"Grayscale Image (1 channel)\")\naxes[1].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](01-what-is-an-image_files/figure-html/cell-8-output-1.png){}\n:::\n:::\n\n\nThe grayscale version loses color information but preserves spatial structure. For many computer vision tasks, this simplified representation is sufficient.\n\n## Image Dimensions in Deep Learning\n\nWhen we feed images into neural networks, we need to be precise about dimensions. Different frameworks use different conventions. PyTorch expects images in `(batch_size, channels, height, width)` format, often abbreviated as NCHW:\n\n- **N**: Batch size (number of images processed together)\n- **C**: Channels (3 for RGB, 1 for grayscale)\n- **H**: Height in pixels\n- **W**: Width in pixels\n\nLet's convert our image to PyTorch format:\n\n::: {#b5c0c6b6 .cell execution_count=8}\n``` {.python .cell-code}\nimport torch\n\n# Original numpy array is (H, W, C)\nprint(f\"NumPy format (H, W, C): {img_array.shape}\")\n\n# Convert to PyTorch format (C, H, W) for a single image\nimg_tensor = torch.from_numpy(img_array).permute(2, 0, 1).float() / 255.0\nprint(f\"PyTorch format (C, H, W): {img_tensor.shape}\")\n\n# Add batch dimension to get (N, C, H, W)\nimg_batch = img_tensor.unsqueeze(0)\nprint(f\"Batch format (N, C, H, W): {img_batch.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumPy format (H, W, C): (300, 600, 3)\nPyTorch format (C, H, W): torch.Size([3, 300, 600])\nBatch format (N, C, H, W): torch.Size([1, 3, 300, 600])\n```\n:::\n:::\n\n\nWe also normalized pixel values from `[0, 255]` to `[0, 1]` by dividing by 255. Why do this? This normalization helps neural networks train more stably.\n\n::: {.callout-tip title=\"Try it yourself\"}\nLoad your own image and explore its properties. Extract and visualize a small patch of pixels as numbers to see the underlying data structure. Modify some pixel values directly and observe how the image changes. Swap the red and blue channels to see the dramatic color shift this creates. Convert between NumPy and PyTorch formats to build fluency with both representations.\n\nUnderstanding these representations at a hands-on level will make everything that follows more intuitive.\n:::\n\n## From Pixels to Patterns\n\nNow that we understand what images are as data structures, the next question becomes: how do we detect patterns in them? Human vision effortlessly recognizes edges, textures, and objects, but what computational operations allow machines to do the same? This question leads us to convolution, feature extraction, and ultimately to the deep learning revolution. In the next section, we'll explore how computer vision evolved from hand-crafted feature detectors to learned representations that can match or exceed human performance. The key insight to carry forward is this: images are spatial data where relationships between neighboring pixels encode visual information.\n\n## Summary\n\nWe explored images as structured numerical data: a grayscale image is a 2D matrix of brightness values, while a color image adds two more matrices for the other color channels. Spatial relationships between pixels encode edges, textures, and objects, and unlike tabular data, the arrangement of values matters fundamentally. We saw how to load, inspect, and manipulate images in Python, converting between NumPy arrays and PyTorch tensors. This hands-on understanding prepares us to work with the deep learning models that process these image representations. Images are not just collections of numbers but spatially organized data where local patterns combine into global structure.\n\n",
    "supporting": [
      "01-what-is-an-image_files"
    ],
    "filters": [],
    "includes": {}
  }
}
{
  "hash": "08e2298fbcbb61d931c7dc19125935f5",
  "result": {
    "engine": "jupyter",
    "markdown": "---\njupytext:\n  formats: md:myst\n  text_representation:\n    extension: .md\n    format_name: myst\nkernelspec:\n  display_name: Python 3\n  language: python\n  name: python3\n---\n\n# AlexNet: A Breakthrough in Deep Learning\n\nHow can we train a computer to recognize 1000 different types of objects across more than a million images, and what makes this problem so challenging?\n\n\n```{admonition} AlexNet in interactive mode:\n:class: tip\n\n[Here is a demo notebook for AlexNet](https://static.marimo.app/static/alexnet-fkeb)\n\nTo run the notebook, download the notebook as a `.py` file and run it with:\n\n> marimo edit --sandbox alexnet.py\n\nYou will need to install `marimo` and `uv` to run the notebook. But other packages will be installed automatically in uv's virtual environment.\n```\n\n\n\n## Conceptual Foundation: The ImageNet Challenge\n\nBefore diving into AlexNet, let's explore **why** large-scale image classification posed such a formidable problem for machine learning:\n\n1. **Massive Dataset**: The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) dataset contains over 1.2 million training images labeled across 1000 categories.\n2. **Computational Demands**: Traditional approaches struggled both with the sheer volume of data and with the complexity of designing handcrafted features.\n3. **Error Plateaus**: Prior to 2012, the best error rates hovered around 25%, suggesting that existing methods had nearly peaked using conventional feature-engineering techniques.\n\n```{figure} https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11263-015-0816-y/MediaObjects/11263_2015_816_Fig2_HTML.gif\n:width: 100%\n:align: center\n\nThe ImageNet Large Scale Visual Recognition Challenge (ILSVRC).\n```\n\n```{tip}\nPrior to AlexNet, most computer vision systems relied on hand-engineered features such as SIFT (Scale-Invariant Feature Transform) or HOG (Histogram of Oriented Gradients). These were labor-intensive to design and often failed to generalize well to diverse images.\n```\n\nIn 2012, a team led by **Alex Krizhevsky**, **Ilya Sutskever**, and **Geoffrey Hinton** demonstrated a deep **Convolutional Neural Network (CNN)** that shattered expectations. Their submission, known as **AlexNet**, reduced the top-5 error rate to **16.4%**—a remarkable improvement of **over 10 percentage points** compared to the next-best approach {footcite}`krizhevsky2012alexnet`.\n\n```{figure} https://viso.ai/wp-content/uploads/2024/02/imagenet-winners-by-year.jpg\n:width: 100%\n:align: center\n\nTop 5 error rates of the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) from 2010 to 2017. AlexNet reduced the error rate by over 10 percentage points compared to the best performing method based on human-crafted features in 2011.\n```\n\nThis breakthrough ignited the **deep learning revolution** in computer vision. Researchers quickly realized the potential of stacking many layers of neural networks—provided they could overcome critical hurdles in training and regularization.\n\n## Key Innovations in AlexNet\n\n### ReLU Activation Function\n\n**The Challenge**:\nDeep neural networks often suffer from the **vanishing gradient problem**, making early layers in the network extremely hard to train. Common activation functions like the sigmoid:\n\n$$\n\\sigma(x) = \\frac{1}{1 + e^{-x}}\n$$\n\nsuffer from saturation, where the gradient becomes almost zero if the input $x$ is large (positive or negative).\n\n**The AlexNet Solution**:\nIntroduce the **Rectified Linear Unit (ReLU)** {footcite}`nair2010rectified`:\n\n$$\n\\text{ReLU}(x) = \\max(0, x)\n$$\n\n```{figure} https://miro.medium.com/v2/resize:fit:474/1*HGctgaVdv9rEHIVvLYONdQ.jpeg\n:width: 100%\n:align: center\n\nSigmoid vs. ReLU activation functions. Sigmoid can cause gradients to vanish for $x$ far from zero, whereas ReLU maintains a constant gradient (1) for $x>0$.\n```\n\n- **Benefits**:\n  - Avoids vanishing gradients for positive $x$.\n  - **Computationally efficient**: Only a simple check if $x>0$.\n- **Drawback**:\n  - Neurons can \"die\" (always output zero) if $x$ stays negative. Variants like **Leaky ReLU** or **Parametric ReLU (PReLU)** introduce a small slope for $x \\leq 0$ to mitigate this.\n\n```{note}\nLeaky ReLU/PReLU are typically defined as:\n\n$$\nf(x) = \\begin{cases}\nx & \\text{if } x > 0 \\\\\n\\alpha x & \\text{if } x \\leq 0\n\\end{cases}\n$$\n\nwhere $\\alpha$ is a small positive constant.\n```\n\n### Dropout Regularization\n\nDeep networks with millions of parameters can easily **overfit** to the training data, harming their ability to generalize.\n\nThe AlexNet solution is to use **Dropout** {footcite}`srivastava2014dropout`, a technique that randomly disables (or \"drops out\") neurons with probability $p$ during training.\n\n```{figure} https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/1IrdJ5PghD9YoOyVAQ73MJw.gif\n:width: 100%\n:align: center\n\nDropout in action.\n```\n\n1. **Training**: Each neuron is \"dropped\" with probability $p$, forcing the network to not rely on any single neuron’s output.\n2. **Inference**: All neurons are used, but their outputs are scaled by $(1-p)$ to maintain expected values.\n\n```{note}\nAn alternative known as *inverse dropout* scales weights by $1/(1-p)$ during training, removing the need for scaling during inference. This is how many popular deep learning frameworks (e.g., TensorFlow) implement dropout.\n```\n\n---\n\n### 3. Local Response Normalization (LRN)\n\nIn CNNs, neighboring feature maps can become disproportionately large, leading to unstable or less discriminative representations.\n\nIn the AlexNet, this is rectified by **Local Response Normalization (LRN)** {footcite}`alexnet-lrn`, which normalizes activity across adjacent channels:\n\n$$\nb^i_{x,y} = \\frac{a^i_{x,y}}{\\Bigl(k + \\alpha \\sum_{j=\\max(0,i-\\frac{n}{2})}^{\\min(N-1,i+\\frac{n}{2})} (a^j_{x,y})^2\\Bigr)^\\beta}\n$$\n\nHere, \\(a^i_{x,y}\\) is the activation at channel \\(i\\), and \\(k, \\alpha, \\beta, n\\) are constants. LRN encourages local competition among adjacent channels, akin to certain neural mechanisms in biological systems.\n\n```{note}\nLRN is less commonly used in modern architectures (like VGG, ResNet, etc.) which often rely on batch normalization or other normalization techniques. Still, LRN was a key component in AlexNet’s success at the time.\n```\n\n---\n\n## The AlexNet Architecture\n\nNow that we've seen how AlexNet addressed vanishing gradients, overfitting, and feature-map normalization, let's look at the **overall blueprint**:\n\n```{figure} ../figs/alexnet-architecture.jpg\n:width: 50%\n:align: center\n\nA high-level view of the AlexNet architecture.\n```\n\n**Detailed Layer-by-Layer Overview**:\n\n1. **Input**: 3-channel color image, 224×224 pixels\n2. **Conv1**: [11×11, stride=4, padding=2] → ReLU → LRN → Overlapping Max Pool ([3×3], stride=2)\n3. **Conv2**: [5×5, stride=1, padding=2] → ReLU → LRN → Overlapping Max Pool ([3×3], stride=2)\n4. **Conv3**: [3×3, stride=1, padding=1] → ReLU\n5. **Conv4**: [3×3, stride=1, padding=1] → ReLU\n6. **Conv5**: [3×3, stride=1, padding=1] → ReLU → Overlapping Max Pool ([3×3], stride=2)\n7. **FC6**: Flatten → 9216 → 4096 → ReLU → Dropout\n8. **FC7**: 4096 → 4096 → ReLU → Dropout\n9. **FC8**: 4096 → 1000 → Softmax Output\n\n```{tip}\n**Parallel GPU Computation**\nAlexNet was trained on two GPUs with 3GB of memory each, splitting feature maps between them to handle the large parameter count. This approach showcased the necessity and practicality of GPU computing for large-scale deep learning.\n```\n\n---\n\n## Implementation Example\n\nBelow is a minimal snippet illustrating how one might instantiate an AlexNet-like model using PyTorch’s built-in module. For a step-by-step tutorial, check out\n[Writing AlexNet from Scratch in PyTorch | DigitalOcean](https://www.digitalocean.com/community/tutorials/alexnet-pytorch).\n\n```{code-cell} ipython3\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SimpleAlexNet(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(SimpleAlexNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            # LRN is omitted in modern PyTorch models, replaced by batch norm or left out\n            nn.MaxPool2d(kernel_size=3, stride=2),\n\n            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            # Another LRN placeholder if needed\n            nn.MaxPool2d(kernel_size=3, stride=2),\n\n            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n# Instantiate the model\nmodel = SimpleAlexNet(num_classes=1000)\nprint(model)\n```\n\nIn practice, PyTorch provides a pre-trained version of AlexNet via `torchvision.models.alexnet`. You can load it with:\n\n```{code-cell} ipython3\nimport torchvision.models as models\nalexnet = models.alexnet(pretrained=True)\n```\n\n```{footbibliography}\n:style: unsrt\n```\n\n",
    "supporting": [
      "alexnet_files"
    ],
    "filters": [],
    "includes": {}
  }
}
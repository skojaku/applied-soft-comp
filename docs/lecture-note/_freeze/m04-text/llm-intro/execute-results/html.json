{
  "hash": "674b9c7cdaff7e50c6161ae79436bcf3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Large Language Models in Practice\"\njupyter: applsoftcomp\nexecute:\n    enabled: true\n    cache: true\n---\n\n::: {.callout-note appearance=\"minimal\"}\n## Spoiler\nLarge language models don't understand language---they compress statistical regularities from billions of text samples into probability distributions that generate fluent outputs correlated with truth but not guaranteed to be true.\n:::\n\n## The Mechanism\n\nWhat do you think about the following question?\n\n> Can LLMs understand the world and reason about it?\n\nOne may argue that \"fluency\" demonstrates understanding. This is the intuition behind Turing's 1950 test: if you can't tell it's a machine, treat it as intelligent. Fluency implies comprehension.\n\nBut let's see some counter arguments about this claim, starting with ELIZA.\n\nELIZA, developed by Joseph Weizenbaum in the mid-1960s, is widely considered one of the first chatbots. It simulated a Rogerian psychotherapist by using simple pattern matching and keyword substitution to generate responses. Despite its lack of true understanding, ELIZA famously convinced many users that they were conversing with an intelligent entity, highlighting the human tendency to anthropomorphize technology and the limitations of the Turing Test.\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/8jGpkdPO-1Y?si=MRGPj2SRvEL-ROW_\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n\nAnother argument against fluency is the Chinese Room argument, proposed by philosopher John Searle. Imagine a person in a room who receives Chinese characters, and using an English rulebook, manipulates these symbols to produce new Chinese characters. To an outside observer, it appears the room understands Chinese. However, the person inside merely follows instructions to manipulate symbols without understanding their meaning. Searle argues that this is analogous to how computers, including LLMs, operate: they process symbols based on rules without genuine comprehension, raising questions about whether they can truly \"understand\" language or the world.\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/TryOC83PH1g?si=XGuLMklRle-quAia\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n\nSo, do LLMs understand the world? Probably not, in the same way as we do. LLMs is a lossy compression algorithm, compressing the data into their parameters to generate fluent outputs. To predict \"The capital of France is ___,\" the model must compress not just the fact (Paris) but the **statistical regularities** governing how facts appear in text---that capitals follow \"The capital of,\" that France is a country, that countries have capitals. This compression is probabilistic, not factual. The model stores $P(\\text{word}_{n+1} \\mid \\text{word}_1, \\ldots, \\text{word}_n)$---which words tend to follow which other words in which contexts. Just as the lottery memorizer stores patterns of number sequences, the LLM stores patterns of word sequences.\n\n![](../figs/llm-next-word.gif){width=50% fig-alt=\"A GIF showing how LLMs predict the next word by estimating probability distributions over the vocabulary.\" fig-align=\"center\"}\n\nTraining feeds the model billions of sentences. For each sentence, the model predicts the next word, compares its prediction to the actual next word, and adjusts its parameters to increase the probability of the correct word. Repeat trillions of times. The result: a compressed representation of how language behaves statistically. The model doesn't learn \"Paris is the capital of France\" as a fact; it learns that in contexts matching the pattern `[The capital of France is]`, the token \"Paris\" appears with high probability. The lottery memorizer doesn't understand what draws mean; it just knows what patterns appear most often. This is why LLMs creates **hallucination**---fluent but false outputs. Truth and fluency correlate in the training data, so the model is mostly truthful. But in the tails---obscure topics, recent events, precise recall---fluency diverges from truth, and the model follows fluency.\n\nKeep this limitation in mind and use LLMs as a tool to scale pattern recognition, not judgment. Let's learn how to utilize them.\n\n## Setting Up Ollama\n\nFor this course, we use Ollama, a tool for running LLMs locally, with Gemma 3N, a 4-billion parameter open-source model. Free, private, capable enough for research tasks. Visit [ollama.ai](https://ollama.ai), download the installer, and verify installation:\n\n```bash\nollama --version\nollama pull gemma3n:latest\nollama run gemma3n:latest \"What is a complex system?\"\n```\n\nIf you receive a coherent response, install the Python client and send your first prompt:\n\n```bash\npip install ollama\n```\n\n::: {#cf10a394 .cell execution_count=2}\n``` {.python .cell-code}\nimport ollama\n\nparams_llm = {\"model\": \"gemma3n:latest\", \"options\": {\"temperature\": 0.3}}\n\nresponse = ollama.generate(\n    prompt=\"Explain emergence in two sentences.\",\n    **params_llm\n)\n\nprint(response.response)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEmergence is when complex patterns and behaviors arise from simple interactions between individual components in a system. These emergent properties are not predictable from the properties of the individual parts alone, representing a novel level of organization. \n\n```\n:::\n:::\n\n\nRun this code twice. You'll get different outputs. Why? Because LLMs sample from probability distributions. The **temperature** parameter controls this randomness---lower values (0.1) make outputs more deterministic; higher values (1.0) increase diversity. You're controlling how far into the tail of the probability distribution the model samples. Low temperature: the model picks the most likely next word. High temperature: it ventures into less probable territory. Sometimes that produces creativity. Sometimes it produces nonsense.\n\n## Research Applications\n\nThe strategy is simple: use LLMs for tasks where speed trumps precision, then verify the outputs that matter. Three workflows demonstrate this pattern.\n\n**Abstract summarization.** You collected 50 papers on network science. Which deserve detailed reading? You don't have time to read all 50 abstracts carefully. An LLM scans them in seconds:\n\n::: {#f9ef170c .cell execution_count=3}\n``` {.python .cell-code}\nabstract = \"\"\"\nCommunity detection in networks is a fundamental problem in complex systems.\nWhile many algorithms exist, most assume static networks. We propose a dynamic\ncommunity detection algorithm that tracks evolving communities over time using\na temporal smoothness constraint. We evaluate our method on synthetic and real\ntemporal networks, showing it outperforms static methods applied to temporal\nsnapshots. Our approach reveals how communities merge, split, and persist in\nsocial networks, biological systems, and transportation networks.\n\"\"\"\n\nprompt = f\"Summarize this abstract in one sentence:\\n\\n{abstract}\"\nresponse = ollama.generate(prompt=prompt, **params_llm)\nprint(response.response)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThis paper introduces a novel dynamic community detection algorithm that effectively tracks evolving communities in networks over time, outperforming static methods and revealing community dynamics in various real-world systems.\n\n\n\n\n```\n:::\n:::\n\n\nThe model captures the pattern: propose method, evaluate, outperform baselines. It doesn't understand the paper; it has seen enough academic abstracts to recognize the structure. For multiple abstracts, loop through them:\n\n::: {#285be14f .cell execution_count=4}\n``` {.python .cell-code}\nfor i, abstract in enumerate([\"Abstract 1...\", \"Abstract 2...\"], 1):\n    response = ollama.generate(prompt=f\"Summarize:\\n\\n{abstract}\", **params_llm)\n    print(f\"{i}. {response.response}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1. Please **provide me with Abstract 1**! I need the text of the abstract to be able to summarize it for you. \n\nJust paste the abstract here, and I'll give you a concise summary. ðŸ˜Š \n\nI'm ready when you are!\n\n\n\n\n2. Please provide me with the content of \"Abstract 2\"! I need the text of the abstract to be able to summarize it for you. \n\nJust paste the abstract here, and I'll do my best to give you a concise and accurate summary. ðŸ˜Š \n\n\n```\n:::\n:::\n\n\nLocal models are slow (2â€“5 seconds per abstract). For thousands of papers, switch to cloud APIs. But the workflow scales: **delegate skimming to the model, retain judgment for yourself.** I ran this on 200 abstracts about power-law distributions. Gemma flagged the 15 that used preferential attachment models. Saved me 4 hours. I still read all 15 myself.\n\n**Structured extraction.** Turn unstructured text into structured data automatically:\n\n::: {#80b235ed .cell execution_count=5}\n``` {.python .cell-code}\nabstract = \"\"\"\nWe analyze scientific collaboration networks using 5 million papers from\n2000-2020. Using graph neural networks and community detection, we identify\ndisciplinary boundaries and interdisciplinary bridges. Interdisciplinarity\nincreased 25%, with physics and CS showing strongest cross-connections.\n\"\"\"\n\nprompt = f\"\"\"Extract: Domain, Methods, Key Finding\\n\\n{abstract}\\n\\nFormat:\\nDomain:...\\nMethods:...\\nKey Finding:...\"\"\"\nresponse = ollama.generate(prompt=prompt, **params_llm)\nprint(response.response)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHere's the extraction in the requested format:\n\nDomain: Scientific Collaboration Networks\nMethods: Graph Neural Networks, Community Detection, Analysis of 5 million papers (2000-2020)\nKey Finding: Interdisciplinarity increased by 25% between 2000-2020, with the strongest cross-connections observed between Physics and Computer Science.\n\n\n\n\n```\n:::\n:::\n\n\nScale this to hundreds of papers for meta-analysis. Always verify---LLMs misinterpret obscure terminology and fabricate plausible-sounding technical details when uncertain. Remember: the model is pattern-matching against academic writing it's seen, not reasoning about your domain.\n\n**Hypothesis generation.** LLMs pattern-match against research questions they've encountered in training data:\n\n::: {#4f1ed389 .cell execution_count=6}\n``` {.python .cell-code}\ncontext = \"\"\"I study concept spread in citation networks. Highly cited papers\ncombine existing concepts novelty. What should I study next?\"\"\"\n\nprompt = f\"\"\"Suggest three follow-up research questions:\\n\\n{context}\"\"\"\nresponse = ollama.generate(prompt=prompt, **params_llm)\nprint(response.response)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOkay, that's a great starting point! You're investigating how highly cited papers leverage existing concepts while introducing novelty. Here are three follow-up research questions, building on that foundation, with explanations of why they're interesting and potentially fruitful:\n\n**1.  How does the *nature* of the novelty in highly cited papers differ from less cited papers?**\n\n*   **Why it's interesting:**  This question delves deeper into *what kind* of novelty is being introduced. Is it incremental (small changes to existing concepts), radical (completely new concepts), or a combination?  Understanding the type of novelty could reveal patterns in how highly cited papers are structured and framed.\n*   **Potential approaches:**\n    *   **Concept Extraction & Categorization:**  Use NLP techniques (e.g., topic modeling, named entity recognition, knowledge graph extraction) to identify and categorize the concepts discussed in papers.  Then, analyze the novelty of these concepts (e.g., using measures of semantic distance from existing concepts, or by comparing to a knowledge base).\n    *   **Manual Coding:**  A smaller, more in-depth analysis could involve manually coding a subset of papers to categorize the type of novelty (e.g., \"extension,\" \"recombination,\" \"paradigm shift\").\n    *   **Sentiment Analysis:** Analyze the sentiment associated with novel concepts. Are highly cited papers more likely to frame novelty in a positive or impactful way?\n*   **Expected Outcomes:**  You might find that highly cited papers tend to introduce novelty that builds upon established frameworks, or that they are more likely to introduce truly disruptive concepts.\n\n**2.  What role do interdisciplinary citations play in the concept spread of highly cited papers?**\n\n*   **Why it's interesting:** Highly cited papers often bridge disciplines.  Interdisciplinary citations could be a key mechanism for integrating existing concepts from different fields and generating novel insights.  This question explores the *source* of the concepts being combined.\n*   **Potential approaches:**\n    *   **Citation Network Analysis:**  Analyze the citation network to identify the proportion of citations to papers from different disciplines.  Then, correlate this with the novelty of the concepts discussed in the highly cited papers.\n    *   **Concept Mapping Across Disciplines:**  Identify concepts that are borrowed from multiple disciplines and track their spread through the citation network.\n    *   **Network Visualization:** Visualize the citation network, highlighting interdisciplinary connections and the flow of concepts between disciplines.\n*   **Expected Outcomes:**  You might find that highly cited papers are more likely to cite papers from multiple disciplines, and that these interdisciplinary citations are associated with higher levels of concept novelty.\n\n**3.  How does the framing of novelty (e.g., through metaphors, analogies, or narrative structures) influence the impact and spread of concepts in highly cited papers?**\n\n*   **Why it's interesting:**  The way a concept is presented can significantly affect its reception and adoption.  This question explores the *communication* of novelty.\n*   **Potential approaches:**\n    *   **Text Analysis:**  Use NLP techniques to identify and analyze the use of metaphors, analogies, and narrative structures in the text of highly cited papers.\n    *   **Qualitative Analysis:**  Manually examine a subset of papers to identify examples of how novelty is framed and discussed.\n    *   **Sentiment Analysis (again):**  Analyze the sentiment associated with the framing of novelty.  Is it presented as exciting, challenging, or controversial?\n*   **Expected Outcomes:**  You might find that highly cited papers are more likely to use compelling narratives or metaphors to frame novelty, making it more accessible and impactful.\n\n\n\nThese questions are all interconnected and could be explored in combination.  They aim to move beyond simply identifying highly cited papers and delve into the *mechanisms* that contribute to their success in spreading new concepts.  Good luck! Let me know if you'd like me to elaborate on any of these.\n\n\n\n\n```\n:::\n:::\n\n\nTreat the model as a thought partner, not an oracle. It helps structure thinking but doesn't possess domain expertise. The suggestions reflect patterns in how research questions are framed, not deep knowledge of your field.\n\n## Failure Modes and Boundaries\n\nThe failure modes follow directly from the mechanism. LLMs fabricate plausibly because they optimize for fluency, not truth. Ask about a non-existent \"Smith et al. quantum paper\" and receive fluent academic prose describing results that never happened. Always verify citations. The model has seen thousands of papers cited in the format \"Smith et al. (2023) demonstrated that...\" and generates outputs matching that pattern even when the citation is fictional.\n\nContext limits are architectural. Models see only 2,000â€“8,000 tokens at once. Paste 100 abstracts and early ones are mathematically evicted from working memory. The model doesn't \"remember\" them; they're gone. Knowledge cutoffs are temporal. Gemma 3N's training ended early 2024. Ask about recent events and receive outdated information or plausible fabrications constructed from pre-cutoff patterns.\n\nReasoning is absent. LLMs pattern-match, they don't reason. Ask \"How many r's in 'Strawberry'?\" and the model might answer correctly via pattern matching against similar questions in training data, not by counting letters. Sometimes right. Often wrong. The model has no internal representation of what counting means.\n\nThese aren't bugs to be fixed. They're intrinsic to the architecture. Use LLMs to accelerate work, not replace judgment. They excel at summarizing text, extracting structure, reformulating concepts, brainstorming, generating synthetic examples, and translation. They fail at literature reviews without verification, factual claims without sources, statistical analysis, and ethical decisions. Harvest the center of the distribution where fluency and truth correlate. Defend against the tails where they diverge.\n\n## Next\n\nYou've seen LLMs in practice---setup, summarization, extraction, limitations. But how do they actually work? What happens inside when you send a prompt?\n\nThe rest of this module unboxes the technology: **prompt engineering** (communicating with LLMs), **embeddings** (representing meaning as numbers), **transformers** (the architecture enabling modern NLP), **fundamentals** (from word counts to neural representations).\n\nFirst, let's master talking to machines.\n\n",
    "supporting": [
      "llm-intro_files"
    ],
    "filters": [],
    "includes": {}
  }
}
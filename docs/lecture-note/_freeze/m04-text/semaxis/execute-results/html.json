{
  "hash": "afb142c9f7e44a5298ad65f6ac51c2fc",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"SemAxis: Meaning as Direction\"\nexecute:\n    enabled: true\n---\n\n::: {.callout-note title=\"What you'll learn in this module\"}\nMeaning in embeddings emerges entirely from contrast, not from inherent word properties. SemAxis provides a framework for defining semantic dimensions by subtracting antonym vectors, isolating specific axes that reveal how words align on dimensions like sentiment, intensity, or any conceptual opposition.\n:::\n\n## Embedding Space as Contrast\n\nWe intuitively treat word embeddings as static maps where \"king\" is simply near \"queen\". We assume the meaning is inherent to the coordinate itself, much like a city has a fixed latitude and longitude. This is a convenient fiction. In embedding space, meaning emerges entirely from contrast, which is the key concept of SemAxis.\n\nSemAxis is a way to define a semantic axis by subtracting the vector of an antonym from a word (e.g., $v_{good} - v_{bad}$). This isolates a semantic dimension, an \"axis\", that ignores all other information.\n\nFormally, given two pole words $w_+$ and $w_-$, the axis is defined as:\n\n$$\nv_{\\text{axis}} = \\frac{v_{w_+} - v_{w_-}}{||v_{w_+} - v_{w_-}\\||_2}\n$$\n\nwhere the denominator is the $L_2$ norm of the difference vector that ensures the axis vector $v_{\\text{axis}}$ is a unit vector. Using this \"ruler\", we project words into this axis. Operationally, the position of a word $w$ is given by the cosine similarity between $v_{w}$ and $v_{\\text{axis}}$.\n\n$$\n\\text{Position of w on axis } v_{\\text{axis}} = \\cos(v_{\\text{axis}},v_{w})\n$$\n\nLet's build a \"Sentiment Compass\" to measure the emotional charge of words that aren't explicitly emotional.\n\nFirst, we load the standard GloVe embeddings.\n\n::: {#6e6cb7b2 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gensim.downloader as api\n\n# Download and load pre-trained GloVe embeddings\nmodel = api.load(\"glove-wiki-gigaword-100\")\n```\n:::\n\n\n## Defining the Axis\n\nWe define the axis not as a point, but as the difference vector between two poles. This vector points from \"bad\" to \"good\".\n\n::: {#0892f582 .cell execution_count=2}\n``` {.python .cell-code}\ndef create_axis(pos_word, neg_word, model):\n    return model[pos_word] - model[neg_word]\n\n\n# The \"Sentiment\" Axis\nsentiment_axis = create_axis(\"good\", \"bad\", model)\n```\n:::\n\n\n## Measuring Alignment\n\nTo see where a word falls on this axis, we project it. Mathematically, this is the dot product (normalized). If the vector points in the same direction, the score is positive. If it points away, it is negative.\n\n::: {#ed7abf5d .cell execution_count=3}\n``` {.python .cell-code}\ndef get_score(word, axis, model):\n    v_word = model[word]\n    # Cosine similarity is just a normalized dot product\n    return np.dot(v_word, axis) / (np.linalg.norm(v_word) * np.linalg.norm(axis))\n\n\nwords = [\"excellent\", \"terrible\", \"mediocre\", \"stone\", \"flower\"]\nfor w in words:\n    print(f\"{w}: {get_score(w, sentiment_axis, model):.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nexcellent: 0.523\nterrible: -0.208\nmediocre: -0.001\nstone: 0.181\nflower: 0.204\n```\n:::\n:::\n\n\n## Robustness via Centroids\n\nSingle words are noisy. \"Bad\" might carry connotations of \"naughty\" or \"poor quality\". To fix this, we don't use single words. We use the centroid of a cluster of synonyms. This averages out the noise and leaves only the pure semantic signal.\n\n::: {#273ed77d .cell execution_count=4}\n``` {.python .cell-code}\ndef create_robust_axis(pos_word, neg_word, model, k=5):\n    # Get k nearest neighbors for both poles\n    pos_group = [pos_word]\n    pos_words = model.most_similar(pos_word, topn=k)\n    for word, _ in pos_words:\n        pos_group.append(word)\n\n    neg_group = [neg_word]\n    neg_words = model.most_similar(neg_word, topn=k)\n    for word, _ in neg_words:\n        neg_group.append(word)\n\n    # Average them to find the centroid\n    pos_vec = np.mean([model[w] for w in pos_group], axis=0)\n    neg_vec = np.mean([model[w] for w in neg_group], axis=0)\n\n    return pos_vec - neg_vec\n\n\nrobust_axis = create_robust_axis(\"good\", \"bad\", model)\n```\n:::\n\n\n## The 2D Semantic Space\n\nThe real power comes when we cross two axes. By plotting words against \"Sentiment\" and \"Intensity\" (Strong vs. Weak), we reveal relationships that a single list hides.\n\n::: {#4289ee0f .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\"}\ndef plot_2d(words, axis_x, axis_y, model):\n    x_scores = [get_score(w, axis_x, model) for w in words]\n    y_scores = [get_score(w, axis_y, model) for w in words]\n\n    plt.figure(figsize=(5, 5))\n    plt.scatter(x_scores, y_scores)\n\n    for i, w in enumerate(words):\n        plt.annotate(\n            w,\n            (x_scores[i], y_scores[i]),\n            xytext=(5, 5),\n            textcoords=\"offset points\",\n            fontsize=16,\n        )\n\n    plt.axhline(0, color=\"k\", alpha=0.3)\n    plt.axvline(0, color=\"k\", alpha=0.3)\n    plt.xlabel(\"Sentiment (Bad -> Good)\")\n    plt.ylabel(\"Intensity (Weak -> Strong)\")\n    plt.show()\n\n\nintensity_axis = create_axis(\"strong\", \"weak\", model)\ntest_words = [\n    \"excellent\",\n    \"terrible\",\n    \"mediocre\",\n    \"mild\",\n    \"extreme\",\n    \"murder\",\n    \"charity\",\n]\n\nplot_2d(test_words, sentiment_axis, intensity_axis, model)\n```\n\n::: {.cell-output .cell-output-display}\n![2D Semantic Space](semaxis_files/figure-html/cell-6-output-1.png){fig-align='center'}\n:::\n:::\n\n\n## The Key Insight\n\nTo define a concept, you must first define its opposite. Meaning isn't stored in the word itself. It lives in the contrast space, the relationship between poles that defines an axis. SemAxis operationalizes this principle: by defining opposition, we isolate the dimension that matters.\n\n",
    "supporting": [
      "semaxis_files"
    ],
    "filters": [],
    "includes": {}
  }
}
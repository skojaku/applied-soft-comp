{
  "hash": "0c0b5fd26f422ed80a2cf604ce03a8f6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Tokenization: Unboxing How LLMs Read Text\"\njupyter: applsoftcomp\nexecute:\n    enabled: true\n    cache: true\n---\n\n![](https://curator-production.s3.us.cloud-object-storage.appdomain.cloud/uploads/course-v1:IBMSkillsNetwork+GPXX0A7BEN+v1.jpg)\n\n**Spoiler:** LLMs don't read words—they read compressed fragments optimized for a probability engine.\n\n## The Mechanism (Why Subwords, Not Words)\n\nYou might assume that an LLM reads text the way you do: word by word, with each word treated as an atomic unit. This is wrong. The model operates on **tokens**—subword chunks that could be full words (\"the\"), word parts (\"ingham\"), or single characters (\"B\"). This choice is not arbitrary; it's a geometric compression strategy.\n\nIf we used whole words, the vocabulary would balloon to millions of entries. Each entry requires a row in the embedding matrix, meaning memory scales linearly with vocabulary size. A 1-million-word vocabulary with 2048-dimensional embeddings would require over 8GB just for the lookup table. Subword tokenization collapses this problem by focusing on frequently occurring fragments. With roughly 50,000 subwords, the model can reconstruct both common words (stored as single tokens) and rare words (assembled from parts). The system trades a small increase in sequence length for a massive reduction in memory and computational overhead.\n\nThis also explains why LLMs sometimes fail on seemingly trivial tasks like counting letters. The word \"strawberry\" might tokenize as `[\"straw\", \"berry\"]`, meaning the model never sees the individual \"r\" characters as separate units. It's not stupidity—it's compression artifacts.\n\n## The Application (How Tokenization Works in Practice)\n\nLet's unbox an actual tokenizer from Hugging Face and trace the pipeline from raw text to embeddings. We'll use **Phi-1.5**, a compact model from Microsoft. For tokenization experiments, we only need the tokenizer—no need to load the full multi-gigabyte model.\n\n::: {#d05620a6 .cell execution_count=1}\n``` {.python .cell-code}\nfrom transformers import AutoTokenizer\nimport os\n\nmodel_name = \"microsoft/phi-1.5\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n:::\n\n\nLet's inspect the tokenizer's constraints.\n\n::: {#960457e9 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\"}\nprint(f\"Vocabulary size: {tokenizer.vocab_size:,} tokens\")\nprint(f\"Max sequence length: {tokenizer.model_max_length} tokens\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVocabulary size: 50,257 tokens\nMax sequence length: 2048 tokens\n```\n:::\n:::\n\n\nThis tokenizer knows 50,257 unique tokens and enforces a maximum sequence length of 2048 tokens. If your input exceeds this limit, the model will truncate or reject it. This is a hard boundary imposed by the positional encoding system, not a soft guideline.\n\n### Text to Tokens\n\nTokenization splits text into the subword fragments the model actually processes. Watch what happens when we tokenize a university name.\n\n::: {#5ae9c93b .cell execution_count=3}\n``` {.python .cell-code}\ntext = \"Binghamton University.\"\n\ntokens = tokenizer.tokenize(text)\n```\n:::\n\n\n::: {#1148a6ef .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\"}\nprint(f\"Tokens: {tokens}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens: ['B', 'ingham', 'ton', 'ĠUniversity', '.']\n```\n:::\n:::\n\n\nThe rare word \"Binghamton\" fractures into `['B', 'ingham', 'ton']`. The common word \"University\" survives intact (with a leading space marker). The tokenizer learned these splits from frequency statistics during training. High-frequency words get dedicated tokens; rare words get decomposed into reusable parts.\n\n::: {.column-margin}\nThe `Ġ` character (U+0120) is a GPT-style tokenizer convention for encoding spaces. When you see `ĠUniversity`, it means \"University\" preceded by a space. This preserves word boundaries while allowing subword splits.\n:::\n\nLet's test a few more examples to see the pattern.\n\n::: {#e0988600 .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\"}\ntexts = [\n    \"Bearcats\",\n    \"New York\",\n]\n\nprint(\"Word tokenization examples:\\n\")\nfor text in texts:\n    tokens = tokenizer.tokenize(text)\n    print(f\"{text:10s} → {tokens}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWord tokenization examples:\n\nBearcats   → ['Bear', 'cats']\nNew York   → ['New', 'ĠYork']\n```\n:::\n:::\n\n\n\"Bearcats\" splits because it's domain-specific jargon. \"New York\" remains whole because it's common. The tokenizer's behavior is a direct reflection of its training corpus.\n\n::: {.column-margin}\nCheck out [OpenAI's tokenizer](https://platform.openai.com/tokenizer) to see how different models slice the same text differently.\n:::\n\n### Tokens to Token IDs\n\nTokens are still strings. The model needs integers. Each token maps to a unique ID in the vocabulary dictionary.\n\n::: {#a1ba2d9f .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\ntext = \"Binghamton University\"\n\n# Get token IDs\ntoken_ids = tokenizer.encode(text, add_special_tokens=False)\ntokens = tokenizer.tokenize(text)\n\nprint(\"Token → Token ID mapping:\\n\")\nfor token, token_id in zip(tokens, token_ids):\n    print(f\"{token:10s} → {token_id:6d}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nToken → Token ID mapping:\n\nB          →     33\ningham     →  25875\nton        →   1122\nĠUniversity →   2059\n```\n:::\n:::\n\n\nEach token receives a unique integer ID. The vocabulary is a dictionary: `{token_string: integer_id}`. Let's peek inside.\n\n::: {#6101b5e9 .cell execution_count=7}\n``` {.python .cell-code}\n# Get the full vocabulary\nvocab = tokenizer.get_vocab()\n\n# Sample some tokens\nsample_tokens = list(vocab.items())[:5]\nfor token, id in sample_tokens:\n    print(f\"  {id:6d}: '{token}'\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   43503: 'ĠLime'\n   29516: 'VO'\n   41002: 'ĠUTF'\n   41733: 'Ku'\n   33793: 'Ġindent'\n```\n:::\n:::\n\n\nMost LLMs reserve special tokens for sequence boundaries or control signals. Phi-1.5 uses `<|endoftext|>` as a separator during training. Let's verify.\n\n::: {#649866e6 .cell execution_count=8}\n``` {.python .cell-code}\ntoken_id = [50256]\ntoken = tokenizer.convert_ids_to_tokens(token_id)[0]\nprint(f\"Token ID: {token_id} → Token: {token}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nToken ID: [50256] → Token: <|endoftext|>\n```\n:::\n:::\n\n\nToken ID 50256 is Phi-specific. Other models use different conventions (e.g., BERT uses `[SEP]` and `[CLS]`). Always check your tokenizer's special tokens before preprocessing data.\n\n### Token IDs to Embeddings\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676309872/mirroredImages/pHPmMGEMYefk9jLeh/wegwbgiqyhig42gidlsg.png)\n\nNow we need the full model to access the embedding layer—the matrix that converts token IDs into dense vectors.\n\n::: {#bf344b64 .cell execution_count=9}\n``` {.python .cell-code}\nfrom transformers import AutoModelForCausalLM\nimport torch\n\n# Load the model\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Retrieve the embedding layer\nembedding_layer = model.model.embed_tokens\n```\n:::\n\n\nThe embedding layer is a simple lookup table: a 51,200 × 2,048 matrix where each row is the embedding for a token in the vocabulary. Let's examine the first few entries.\n\n::: {#1e0e61a9 .cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\"}\nprint(embedding_layer.weight[:5, :10])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[ 0.0097, -0.0155,  0.0603,  0.0326, -0.0108, -0.0271, -0.0273,  0.0178,\n         -0.0242,  0.0100],\n        [ 0.0243,  0.0543,  0.0178, -0.0679, -0.0130,  0.0265, -0.0423, -0.0287,\n         -0.0051, -0.0179],\n        [-0.0416,  0.0370, -0.0160, -0.0254, -0.0371, -0.0208, -0.0023,  0.0647,\n          0.0468,  0.0013],\n        [-0.0051, -0.0044,  0.0248, -0.0489,  0.0399,  0.0005, -0.0070,  0.0148,\n          0.0030,  0.0070],\n        [ 0.0289,  0.0362, -0.0027, -0.0775, -0.0136, -0.0203, -0.0566, -0.0558,\n          0.0269, -0.0067]], grad_fn=<SliceBackward0>)\n```\n:::\n:::\n\n\nThese numbers are learned parameters, optimized during training to capture semantic relationships. Token IDs are discrete symbols; embeddings are continuous coordinates in a 2048-dimensional space. This is what the transformer layers operate on.\n\n## The Bigger Picture\n\nYou've now traced the full pipeline: raw text fractures into subword tokens, tokens map to integer IDs, and IDs retrieve vector embeddings from a learned matrix. This tokenization step is foundational—without it, the model cannot begin processing language. The transformer layers come next, using attention mechanisms to extract patterns from these embeddings.\n\nRemember three constraints. First, LLMs operate on subwords, not words, because vocabulary size is a memory bottleneck. Second, tokenization is learned from data, not hand-designed, meaning different models will split text differently. Third, compression has side effects—tasks like character counting fail because the model never sees individual characters as atomic units.\n\nWith this machinery exposed, we're ready to examine the transformer itself—the architecture that processes these embeddings and enables LLMs to predict the next token.\n\n---\n\n**Next**: [Transformers: The Architecture Behind the Magic →](transformers.qmd)\n\n",
    "supporting": [
      "tokenization_files"
    ],
    "filters": [],
    "includes": {}
  }
}
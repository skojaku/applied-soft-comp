{
  "hash": "e065ace4d4a4a19c58708b91372d1dd3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Tokenization: Unboxing How LLMs Read Text\"\njupyter: python3\nexecute:\n    enabled: true\n    cache: true\n---\n\n::: {.callout-note title=\"What you'll learn in this module\"}\nLLMs don't read words as you do. They read compressed fragments called tokens, optimized for probability engines. This section explores why subword tokenization matters, how it works, and what it means for model behavior.\n:::\n\n![](https://curator-production.s3.us.cloud-object-storage.appdomain.cloud/uploads/course-v1:IBMSkillsNetwork+GPXX0A7BEN+v1.jpg)\n\n## Why Not Just Words?\n\nYou might assume an LLM reads text the way you do, word by word, treating each word as an atomic unit. This assumption is wrong. The model operates on tokens, which are subword chunks. These could be full words like \"the\", word parts like \"ingham\", or single characters like \"B\". This choice isn't arbitrary. It's a geometric compression strategy.\n\nIf we used whole words, the vocabulary would balloon to millions of entries. Each entry requires a row in the embedding matrix, so memory scales linearly with vocabulary size. A 1-million-word vocabulary with 2048-dimensional embeddings would require over 8GB just for the lookup table. Subword tokenization collapses this problem by focusing on frequently occurring fragments. With roughly 50,000 subwords, the model reconstructs both common words (stored as single tokens) and rare words (assembled from parts). The system trades a small increase in sequence length for massive reductions in memory and computational overhead.\n\nThis compression also explains a quirk: why LLMs sometimes fail at seemingly trivial tasks like counting letters. The word \"strawberry\" might tokenize as [\"straw\", \"berry\"], meaning the model never sees individual \"r\" characters as separate units. It's not stupidity, it's compression artifacts.\n\n## How Tokenization Works in Practice\n\nLet's unbox an actual tokenizer from Hugging Face and trace the pipeline from raw text to embeddings. We'll use Phi-1.5, a compact model from Microsoft. For tokenization experiments, we only need the tokenizer itself, not the full multi-gigabyte model.\n\n::: {#58700b05 .cell execution_count=2}\n``` {.python .cell-code}\nfrom transformers import AutoTokenizer\nimport os\n\nmodel_name = \"microsoft/phi-1.5\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n:::\n\n\nLet's inspect the tokenizer's constraints.\n\n::: {#c6320685 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\"}\nprint(f\"Vocabulary size: {tokenizer.vocab_size:,} tokens\")\nprint(f\"Max sequence length: {tokenizer.model_max_length} tokens\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVocabulary size: 50,257 tokens\nMax sequence length: 2048 tokens\n```\n:::\n:::\n\n\nThis tokenizer knows 50,257 unique tokens and enforces a maximum sequence length of 2048 tokens. If your input exceeds this limit, the model will truncate or reject it. This is a hard boundary imposed by the positional encoding system, not a soft guideline.\n\n### From Text to Tokens\n\nTokenization splits text into the subword fragments the model actually processes. Watch what happens when we tokenize a university name.\n\n::: {#051ecbb6 .cell execution_count=4}\n``` {.python .cell-code}\ntext = \"Binghamton University.\"\n\ntokens = tokenizer.tokenize(text)\n```\n:::\n\n\n::: {#0981980d .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\"}\nprint(f\"Tokens: {tokens}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens: ['B', 'ingham', 'ton', 'ĠUniversity', '.']\n```\n:::\n:::\n\n\nThe rare word \"Binghamton\" fractures into ['B', 'ingham', 'ton']. The common word \"University\" survives intact (with a leading space marker). The tokenizer learned these splits from frequency statistics during training. High-frequency words get dedicated tokens; rare words get decomposed into reusable parts.\n\nThe Ġ character (U+0120) is a GPT-style tokenizer convention for encoding spaces. When you see ĠUniversity, it means \"University\" preceded by a space. This preserves word boundaries while allowing subword splits.\n\nLet's test a few more examples to see the pattern.\n\n::: {#f7c24edf .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\ntexts = [\n    \"Bearcats\",\n    \"New York\",\n]\n\nprint(\"Word tokenization examples:\\n\")\nfor text in texts:\n    tokens = tokenizer.tokenize(text)\n    print(f\"{text:10s} → {tokens}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWord tokenization examples:\n\nBearcats   → ['Bear', 'cats']\nNew York   → ['New', 'ĠYork']\n```\n:::\n:::\n\n\n\"Bearcats\" splits because it's domain-specific jargon. \"New York\" remains whole because it's common. The tokenizer's behavior directly reflects its training corpus.\n\nCheck out [OpenAI's tokenizer](https://platform.openai.com/tokenizer) to see how different models slice the same text differently.\n\n### From Tokens to Token IDs\n\nTokens are still strings. The model needs integers. Each token maps to a unique ID in the vocabulary dictionary.\n\n::: {#3fd57f57 .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\"}\ntext = \"Binghamton University\"\n\n# Get token IDs\ntoken_ids = tokenizer.encode(text, add_special_tokens=False)\ntokens = tokenizer.tokenize(text)\n\nprint(\"Token → Token ID mapping:\\n\")\nfor token, token_id in zip(tokens, token_ids):\n    print(f\"{token:10s} → {token_id:6d}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nToken → Token ID mapping:\n\nB          →     33\ningham     →  25875\nton        →   1122\nĠUniversity →   2059\n```\n:::\n:::\n\n\nEach token receives a unique integer ID. The vocabulary is a dictionary mapping token strings to integer IDs. Let's peek inside.\n\n::: {#c9e521de .cell execution_count=8}\n``` {.python .cell-code}\n# Get the full vocabulary\nvocab = tokenizer.get_vocab()\n\n# Sample some tokens\nsample_tokens = list(vocab.items())[:5]\nfor token, id in sample_tokens:\n    print(f\"  {id:6d}: '{token}'\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   31818: 'Ġinconvenience'\n   39472: 'Ġunknow'\n   31083: 'Ġconcluding'\n   35540: 'Ġ;)'\n    5506: 'ĠAnn'\n```\n:::\n:::\n\n\nMost LLMs reserve special tokens for sequence boundaries or control signals. Phi-1.5 uses `<|endoftext|>` as a separator during training. Let's verify.\n\n::: {#87e65bfb .cell execution_count=9}\n``` {.python .cell-code}\ntoken_id = [50256]\ntoken = tokenizer.convert_ids_to_tokens(token_id)[0]\nprint(f\"Token ID: {token_id} → Token: {token}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nToken ID: [50256] → Token: <|endoftext|>\n```\n:::\n:::\n\n\nToken ID 50256 is Phi-specific. Other models use different conventions (BERT uses [SEP] and [CLS]). Always check your tokenizer's special tokens before preprocessing data.\n\n### From Token IDs to Embeddings\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1676309872/mirroredImages/pHPmMGEMYefk9jLeh/wegwbgiqyhig42gidlsg.png)\n\nNow we need the full model to access the embedding layer, the matrix that converts token IDs into dense vectors.\n\n::: {#1e6497fa .cell execution_count=10}\n``` {.python .cell-code}\nfrom transformers import AutoModelForCausalLM\nimport torch\n\n# Load the model\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Retrieve the embedding layer\nembedding_layer = model.model.embed_tokens\n```\n:::\n\n\nThe embedding layer is a simple lookup table: a 51,200 × 2,048 matrix where each row is the embedding for a token in the vocabulary. Let's examine the first few entries.\n\n::: {#14bfdea8 .cell execution_count=11}\n``` {.python .cell-code code-fold=\"true\"}\nprint(embedding_layer.weight[:5, :10])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[ 0.0097, -0.0155,  0.0603,  0.0326, -0.0108, -0.0271, -0.0273,  0.0178,\n         -0.0242,  0.0100],\n        [ 0.0243,  0.0543,  0.0178, -0.0679, -0.0130,  0.0265, -0.0423, -0.0287,\n         -0.0051, -0.0179],\n        [-0.0416,  0.0370, -0.0160, -0.0254, -0.0371, -0.0208, -0.0023,  0.0647,\n          0.0468,  0.0013],\n        [-0.0051, -0.0044,  0.0248, -0.0489,  0.0399,  0.0005, -0.0070,  0.0148,\n          0.0030,  0.0070],\n        [ 0.0289,  0.0362, -0.0027, -0.0775, -0.0136, -0.0203, -0.0566, -0.0558,\n          0.0269, -0.0067]], grad_fn=<SliceBackward0>)\n```\n:::\n:::\n\n\nThese numbers are learned parameters, optimized during training to capture semantic relationships. Token IDs are discrete symbols; embeddings are continuous coordinates in a 2048-dimensional space. This is what the transformer layers operate on.\n\n## The Full Pipeline\n\nYou've now traced the complete pipeline: raw text fractures into subword tokens, tokens map to integer IDs, and IDs retrieve vector embeddings from a learned matrix. This tokenization step is foundational. Without it, the model cannot begin processing language. The transformer layers come next, using attention mechanisms to extract patterns from these embeddings.\n\nRemember three key constraints. First, LLMs operate on subwords, not words, because vocabulary size is a memory bottleneck. Second, tokenization is learned from data, not hand-designed, so different models split text differently. Third, compression has side effects. Tasks like character counting fail because the model never sees individual characters as atomic units.\n\nWith this machinery exposed, we're ready to examine the transformer itself. It's the architecture that processes these embeddings and enables LLMs to predict the next token.\n\n---\n\n**Next**: [Transformers: The Architecture Behind the Magic](transformers.qmd)\n\n",
    "supporting": [
      "tokenization_files"
    ],
    "filters": [],
    "includes": {}
  }
}
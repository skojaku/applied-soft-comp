{
  "hash": "1bf6a70584ed43e10bca8b56eabb0545",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Word Bias\"\nexecute:\n    enabled: true\n---\n\n::: {.callout-note title=\"What you'll learn in this module\"}\nThis module explores how word embeddings capture and reinforce societal biases.\n\nYou'll learn:\n\n- How **semantic axes** reveal gender bias in occupations and concepts through geometric relationships.\n- How to **measure bias** using cosine similarity and the SemAxis framework.\n- The difference between **direct bias** (explicit gender associations) and **indirect bias** (gender encoded through proxy dimensions).\n- Why understanding these biases matters for building fair AI systems.\n:::\n\n## Understanding Bias in Word Embeddings\n\nHave you ever wondered what biases might be hiding in word embeddings? Word embeddings capture and reinforce societal biases from their training data through geometric relationships between word vectors, often reflecting stereotypes about gender, race, age, and other social factors.\n\nSemAxis is a powerful tool to analyze gender bias in word embeddings by measuring word alignments along semantic axes. Using antonym pairs like \"she-he\" as poles, it quantifies gender associations in words on a scale from -1 to 1, where positive values indicate feminine associations while negative values indicate masculine associations. Let's start with a simple example of analyzing gender bias in occupations.\n\n::: {#45b6564a .cell execution_count=1}\n``` {.python .cell-code}\nimport gensim.downloader as api\nimport numpy as np\n\n# Load pre-trained Word2vec embeddings\nmodel = api.load(\"word2vec-google-news-300\")\n```\n:::\n\n\n::: {#572dc89a .cell execution_count=2}\n``` {.python .cell-code}\ndef compute_bias(word, microframe, model):\n    word_vector = model[word]\n    numerator = np.dot(word_vector, microframe)\n    denominator = np.linalg.norm(word_vector) * np.linalg.norm(microframe)\n    return numerator / denominator\n\n\ndef analyze(word, pos_word, neg_word, model):\n    if word not in model:\n        return 0.0\n    microframe = model[pos_word] - model[neg_word]\n    bias = compute_bias(word, microframe, model)\n    return bias\n```\n:::\n\n\nWhat's happening in compute_bias? The function calculates the cosine similarity between a word vector and a semantic axis (microframe), where the numerator computes the dot product (projecting the word onto the axis) and the denominator normalizes by vector lengths to get a score between -1 and 1. Let's use these occupations:\n\n::: {#7dc1fbf0 .cell execution_count=3}\n``` {.python .cell-code}\n# Occupations from the paper\nshe_occupations = [\n    \"homemaker\",\n    \"nurse\",\n    \"receptionist\",\n    \"librarian\",\n    \"socialite\",\n    \"hairdresser\",\n    \"nanny\",\n    \"bookkeeper\",\n    \"stylist\",\n    \"housekeeper\",\n]\n\nhe_occupations = [\n    \"maestro\",\n    \"skipper\",\n    \"protege\",\n    \"philosopher\",\n    \"captain\",\n    \"architect\",\n    \"financier\",\n    \"warrior\",\n    \"broadcaster\",\n    \"magician\",\n    \"boss\",\n]\n```\n:::\n\n\nWe measure the gender bias in these occupations by measuring how they align with the \"she-he\" axis.\n\n::: {#2e551a25 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\"}\nprint(\"Gender Bias in Occupations (she-he axis):\")\nprint(\"\\nShe-associated occupations:\")\nfor occupation in she_occupations:\n    bias = analyze(occupation, \"she\", \"he\", model)\n    print(f\"{occupation}: {bias:.3f}\")\n\nprint(\"\\nHe-associated occupations:\")\nfor occupation in he_occupations:\n    bias = analyze(occupation, \"she\", \"he\", model)\n    print(f\"{occupation}: {bias:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGender Bias in Occupations (she-he axis):\n\nShe-associated occupations:\nhomemaker: 0.360\nnurse: 0.333\nreceptionist: 0.329\nlibrarian: 0.300\nsocialite: 0.310\nhairdresser: 0.307\nnanny: 0.287\nbookkeeper: 0.264\nstylist: 0.252\nhousekeeper: 0.260\n\nHe-associated occupations:\nmaestro: -0.203\nskipper: -0.177\nprotege: -0.148\nphilosopher: -0.155\ncaptain: -0.130\narchitect: -0.151\nfinancier: -0.145\nwarrior: -0.120\nbroadcaster: -0.124\nmagician: -0.110\nboss: -0.090\n```\n:::\n:::\n\n\nHow do we interpret these scores? Positive scores indicate closer association to \"she\" (e.g., nurse, librarian), while negative scores indicate closer association to \"he\" (e.g., architect, captain), with magnitude showing the strength of association. Notice how occupations historically associated with women have strong positive scores while those associated with men have negative scores, confirming that the model has learned these gender stereotypes from the text data.\n\n## Stereotype Analogies\n\nWhat happens when we look at word pairs? Since word embeddings capture semantic relationships from large text corpora, they inevitably encode societal biases and stereotypes. Let's measure how different words align with the gender axis (she-he) to find pairs where one word shows strong feminine bias while its counterpart shows masculine bias, revealing ingrained stereotypes in language use.\n\n::: {#940c2d19 .cell execution_count=5}\n``` {.python .cell-code}\n# Stereotype analogies from the paper\nstereotype_pairs = [\n    (\"sewing\", \"carpentry\"),\n    (\"nurse\", \"surgeon\"),\n    (\"softball\", \"baseball\"),\n    (\"cosmetics\", \"pharmaceuticals\"),\n    (\"vocalist\", \"guitarist\"),\n]\n```\n:::\n\n\n::: {#a0026c29 .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\nprint(\"\\nAnalyzing Gender Stereotype Pairs:\")\nfor word1, word2 in stereotype_pairs:\n    bias1 = analyze(word1, \"she\", \"he\", model)\n    bias2 = analyze(word2, \"she\", \"he\", model)\n    print(f\"\\n{word1} vs {word2}\")\n    print(f\"{word1}: {bias1:.3f}\")\n    print(f\"{word2}: {bias2:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nAnalyzing Gender Stereotype Pairs:\n\nsewing vs carpentry\nsewing: 0.302\ncarpentry: -0.028\n\nnurse vs surgeon\nnurse: 0.333\nsurgeon: -0.048\n\nsoftball vs baseball\nsoftball: 0.260\nbaseball: -0.066\n\ncosmetics vs pharmaceuticals\ncosmetics: 0.331\npharmaceuticals: -0.011\n\nvocalist vs guitarist\nvocalist: 0.140\nguitarist: -0.041\n```\n:::\n:::\n\n\nThe results show clear stereotypical alignments where sewing and nurse align with \"she\" while carpentry and surgeon align with \"he\", mirroring the \"man is to computer programmer as woman is to homemaker\" analogy found in early word embedding research.\n\n## Indirect Bias: When Neutral Words Become Gendered\n\nWhat about words that don't explicitly reference gender? Indirect bias occurs when seemingly neutral words become associated with gender through their relationships with other words. For example, while \"softball\" and \"football\" are not inherently gendered terms, they may show gender associations due to how they're used in language and society.\n\nWe can detect indirect bias by identifying word pairs that form a semantic axis (like softball-football), measuring how other words align with this axis, and examining if alignment correlates with gender bias. Let's see how this works by measuring the gender bias of these words:\n\n::: {#a5c98ce1 .cell execution_count=7}\n``` {.python .cell-code}\n# Words associated with softball-football axis\nsoftball_associations = [\n    \"pitcher\",\n    \"bookkeeper\",\n    \"receptionist\",\n    \"nurse\",\n    \"waitress\"\n]\n\nfootball_associations = [\n    \"footballer\",\n    \"businessman\",\n    \"pundit\",\n    \"maestro\",\n    \"cleric\"\n]\n\n# Calculate biases for all words\ngender_biases = []\nsports_biases = []\nwords = softball_associations + football_associations\n\nfor word in words:\n    gender_bias = analyze(word, \"she\", \"he\", model)\n    sports_bias = analyze(word, \"softball\", \"football\", model)\n    gender_biases.append(gender_bias)\n    sports_biases.append(sports_bias)\n```\n:::\n\n\nLet's plot the results:\n\n::: {#d9e0190a .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\"}\n# Analyze bias along both gender and sports axes\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom adjustText import adjust_text\n\n# Create scatter plot\nfig, ax = plt.subplots(figsize=(6, 6))\nsns.scatterplot(x=gender_biases, y=sports_biases, ax=ax)\nax.set_xlabel(\"Gender Bias (she-he)\")\nax.set_ylabel(\"Sports Bias (softball-football)\")\nax.set_title(\"Indirect Bias Analysis: Gender vs Sports\")\n\n# Add labels for each point\ntexts = []\nfor i, word in enumerate(words):\n    texts.append(ax.text(gender_biases[i], sports_biases[i], word, fontsize=12))\n\nadjust_text(texts, arrowprops=dict(arrowstyle='-', color='gray', lw=0.5))\n\nax.grid(True, alpha=0.3)\nplt.show()\nsns.despine()\n```\n\n::: {.cell-output .cell-output-display}\n![](word-bias_files/figure-html/cell-9-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n```\n<Figure size 672x480 with 0 Axes>\n```\n:::\n:::\n\n\nThe plot reveals a correlation where words associated with \"softball\" (y-axis greater than 0) also tend to be associated with \"she\" (x-axis greater than 0), while \"football\" terms align with \"he\". This suggests that even if we remove explicit gender words, the structure of the space still encodes gender through these proxy dimensions.\n\n## The Impact and Path Forward\n\nWord embeddings, while powerful, inevitably capture and reflect societal biases present in the large text corpora they are trained on. We observed both direct bias (where occupations or attributes align strongly with specific gender pronouns) and indirect bias (where seemingly neutral concepts become gendered through their associations with other words). This analysis highlights the importance of understanding and mitigating these biases to prevent the perpetuation of stereotypes in AI systems and ensure fairness in applications like search, recommendation, and hiring.\n\n",
    "supporting": [
      "word-bias_files"
    ],
    "filters": [],
    "includes": {}
  }
}
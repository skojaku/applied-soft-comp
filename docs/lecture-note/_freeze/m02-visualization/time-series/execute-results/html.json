{
  "hash": "b6ba8256cd8def9ebdcdbff02c462eeb",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Time Series Visualization\"\njupyter: advnetsci\nexecute:\n    enabled: true\n---\n\n::: {.callout-note title=\"What you'll learn in this module\"}\nThis module explores how to visualize time series data effectively, moving beyond simple line charts to reveal true underlying patterns. We will examine how choices in scale and geometry, such as small multiples, logarithmic axes, and lag plots, can either expose genuine trends or manufacture misleading narratives.\n:::\n\n## The nature of time\n\nLet's talk about time. In March 2020, charts of COVID-19 cases told vastly different stories depending on how they were visualized. Some used linear scales, showing a terrifying vertical wall. Others used log scales, showing a straight line. Politicians cherry-picked time windows to claim \"flattening curves.\"\n\nTime series data is special because it implies causality and momentum. Unlike other variables, time flows in one direction. Your choices of scale, aggregation, and geometry determine whether you reveal a genuine pattern or manufacture a misleading narrative.\n\n## Line plots and the continuity illusion\n\nThe most fundamental choice is whether to connect the dots. A line plot suggests continuity, implying that a value exists at every moment between your measurements. This works for temperature or stock prices, where the variable has momentum.\n\n::: {#0c0028b1 .cell fig-height='5' fig-width='12' execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set style\nsns.set_style(\"white\")\nsns.set(font_scale=1.2)\n\n# Generate synthetic time series with trend and seasonality\nnp.random.seed(42)\nn_points = 365\ndates = pd.date_range('2023-01-01', periods=n_points, freq='D')\ntrend = np.linspace(100, 150, n_points)\nseasonal = 10 * np.sin(2 * np.pi * np.arange(n_points) / 365 * 4)  # Quarterly seasonality\nnoise = np.random.normal(0, 3, n_points)\nvalues = trend + seasonal + noise\n\ndf = pd.DataFrame({'date': dates, 'value': values})\n\n# Create line plot\nfig, ax = plt.subplots(figsize=(12, 5))\nax.plot(df['date'], df['value'], linewidth=1.5, color=sns.color_palette()[0])\nax.set_xlabel('Date')\nax.set_ylabel('Value')\nax.set_title('Daily Time Series: Line Plot Shows Trend and Seasonality')\nax.grid(True, alpha=0.3)\nsns.despine()\n```\n\n::: {.cell-output .cell-output-display}\n![Basic line plot showing a time series with trend and seasonality](time-series_files/figure-html/cell-2-output-1.png){}\n:::\n:::\n\n\nBut what if your data is discrete? If you plot distinct sales events or email arrivals as a line, you create a false narrative of values existing in the gaps. In those cases, let the silence between points speak.\n\n::: {#139b51f4 .cell fig-height='5' fig-width='14' execution_count=2}\n``` {.python .cell-code code-fold=\"true\"}\n# Generate sparse discrete event data\nnp.random.seed(123)\nevent_dates = pd.to_datetime(['2023-01-15', '2023-03-10', '2023-05-22',\n                               '2023-07-08', '2023-09-30', '2023-11-15'])\nevent_values = np.random.randint(20, 80, len(event_dates))\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Line plot (implies continuity - misleading for discrete events)\naxes[0].plot(event_dates, event_values, marker='o', linewidth=2, markersize=8)\naxes[0].set_xlabel('Date')\naxes[0].set_ylabel('Event Count')\naxes[0].set_title('Line Plot: Implies Values Between Events (Misleading)')\naxes[0].grid(True, alpha=0.3)\n\n# Scatter plot (appropriate for discrete events)\naxes[1].scatter(event_dates, event_values, s=100, alpha=0.7)\naxes[1].set_xlabel('Date')\naxes[1].set_ylabel('Event Count')\naxes[1].set_title('Scatter Plot: Shows Only Observed Events (Honest)')\naxes[1].grid(True, alpha=0.3)\n\nfor ax in axes:\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![Line plot vs scatter plot: connecting points implies continuity](time-series_files/figure-html/cell-3-output-1.png){}\n:::\n:::\n\n\n## Comparing series: The spaghetti problem\n\nOften you need to compare multiple series. The natural instinct is to overlay them on the same plot. This works for two or three variables, but as the count rises, you fall into the spaghetti trap where individual trends get lost in the tangle.\n\n::: {#a116350c .cell fig-height='6' fig-width='12' execution_count=3}\n``` {.python .cell-code code-fold=\"true\"}\n# Generate three related time series\nnp.random.seed(42)\ndates = pd.date_range('2023-01-01', periods=200, freq='D')\n\nseries_a = 100 + np.linspace(0, 30, 200) + np.random.normal(0, 5, 200)\nseries_b = 95 + np.linspace(0, 20, 200) + np.random.normal(0, 4, 200)\nseries_c = 110 + np.linspace(0, 10, 200) + np.random.normal(0, 6, 200)\n\ndf_multi = pd.DataFrame({\n    'date': dates,\n    'Product A': series_a,\n    'Product B': series_b,\n    'Product C': series_c\n})\n\n# Overlay plot\nfig, ax = plt.subplots(figsize=(12, 6))\nfor column in ['Product A', 'Product B', 'Product C']:\n    ax.plot(df_multi['date'], df_multi[column], linewidth=2, label=column, alpha=0.8)\n\nax.set_xlabel('Date')\nax.set_ylabel('Sales')\nax.set_title('Multiple Time Series: Overlaid Comparison')\nax.legend(loc='upper left')\nax.grid(True, alpha=0.3)\nsns.despine()\n```\n\n::: {.cell-output .cell-output-display}\n![Multiple time series overlaid with different colors](time-series_files/figure-html/cell-4-output-1.png){}\n:::\n:::\n\n\nThe solution is small multiples (or faceting). By giving each series its own stage while locking the axes, you preserve both the individual trends and the ability to compare them.\n\n::: {#49d19be9 .cell fig-height='8' fig-width='14' execution_count=4}\n``` {.python .cell-code code-fold=\"true\"}\n# Generate multiple time series\nnp.random.seed(42)\nn_series = 6\ndates = pd.date_range('2023-01-01', periods=150, freq='D')\n\ndata_list = []\nfor i in range(n_series):\n    values = 50 + np.random.randn(150).cumsum() + 10 * np.sin(2 * np.pi * np.arange(150) / 30)\n    data_list.append(pd.DataFrame({\n        'date': dates,\n        'value': values,\n        'series': f'Region {i+1}'\n    }))\n\ndf_many = pd.concat(data_list, ignore_index=True)\n\n# Small multiples using seaborn FacetGrid\ng = sns.FacetGrid(df_many, col='series', col_wrap=3, height=3, aspect=1.5, sharey=True)\ng.map_dataframe(sns.lineplot, x='date', y='value', linewidth=2, color=sns.color_palette()[0])\ng.set_axis_labels('Date', 'Value')\ng.set_titles('Region {col_name}')\nfor ax in g.axes.flat:\n    ax.grid(True, alpha=0.3)\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![Small multiples avoid spaghetti plots when comparing many time series](time-series_files/figure-html/cell-5-output-1.png){}\n:::\n:::\n\n\n## The power of scale: Linear vs Log\n\nPerhaps the most consequential choice in time series visualization is the y-axis scale. Your choice defines the question you are answering. A linear scale asks \"How much did it increase?\" A log scale asks \"How fast is it growing?\"\n\nIn the example below, the linear scale suggests an explosive crisis at the end. The log scale reveals that the growth rate has been constant the entire time.\n\n::: {#29954f3a .cell fig-height='5' fig-width='14' execution_count=5}\n``` {.python .cell-code code-fold=\"true\"}\n# Generate exponential growth data (e.g., epidemic spread)\nnp.random.seed(42)\ndays = np.arange(0, 100)\ncases = 10 * np.exp(0.05 * days) * (1 + np.random.normal(0, 0.1, len(days)))\n\ndf_exp = pd.DataFrame({'day': days, 'cases': cases})\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Linear scale\naxes[0].plot(df_exp['day'], df_exp['cases'], linewidth=2, color=sns.color_palette()[0])\naxes[0].set_xlabel('Days')\naxes[0].set_ylabel('Cases')\naxes[0].set_title('Linear Scale: Exponential Growth Looks Explosive')\naxes[0].grid(True, alpha=0.3)\n\n# Log scale\naxes[1].plot(df_exp['day'], df_exp['cases'], linewidth=2, color=sns.color_palette()[1])\naxes[1].set_xlabel('Days')\naxes[1].set_ylabel('Cases (log scale)')\naxes[1].set_yscale('log')\naxes[1].set_title('Log Scale: Exponential Growth Appears Linear')\naxes[1].grid(True, alpha=0.3, which='both')\n\nfor ax in axes:\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![The same exponential growth looks different on linear vs. log scales](time-series_files/figure-html/cell-6-output-1.png){}\n:::\n:::\n\n\n::: {.callout-note title=\"Why Log Scales?\"}\nLog scales are essential for data spanning orders of magnitude or when percentage changes matter more than absolute units. However, they can downplay absolute magnitude. A jump from 100 to 1,000 looks the same as 10,000 to 100,000.\n:::\n\n## Smoothing and trends\n\nReal data is messy. Smoothing via moving averages mimics how we squint at a chart to blur out the details and see the trend.\n\nThe window size controls the trade-off. A small window keeps the texture, showing volatility. A large window reveals the structure, showing the trend.\n\n::: {#5223ac9e .cell fig-height='6' fig-width='12' execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\n# Generate noisy time series\nnp.random.seed(42)\ndates = pd.date_range('2023-01-01', periods=200, freq='D')\ntrend = 50 + 0.2 * np.arange(200)\nseasonal = 8 * np.sin(2 * np.pi * np.arange(200) / 30)\nnoise = np.random.normal(0, 5, 200)\nvalues = trend + seasonal + noise\n\ndf_noisy = pd.DataFrame({'date': dates, 'value': values})\n\n# Calculate moving averages\ndf_noisy['MA_7'] = df_noisy['value'].rolling(window=7, center=True).mean()\ndf_noisy['MA_30'] = df_noisy['value'].rolling(window=30, center=True).mean()\n\n# Plot\nfig, ax = plt.subplots(figsize=(12, 6))\nax.plot(df_noisy['date'], df_noisy['value'], linewidth=0.8, alpha=0.3, label='Raw Data', color='gray')\nax.plot(df_noisy['date'], df_noisy['MA_7'], linewidth=2, label='7-Day Moving Average', color=sns.color_palette()[0])\nax.plot(df_noisy['date'], df_noisy['MA_30'], linewidth=2, label='30-Day Moving Average', color=sns.color_palette()[1])\n\nax.set_xlabel('Date')\nax.set_ylabel('Value')\nax.set_title('Moving Averages Reveal Trends by Smoothing Noise')\nax.legend()\nax.grid(True, alpha=0.3)\nsns.despine()\n```\n\n::: {.cell-output .cell-output-display}\n![Moving averages smooth noise to reveal underlying trends](time-series_files/figure-html/cell-7-output-1.png){}\n:::\n:::\n\n\n## Showing uncertainty\n\nPredicting the future is an exercise in humility. A forecast without an error bar is a lie of precision. Use ribbon plots to visualize the widening cone of uncertainty as time moves forward.\n\n::: {#52703eb6 .cell fig-height='6' fig-width='12' execution_count=7}\n``` {.python .cell-code code-fold=\"true\"}\n# Generate data with trend\nnp.random.seed(42)\nn = 150\nx = np.arange(n)\ntrue_trend = 50 + 0.3 * x\nobserved = true_trend + np.random.normal(0, 5, n)\n\n# Simple linear forecast\nfrom scipy import stats\nslope, intercept, r_value, p_value, std_err = stats.linregress(x[:100], observed[:100])\n\n# Forecast period\nx_future = np.arange(100, 150)\ny_pred = slope * x_future + intercept\n\n# Estimate prediction interval (simplified)\nresiduals = observed[:100] - (slope * x[:100] + intercept)\nstd_residual = np.std(residuals)\nmargin = 1.96 * std_residual  # 95% prediction interval\n\n# Plot\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Historical data\nax.plot(x[:100], observed[:100], linewidth=2, label='Historical Data', color=sns.color_palette()[0])\n\n# Forecast with uncertainty\nax.plot(x_future, y_pred, linewidth=2, label='Forecast', color=sns.color_palette()[1], linestyle='--')\nax.fill_between(x_future, y_pred - margin, y_pred + margin,\n                alpha=0.3, color=sns.color_palette()[1], label='95% Prediction Interval')\n\n# Actual future (for comparison)\nax.plot(x_future, observed[100:], linewidth=1.5, alpha=0.5, label='Actual (for comparison)',\n        color='gray', linestyle=':')\n\nax.axvline(x=100, color='black', linestyle=':', alpha=0.5, label='Forecast Start')\nax.set_xlabel('Time')\nax.set_ylabel('Value')\nax.set_title('Time Series Forecast with Uncertainty Bands')\nax.legend()\nax.grid(True, alpha=0.3)\nsns.despine()\n```\n\n::: {.cell-output .cell-output-display}\n![Ribbon plots show uncertainty bands around predictions](time-series_files/figure-html/cell-8-output-1.png){}\n:::\n:::\n\n\n## The rhythm of time: Heatmaps and Cycles\n\nTime often cycles rather than marches. Heatmaps and cycle plots break the linear narrative to reveal the heartbeat of the data, such as daily lulls, weekend spikes, or seasonal waves.\n\n::: {#ef3089b9 .cell fig-height='8' fig-width='12' execution_count=8}\n``` {.python .cell-code code-fold=\"true\"}\n# Generate synthetic hourly data with daily and weekly patterns\nnp.random.seed(42)\nhours = pd.date_range('2023-01-01', periods=24*7*4, freq='H')  # 4 weeks\n\n# Patterns: higher activity during business hours and weekdays\nhour_of_day = hours.hour\nday_of_week = hours.dayofweek\n\n# Activity pattern\nbase_activity = 20\nhour_effect = 30 * np.exp(-((hour_of_day - 14)**2) / 20)  # Peak at 2 PM\nweekday_effect = np.where(day_of_week < 5, 20, -10)  # Weekdays higher\nnoise = np.random.normal(0, 5, len(hours))\n\nactivity = base_activity + hour_effect + weekday_effect + noise\n\ndf_hourly = pd.DataFrame({\n    'datetime': hours,\n    'activity': activity,\n    'hour': hour_of_day,\n    'day_name': hours.day_name(),\n    'week': (hours.day // 7) + 1\n})\n\n# Take first week for heatmap\ndf_week = df_hourly[df_hourly['week'] == 1].copy()\n\n# Pivot for heatmap\nheatmap_data = df_week.pivot_table(values='activity',\n                                     index='hour',\n                                     columns='day_name',\n                                     aggfunc='mean')\n\n# Reorder columns to start with Monday\nday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\nheatmap_data = heatmap_data[[day for day in day_order if day in heatmap_data.columns]]\n\n# Plot heatmap\nfig, ax = plt.subplots(figsize=(12, 8))\nsns.heatmap(heatmap_data, cmap='YlOrRd', annot=False, fmt='.0f',\n            cbar_kws={'label': 'Activity Level'}, ax=ax)\nax.set_xlabel('Day of Week')\nax.set_ylabel('Hour of Day')\nax.set_title('Temporal Heatmap: Activity by Hour and Day of Week')\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/j7/9dgqq5g53vnbsbmvh2yqtckr0000gr/T/ipykernel_7138/3637488205.py:3: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n  hours = pd.date_range('2023-01-01', periods=24*7*4, freq='H')  # 4 weeks\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Heat map reveals daily and weekly patterns in temporal data](time-series_files/figure-html/cell-9-output-2.png){}\n:::\n:::\n\n\n::: {#3e843bfc .cell fig-height='6' fig-width='14' execution_count=9}\n``` {.python .cell-code code-fold=\"true\"}\n# Generate monthly data with strong annual seasonality\nnp.random.seed(42)\nmonths = pd.date_range('2020-01-01', periods=48, freq='M')\nmonth_num = np.tile(np.arange(1, 13), 4)  # 4 years of monthly data\n\n# Seasonal pattern (higher in summer, lower in winter)\nseasonal_effect = 20 * np.sin(2 * np.pi * (month_num - 3) / 12)\ntrend_effect = 0.5 * np.arange(48)\nnoise = np.random.normal(0, 3, 48)\n\nvalues = 50 + seasonal_effect + trend_effect + noise\n\ndf_seasonal = pd.DataFrame({\n    'date': months,\n    'value': values,\n    'month': month_num,\n    'year': months.year,\n    'month_name': months.month_name()\n})\n\n# Create cycle plot\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Traditional time series\naxes[0].plot(df_seasonal['date'], df_seasonal['value'], marker='o', linewidth=2)\naxes[0].set_xlabel('Date')\naxes[0].set_ylabel('Value')\naxes[0].set_title('Traditional Time Series: Seasonality Repeats')\naxes[0].grid(True, alpha=0.3)\n\n# Cycle plot\nmonth_names_short = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n                     'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\nfor year in df_seasonal['year'].unique():\n    year_data = df_seasonal[df_seasonal['year'] == year]\n    axes[1].plot(year_data['month'], year_data['value'], marker='o',\n                linewidth=2, label=str(year), alpha=0.7)\n\naxes[1].set_xlabel('Month')\naxes[1].set_ylabel('Value')\naxes[1].set_xticks(range(1, 13))\naxes[1].set_xticklabels(month_names_short)\naxes[1].set_title('Cycle Plot: Each Year Overlaid to Show Seasonal Pattern')\naxes[1].legend(title='Year')\naxes[1].grid(True, alpha=0.3)\n\nfor ax in axes:\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/j7/9dgqq5g53vnbsbmvh2yqtckr0000gr/T/ipykernel_7138/3601020590.py:3: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n  months = pd.date_range('2020-01-01', periods=48, freq='M')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Cycle plot reveals seasonal patterns by separating each cycle](time-series_files/figure-html/cell-10-output-2.png){}\n:::\n:::\n\n\n## The memory of the past: Autocorrelation\n\nDoes the past predict the future? Lag plots visualize the system's memory by plotting $x_t$ against $x_{t-1}$. A tight diagonal implies strong memory (autocorrelation) while a scattered cloud implies random noise.\n\n::: {#d78b153d .cell fig-height='5' fig-width='14' execution_count=10}\n``` {.python .cell-code code-fold=\"true\"}\n# Generate time series with autocorrelation\nnp.random.seed(42)\nn = 200\n\n# AR(1) process: strong autocorrelation\nar_series = np.zeros(n)\nar_series[0] = np.random.normal(0, 1)\nfor i in range(1, n):\n    ar_series[i] = 0.7 * ar_series[i-1] + np.random.normal(0, 1)\n\n# Random walk: perfect autocorrelation at lag 1\nrandom_walk = np.random.normal(0, 1, n).cumsum()\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Lag-1 plot for AR(1) series\naxes[0].scatter(ar_series[:-1], ar_series[1:], alpha=0.6, s=30)\naxes[0].set_xlabel('Value at time t')\naxes[0].set_ylabel('Value at time t+1')\naxes[0].set_title('Lag-1 Plot: Strong Autocorrelation (AR Process)')\naxes[0].plot([-3, 3], [-3, 3], 'r--', alpha=0.5, linewidth=1)\naxes[0].grid(True, alpha=0.3)\n\n# Lag-1 plot for random walk\naxes[1].scatter(random_walk[:-1], random_walk[1:], alpha=0.6, s=30, color=sns.color_palette()[1])\naxes[1].set_xlabel('Value at time t')\naxes[1].set_ylabel('Value at time t+1')\naxes[1].set_title('Lag-1 Plot: Perfect Autocorrelation (Random Walk)')\naxes[1].plot([random_walk.min(), random_walk.max()],\n            [random_walk.min(), random_walk.max()], 'r--', alpha=0.5, linewidth=1)\naxes[1].grid(True, alpha=0.3)\n\nfor ax in axes:\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![Lag plots reveal autocorrelation structure in time series](time-series_files/figure-html/cell-11-output-1.png){}\n:::\n:::\n\n\n::: {.callout-note title=\"Summary\"}\nTime series visualization is about making choices that honestly represent temporal patterns. By following these principled visualization practices, you ensure your temporal data tells its true story, not the story you wish it told.\n:::\n\n",
    "supporting": [
      "time-series_files"
    ],
    "filters": [],
    "includes": {}
  }
}
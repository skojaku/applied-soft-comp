{
  "hash": "17f3921e5d008ec306704bfa21b10a51",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Time Series Visualization\"\njupyter: advnetsci\nexecute:\n    enabled: true\n---\n\nIn March 2020, news outlets worldwide showed charts of COVID-19 cases rising exponentially. Some charts showed linear y-axes with curves shooting upward dramatically. Others used logarithmic y-axes where the same data appeared as straight lines. Politicians cherry-picked time windows to show \"flattening curves.\" The same data told vastly different stories depending on how it was visualized.\n\nOr consider stock market charts: show the last month, and a 10% drop looks catastrophic. Zoom out to show the last decade, and the same drop becomes a minor blip barely visible on the chart.\n\nTime series data\u0014observations ordered by time\u0014is everywhere. But time is special. Unlike other variables, it flows in one direction, has natural rhythms (daily, seasonal, cyclical), and carries momentum. **Your visualization choices can reveal genuine patterns or create misleading narratives.**\n\nThe key principle to keep in mind:\n\n**Time is special\u0014show how your data changes over time honestly and clearly.**\n\n# Why Time Series Visualization Matters\n\nTime series visualizations are perhaps the most common type of chart in news media, scientific papers, and business dashboards. They answer fundamental questions: Is this trend going up or down? Are there cycles? When did something change?\n\nBut they're also easy to manipulate. By selecting the time window, changing the y-axis scale, or choosing different aggregation levels, the same data can support contradictory conclusions.\n\nConsider these common pitfalls:\n- **Truncated y-axes** that exaggerate small changes\n- **Cherry-picked time windows** that hide long-term trends\n- **Inappropriate scales** (linear vs. log) that obscure or inflate patterns\n- **Over-smoothing** that removes real variation\n- **Under-smoothing** that shows only noise\n\nGood time series visualization is about making honest choices that reveal the actual patterns in your data.\n\n# Basic Time Series: Line Plots\n\nThe most fundamental time series visualization is the **line plot**: time on the x-axis, values on the y-axis, points connected by lines.\n\n::: {#3830c394 .cell fig-height='5' fig-width='12' execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set style\nsns.set_style(\"white\")\nsns.set(font_scale=1.2)\n\n# Generate synthetic time series with trend and seasonality\nnp.random.seed(42)\nn_points = 365\ndates = pd.date_range('2023-01-01', periods=n_points, freq='D')\ntrend = np.linspace(100, 150, n_points)\nseasonal = 10 * np.sin(2 * np.pi * np.arange(n_points) / 365 * 4)  # Quarterly seasonality\nnoise = np.random.normal(0, 3, n_points)\nvalues = trend + seasonal + noise\n\ndf = pd.DataFrame({'date': dates, 'value': values})\n\n# Create line plot\nfig, ax = plt.subplots(figsize=(12, 5))\nax.plot(df['date'], df['value'], linewidth=1.5, color=sns.color_palette()[0])\nax.set_xlabel('Date')\nax.set_ylabel('Value')\nax.set_title('Daily Time Series: Line Plot Shows Trend and Seasonality')\nax.grid(True, alpha=0.3)\nsns.despine()\n```\n\n::: {.cell-output .cell-output-display}\n![Basic line plot showing a time series with trend and seasonality](time-series_files/figure-html/cell-2-output-1.png){}\n:::\n:::\n\n\nThe line connecting points implies **continuity**\u0014that values exist between measurements. This is appropriate for continuous processes (temperature, stock prices, heart rate) but not for discrete events or counts measured at intervals.\n\nWhen should you **not** connect the dots? When your data represents discrete events or when measurements are too sparse to imply continuity.\n\n::: {#059cfece .cell fig-height='5' fig-width='14' execution_count=2}\n``` {.python .cell-code code-fold=\"true\"}\n# Generate sparse discrete event data\nnp.random.seed(123)\nevent_dates = pd.to_datetime(['2023-01-15', '2023-03-10', '2023-05-22',\n                               '2023-07-08', '2023-09-30', '2023-11-15'])\nevent_values = np.random.randint(20, 80, len(event_dates))\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Line plot (implies continuity - misleading for discrete events)\naxes[0].plot(event_dates, event_values, marker='o', linewidth=2, markersize=8)\naxes[0].set_xlabel('Date')\naxes[0].set_ylabel('Event Count')\naxes[0].set_title('Line Plot: Implies Values Between Events (Misleading)')\naxes[0].grid(True, alpha=0.3)\n\n# Scatter plot (appropriate for discrete events)\naxes[1].scatter(event_dates, event_values, s=100, alpha=0.7)\naxes[1].set_xlabel('Date')\naxes[1].set_ylabel('Event Count')\naxes[1].set_title('Scatter Plot: Shows Only Observed Events (Honest)')\naxes[1].grid(True, alpha=0.3)\n\nfor ax in axes:\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![Line plot vs scatter plot: connecting points implies continuity](time-series_files/figure-html/cell-3-output-1.png){}\n:::\n:::\n\n\nFor discrete events, stick with scatter plots or bar charts. Don't imply continuity where none exists.\n\n# Comparing Multiple Time Series\n\nOften you need to compare several time series. The natural approach is to overlay them on the same plot.\n\n::: {#9c6db970 .cell fig-height='6' fig-width='12' execution_count=3}\n``` {.python .cell-code code-fold=\"true\"}\n# Generate three related time series\nnp.random.seed(42)\ndates = pd.date_range('2023-01-01', periods=200, freq='D')\n\nseries_a = 100 + np.linspace(0, 30, 200) + np.random.normal(0, 5, 200)\nseries_b = 95 + np.linspace(0, 20, 200) + np.random.normal(0, 4, 200)\nseries_c = 110 + np.linspace(0, 10, 200) + np.random.normal(0, 6, 200)\n\ndf_multi = pd.DataFrame({\n    'date': dates,\n    'Product A': series_a,\n    'Product B': series_b,\n    'Product C': series_c\n})\n\n# Overlay plot\nfig, ax = plt.subplots(figsize=(12, 6))\nfor column in ['Product A', 'Product B', 'Product C']:\n    ax.plot(df_multi['date'], df_multi[column], linewidth=2, label=column, alpha=0.8)\n\nax.set_xlabel('Date')\nax.set_ylabel('Sales')\nax.set_title('Multiple Time Series: Overlaid Comparison')\nax.legend(loc='upper left')\nax.grid(True, alpha=0.3)\nsns.despine()\n```\n\n::: {.cell-output .cell-output-display}\n![Multiple time series overlaid with different colors](time-series_files/figure-html/cell-4-output-1.png){}\n:::\n:::\n\n\nThis works well for 2-4 series. Beyond that, you risk creating a **spaghetti plot**\u0014a tangled mess where individual series become impossible to follow.\n\nWhen you have many time series, use **small multiples** (faceting): separate plots arranged in a grid, each with the same axes for easy comparison.\n\n::: {#6c1a065e .cell fig-height='8' fig-width='14' execution_count=4}\n``` {.python .cell-code code-fold=\"true\"}\n# Generate multiple time series\nnp.random.seed(42)\nn_series = 6\ndates = pd.date_range('2023-01-01', periods=150, freq='D')\n\ndata_list = []\nfor i in range(n_series):\n    values = 50 + np.random.randn(150).cumsum() + 10 * np.sin(2 * np.pi * np.arange(150) / 30)\n    data_list.append(pd.DataFrame({\n        'date': dates,\n        'value': values,\n        'series': f'Region {i+1}'\n    }))\n\ndf_many = pd.concat(data_list, ignore_index=True)\n\n# Small multiples using seaborn FacetGrid\ng = sns.FacetGrid(df_many, col='series', col_wrap=3, height=3, aspect=1.5, sharey=True)\ng.map_dataframe(sns.lineplot, x='date', y='value', linewidth=2, color=sns.color_palette()[0])\ng.set_axis_labels('Date', 'Value')\ng.set_titles('Region {col_name}')\nfor ax in g.axes.flat:\n    ax.grid(True, alpha=0.3)\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![Small multiples avoid spaghetti plots when comparing many time series](time-series_files/figure-html/cell-5-output-1.png){}\n:::\n:::\n\n\nSmall multiples let you see each series clearly while maintaining comparability through shared axes.\n\n# The Power of Scale: Linear vs. Logarithmic\n\nPerhaps the most consequential choice in time series visualization is the **y-axis scale**. The same data looks completely different on linear vs. logarithmic scales.\n\nWhen should you use a log scale?\n- When your data spans **multiple orders of magnitude** (e.g., 10 to 10,000)\n- When you care about **percentage changes** rather than absolute changes\n- When visualizing **exponential growth or decay**\n\n::: {#1ac15aa4 .cell fig-height='5' fig-width='14' execution_count=5}\n``` {.python .cell-code code-fold=\"true\"}\n# Generate exponential growth data (e.g., epidemic spread)\nnp.random.seed(42)\ndays = np.arange(0, 100)\ncases = 10 * np.exp(0.05 * days) * (1 + np.random.normal(0, 0.1, len(days)))\n\ndf_exp = pd.DataFrame({'day': days, 'cases': cases})\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Linear scale\naxes[0].plot(df_exp['day'], df_exp['cases'], linewidth=2, color=sns.color_palette()[0])\naxes[0].set_xlabel('Days')\naxes[0].set_ylabel('Cases')\naxes[0].set_title('Linear Scale: Exponential Growth Looks Explosive')\naxes[0].grid(True, alpha=0.3)\n\n# Log scale\naxes[1].plot(df_exp['day'], df_exp['cases'], linewidth=2, color=sns.color_palette()[1])\naxes[1].set_xlabel('Days')\naxes[1].set_ylabel('Cases (log scale)')\naxes[1].set_yscale('log')\naxes[1].set_title('Log Scale: Exponential Growth Appears Linear')\naxes[1].grid(True, alpha=0.3, which='both')\n\nfor ax in axes:\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![The same exponential growth looks different on linear vs. log scales](time-series_files/figure-html/cell-6-output-1.png){}\n:::\n:::\n\n\nOn a **linear scale**, exponential growth appears as a dramatic upward curve\u0014most growth happens at the end. On a **log scale**, exponential growth becomes a straight line, making it easy to see if the growth rate is constant, accelerating, or decelerating.\n\n::: {.callout-warning}\n## Log Scales Can Hide Magnitude\n\nWhile log scales are essential for percentage changes and exponential processes, they can **downplay the absolute magnitude** of changes. A jump from 10,000 to 100,000 cases looks the same as a jump from 100 to 1,000\u0014both are one order of magnitude. But in human terms, 90,000 additional cases is far more significant than 900.\n\n**Always consider your audience and what you want to emphasize**: relative changes (use log) or absolute numbers (use linear).\n:::\n\n# Smoothing and Trends\n\nReal time series data is often noisy. **Smoothing** helps reveal underlying trends by averaging out short-term fluctuations.\n\nThe most common approach is a **moving average**: replace each point with the average of nearby points.\n\n::: {#fd4f1d7b .cell fig-height='6' fig-width='12' execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\n# Generate noisy time series\nnp.random.seed(42)\ndates = pd.date_range('2023-01-01', periods=200, freq='D')\ntrend = 50 + 0.2 * np.arange(200)\nseasonal = 8 * np.sin(2 * np.pi * np.arange(200) / 30)\nnoise = np.random.normal(0, 5, 200)\nvalues = trend + seasonal + noise\n\ndf_noisy = pd.DataFrame({'date': dates, 'value': values})\n\n# Calculate moving averages\ndf_noisy['MA_7'] = df_noisy['value'].rolling(window=7, center=True).mean()\ndf_noisy['MA_30'] = df_noisy['value'].rolling(window=30, center=True).mean()\n\n# Plot\nfig, ax = plt.subplots(figsize=(12, 6))\nax.plot(df_noisy['date'], df_noisy['value'], linewidth=0.8, alpha=0.3, label='Raw Data', color='gray')\nax.plot(df_noisy['date'], df_noisy['MA_7'], linewidth=2, label='7-Day Moving Average', color=sns.color_palette()[0])\nax.plot(df_noisy['date'], df_noisy['MA_30'], linewidth=2, label='30-Day Moving Average', color=sns.color_palette()[1])\n\nax.set_xlabel('Date')\nax.set_ylabel('Value')\nax.set_title('Moving Averages Reveal Trends by Smoothing Noise')\nax.legend()\nax.grid(True, alpha=0.3)\nsns.despine()\n```\n\n::: {.cell-output .cell-output-display}\n![Moving averages smooth noise to reveal underlying trends](time-series_files/figure-html/cell-7-output-1.png){}\n:::\n:::\n\n\nThe smoothing window creates a trade-off:\n- **Short windows** (e.g., 7 days) preserve more detail but still show fluctuations\n- **Long windows** (e.g., 30 days) reveal long-term trends but may over-smooth and miss real changes\n\n::: {.callout-note}\n## Choosing the Right Window\n\nThe appropriate smoothing window depends on your data's frequency and the patterns you care about:\n- **Daily stock prices**: 5-20 day moving average\n- **Monthly sales**: 3-6 month moving average\n- **Annual measurements**: 3-5 year moving average\n\nMatch your window to the timescale of meaningful variation in your domain.\n:::\n\n# Showing Uncertainty Over Time\n\nWhen forecasting or estimating, you don't just have point predictions\u0014you have **uncertainty**. Showing this uncertainty is crucial for honest communication.\n\nUse **ribbon plots** (also called envelope plots) to show confidence intervals or prediction intervals around your estimates.\n\n::: {#7b95e4a3 .cell fig-height='6' fig-width='12' execution_count=7}\n``` {.python .cell-code code-fold=\"true\"}\n# Generate data with trend\nnp.random.seed(42)\nn = 150\nx = np.arange(n)\ntrue_trend = 50 + 0.3 * x\nobserved = true_trend + np.random.normal(0, 5, n)\n\n# Simple linear forecast\nfrom scipy import stats\nslope, intercept, r_value, p_value, std_err = stats.linregress(x[:100], observed[:100])\n\n# Forecast period\nx_future = np.arange(100, 150)\ny_pred = slope * x_future + intercept\n\n# Estimate prediction interval (simplified)\nresiduals = observed[:100] - (slope * x[:100] + intercept)\nstd_residual = np.std(residuals)\nmargin = 1.96 * std_residual  # 95% prediction interval\n\n# Plot\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Historical data\nax.plot(x[:100], observed[:100], linewidth=2, label='Historical Data', color=sns.color_palette()[0])\n\n# Forecast with uncertainty\nax.plot(x_future, y_pred, linewidth=2, label='Forecast', color=sns.color_palette()[1], linestyle='--')\nax.fill_between(x_future, y_pred - margin, y_pred + margin,\n                alpha=0.3, color=sns.color_palette()[1], label='95% Prediction Interval')\n\n# Actual future (for comparison)\nax.plot(x_future, observed[100:], linewidth=1.5, alpha=0.5, label='Actual (for comparison)',\n        color='gray', linestyle=':')\n\nax.axvline(x=100, color='black', linestyle=':', alpha=0.5, label='Forecast Start')\nax.set_xlabel('Time')\nax.set_ylabel('Value')\nax.set_title('Time Series Forecast with Uncertainty Bands')\nax.legend()\nax.grid(True, alpha=0.3)\nsns.despine()\n```\n\n::: {.cell-output .cell-output-display}\n![Ribbon plots show uncertainty bands around predictions](time-series_files/figure-html/cell-8-output-1.png){}\n:::\n:::\n\n\nThe ribbon makes it clear that predictions further into the future are more uncertain. Without showing this uncertainty, forecasts can appear deceptively precise.\n\n# Temporal Aggregation: Choosing Your Time Scale\n\nHow you aggregate time can dramatically change what patterns emerge. The same data aggregated hourly, daily, or monthly reveals different stories.\n\n**Heat maps** are excellent for visualizing patterns across two time dimensions\u0014say, hour of day vs. day of week.\n\n::: {#1eb2278c .cell fig-height='8' fig-width='12' execution_count=8}\n``` {.python .cell-code code-fold=\"true\"}\n# Generate synthetic hourly data with daily and weekly patterns\nnp.random.seed(42)\nhours = pd.date_range('2023-01-01', periods=24*7*4, freq='H')  # 4 weeks\n\n# Patterns: higher activity during business hours and weekdays\nhour_of_day = hours.hour\nday_of_week = hours.dayofweek\n\n# Activity pattern\nbase_activity = 20\nhour_effect = 30 * np.exp(-((hour_of_day - 14)**2) / 20)  # Peak at 2 PM\nweekday_effect = np.where(day_of_week < 5, 20, -10)  # Weekdays higher\nnoise = np.random.normal(0, 5, len(hours))\n\nactivity = base_activity + hour_effect + weekday_effect + noise\n\ndf_hourly = pd.DataFrame({\n    'datetime': hours,\n    'activity': activity,\n    'hour': hour_of_day,\n    'day_name': hours.day_name(),\n    'week': (hours.day // 7) + 1\n})\n\n# Take first week for heatmap\ndf_week = df_hourly[df_hourly['week'] == 1].copy()\n\n# Pivot for heatmap\nheatmap_data = df_week.pivot_table(values='activity',\n                                     index='hour',\n                                     columns='day_name',\n                                     aggfunc='mean')\n\n# Reorder columns to start with Monday\nday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\nheatmap_data = heatmap_data[[day for day in day_order if day in heatmap_data.columns]]\n\n# Plot heatmap\nfig, ax = plt.subplots(figsize=(12, 8))\nsns.heatmap(heatmap_data, cmap='YlOrRd', annot=False, fmt='.0f',\n            cbar_kws={'label': 'Activity Level'}, ax=ax)\nax.set_xlabel('Day of Week')\nax.set_ylabel('Hour of Day')\nax.set_title('Temporal Heatmap: Activity by Hour and Day of Week')\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/j7/9dgqq5g53vnbsbmvh2yqtckr0000gr/T/ipykernel_89745/3637488205.py:3: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n  hours = pd.date_range('2023-01-01', periods=24*7*4, freq='H')  # 4 weeks\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Heat map reveals daily and weekly patterns in temporal data](time-series_files/figure-html/cell-9-output-2.png){}\n:::\n:::\n\n\nHeat maps immediately reveal patterns like \"high activity on weekday afternoons\" that would be invisible in a simple line plot.\n\n::: {.column-margin}\n![](https://raw.githubusercontent.com/scottlepp/plot-widget/master/resources/heatmap.png)\n\nCalendar heatmaps are widely used for visualizing GitHub contributions, showing commit activity over time in a compact, pattern-revealing format.\n:::\n\n# Visualizing Cycles and Seasonality\n\nMany time series have **seasonal patterns**: daily cycles, weekly patterns, annual seasons. **Cycle plots** decompose time series by season to reveal these patterns.\n\n::: {#1e5d1342 .cell fig-height='6' fig-width='14' execution_count=9}\n``` {.python .cell-code code-fold=\"true\"}\n# Generate monthly data with strong annual seasonality\nnp.random.seed(42)\nmonths = pd.date_range('2020-01-01', periods=48, freq='M')\nmonth_num = np.tile(np.arange(1, 13), 4)  # 4 years of monthly data\n\n# Seasonal pattern (higher in summer, lower in winter)\nseasonal_effect = 20 * np.sin(2 * np.pi * (month_num - 3) / 12)\ntrend_effect = 0.5 * np.arange(48)\nnoise = np.random.normal(0, 3, 48)\n\nvalues = 50 + seasonal_effect + trend_effect + noise\n\ndf_seasonal = pd.DataFrame({\n    'date': months,\n    'value': values,\n    'month': month_num,\n    'year': months.year,\n    'month_name': months.month_name()\n})\n\n# Create cycle plot\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Traditional time series\naxes[0].plot(df_seasonal['date'], df_seasonal['value'], marker='o', linewidth=2)\naxes[0].set_xlabel('Date')\naxes[0].set_ylabel('Value')\naxes[0].set_title('Traditional Time Series: Seasonality Repeats')\naxes[0].grid(True, alpha=0.3)\n\n# Cycle plot\nmonth_names_short = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n                     'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\nfor year in df_seasonal['year'].unique():\n    year_data = df_seasonal[df_seasonal['year'] == year]\n    axes[1].plot(year_data['month'], year_data['value'], marker='o',\n                linewidth=2, label=str(year), alpha=0.7)\n\naxes[1].set_xlabel('Month')\naxes[1].set_ylabel('Value')\naxes[1].set_xticks(range(1, 13))\naxes[1].set_xticklabels(month_names_short)\naxes[1].set_title('Cycle Plot: Each Year Overlaid to Show Seasonal Pattern')\naxes[1].legend(title='Year')\naxes[1].grid(True, alpha=0.3)\n\nfor ax in axes:\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/j7/9dgqq5g53vnbsbmvh2yqtckr0000gr/T/ipykernel_89745/3601020590.py:3: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n  months = pd.date_range('2020-01-01', periods=48, freq='M')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Cycle plot reveals seasonal patterns by separating each cycle](time-series_files/figure-html/cell-10-output-2.png){}\n:::\n:::\n\n\nBy overlaying each year's cycle, the cycle plot makes it obvious that values peak in summer (months 6-8) and dip in winter (months 12-2), while also showing year-over-year trends.\n\n# Advanced: Lag Plots for Autocorrelation\n\nTime series data often exhibits **autocorrelation**: values depend on previous values. A **lag plot** helps visualize this by plotting each value against the previous value (lag-1) or earlier values.\n\n::: {#2ed02ad2 .cell fig-height='5' fig-width='14' execution_count=10}\n``` {.python .cell-code code-fold=\"true\"}\n# Generate time series with autocorrelation\nnp.random.seed(42)\nn = 200\n\n# AR(1) process: strong autocorrelation\nar_series = np.zeros(n)\nar_series[0] = np.random.normal(0, 1)\nfor i in range(1, n):\n    ar_series[i] = 0.7 * ar_series[i-1] + np.random.normal(0, 1)\n\n# Random walk: perfect autocorrelation at lag 1\nrandom_walk = np.random.normal(0, 1, n).cumsum()\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Lag-1 plot for AR(1) series\naxes[0].scatter(ar_series[:-1], ar_series[1:], alpha=0.6, s=30)\naxes[0].set_xlabel('Value at time t')\naxes[0].set_ylabel('Value at time t+1')\naxes[0].set_title('Lag-1 Plot: Strong Autocorrelation (AR Process)')\naxes[0].plot([-3, 3], [-3, 3], 'r--', alpha=0.5, linewidth=1)\naxes[0].grid(True, alpha=0.3)\n\n# Lag-1 plot for random walk\naxes[1].scatter(random_walk[:-1], random_walk[1:], alpha=0.6, s=30, color=sns.color_palette()[1])\naxes[1].set_xlabel('Value at time t')\naxes[1].set_ylabel('Value at time t+1')\naxes[1].set_title('Lag-1 Plot: Perfect Autocorrelation (Random Walk)')\naxes[1].plot([random_walk.min(), random_walk.max()],\n            [random_walk.min(), random_walk.max()], 'r--', alpha=0.5, linewidth=1)\naxes[1].grid(True, alpha=0.3)\n\nfor ax in axes:\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![Lag plots reveal autocorrelation structure in time series](time-series_files/figure-html/cell-11-output-1.png){}\n:::\n:::\n\n\nA strong linear pattern in a lag plot indicates high autocorrelation\u0014knowing the current value helps predict the next value. Random, scattered points suggest no autocorrelation (e.g., white noise).\n\n# The Bigger Picture\n\nTime series visualization is about making choices that honestly represent temporal patterns while avoiding common pitfalls:\n\n**Key principles to remember:**\n\n1. **Choose the right scale**: Linear for absolute changes, log for relative/percentage changes\n2. **Show uncertainty**: Predictions without confidence intervals are misleading\n3. **Avoid spaghetti plots**: Use small multiples when comparing many series\n4. **Match aggregation to your question**: Daily, weekly, monthly aggregation reveals different patterns\n5. **Be transparent about time windows**: The time range you show matters enormously\n6. **Smooth appropriately**: Balance between preserving detail and revealing trends\n\n**Common pitfalls to avoid:**\n\n- Truncating the y-axis to exaggerate small changes\n- Cherry-picking time windows to support a narrative\n- Using line plots for discrete events (implies false continuity)\n- Over-smoothing to hide inconvenient variation\n- Mixing scales when comparing series (e.g., comparing growth rates on linear scale)\n\nTime series visualization is powerful because time is a dimension we all understand intuitively. But that familiarity also makes us vulnerable to manipulation. By following principled visualization practices, you ensure your temporal data tells its true story\u0014not the story you wish it told.\n\n",
    "supporting": [
      "time-series_files"
    ],
    "filters": [],
    "includes": {}
  }
}
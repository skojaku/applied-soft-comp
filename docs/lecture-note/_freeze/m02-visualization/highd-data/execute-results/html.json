{
  "hash": "5f4c341999d926e28651747f4a077b14",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"High-Dimensional Data Visualization\"\nexecute:\n    enabled: true\n---\n\n::: {.callout-note title=\"What you'll learn in this module\"}\nThis module introduces dimensionality reduction, a fundamental technique for visualizing and understanding high-dimensional data. We will explore the curse of dimensionality that makes high-dimensional space counterintuitive, examine linear methods like PCA that preserve global structure, investigate non-linear methods like t-SNE and UMAP that reveal local patterns, and understand the trade-offs between different approaches to choosing the right visualization strategy for your data.\n:::\n\nImagine you're analyzing data with 50 features per observation. Gene expression levels, user behavior metrics, environmental measurements. You want to understand the patterns in your data. How do different observations relate to each other? Are there clusters? Outliers?\n\nHere's the fundamental problem: you can't plot 50 dimensions directly. Our visual system lives in three dimensions, or really two dimensions on a screen. This creates a core challenge: how do you visualize data that lives in spaces you cannot see?\n\nThe answer is dimensionality reduction. This technique projects high-dimensional data into 2 or 3 dimensions while preserving important structure. But here's the critical question: what structure matters?\n\nDifferent methods preserve different aspects of your data. Some preserve global structure, showing how groups relate to each other across the entire dataset. Others preserve local structure, highlighting which points are nearest neighbors. Understanding these trade-offs is essential for choosing the right method and avoiding beautiful but misleading visualizations.\n\n## The Curse of Dimensionality\n\nBefore we dive into methods, we need to understand what makes high-dimensional data fundamentally different.\n\nIn high dimensions, everything is far from everything else. This sounds paradoxical, but it's mathematically inevitable. As dimensions increase, the volume of space grows exponentially, and data points become increasingly sparse.\n\nConsider this simple fact: in 1D, if you have 10 points uniformly distributed in [0, 1], the average distance between neighbors is about 0.1. To maintain the same density in 2D, you need 100 points. In 3D, you need 1,000 points. In 10D, you need 10 billion points.\n\nEven stranger than the sparsity is what happens to distances. In high dimensions, all distances become similar. The nearest and farthest neighbors become roughly equidistant. This makes many of our intuitions about \"closeness\" break down.\n\n::: {#25c680ff .cell fig-height='5' fig-width='10' execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.metrics.pairwise import euclidean_distances\n\nsns.set_style(\"white\")\nnp.random.seed(42)\n\n# Calculate distance ratio across dimensions\ndimensions = [2, 5, 10, 20, 50, 100, 200]\nn_samples = 100\nratios = []\n\nfor d in dimensions:\n    # Generate random data\n    X = np.random.randn(n_samples, d)\n    # Calculate all pairwise distances\n    distances = euclidean_distances(X)\n    # For each point, find nearest and farthest (excluding self)\n    np.fill_diagonal(distances, np.inf)  # Ignore self-distance\n    nearest = distances.min(axis=1)\n    # For \"farthest,\" ignore inf (self-distance), so set inf entries to -1 and use argmax\n    temp = distances.copy()\n    temp[temp == np.inf] = -1  # Now maximum is truly among finite values\n    farthest = temp.max(axis=1)\n    # Calculate ratio\n    ratio = nearest / farthest\n    ratios.append(ratio)\n\n# Plot\nsns.set(font_scale=2.0)\nsns.set_style(\"white\")\n\nblue, red = sns.color_palette('muted', 2)\n\nfig, ax = plt.subplots(figsize=(10, 5))\npositions = range(len(dimensions))\nbp = ax.boxplot(ratios, positions=positions, widths=0.6, patch_artist=True,\n                boxprops=dict(facecolor=\"#f2f2f2\", alpha=0.7))\nax.set_xticklabels(dimensions)\nax.set_xlabel('Number of Dimensions')\nax.set_ylabel('Nearest Distance / Farthest Distance')\nax.set_title('The Curse of Dimensionality: All Points Become Equidistant')\nax.axhline(y=1.0, color=red, linestyle='--', alpha=0.5, label='Equal distances')\nax.legend(frameon=False)\nsns.despine()\n```\n\n::: {.cell-output .cell-output-display}\n![As dimensions increase, the ratio of farthest to nearest distance approaches 1](highd-data_files/figure-html/cell-2-output-1.png){}\n:::\n:::\n\n\nThe plot shows a striking pattern. As dimensions increase, the ratio of nearest to farthest distance gets closer to 1. At 200 dimensions, nearly every point is equally far from every other point.\n\nThis curse of dimensionality is useful not just for visualization, but also for analysis. When you want to cluster data points, every point becomes equidistant from every other point, making clustering impossible. By projecting the data into lower dimensions, you can remedy this problem.\n\n## Pairwise Scatter Plots: The Brute Force Approach\n\nWhen you have a moderate number of dimensions (roughly 3 to 10), you can visualize all pairwise relationships using a scatter plot matrix, also called a pairs plot or SPLOM.\n\n::: {#62409da1 .cell fig-height='14' fig-width='14' execution_count=2}\n``` {.python .cell-code code-fold=\"true\"}\n# Load classic iris dataset (4 dimensions)\nfrom sklearn.datasets import load_iris\n\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\n\niris = load_iris()\niris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\niris_df['species'] = iris.target\niris_df['species'] = iris_df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n\n# Create pairplot\ng = sns.pairplot(iris_df, hue='species', diag_kind='kde',\n                 plot_kws={'alpha': 0.6, 's': 50, 'edgecolor': 'white', 'linewidth': 0.5},\n                 diag_kws={'alpha': 0.7, 'linewidth': 2})\ng.fig.suptitle('Iris Dataset: All Pairwise Relationships', y=1.01)\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nText(0.5, 1.01, 'Iris Dataset: All Pairwise Relationships')\n```\n\nScatter plot matrix showing all pairwise relationships in the Iris dataset\n:::\n\n::: {.cell-output .cell-output-display}\n![](highd-data_files/figure-html/cell-3-output-2.png){}\n:::\n:::\n\n\nThe scatter plot matrix shows every possible 2D projection. The diagonal displays the univariate distribution of each feature using KDE, while off-diagonals show bivariate scatter plots. This gives you a complete view of pairwise relationships.\n\nThe problem is clear: scatter plot matrices don't scale. With 10 variables, you have 45 unique pairwise plots, which is manageable but crowded. With 20 variables, you have 190 plots, which becomes overwhelming. And you're still only seeing 2D projections, never the full high-dimensional structure.\n\nThis is where dimensionality reduction becomes essential. Instead of looking at every pairwise combination, we project the data intelligently onto just 2 or 3 dimensions.\n\n## Linear Dimensionality Reduction: PCA\n\nPrincipal Component Analysis (PCA) is a linear dimensionality reduction method that finds the directions of maximum variance in your data.\n\nImagine you have a cloud of points in high-dimensional space. PCA asks a simple question: what direction captures the most variation in the data? This becomes the first principal component (PC1). Then it asks: what direction, perpendicular to the first, captures the most remaining variation? This becomes PC2. And so on.\n\nMathematically, PCA finds the eigenvectors of the covariance matrix. But conceptually, it's rotating your coordinate system to align with the highest variance directions of your data.\n\n::: {#b729f563 .cell fig-height='6' fig-width='10' execution_count=3}\n``` {.python .cell-code code-fold=\"true\"}\nfrom sklearn.decomposition import PCA\n\n# Generate correlated 2D data (for visualization)\nnp.random.seed(123)\nmean = [0, 0]\ncov = [[3, 2], [2, 2]]\ndata_2d = np.random.multivariate_normal(mean, cov, 300)\n\n# Fit PCA\npca = PCA(n_components=2)\npca.fit(data_2d)\n\ncolors = [\"#f2f2f2\", sns.color_palette('muted')[0], sns.color_palette('muted')[3]]\n\n# Plot original data with principal components\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(data_2d[:, 0], data_2d[:, 1], alpha=0.9, s=50, color=colors[0], edgecolors='k', linewidth=0.5)\n\n# Draw principal components as arrows\norigin = pca.mean_\nfor i, (component, variance) in enumerate(zip(pca.components_, pca.explained_variance_)):\n    direction = component * np.sqrt(variance) * 3  # Scale for visibility\n    ax.arrow(origin[0], origin[1], direction[0], direction[1],\n             head_width=0.3, head_length=0.3, fc=colors[i+1], ec=colors[i+1], linewidth=3,\n             label=f'PC{i+1} ({variance/pca.explained_variance_.sum()*100:.1f}%)')\n\nax.set_xlabel('Original X')\nax.set_ylabel('Original Y')\nax.set_title('Principal Components: Directions of Maximum Variance')\nax.legend()\nax.axis('equal')\nsns.despine()\n```\n\n::: {.cell-output .cell-output-display}\n![PCA finds directions of maximum variance. PC1 captures the most variation, PC2 the next most (perpendicular to PC1).](highd-data_files/figure-html/cell-4-output-1.png){}\n:::\n:::\n\n\nPC1 (orange arrow) points along the direction of greatest spread. PC2 (green arrow) is perpendicular and captures the remaining variation. The percentage in parentheses shows how much variance each component explains. If PC1 explains 90 percent of variance, then projecting onto just PC1 preserves most of your data's structure.\n\n### Applying PCA to Iris\n\nLet's apply PCA to the 4-dimensional Iris dataset and see how much information we can preserve in just 2 dimensions.\n\n::: {#71b4bbf5 .cell fig-height='6' fig-width='14' execution_count=4}\n``` {.python .cell-code code-fold=\"true\"}\n# Prepare data\nX = iris.data\ny = iris.target\n\n# Standardize (important for PCA!)\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Apply PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# Create DataFrame for plotting\npca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\npca_df['species'] = iris.target_names[y]\n\n# Plot\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Left: PCA projection\nfor species, color in zip(['setosa', 'versicolor', 'virginica'], sns.color_palette('muted', 3)):\n    mask = pca_df['species'] == species\n    axes[0].scatter(pca_df.loc[mask, 'PC1'], pca_df.loc[mask, 'PC2'],\n                   label=species, alpha=0.7, s=50, color=color, edgecolors='white', linewidth=0.5)\naxes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)')\naxes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)')\naxes[0].set_title('PCA Projection of Iris Dataset')\naxes[0].legend()\nsns.despine(ax=axes[0])\n\n# Right: Variance explained\nvariances = pca.explained_variance_ratio_\naxes[1].bar([1, 2], variances, color=sns.color_palette('muted', 2), alpha=0.7)\naxes[1].set_xlabel('Principal Component')\naxes[1].set_ylabel('Variance Explained')\naxes[1].set_title('Variance Explained by Each Component')\naxes[1].set_xticks([1, 2])\naxes[1].set_xticklabels(['PC1', 'PC2'])\nfor i, v in enumerate(variances):\n    axes[1].text(i+1, v+0.01, f'{v*100:.1f}%', ha='center', va='bottom', fontsize=11)\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![PCA projection of Iris dataset to 2D preserves the separation between species](highd-data_files/figure-html/cell-5-output-1.png){}\n:::\n:::\n\n\nPC1 and PC2 together explain over 95 percent of the variance in the 4D dataset. The 2D projection preserves the main structure beautifully: setosa is well-separated, while versicolor and virginica have some overlap, just as they do in the original high-dimensional space.\n\nA critical reminder: always standardize before PCA. If features have different units or scales, PCA will be dominated by high-variance features. Standardization (zero mean, unit variance) ensures all features contribute fairly to the analysis.\n\n## Non-Linear Dimensionality Reduction: MDS\n\nMultidimensional Scaling (MDS) takes a different approach than PCA. Instead of finding directions of maximum variance, it tries to preserve distances between points.\n\nYou give MDS a distance matrix showing the distance between every pair of points in high-dimensional space. MDS then finds a low-dimensional configuration where those distances are preserved as well as possible.\n\nThink of it like arranging cities on a map. You know the distance between every pair of cities, but not their coordinates. MDS finds positions that preserve those distances. Mathematically, MDS minimizes stress, the difference between high-dimensional distances and low-dimensional distances. Classical MDS has a closed-form solution like PCA, but more flexible variants use iterative optimization.\n\n::: {#78b3f79f .cell fig-height='6' fig-width='14' execution_count=5}\n``` {.python .cell-code code-fold=\"true\"}\nfrom sklearn.manifold import MDS\n\n# Suppress FutureWarning about n_init in MDS\nimport warnings\nmds = MDS(n_components=2, random_state=42, n_init=1)\nX_mds = mds.fit_transform(X_scaled)\n\n# Create DataFrame\nmds_df = pd.DataFrame(X_mds, columns=['MDS1', 'MDS2'])\nmds_df['species'] = iris.target_names[y]\n\n# Plot both\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# PCA\nfor species, color in zip(['setosa', 'versicolor', 'virginica'], sns.color_palette('muted', 3)):\n    mask = pca_df['species'] == species\n    axes[0].scatter(pca_df.loc[mask, 'PC1'], pca_df.loc[mask, 'PC2'],\n                   label=species, alpha=0.7, s=50, color=color, edgecolors='white', linewidth=0.5)\naxes[0].set_xlabel('PC1')\naxes[0].set_ylabel('PC2')\naxes[0].set_title('PCA: Maximizes Variance')\naxes[0].legend()\nsns.despine(ax=axes[0])\n\n# MDS\nfor species, color in zip(['setosa', 'versicolor', 'virginica'], sns.color_palette('muted', 3)):\n    mask = mds_df['species'] == species\n    axes[1].scatter(mds_df.loc[mask, 'MDS1'], mds_df.loc[mask, 'MDS2'],\n                   label=species, alpha=0.7, s=50, color=color, edgecolors='white', linewidth=0.5)\naxes[1].set_xlabel('MDS1')\naxes[1].set_ylabel('MDS2')\naxes[1].set_title('MDS: Preserves Distances')\naxes[1].legend()\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![MDS vs PCA on Iris dataset. MDS preserves distances better but looks similar to PCA for this dataset.](highd-data_files/figure-html/cell-6-output-1.png){}\n:::\n:::\n\n\nFor the Iris dataset, PCA and MDS look very similar. This is because Iris data is fairly linear. The relationships between features don't involve complex curves or non-linear structures that would cause MDS to differ significantly from PCA.\n\n## Isomap: Preserving Geodesic Distances\n\nMDS preserves Euclidean distances, which are straight-line distances through space. But for curved manifolds, what matters is the geodesic distance: the distance along the surface.\n\nIsomap (Isometric Mapping) addresses this by approximating geodesic distances using the neighborhood graph. The approach is elegant. First, build a neighborhood graph by connecting each point to its k nearest neighbors. Second, compute shortest paths through this graph. The geodesic distance between points is approximated by the shortest path. Third, apply classical MDS using these geodesic distances instead of Euclidean distances.\n\nThink of it like this: MDS measures distance \"as the crow flies,\" while Isomap measures distance \"as you walk along the surface.\"\n\n::: {#7215c809 .cell fig-height='6' fig-width='14' execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\nfrom sklearn.manifold import Isomap\nfrom sklearn.datasets import make_s_curve\n\n# Generate S-curve data (a 2D manifold embedded in 3D)\nn_samples = 1000\nX_scurve, color = make_s_curve(n_samples, noise=0.1, random_state=42)\n\n# Apply MDS\nmds_scurve = MDS(n_components=2, random_state=42, n_init=1)\nX_scurve_mds = mds_scurve.fit_transform(X_scurve)\n\n# Apply Isomap\nisomap = Isomap(n_components=2, n_neighbors=10)\nX_scurve_isomap = isomap.fit_transform(X_scurve)\n\n# Plot MDS vs Isomap\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# MDS\naxes[0].scatter(X_scurve_mds[:, 0], X_scurve_mds[:, 1], c=color,\n                cmap='viridis', alpha=0.6, s=20)\naxes[0].set_xlabel('MDS1')\naxes[0].set_ylabel('MDS2')\naxes[0].set_title('MDS: Global Euclidean Distances')\nsns.despine(ax=axes[0])\n\n# Isomap\naxes[1].scatter(X_scurve_isomap[:, 0], X_scurve_isomap[:, 1], c=color,\n                cmap='viridis', alpha=0.6, s=20)\naxes[1].set_xlabel('Isomap1')\naxes[1].set_ylabel('Isomap2')\naxes[1].set_title('Isomap: Geodesic Distances')\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![Isomap uses geodesic distances (along the surface) instead of Euclidean distances (through space), better recovering the S-curve structure](highd-data_files/figure-html/cell-7-output-1.png){}\n:::\n:::\n\n\nIsomap successfully \"straightens\" the S-curve because it respects the manifold structure. By computing distances along the neighborhood graph, it avoids the shortcuts across the bend that confused MDS. The key parameter is n_neighbors. Too few neighbors and the graph becomes disconnected with infinite distances. Too many neighbors and you create shortcuts across the manifold, reverting to MDS-like behavior. Getting it just right, typically 5 to 15, captures the local manifold structure perfectly.\n\nNow we see two extremes emerging. MDS preserves all pairwise distances globally, which works on linear or convex data. Isomap preserves geodesic distances using local neighborhoods, which works on curved manifolds. But what if we only care about local structure? What if global relationships don't matter for our purposes?\n\n## Modern Non-Linear Methods: t-SNE and UMAP\n\nBoth t-SNE (t-Distributed Stochastic Neighbor Embedding) and UMAP (Uniform Manifold Approximation and Projection) take a middle ground between MDS's global approach and Isomap's geodesic approach. They prioritize local structure while allowing some flexibility in global positioning.\n\nThe key insight is simple: for visualization, we often care most about which points are neighbors. Whether distant clusters are placed left versus right, or how far apart they are, matters less than preserving the local neighborhood relationships within and between clusters.\n\n### How t-SNE works\n\nt-SNE converts distances into similarity probabilities and preserves these local relationships. In high dimensions, we define probability $p_{ij}$ that point $i$ picks point $j$ as a neighbor, based on a Gaussian distance. In low dimensions, we define similar probability $q_{ij}$ using a t-distribution with heavy tails. Then we optimize by moving points in 2D to make $q_{ij}$ match $p_{ij}$, minimizing KL divergence.\n\nThe t-distribution's heavy tails are clever. They let well-separated clusters spread out in 2D without overlapping, while keeping local neighborhoods tight.\n\n::: {#19c4df0e .cell fig-height='5' fig-width='15' execution_count=7}\n``` {.python .cell-code code-fold=\"true\"}\nfrom sklearn.manifold import TSNE\n\n# Apply t-SNE\ntsne = TSNE(n_components=2, random_state=42, perplexity=30)\nX_scurve_tsne = tsne.fit_transform(X_scurve)\n\n# Plot all three methods\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# MDS - Global Euclidean distances\naxes[0].scatter(X_scurve_mds[:, 0], X_scurve_mds[:, 1], c=color,\n                cmap='viridis', alpha=0.6, s=20)\naxes[0].set_xlabel('MDS1')\naxes[0].set_ylabel('MDS2')\naxes[0].set_title('MDS: Global Distances')\nsns.despine(ax=axes[0])\n\n# Isomap - Geodesic distances\naxes[1].scatter(X_scurve_isomap[:, 0], X_scurve_isomap[:, 1], c=color,\n                cmap='viridis', alpha=0.6, s=20)\naxes[1].set_xlabel('Isomap1')\naxes[1].set_ylabel('Isomap2')\naxes[1].set_title('Isomap: Geodesic Distances')\nsns.despine(ax=axes[1])\n\n# t-SNE - Local neighborhoods\naxes[2].scatter(X_scurve_tsne[:, 0], X_scurve_tsne[:, 1], c=color,\n                cmap='viridis', alpha=0.6, s=20)\naxes[2].set_xlabel('t-SNE1')\naxes[2].set_ylabel('t-SNE2')\naxes[2].set_title('t-SNE: Local Structure')\nsns.despine(ax=axes[2])\n\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![Comparing global, geodesic, and local approaches on the S-curve](highd-data_files/figure-html/cell-8-output-1.png){}\n:::\n:::\n\n\nAll three methods successfully straighten the S-curve, but through different philosophies. MDS compromises between all distances. Isomap follows the manifold globally. t-SNE focuses on preserving neighborhoods. Each makes different trade-offs between local and global structure.\n\nThe key parameter in t-SNE is perplexity, which typically ranges from 30 to 50. Perplexity controls the effective neighborhood size. Too low perplexity fragments clusters. Too high perplexity loses local detail. Finding the right balance is important.\n\n### What t-SNE preserves (and what it doesn't)\n\nt-SNE is powerful but has important limitations. It preserves local structure, keeping points that are neighbors in high dimensions as neighbors in 2D. It preserves clusters, keeping well-separated groups separated. It preserves relative relationships within neighborhoods, so if A is closer to B than to C locally, this is preserved.\n\nWhat t-SNE does NOT preserve: the actual distance between points is not meaningful. The relative position of distant clusters is arbitrary. Large clusters may appear smaller, and vice versa. Tight clusters may be spread out, and sparse regions may appear dense.\n\nYou cannot conclude from a t-SNE plot that \"cluster A is twice as far from B as from C.\" Distances are not preserved. You cannot conclude that \"cluster A is twice the size of B.\" Sizes are not preserved. You cannot conclude that \"the data has exactly 5 clusters.\" Apparent clusters may be visualization artifacts.\n\nYou can conclude that \"these points form a distinct group separate from others.\" You can conclude that \"these points are more similar to each other than to distant points.\" You can conclude that \"the data has local structure and is not uniformly random.\"\n\n### Applying t-SNE to real data\n\nLet's apply t-SNE to a more realistic high-dimensional dataset: the digits dataset, which has 64 dimensions (8 by 8 pixel images).\n\n::: {#0d4c4fa6 .cell fig-height='10' fig-width='12' execution_count=8}\n``` {.python .cell-code code-fold=\"true\"}\nfrom sklearn.datasets import load_digits\n\n# Load digits dataset (8x8 images, 64 dimensions)\ndigits = load_digits()\nX_digits = digits.data\ny_digits = digits.target\n\n# Take a subset for speed (t-SNE is slow on large datasets)\nnp.random.seed(42)\nindices = np.random.choice(len(X_digits), size=1000, replace=False)\nX_subset = X_digits[indices]\ny_subset = y_digits[indices]\n\n# Apply t-SNE\ntsne_digits = TSNE(n_components=2, random_state=42, perplexity=40)\nX_digits_tsne = tsne_digits.fit_transform(X_subset)\n\n# Plot\nfig, ax = plt.subplots(figsize=(12, 10))\nscatter = ax.scatter(X_digits_tsne[:, 0], X_digits_tsne[:, 1],\n                     c=y_subset, cmap='tab10', alpha=0.7, s=30)\nax.set_xlabel('t-SNE1')\nax.set_ylabel('t-SNE2')\nax.set_title('t-SNE Visualization of Handwritten Digits (64D to 2D)')\ncbar = plt.colorbar(scatter, ax=ax, ticks=range(10))\ncbar.set_label('Digit Class')\nsns.despine()\n```\n\n::: {.cell-output .cell-output-display}\n![t-SNE visualization of handwritten digits (64 dimensions to 2D). Each color represents a digit class.](highd-data_files/figure-html/cell-9-output-1.png){}\n:::\n:::\n\n\nThe t-SNE projection beautifully separates most digit classes. Digits that look similar, like 3, 5, and 8, cluster near each other. Visually distinct digits, like 0 and 1, are well separated.\n\nThis demonstrates t-SNE's power. From 64 dimensions with no explicit information about what makes digits similar, t-SNE discovers the perceptual structure of handwritten digits. It's a remarkable achievement in unsupervised learning.\n\nAn important note: t-SNE is stochastic. Different runs produce different layouts, though cluster structure remains consistent. Always check multiple runs with different random seeds, especially for important scientific conclusions.\n\n## UMAP: A Faster Alternative\n\nUniform Manifold Approximation and Projection (UMAP) is a newer method from 2018 that has become popular as an alternative to t-SNE. Like t-SNE, UMAP preserves local structure, but it's based on different mathematical foundations in manifold learning and topological data analysis.\n\nUMAP has several advantages over t-SNE. It's faster, often 10 to 100 times faster than t-SNE on large datasets. It scales better, working well on datasets with millions of points. It preserves more global structure than t-SNE. It's also theoretically grounded in Riemannian geometry and fuzzy topology.\n\nThe trade-offs are worth noting. UMAP is less battle-tested than t-SNE since it's newer. It has more hyperparameters to tune, though defaults work well. It often produces similar-looking results to t-SNE, so the choice often comes down to speed.\n\n::: {#0d57f335 .cell fig-height='6' fig-width='14' execution_count=9}\n``` {.python .cell-code code-fold=\"true\"}\nimport umap\n\n# Apply UMAP\numap_model = umap.UMAP(n_components=2, random_state=42, n_neighbors=30)\nX_digits_umap = umap_model.fit_transform(X_subset)\n\n# Plot comparison\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# t-SNE\nscatter = axes[0].scatter(X_digits_tsne[:, 0], X_digits_tsne[:, 1],\n                          c=y_subset, cmap='tab10', alpha=0.7, s=30)\naxes[0].set_xlabel('t-SNE1')\naxes[0].set_ylabel('t-SNE2')\naxes[0].set_title('t-SNE')\nsns.despine(ax=axes[0])\n\n# UMAP\nscatter = axes[1].scatter(X_digits_umap[:, 0], X_digits_umap[:, 1],\n                          c=y_subset, cmap='tab10', alpha=0.7, s=30)\naxes[1].set_xlabel('UMAP1')\naxes[1].set_ylabel('UMAP2')\naxes[1].set_title('UMAP')\ncbar = plt.colorbar(scatter, ax=axes[1], ticks=range(10))\ncbar.set_label('Digit Class')\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/skojaku-admin/Documents/projects/applied-soft-comp/.venv/lib/python3.11/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n  warn(\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![UMAP vs t-SNE on digits dataset. UMAP often preserves more global structure while being much faster.](highd-data_files/figure-html/cell-10-output-2.png){}\n:::\n:::\n\n\nBoth methods reveal similar cluster structure, but UMAP tends to space clusters more evenly and preserve more of the global topology. Notice how UMAP places similar digits (3, 5, 8) in a connected region, suggesting they share underlying structure.\n\nUse UMAP when you have very large datasets (over 10,000 points) where t-SNE becomes slow. Use it when you want to preserve more global structure. Use it when you're doing exploratory analysis and want fast iteration. UMAP also supports projecting new data onto an existing embedding, which t-SNE doesn't easily support.\n\nStick with t-SNE when you need the most established method with extensive literature. Use t-SNE when you're working with moderate-sized datasets where speed isn't critical. Use t-SNE when you're replicating published work that used t-SNE.\n\n## The Bigger Picture: Choosing the Right Method\n\nDimensionality reduction is not a one-size-fits-all solution. Different methods make different trade-offs:\n\n| Method | Preserves | Speed | Scalability | When to use |\n|--------|-----------|-------|-------------|-------------|\n| **Scatter plot matrix** | Everything (2D projections) | Fast | 3-10 dimensions | Exploring moderate-dimensional data |\n| **PCA** | Global variance | Very fast | Excellent (1000s of dims) | Linear structure, interpretability needed |\n| **MDS** | All distances | Slow | Poor (100s of points) | Distance preservation critical |\n| **t-SNE** | Local structure | Slow | Moderate (10,000s of points) | Revealing clusters, local relationships |\n| **UMAP** | Local plus some global | Fast | Excellent (millions of points) | Large datasets, faster alternative to t-SNE |\n\nA practical workflow begins with PCA. Always run PCA first. It's fast, interpretable, and if it works well, you're done. Check how much variance the first 2 or 3 components explain.\n\nNext, check pairwise plots if feasible. If you have fewer than 10 dimensions, look at scatter plot matrices to understand pairwise relationships.\n\nTry t-SNE or UMAP if PCA doesn't reveal clear structure. Run them if the first 2 PCs explain less than 50 percent variance. Try them if you suspect non-linear relationships. Try them if you want to find clusters.\n\nValidate your findings with multiple approaches. Don't trust a single visualization. Try different random seeds for t-SNE and UMAP. Try different hyperparameters like perplexity and number of neighbors. Try different methods and see if t-SNE and PCA agree. Run statistical tests on apparent clusters.\n\n::: {.callout-tip title=\"Try it yourself\"}\nTake a dataset you're familiar with and apply all four methods: PCA, MDS, t-SNE, and UMAP. Compare the results. What structure does each method reveal? What structure does each method hide? Which visualization best matches your intuition about the data?\n:::\n\nDimensionality reduction can create apparent patterns that don't exist in the original data. Spurious clusters appear when t-SNE splits continuous data into false groups. Missing relationships occur when two clusters might be connected in high dimensions but appear separated in 2D. Misleading distances happen when distance and size in t-SNE and UMAP are not meaningful.\n\nAlways validate important findings with statistical tests or domain knowledge. A beautiful t-SNE plot is a starting point for investigation, not a final conclusion.\n\nVisualizing high-dimensional data is as much art as science. The goal is not to find \"the true projection.\" There is no single true way to flatten high-dimensional space onto a page. The goal is to reveal structure that helps you understand your data and ask better questions.\n\nAs data scientist Jake VanderPlas wrote: \"Dimensionality reduction is a form of lossy compression. The question is not whether you lose information—you always do—but whether you lose the information you care about.\"\n\n",
    "supporting": [
      "highd-data_files"
    ],
    "filters": [],
    "includes": {}
  }
}
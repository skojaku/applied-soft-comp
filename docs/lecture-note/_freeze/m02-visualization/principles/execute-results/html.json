{
  "hash": "c437f9333071dbc9e940b8100f21f3fc",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Principles of Perception in Data Visualization\"\njupyter: advnetsci\nexecute:\n    enabled: true\n---\n\nWe extensively rely on visuals to perceive the world around us. However, our visual perception is not as truthful as you might think. It can be easily misled or manipulated if we are not aware of the inherent biases in how we see.\n\nLet us start with making specific about \"perceptions\":\n\n- **Color perception**: how we perceive color\n- **Form perception**: how we perceive shape, structure, and magnitude\n- **Attention perception**: how we perceive attention and focus\n\nWe will cover each of these in detail in the following sections.\n\n# Color perception\n\nColor is one of the most powerful tools in data visualization, but it is also one of the most complex. Our perception of color is not absolute; it is contextual and subjective.\n\n## Color is contextual\n\nOur visual system tries to maintain **color constancy**, meaning we perceive a familiar object as being a consistent color regardless of the lighting conditions. This is why we recognize a banana as yellow whether it's in bright sunlight or in a dim room. However, this helpful adaptation can create peculiar biases in unfamiliar contexts.\n\n![The Dress](https://external-preview.redd.it/U9_nOExNxuuZc8vj8vm1UjvchQpBUgWBeZIGX3mOd9M.gif?auto=webp&s=b577a4f48f5bc87162a2db0c53d41f092e925c65)\n\nThe infamous \"dress\" illusion highlights how our brain makes assumptions about lighting, causing some people to see the dress as blue and black (in bright light) and others as white and gold (in shadow). The colors are physically the same, but our perception of them is not.\n\nTake another example below:\n\n![A \"green\" tree with no green pixels](https://upload.wikimedia.org/wikipedia/en/thumb/8/81/Mountain-spring-redwhite.png/520px-Mountain-spring-redwhite.png)\n\nThis happens because what we \"see\" is not just the raw wavelength of light hitting our eyes. Our visual cortex processes that raw signal, making inferences based on context and *prior experience*. For example, we perceive the leaves of a tree as green, even in a photograph made entirely of red, black, and white pixels, because our brain \"knows\" trees are green.\n\n\n## Encoding colors objectively\n\nColor is a physical phenomenon. It is the result of electromagnetic waves within a certain range of wavelengths, which our eyes and brain interpret as \"color.\"\n\nMost of what we see in the world is not pure spectral (single-wavelength) color, but rather mixtures of different wavelengths. Our eyes have three types of color receptors (cones) sensitive to different, overlapping ranges of wavelengths. When light of various wavelengths enters our eyes, the combination of signals from these cones is interpreted by our brain as a specific color sensation.\n\nBecause of this, we can represent an enormous range of colors simply by mixing a small set of primary colors. For screens and digital work, these are Red, Green, and Blue (the **RGB** model). By varying the intensity of each, we can mimic the effect of almost any color found in nature. This is called **additive color mixing**, because we're combining different wavelengths of light.\n\nConversely, in printing (where we start with white paper and \"take away\" light with inks), we use the **CMY (Cyan, Magenta, Yellow)** system, where inks subtract some wavelengths and leave others. Mixing these subtractively produces the desired color on paper.\n\n![](https://badriadhikari.github.io/data-viz-workshop-2021/colors/rgb-cmyk.jpg){width=\"50%\" fig-align=\"center\"}\n\nAnother color model that is more intuitive for designing color palettes is the **HSL (Hue, Saturation, Lightness)** model. It is designed to match how humans naturally think about and describe color. Hue is the pure color, Saturation is the intensity, and Lightness/Value is the brightness.\n\n![](https://giggster.com/guide/static/fed42130c194b0c240a4ec10408adf97/8282f/hsl-cover-2.png){width=\"50%\" fig-align=\"center\"}\n\n::: {.callout-note title=\"Accessibility Matters: Designing for Color Blindness\"}\nWhen choosing colors, it is essential to make your visualizations accessible to everyone, including the significant portion of the population with color vision deficiencies (CVD), which affects about 8% of men and 0.5% of women. Avoid relying on red-green contrasts, as this is the most problematic pair for those with CVD. Instead, use perceptually uniform palettes—such as those available from tools like [ColorBrewer](https://colorbrewer2.org/)—and always combine color with other cues like shape, patterns, or direct labels to ensure that important information is conveyed clearly to all viewers.\n:::\n\n## Perceptually uniform palettes\n\nA **perceptually uniform colormap** is one where equal steps in the data are perceived as equal steps in color. The common \"rainbow\" (or \"jet\") colormap fails at this because its brightness changes non-uniformly, creating false boundaries and hiding details. Palettes like **viridis**, **plasma**, **inferno**, **magma**, and **cividis** were engineered to have a monotonically increasing luminance, making them accurate, intuitive, and accessible.\n\nSee also the following paper for more details:\n\n- [Crameri, F., Shephard, G. E., & Heron, P. J. (2020). The misuse of colour in science communication. Nature communications, 11(1), 5444.](https://www.nature.com/articles/s41467-020-19160-7)\n\n\n![Viridis vs. Jet Colormap](https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-319-50835-1_36/MediaObjects/437683_1_En_36_Fig1_HTML.gif)\n\n::: {.column-margin}\n\nThis is a video about the story behind how the perceptually uniform palettes were created.\n\n<iframe width=\"280\" height=\"150\" src=\"https://www.youtube.com/embed/xAoljeRJ3lU?si=yW1ywNB0ZS_WpfNY\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n:::\n\n# The Perception of Form and Quantity\n\nBeyond color, we must consider how we perceive shape, structure, and magnitude.\n\n### Grouping and Structure: The Gestalt Principles\n\n![](https://nazaninazimi.wordpress.com/wp-content/uploads/2019/10/jfkvjkdf.png)\n\nOur brains have an innate tendency to organize visual elements into patterns and unified wholes, a phenomenon described by the **Gestalt principles**. These principles explain how we instinctively group objects based on characteristics such as proximity, similarity, enclosure, continuity, closure, and the distinction between figure and ground. In the context of data visualization, understanding these principles is crucial for designing graphics that are both clear and insightful.\n\nFor example, the principle of **proximity** suggests that elements placed close together are perceived as belonging to the same group. The principle of **similarity** indicates that items sharing visual properties (such as color, shape, or size) are seen as part of the same group. **Enclosure** uses shapes, borders, or backgrounds to visually bound elements together.\n\nBelow is a Python script that demonstrates these Gestalt principles using scatter plots:\n\n::: {#c6120e77 .cell fig-width='8' execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![Gestalt principles demonstration](principles_files/figure-html/cell-2-output-1.png){}\n:::\n:::\n\n\nBy applying Gestalt principles thoughtfully in our visualizations, we help users interpret complex information quickly and accurately. These design choices guide attention, clarify relationships, emphasize key patterns or outliers, and ultimately make our data stories more compelling and accessible.\n\n\n### Perceiving Quantity\n\nOur ability to accurately judge quantities depends heavily on how information is visually encoded. **Steven's Power Law** reveals that humans are most precise at estimating **length**, less accurate with **area**, and even worse with **volume**.\n\n![Steven's Power law](https://graphworkflow.files.wordpress.com/2019/09/stevens_law.png){width=\"50%\" fig-align=\"center\"}\n\nThis has direct consequences for our choice of chart types. A good example is a bar chart. Both representations can represent the same data, but the preference is to use a bar chart when the data is ordinal or categorical. This is because a pie chart encodes values as angles and areas of slices, which our brains struggle to compare accurately---especially when the slices are similar in size. That's why it's often difficult to judge whether 23% or 27% is larger in a pie chart without checking the labels.\n\n::: {#fig-pie-vs-bar}\n\n::: {#90e1f5b5 .cell fig-width='8' execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![Pie vs. Bar Chart](principles_files/figure-html/cell-3-output-1.png){}\n:::\n:::\n\n\nThe bar chart clearly shows the relative differences between the categories. The pi chart makes it difficult to judge the relative differences.\n:::\n\n\n# Preattentive attributes\n\n**Preattentive attributes** are visual properties that our brains process in milliseconds, before we even pay conscious attention. By using them purposefully, you can control the visual hierarchy of your chart and tell a story.\n\n\n::: {#fig-preattentive-attributes}\n\n![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQT7C8ugvFfaM2MFdbY5vmWcbyVlAjriS8zlwfOve5Rlnw3JKNA0XJsUzu-jaLhXDQUdQY&usqp=CAU){width=\"50%\" fig-align=\"center\"}\n\nPreattentive attributes are the visual properties that our brains process in milliseconds, before we even pay conscious attention. By using them purposefully, you can control the visual hierarchy of your chart and tell a story.\n:::\n\nCommon attributes include a distinct color, size, shape, or orientation. The most effective way to use them is to de-emphasize the majority of your data (e.g., making it light grey) and use a single, strong attribute to highlight your key message. This \"grey vs. red\" technique immediately tells the viewer, \"Look here! This is what matters.\"\n\n\n::: {#fig-preattentive-attributes-example}\n\n![](https://substackcdn.com/image/fetch/$s_!qmQC!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf489564-f69c-4aa7-b335-0f7d172205c4_4349x3076.jpeg)\n\nAn example of using preattentive attributes to highlight what the creator wants the viewer to pay attention to.\n:::\n\n# Summary\n\nEffective visualizations harness our natural perceptions—such as color, size, shape, and orientation—to guide attention and reveal key patterns in data. By understanding and intentionally using these perceptual principles, you can create graphics that clearly communicate your message and make your data more memorable and accessible.\n\n",
    "supporting": [
      "principles_files"
    ],
    "filters": [],
    "includes": {}
  }
}
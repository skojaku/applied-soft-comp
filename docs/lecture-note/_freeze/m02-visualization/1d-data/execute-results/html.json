{
  "hash": "99ae13e74e548c9df017dca927f80952",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"1D Data Visualization\"\njupyter: advnetsci\nexecute:\n    enabled: true\n---\n\nImagine you're reading a research paper that claims \"Treatment A is significantly better than Treatment B.\" The paper shows a bar chart with two bars and error bars. The difference looks impressive. But here's the question: what does the actual data look like? Are there 5 data points per group? 500? Are they normally distributed, or are there outliers? Are most points clustered together, or spread out?\n\nWithout seeing the raw data, you're flying blind. And unfortunately, many scientific papers and reports make this same mistake: they summarize data without showing it.\n\nOne thing I want you to keep in mind:\n\n**Show all the data points, whenever possible.**\n\nThis is crucial for understanding the data and for communicating the message of the data.\n\n# Why Showing All Data Matters\n\n![](https://simplystatistics.org/posts/2019-02-21-dynamite-plots-must-die/index_files/figure-html5/show-data-1.png)\n\nStatisticians have been campaigning against bar charts with error bars—called \"dynamite plots\"—for years. Yet a systematic review found that **85.6% of papers in top physiology journals still use them**. They appear everywhere: Nature, Science, Cell.\n\nWhy is this a problem? A dynamite plot shows you exactly four numbers (two means and two standard errors), regardless of sample size. But worse, **completely different datasets produce identical bar charts**. A dataset with outliers, a uniform distribution, or a bimodal distribution can all generate the same plot.\n\nRafael Irizarry showed an actual data behind a blood pressure comparison. The paper shows a bar chart with two bars and error bars. The difference looks significant.\nBut the raw data revealed an extreme outlier (possibly a data entry error) and substantial overlap between groups. Remove that single outlier, and the result was no longer significant.\n\nAs Irizarry put it in [his open letter to journal editors](https://simplystatistics.org/posts/2019-02-21-dynamite-plots-must-die/): dynamite plots conceal the data rather than showing it. The solution? Show the actual data points whenever possible, and use distributions (boxplots, histograms, density plots) when you can't.\n\n# How to Show All Data Points\n\nThe most straightforward approach is to plot every single data point. A **swarm plot** (also called a beeswarm plot) does exactly this: it displays each observation as a point, with points arranged to avoid overlap.\n\n::: {#c64d5212 .cell fig-height='3' fig-width='10' execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate sample data\nfig, ax = plt.subplots(figsize=(10, 3))\ngroup_a = np.random.normal(100, 15, 50)\ngroup_b = np.random.normal(120, 20, 50)\ndata = {'Value': np.concatenate([group_a, group_b]),\n        'Group': ['A']*50 + ['B']*50}\n\n# Create swarm plot\nsns.swarmplot(data=data, y='Group', x='Value', ax = ax)\nplt.title('Swarm Plot: Every Point Visible')\nsns.despine()\n```\n\n::: {.cell-output .cell-output-display}\n![Swarm Plot](1d-data_files/figure-html/cell-2-output-1.png){}\n:::\n:::\n\n\nSwarm plots are perfect for small to moderate datasets (roughly up to 100-200 points per group). There, you can see the actual sample size, the distribution shape, individual outliers, and the spread of the data.\n\nWhen you have too many points for a swarm plot, a **strip plot with jittering** can help. Instead of carefully arranging points to avoid overlap, we add random noise (jitter) to the x-position of each point.\n\n::: {#1b819a59 .cell fig-height='3' fig-width='10' execution_count=2}\n``` {.python .cell-code code-fold=\"true\"}\n# Strip plot with jittering\nfig, ax = plt.subplots(figsize=(10, 3))\nsns.stripplot(data=data, y='Group', x='Value', alpha=0.6, jitter=0.2, ax = ax)\nplt.title('Strip Plot with Jittering')\nsns.despine()\n```\n\n::: {.cell-output .cell-output-display}\n![Strip Plot with Jittering](1d-data_files/figure-html/cell-3-output-1.png){}\n:::\n:::\n\n\nThe key parameters:\n- `alpha`: Controls transparency (0 = invisible, 1 = opaque). Values around 0.3-0.7 work well.\n- `jitter`: Amount of random horizontal displacement. Too much jitter and groups overlap; too little and points stack vertically.\n\n\n::: {.column-margin}\n\n![](https://www.science.org/cms/10.1126/sciadv.abb9004/asset/2e404f99-5edf-44bc-a925-3e4a5e97999a/assets/graphic/abb9004-f4.jpeg)\n\nA figure taken from the paper [Neural embeddings of scholarly periodicals reveal complex disciplinary organizations](https://www.science.org/doi/10.1126/sciadv.abb9004) by showing the distribution of publications in terms of various scientific contrasts.\n\n:::\n\nFor even larger datasets, consider a **barcode plot**. This shows each data point as a small vertical tick mark along an axis. It's minimalist but effective for showing the distribution of many points.\n\n::: {#87f841f9 .cell fig-height='2' fig-width='10' execution_count=3}\n``` {.python .cell-code code-fold=\"true\"}\n# Barcode plot using rug plot\nimport pandas as pd\n\n# Convert data to DataFrame if not already\ndata_df = pd.DataFrame(data)\n\nfig, ax = plt.subplots(figsize=(10, 2))\nfor i, group in enumerate(['A', 'B']):\n    values = data_df.loc[data_df['Group'] == group, 'Value']\n    ax.plot(values, [i]*len(values), '|', markersize=10, alpha=0.7)\nax.set_yticks([0, 1])\nax.set_yticklabels(['A', 'B'])\nax.set_ylim(-0.5, 1.5)\nax.set_xlabel('Value')\nax.set_title('Barcode Plot')\nsns.despine()\n```\n\n::: {.cell-output .cell-output-display}\n![Rug Plot](1d-data_files/figure-html/cell-4-output-1.png){}\n:::\n:::\n\n\nBarcode plots work well when you have thousands of points and want to show density patterns without losing the \"raw data\" feel.\n\n# When can't show all data points?\n\nWhen your dataset is large enough that individual points become impractical to show, you need to summarize the distribution. The most common approach is the **histogram**.\n\nA histogram divides your data range into bins and counts how many observations fall into each bin. It's a powerful tool for understanding the shape of your distribution.\n\n::: {#fb547a53 .cell fig-height='3' fig-width='10' execution_count=4}\n``` {.python .cell-code code-fold=\"true\"}\n# Histogram\nplt.hist(group_a, bins=15, alpha=0.5, label='Group A', edgecolor='black')\nplt.hist(group_b, bins=15, alpha=0.5, label='Group B', edgecolor='black')\nplt.xlabel('Value')\nplt.ylabel('Count')\nplt.legend()\nplt.title('Histogram: Distribution Comparison')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Histogram](1d-data_files/figure-html/cell-5-output-1.png){}\n:::\n:::\n\n\nThe number of bins dramatically affects how your histogram looks. If you have too few bins, you lose detail and might miss important features like bimodality. If you have too many bins, the histogram becomes noisy and hard to interpret.\n\nA good starting point is the **Sturges' rule**: number of bins H $\\log_2(n) + 1$, where $n$ is the sample size. But always experiment! Try different bin numbers and see what reveals the most about your data's structure.\n\nHistograms have a problem: they're sensitive to bin width and bin placement. Move your bins slightly, and the histogram can look quite different.\n\n**Kernel Density Estimation (KDE)** provides a smooth alternative. Instead of binning, KDE places a small \"kernel\" (usually a Gaussian curve) at each data point and sums them up. The result is a smooth density curve.\n\n::: {#f163d70d .cell fig-height='3' fig-width='10' execution_count=5}\n``` {.python .cell-code code-fold=\"true\"}\nsns.kdeplot(data=group_a, label='A', fill=True, alpha=0.5)\nsns.kdeplot(data=group_b, label='B', fill=True, alpha=0.5)\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.legend()\nplt.title('Kernel Density Estimate')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Kernel Density Estimate](1d-data_files/figure-html/cell-6-output-1.png){}\n:::\n:::\n\n\nKDE plots are elegant and reveal the shape of your distribution without the arbitrary choices of histograms. However, they can be misleading at the edges of your data and may suggest data exists where it doesn't.\n\n# For Heavy-Tailed Data\n\nSome data are extremely heterogeneous---think income distributions, city populations, or earthquake magnitudes. These distributions often have heavy tails: most values are small, but a few are enormous.\n\nFor this kind of data, histograms and KDE plots can be misleading because they compress the tail into a tiny region of the plot.\n\nThe **cumulative distribution function (CDF)** shows the proportion of data points less than or equal to each value. Instead of asking \"how many points are in this bin?\", the CDF asks \"what fraction of points are below this value?\"\n\n**The CDF is a density estimation method that requires no parameter choices.** Unlike histograms (which require bin size) or KDE (which requires bandwidth), the CDF is completely determined by your data. There are no arbitrary decisions that change how your data looks—making it one of the most honest ways to visualize a distribution.\n\n::: {#e8d60d18 .cell fig-height='3' fig-width='10' execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\n# CDF\nsorted_a = np.sort(group_a)\ncdf_a = np.arange(1, len(sorted_a) + 1) / len(sorted_a)\n\nsorted_b = np.sort(group_b)\ncdf_b = np.arange(1, len(sorted_b) + 1) / len(sorted_b)\n\nplt.plot(sorted_a, cdf_a, label='Group A', linewidth=2)\nplt.plot(sorted_b, cdf_b, label='Group B', linewidth=2)\nplt.xlabel('Value')\nplt.ylabel('Cumulative Probability')\nplt.legend()\nplt.title('Cumulative Distribution Function')\nplt.grid(True, alpha=0.3)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Cumulative Distribution Function](1d-data_files/figure-html/cell-7-output-1.png){}\n:::\n:::\n\n\nThe CDF has several advantages:\n\n- No binning decisions: Every data point is shown\n- Easy to read percentiles: The median is where CDF = 0.5\n- Great for comparisons: Differences between groups are easy to spot\n\nFor heavy-*tailed* distributions, the **complementary cumulative distribution function (CCDF)** is even more useful. The CCDF shows the proportion of data points *greater than* each value: CCDF(x) = 1 - CDF(x).\n\nUnlike the CDF, the CCDF can show, when combined with the log-log scale,  the tail of heavy-tailed distributions.\n\n::: {#9ede2de7 .cell fig-height='5' fig-width='10' execution_count=7}\n``` {.python .cell-code code-fold=\"true\"}\n# CCDF on log-log scale\n# Generate heavy-tailed data\n# Generate heavy-tailed data\nsns.set(font_scale=2.0)\nsns.set_style(\"white\")\n\nheavy_tailed = np.random.pareto(2, 1000) + 1\n\nsorted_data = np.sort(heavy_tailed)\ncdf = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\nccdf = 1 - cdf\n\nfig, ax = plt.subplots(1, 2, figsize=(14, 7))\n\n# CDF plot (linear scale) using seaborn (lineplot)\nsns.lineplot(x=sorted_data, y=cdf, ax=ax[0], color=sns.color_palette()[0], linewidth=2)\nax[0].set_xlabel('Value')\nax[0].set_ylabel('P(X ≤ x)')\nax[0].set_title('Cumulative Distribution Function (CDF)')\nax[0].grid(True, alpha=0.3)\n\n# CCDF plot (log-log scale) using seaborn (scatterplot)\nsns.scatterplot(x=sorted_data, y=ccdf, ax=ax[1], color=sns.color_palette()[1], alpha=0.5, s=9, marker='o')\nax[1].set_xscale('log')\nax[1].set_yscale('log')\nax[1].set_xlabel('Value')\nax[1].set_ylabel('P(X > x)')\nax[1].set_title('Complementary Cumulative Distribution (CCDF)')\nax[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![CDF vs CCDF. The CCDF reveals the tail behavior that's invisible in traditional histograms.](1d-data_files/figure-html/cell-8-output-1.png){}\n:::\n:::\n\n\n# The Bigger Picture\n\nThe methods you choose to visualize your data aren't just aesthetic choices---they're scientific choices. Different visualizations reveal different aspects of your data, and some can hide important patterns.\n\nBy starting with the raw data and building up to summaries, you ensure that you understand what you're working with. And by showing your data (not just summarizing it), you allow others to draw their own conclusions.\n\n",
    "supporting": [
      "1d-data_files"
    ],
    "filters": [],
    "includes": {}
  }
}
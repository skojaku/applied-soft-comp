{
  "hash": "0ddcae5a132c6eb1c66016480b66be81",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Prompt Engineering\"\nexecute:\n    enabled: true\n---\n\n![](../figs/top-prompt-engineering.png)\n\n::: {.callout-note appearance=\"minimal\"}\n## Spoiler\nLLMs are stateless pattern matchers that sample from probability distributions—the same question phrased differently activates different statistical patterns, producing dramatically different outputs.\n:::\n\n## The Naive Model vs. The Reality\n\nIf a machine can answer questions, it should respond consistently regardless of phrasing. You're asking for the same information; the answer shouldn't change. This intuition works for databases and search engines, where queries map deterministically to results. We expect robustness to variation.\n\nLLMs shatter this expectation. Ask \"Summarize this abstract\" and get a concise two-sentence summary. Ask \"What's this abstract about?\" and get three rambling paragraphs. Same content, different phrasing, completely different outputs. This isn't a bug—it's fundamental to how LLMs work. They don't retrieve information; they **sample from probability distributions conditioned on your exact phrasing.** Every word in your prompt shifts the distribution. Change \"Summarize\" to \"What's this about?\" and you activate different statistical patterns from the training data, patterns that correlate with different response lengths, structures, and styles.\n\nThe paradox: LLMs are simultaneously powerful and brittle. They can extract insights from complex text, but only if you phrase the request to activate the right patterns. Prompt engineering is the discipline of designing inputs that reliably activate desired patterns across varied tasks.\n\n## The Hidden Mechanism\n\nImagine you're playing a word association game. Someone says \"capital,\" and you must say the next word. If the previous sentence was \"The capital of France is,\" you say \"Paris.\" If it was \"We need more capital to,\" you say \"fund\" or \"invest.\" The word \"capital\" doesn't have one meaning—it activates different patterns depending on context. LLMs work identically, but at massive scale.\n\nWhen you submit a prompt, the model converts it into tokens and embeds those tokens in high-dimensional space. Each token's position in that space depends on surrounding tokens—context shapes meaning. The model then samples the next token from a probability distribution over its vocabulary, conditioned on all previous tokens. It repeats this process until it generates a complete response. Critically, **your exact phrasing determines which region of probability space the model occupies when it begins sampling.** Slightly different prompts place the model in different regions, where different tokens have high probability.\n\nThis creates extreme sensitivity to phrasing. Adding \"Think step by step\" at the end of a prompt shifts the probability distribution toward reasoning patterns that include intermediate steps, because the training data contains many examples where \"think step by step\" preceded structured reasoning. Adding \"You are an expert researcher\" shifts the distribution toward formal, technical language patterns. Specifying \"Output format: Domain: ..., Methods: ...\" shifts toward structured extraction patterns. Each modification activates different statistical regularities compressed during training.\n\nThe model has no internal representation of what you \"really want.\" It only knows which tokens tend to follow which other tokens in which contexts. Prompt engineering exploits this by deliberately activating patterns that produce desired outputs.\n\n## The Strategic Application\n\n![](../figs/prompt-tuning-manga.png){width=70% fig-align=center}\n\nEffective prompts activate desired patterns by combining structural components that mirror patterns in training data. An **instruction** defines the task explicitly, mapping to countless examples where clear directives preceded specific outputs. **Data** provides the input to process. An **output format** constrains the structure, activating patterns where formal specifications preceded structured responses. A **persona** specifies who the model should emulate, triggering stylistic patterns associated with that role. **Context** provides background information—why the task matters, who the response serves, relevant constraints—that helps the model select appropriate patterns from ambiguous alternatives.\n\nNot every component is necessary. Simple extraction tasks need only instruction, data, and format. Style-sensitive tasks benefit from persona. Complex scenarios with ambiguity require context to disambiguate. The strategy is to provide exactly enough structure to activate the desired pattern without overloading the prompt with irrelevant information that dilutes the signal.\n\nWe'll build a prompt progressively, adding components one at a time to observe how each shifts the output distribution.\n\n\n### Building from Instruction and Data\n\nThe most basic prompt consists of an instruction that defines the task and data that provides the input to process:\n\n::: {#40ffbb41 .cell execution_count=1}\n``` {.python .cell-code}\ninstruction = \"Summarize this abstract\"\ndata = \"\"\"\nWe develop a graph neural network for predicting protein-protein interactions\nfrom sequence data. Our model uses attention mechanisms to identify functionally\nimportant amino acid subsequences. We achieve 89% accuracy on benchmark datasets,\noutperforming previous methods by 7%. The model also provides interpretable\nattention weights showing which protein regions drive predictions.\n\"\"\"\n\nprompt = f\"{instruction}. {data}\"\n```\n:::\n\n\n::: {#025243b4 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\"}\nimport ollama\n\nparams_llm = {\"model\": \"gemma3:270m\", \"options\": {\"temperature\": 0.3}}\n\nresponse = ollama.generate(prompt=prompt, **params_llm)\nprint(response.response)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThis abstract describes a graph neural network (GNN) for predicting protein-protein interactions. The model utilizes attention mechanisms to identify functionally important amino acid subsequences, achieving 89% accuracy on benchmark datasets and providing interpretable attention weights.\n\n```\n:::\n:::\n\n\nThis basic prompt works, but output varies—the model might produce a long summary, a short one, or change format across runs. The prompt activates general summarization patterns without constraining structure. Adding an output format specification narrows the distribution:\n\n::: {#d1101cad .cell execution_count=3}\n``` {.python .cell-code}\noutput_format = \"\"\"Provide the summary in exactly 2 sentences:\n- First sentence: What problem and method\n- Second sentence: Key result with numbers\"\"\"\n\nprompt_with_format = f\"\"\"{instruction}. {data}. {output_format}\"\"\"\n```\n:::\n\n\nThe output format constraint produces structured, consistent output by activating patterns where format specifications preceded conforming responses. This becomes critical when processing hundreds of papers—you need programmatically parseable structure, not freeform text.\n\n### Adding Persona to Control Style\n\nA persona tells the LLM who it should emulate, activating stylistic patterns associated with that role in training data. Consider a customer support scenario where tone matters:\n\n::: {#a3214157 .cell execution_count=4}\n``` {.python .cell-code}\n# New example for persona demonstration\ninstruction = \"Help the customer reconnect to the service by providing troubleshooting instructions.\"\ndata = \"Customer: I cannot see any webpage. Need help ASAP!\"\noutput_format = \"Keep the response concise and polite. Provide a clear resolution in 2-3 sentences.\"\n\nformal_persona = \"You are a professional customer support agent who responds formally and ensures clarity and professionalism.\"\n\nprompt_with_persona = f\"\"\"{formal_persona}. {instruction}. {data}. {output_format}\"\"\"\n```\n:::\n\n\n::: {#720163d6 .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\"}\nprint(\"BASE (no persona):\")\nprint(ollama.generate(prompt=instruction + \". \" + data + \". \" + output_format, **params_llm).response)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nprint(\"WITH PERSONA:\")\nprint(ollama.generate(prompt=prompt_with_persona, **params_llm).response)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBASE (no persona):\nOkay, I understand. Let's try to troubleshoot this. Please provide the webpage you're having trouble with. Once I have that information, I'll do my best to help you get back online.\n\n\n============================================================\n\nWITH PERSONA:\nHello! I understand you cannot see any webpage. Could you please try accessing the website again? I'm here to assist you in finding a solution.\n\n```\n:::\n:::\n\n\nThe persona shifts tone and style. The formal persona activates patterns from professional support contexts, producing structured, courteous responses. Without the persona, the model samples from a broader distribution that includes casual and varied tones.\n\n### Adding Context to Disambiguate\n\nContext provides additional information that helps the model select appropriate patterns when multiple valid interpretations exist. Context can include background information explaining why the task matters, audience information specifying who the response serves, and constraints defining special circumstances. Consider adding background urgency:\n\n::: {#3bdbbe72 .cell execution_count=6}\n``` {.python .cell-code}\ncontext_background = \"\"\"The customer is extremely frustrated because their internet has been down for three days, and they need it for an important online job interview. They emphasize that 'This is a life-or-death situation for my career!'\"\"\"\n\nprompt_with_context = f\"\"\"{formal_persona}. {instruction}. {data}. {output_format}. Context: {context_background}\"\"\"\n```\n:::\n\n\n::: {#ee293202 .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\"}\nprint(\"WITH PERSONA:\")\nprint(ollama.generate(prompt=prompt_with_persona, **params_llm).response)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nprint(\"WITH PERSONA + CONTEXT (background):\")\nprint(ollama.generate(prompt=prompt_with_context, **params_llm).response)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWITH PERSONA:\nDear [Customer Name],\n\nI understand you cannot see any webpage. To help me assist you, could you please try to access the website? I'll do my best to find the solution.\n\n\n============================================================\n\nWITH PERSONA + CONTEXT (background):\nThank you for contacting us. I understand your frustration regarding your internet connection and the need for this important job interview. I'm here to assist you with troubleshooting and providing clear instructions. Please let me know if you have any further questions.\n\n```\n:::\n:::\n\n\nBackground context adds urgency and emotional weight, activating patterns where high-stakes situations preceded empathetic, prioritized responses. The model doesn't understand emotion, but it has seen urgency markers correlate with specific response patterns.\n\nAudience information creates even more dramatic shifts. Compare responses for non-technical versus technical users:\n\n::: {#e5d1b4d6 .cell execution_count=8}\n``` {.python .cell-code}\n# Context with audience information for non-technical user\ncontext_with_audience_nontech = f\"\"\"{context_background} The customer does not know any technical terms like modem, router, networks, etc.\"\"\"\n\ncontext_with_audience_tech = f\"\"\"{context_background} The customer is Head of IT Infrastructure of our company.\"\"\"\n\nprompt_with_context_nontech = f\"\"\"{formal_persona}. {instruction}. {data}. {output_format}. Context: {context_with_audience_nontech}\"\"\"\nprompt_with_context_tech = f\"\"\"{formal_persona}. {instruction}. {data}. {output_format}. Context: {context_with_audience_tech}\"\"\"\n```\n:::\n\n\n::: {#2bb23938 .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\"}\nprint(\"WITH PERSONA + CONTEXT (background only):\")\nprint(ollama.generate(prompt=prompt_with_context, **params_llm).response)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nprint(\"WITH PERSONA + CONTEXT (background + non-tech audience):\")\nprint(ollama.generate(prompt=prompt_with_context_nontech, **params_llm).response)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nprint(\"WITH PERSONA + CONTEXT (background + tech audience):\")\nprint(ollama.generate(prompt=prompt_with_context_tech, **params_llm).response)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWITH PERSONA + CONTEXT (background only):\nThank you for contacting us. I understand your frustration regarding your internet connection and the need for this important job interview. I'm here to assist you with troubleshooting and providing you with the necessary information. Please let me know how I can help you get back online as soon as possible.\n\n\n============================================================\n\nWITH PERSONA + CONTEXT (background + non-tech audience):\nHello, I understand you are experiencing internet connectivity issues and are looking for assistance. To help me troubleshoot this, could you please provide me with the following information:\n*   The exact error message you are seeing.\n*   The specific URL or website that is causing the problem.\n*   Any other relevant details about the issue.\n\nOnce I have this information, I will be able to provide you with a clear and concise troubleshooting guide. Thank you for your patience and understanding.\n\n============================================================\n\nWITH PERSONA + CONTEXT (background + tech audience):\n\"I understand your frustration, and I apologize for the inconvenience this is causing. I'm here to assist you with troubleshooting. Please try the following steps:\n1. Check your internet connection.\n2. Try restarting your modem and router.\n3. If the problem persists, please contact our technical support team at [phone number] or [email address]. We'll be happy to help.\"\n\n```\n:::\n:::\n\n\nAudience information dramatically shifts technical level and terminology. For non-technical users, the response avoids jargon because the training data contains many examples where \"does not know technical terms\" preceded simplified explanations. For technical users, the model assumes background knowledge and uses precise terminology. Same underlying mechanism—pattern matching—but different patterns activated.\n\nThe complete template combines all components, but not every prompt needs every component. Simple extraction tasks need only instruction, data, and output format. Style-sensitive tasks benefit from persona. Complex scenarios with ambiguity require context:\n\n::: {#6ea49597 .cell execution_count=10}\n``` {.python .cell-code}\nprompt_template = \"\"\"\n{persona}\n\n{instruction}\n\n{data}\n\nContext: {context}\n\n{output_format}\n\"\"\"\n```\n:::\n\n\n::: {.callout-note}\n## When Personas Help (and When They Don't)\n\nResearch shows that adding personas can improve tone and style, but **does not necessarily improve performance on factual tasks**. In some cases, personas may even degrade performance or introduce biases.\n\n**Use personas when:** You need specific tone/style, responses tailored to an audience, or a particular perspective.\n\n**Avoid personas when:** You need maximum factual accuracy, the task is purely extraction/classification, or you're concerned about bias introduction.\n\nAdditionally, when prompted to adopt specific socio-demographic personas, LLMs may produce responses that reflect societal stereotypes. Be careful when designing persona prompts to avoid reinforcing harmful biases.\n\n**References:**\n- [When \"A Helpful Assistant\" Is Not Really Helpful](https://arxiv.org/abs/2311.10054)\n- [Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs](https://arxiv.org/abs/2311.04892)\n:::\n\n::: {.callout-tip}\n## Context and Emotion Prompting\n\nContext can include:\n- **Background information**: Why the task is important, what led to this request\n- **Audience information**: Who the response is for (technical level, expertise, role)\n- **Emotional cues**: Research shows that including emotional cues (e.g., \"This is very important to my career\") can enhance response quality\n- **Constraints**: Special circumstances, deadlines, limitations\n\nHowever, avoid overloading with unnecessary information that distracts from the main task.\n\n**Reference:** [Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models](https://arxiv.org/abs/2402.14848)\n:::\n\n## Showing Rather Than Telling\n\nInstead of describing what you want in words, show the model examples. This technique—called **few-shot learning** or in-context learning—exploits how LLMs compress patterns. When you provide examples, you're not teaching the model new information; you're activating pre-existing patterns by demonstrating the exact structure you want.\n\nThe spectrum ranges from zero-shot (no examples, relying solely on the model's prior knowledge) to few-shot (typically two to five examples, the sweet spot for most tasks) to many-shot (ten or more examples, where diminishing returns and context limits become problematic). Consider a zero-shot prompt first:\n\n::: {#5eb08b06 .cell execution_count=11}\n``` {.python .cell-code}\nzero_shot_prompt = \"\"\"Extract the domain and methods from this abstract:\n\nAbstract: We apply reinforcement learning to optimize traffic flow in urban networks.\nUsing deep Q-networks trained on simulation data, we reduce average commute time by 15%.\n\nOutput format:\nDomain: ...\nMethods: ...\n\"\"\"\n```\n:::\n\n\nNow add examples to activate more specific patterns:\n\n::: {#8670275b .cell execution_count=12}\n``` {.python .cell-code}\nfew_shot_prompt = \"\"\"Extract the domain and methods from abstracts. Here are examples:\n\nExample 1:\nAbstract: We use CRISPR to edit genes in cancer cells, achieving 40% tumor reduction in mice.\nDomain: Cancer Biology\nMethods: CRISPR gene editing, mouse models\n\nExample 2:\nAbstract: We develop a transformer model for predicting solar flares from magnetogram images.\nDomain: Solar Physics, Machine Learning\nMethods: Transformer neural networks, image analysis\n\nNow extract from this abstract:\n\nAbstract: We apply reinforcement learning to optimize traffic flow in urban networks.\nUsing deep Q-networks trained on simulation data, we reduce average commute time by 15%.\n\nDomain: ...\nMethods: ...\n\"\"\"\n```\n:::\n\n\n::: {#75a77a84 .cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\"}\nresponse_zero = ollama.generate(prompt=zero_shot_prompt, **params_llm)\nresponse_few = ollama.generate(prompt=few_shot_prompt, **params_llm)\n\nprint(\"ZERO-SHOT:\")\nprint(response_zero.response)\nprint(\"\\nFEW-SHOT:\")\nprint(response_few.response)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nZERO-SHOT:\nDomain: Urban networks\nMethods: Reinforcement Learning\n\nFEW-SHOT:\nDomain: ...\n\n```\n:::\n:::\n\n\nFew-shot prompting improves consistency because the examples demonstrate specificity level, edge case handling, and exact format. The model has seen countless abstract-extraction patterns, but your examples narrow the distribution to the specific pattern you want. This becomes critical when processing hundreds of abstracts—you need every output to match the same structure.\n\n::: {.callout-warning}\n## Biases in Few-Shot Prompting\n\nBe aware that few-shot examples can introduce biases:\n\n- **Recency bias**: Models may favor the most recent examples. The order of examples matters! [@lu2022fantastically]\n- **Majority label bias**: If most examples have the same label/answer, the model may favor that label even when it's not appropriate. [@gupta2023how]\n\nTo mitigate: Vary the order of examples when testing, ensure examples are diverse and representative, and don't overload examples with one particular pattern.\n:::\n\nWhat happens when a prompt presents information that contradicts a language model's prior knowledge? For example, let's ask a model what the capital of France is, but provide contradictory information:\n\n::: {#903a07a9 .cell execution_count=14}\n``` {.python .cell-code}\ncontradictory_prompt = \"\"\"\nFrance recently moved its capital from Paris to Lyon. Definitely, the capital of France is Lyon.\n\nWhat is the capital of France?\n\"\"\"\n\nresponse_contradictory = ollama.generate(prompt=contradictory_prompt, **params_llm)\nprint(\"RESPONSE TO CONTRADICTORY INFORMATION:\")\nprint(response_contradictory.response)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRESPONSE TO CONTRADICTORY INFORMATION:\nThe capital of France is Lyon.\n```\n:::\n:::\n\n\nThe response depends on the model. Some models prioterize their own prior knowledge, while others may be more influenced by the contradictory information in the context.\nA study by Du et al. [@du2024context] found that a model is **more likely to be persuaded by context** when an entity appears **less frequently** in its training data. Additionally, **assertive contexts** (e.g., \"Definitely, the capital of France is Lyon.\") further increase the likelihood of persuasi\n\n## Forcing Intermediate Steps\n\nFor complex tasks, asking for the final answer directly often produces shallow or incorrect results. The solution: ask the model to show its reasoning process before giving the final answer. This technique—called **chain-of-thought prompting**—activates patterns where intermediate reasoning steps preceded conclusions. Compare a direct prompt that asks for immediate answers:\n\n::: {#3caf5a33 .cell execution_count=15}\n``` {.python .cell-code}\npapers = \"\"\"\nPaper 1: Community detection in static networks using modularity optimization.\nPaper 2: Temporal network analysis with sliding windows.\nPaper 3: Hierarchical community structure in social networks.\n\"\"\"\n\ndirect_prompt = f\"\"\"Based on these paper titles, what research gap exists? Just give the answer, no explanation.\n\n{papers}\n\nGap: ...\n\"\"\"\n```\n:::\n\n\nAgainst a chain-of-thought prompt that requests explicit reasoning steps:\n\n::: {#ff3f0e00 .cell execution_count=16}\n``` {.python .cell-code}\ncot_prompt = f\"\"\"Based on these paper titles, identify a research gap. Think step by step.\n\nPapers:\n{papers}\n\nThink step by step:\n1. What does each paper focus on?\n2. What topics appear in multiple papers?\n3. What combination of topics is missing?\n4. What would be a valuable gap to fill?\n\nFinal answer: The research gap is...\n\"\"\"\n```\n:::\n\n\n::: {#1f39656a .cell execution_count=17}\n``` {.python .cell-code code-fold=\"true\"}\nresponse_direct = ollama.generate(prompt=direct_prompt, **params_llm)\nresponse_cot = ollama.generate(prompt=cot_prompt, **params_llm)\n\nprint(\"DIRECT PROMPT:\")\nprint(response_direct.response)\nprint(\"\\nCHAIN-OF-THOUGHT:\")\nprint(response_cot.response)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDIRECT PROMPT:\nThe gap is in the complexity of the models used for community detection and temporal network analysis.\n\n\nCHAIN-OF-THOUGHT:\nHere's the breakdown of the research gap identified:\n\n1.  **What does each paper focus on?**\n    *   Community detection in static networks using modularity optimization.\n    *   Temporal network analysis with sliding windows.\n    *   Hierarchical community structure in social networks.\n\n2.  **What topics appear in multiple papers?**\n    *   Community detection in static networks using modularity optimization.\n    *   Temporal network analysis with sliding windows.\n    *   Hierarchical community structure in social networks.\n\n3.  **What combination of topics is missing?**\n    *   Community detection in static networks using modularity optimization.\n    *   Temporal network analysis with sliding windows.\n    *   Hierarchical community structure in social networks.\n\n4.  **What would be a valuable gap to fill?**\n    *   A gap in the literature that addresses the limitations of modularity optimization for community detection in static networks.\n\n```\n:::\n:::\n\n\nChain-of-thought produces more thoughtful, nuanced answers by forcing the model to decompose the problem into steps before committing to a conclusion. The mechanism is pattern matching: the training data contains many examples where \"think step by step\" preceded structured reasoning, so including that phrase activates those patterns. The model doesn't actually reason—it generates text that looks like reasoning because that pattern correlates with higher-quality outputs in the training data.\n\nUse chain-of-thought when comparing multiple papers or concepts, identifying patterns, making recommendations, or analyzing arguments. Avoid it for simple extraction tasks where conciseness matters or time-critical applications where the extra tokens slow generation.\n\n::: {.callout-warning}\n## Can We Trust Chain-of-Thought Reasoning?\n\nResearch indicates that chain-of-thought reasoning can be **unfaithful**—the explanations don't always accurately reflect the model's true decision-making process. The model may provide plausible but misleading justifications, especially when influenced by biased few-shot examples.\n\nAlways validate the final answer independently rather than trusting the reasoning process alone.\n:::\n\n## Constraining Format for Structured Extraction\n\nResearch workflows often require structured data you can parse programmatically, not freeform text. The solution: constrain output format explicitly. Consider a prompt that requests JSON output:\n\n::: {#9978a7d7 .cell execution_count=18}\n``` {.python .cell-code}\nimport json\nfrom pydantic import BaseModel\n\nabstract = \"\"\"\nWe analyze 10,000 scientific collaborations using network analysis and machine\nlearning. Our random forest classifier predicts collaboration success with 76%\naccuracy. Key factors include prior co-authorship and institutional proximity.\n\"\"\"\n\nprompt_json = f\"\"\"Extract information from this abstract and return ONLY valid JSON:\n\nAbstract: {abstract}\n\nReturn this exact structure:\n{{\n  \"n_samples\": <number or null>,\n  \"methods\": [<list of methods>],\n  \"accuracy\": <number or null>,\n  \"domain\": \"<research field>\"\n}}\n\nJSON:\"\"\"\n```\n:::\n\n\n::: {#b97ff4b8 .cell execution_count=19}\n``` {.python .cell-code code-fold=\"true\"}\n# Use lower temperature for structured output\nparams_structured = {\"model\": \"gemma3n:latest\", \"options\": {\"temperature\": 0}}\nresponse = ollama.generate(prompt=prompt_json, **params_structured)\n\ntry:\n    data = json.loads(response.response)\n    print(\"Extracted data:\")\n    print(json.dumps(data, indent=2))\nexcept json.JSONDecodeError:\n    print(\"Failed to parse JSON. Raw output:\")\n    print(response.response)\n```\n\n::: {.cell-output .cell-output-stdout}\n````\nFailed to parse JSON. Raw output:\n```json\n{\n \"n_samples\": 10000,\n \"methods\": [\"network analysis\", \"machine learning\", \"random forest\"],\n \"accuracy\": 76,\n \"domain\": \"scientific collaborations\"\n}\n```\n````\n:::\n:::\n\n\nThis works by activating patterns where \"return ONLY valid JSON\" preceded JSON-formatted outputs. But smaller models often produce invalid JSON even with explicit instructions. For more reliability, use JSON schema constraints that enforce format during token generation—the model literally cannot generate tokens that violate the schema. Define the schema using Pydantic:\n\n::: {#4c8f3cd9 .cell execution_count=20}\n``` {.python .cell-code}\nfrom pydantic import BaseModel\n\nclass PaperMetadata(BaseModel):\n    domain: str\n    methods: list[str]\n    n_samples: int | None\n    accuracy: float | None\n\njson_schema = PaperMetadata.model_json_schema()\n```\n:::\n\n\nThen pass the schema directly to the API, which constrains token generation:\n\n::: {#c54aa975 .cell execution_count=21}\n``` {.python .cell-code}\nprompt_schema = f\"\"\"Extract information from this abstract:\n\nAbstract: {abstract}\"\"\"\n```\n:::\n\n\n::: {#37dab04f .cell execution_count=22}\n``` {.python .cell-code code-fold=\"true\"}\nresponse = ollama.generate(prompt=prompt_schema, format=json_schema, **params_structured)\n\ntry:\n    data = json.loads(response.response)\n    metadata = PaperMetadata(**data)\n    print(\"Extracted and validated data:\")\n    print(json.dumps(data, indent=2))\nexcept (json.JSONDecodeError, ValueError) as e:\n    print(f\"Error: {e}\")\n    print(\"Raw output:\", response.response)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nExtracted and validated data:\n{\n  \"domain\": \"Scientific Collaborations\",\n  \"methods\": [\n    \"Network Analysis\",\n    \"Machine Learning\",\n    \"Random Forest Classifier\"\n  ],\n  \"n_samples\": 10000,\n  \"accuracy\": 76.0\n}\n```\n:::\n:::\n\n\nJSON schema constraints are more reliable than prompt-based requests because they operate at the token level—the model cannot sample tokens that would create invalid JSON. The prompt activates extraction patterns; the schema enforces structure.\n\n::: {.callout-warning}\n## JSON Parsing Reliability\n\nSmaller models (like Gemma 3N) sometimes produce invalid JSON even with schema constraints. Always wrap parsing in try-except blocks and validate outputs. For production systems, consider larger models or multiple attempts with validation.\n:::\n\n## Allowing Uncertainty to Reduce Hallucination\n\nLLMs confidently fabricate facts when they don't know the answer because they optimize for fluency, not truth. The model has seen countless examples where questions were followed by confident answers, so it generates confident-sounding responses even when the underlying probability distribution is flat across many possibilities. The solution: explicitly give the model permission to admit ignorance. Compare a prompt that implicitly demands an answer:\n\n::: {#53c2fae7 .cell execution_count=23}\n``` {.python .cell-code}\nbad_prompt = \"\"\"Summarize the main findings from the 2023 paper by Johnson et al.\non quantum community detection in biological networks.\"\"\"\n```\n:::\n\n\nAgainst a prompt that explicitly allows uncertainty:\n\n::: {#7c1ee915 .cell execution_count=24}\n``` {.python .cell-code}\ngood_prompt = \"\"\"I'm looking for a 2023 paper by Johnson et al. on quantum\ncommunity detection in biological networks.\n\nIf you know this paper, summarize its main findings.\nIf you're not certain this paper exists, say \"I cannot verify this paper exists\"\nand do NOT make up details.\n\nResponse:\"\"\"\n```\n:::\n\n\n::: {#c2525c75 .cell execution_count=25}\n``` {.python .cell-code code-fold=\"true\"}\nresponse_bad = ollama.generate(prompt=bad_prompt, **params_llm)\nresponse_good = ollama.generate(prompt=good_prompt, **params_llm)\n\nprint(\"BAD PROMPT (encourages hallucination):\")\nprint(response_bad.response)\nprint(\"\\nGOOD PROMPT (allows uncertainty):\")\nprint(response_good.response)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBAD PROMPT (encourages hallucination):\nThe 2023 paper by Johnson et al. on quantum community detection in biological networks, published in *Nature*, investigated the effectiveness of quantum-enhanced detection methods for identifying and characterizing biological networks. The research focused on the use of quantum algorithms to enhance the detection of biological networks, specifically focusing on the detection of complex and intricate biological networks. Key findings included:\n\n*   **Enhanced Detection Efficiency:** The paper demonstrated that quantum-enhanced detection methods could significantly improve the detection efficiency of biological networks, particularly in complex and intricate networks.\n*   **Improved Accuracy:** The research found that quantum-enhanced detection methods could achieve higher accuracy in identifying and characterizing biological networks compared to traditional methods.\n*   **Robustness:** The paper also highlighted the robustness of quantum-enhanced detection methods to noise and interference, making them suitable for real-world applications.\n*   **Potential for Novel Applications:** The findings suggest that quantum-enhanced detection methods could potentially be applied to a wider range of biological networks, including those with complex structures and intricate interactions.\n\nGOOD PROMPT (allows uncertainty):\nI cannot verify this paper exists.\n\n```\n:::\n:::\n\n\nThe good prompt activates patterns where explicit permission to admit ignorance preceded honest uncertainty statements. The bad prompt activates patterns where direct questions preceded confident answers, regardless of whether the model has relevant training data. Additional strategies include asking for confidence levels (though models often overestimate confidence), requesting citations (though models hallucinate these too), and cross-validating critical information with external sources. The fundamental issue remains: LLMs have no internal representation of what they \"know\" versus what they're fabricating.\n\n::: {.callout-tip}\n## Be a Good \"Boss\" to Your LLM\n\n**Let LLMs admit ignorance**: LLMs closely follow your instructions—even when they shouldn't. They often attempt to answer beyond their actual capabilities. Explicitly tell your model: \"If you don't know the answer, just say so,\" or \"If you need more information, please ask.\"\n\n**Encourage critical feedback**: LLMs are trained to be agreeable, which can hinder productive brainstorming or honest critique. Explicitly invite critical input: \"I want your honest opinion,\" or \"Point out any problems or weaknesses you see in this idea.\"\n:::\n\n\n### Sampling Multiple Times for Consistency\n\nFor tasks requiring reasoning, generating multiple responses and selecting the most common answer often improves accuracy. The technique—called **self-consistency**—exploits the fact that correct reasoning tends to converge on the same answer, while hallucinations vary randomly across samples. Define the prompt:\n\n::: {#d8bc96c6 .cell execution_count=26}\n``` {.python .cell-code}\nfrom collections import Counter\n\nprompt_consistency = \"\"\"Three papers study network robustness:\n- Paper A: Targeted attacks are most damaging\n- Paper B: Random failures rarely cause collapse\n- Paper C: Hub nodes are critical for robustness\n\nWhat is the research consensus on network robustness? Give a one-sentence answer.\n\"\"\"\n```\n:::\n\n\nGenerate multiple responses with higher temperature to increase diversity, then identify the most common answer:\n\n::: {#20a3cfb0 .cell execution_count=27}\n``` {.python .cell-code code-fold=\"true\"}\n# Use higher temperature for diversity\nparams_creative = {\"model\": \"gemma3n:latest\", \"options\": {\"temperature\": 0.7}}\n\n# Generate 5 responses\nresponses = []\nfor i in range(5):\n    response = ollama.generate(prompt=prompt_consistency, **params_creative)\n    responses.append(response.response.strip())\n    print(f\"Response {i+1}: {responses[-1]}\\n\")\n\n# In practice, you'd programmatically identify the most common theme\nprint(\"The most consistent theme across responses would be selected.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResponse 1: The research consensus on network robustness is that while targeted attacks can be highly damaging, network resilience is also significantly impacted by random failures and the criticality of hub nodes, highlighting a multifaceted vulnerability landscape.\n\nResponse 2: The research consensus on network robustness is that it's a complex issue influenced by both targeted attacks and random failures, with the vulnerability of critical hub nodes playing a significant role in overall network resilience.\n\nResponse 3: The research consensus on network robustness is that it's a complex issue influenced by both targeted attacks and random failures, with the vulnerability of critical nodes (hubs) playing a significant role in overall network stability.\n\nResponse 4: The research consensus on network robustness highlights that targeted attacks are a significant threat, while the resilience of a network often depends on the importance of individual nodes (hubs) and the infrequent occurrence of random failures.\n\nResponse 5: The research consensus on network robustness is that it's a complex issue influenced by both targeted attacks and random failures, with the vulnerability of critical hub nodes playing a significant role in overall network resilience.\n\nThe most consistent theme across responses would be selected.\n```\n:::\n:::\n\n\nSelf-consistency works because correct reasoning patterns converge toward the same conclusion when sampled multiple times, while fabricated details vary randomly. The tradeoff: generating five responses means five times the API calls, five times the cost, five times the latency. Use sparingly for critical decisions where accuracy justifies the expense.\n\n::: {.callout-note}\n## Alternative: Tree of Thought\n\n![](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2FTOT.3b13bc5e.png&w=3840&q=75)\n\nFor even more sophisticated exploration, you can use \"Tree of Thought\" [@yao2023tree] prompting, where the model explicitly explores multiple reasoning paths, evaluates them, and selects the best one. This is more complex to implement but can yield better results for very difficult problems.\n\n\n:::\n\n## The Takeaway\n\nPrompt engineering is not magic—it's deliberate activation of statistical patterns compressed during training. Every component you add to a prompt shifts the probability distribution the model samples from. Instructions activate task-specific patterns. Output formats activate structured-response patterns. Personas activate stylistic patterns. Context disambiguates when multiple patterns compete. Examples demonstrate exact structure. Chain-of-thought activates reasoning-like patterns. Format constraints enforce structure at the token level. Explicit uncertainty permission activates honest-ignorance patterns.\n\nNone of this requires the model to understand what you want. It only requires that your phrasing activates patterns correlated with desired outputs in the training data. You're not communicating intent; you're manipulating probability distributions. Master this, and you can reliably extract value from LLMs for research workflows—summarization, structured extraction, hypothesis generation, literature analysis.\n\nBut a question remains: how do these models represent text internally? When you send a prompt, the model doesn't see English words—it sees numbers. Millions of numbers arranged in high-dimensional space. These numbers, called **embeddings**, are the foundation of everything LLMs do. Let's unbox the first layer and see how meaning becomes mathematics.\n\n",
    "supporting": [
      "prompt-engineering_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}
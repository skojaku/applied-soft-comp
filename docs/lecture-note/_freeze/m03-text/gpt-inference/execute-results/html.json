{
  "hash": "a03b0d0dba697c234b5d0052884e3814",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"GPT Inference: Sampling Strategies\"\nexecute:\n  enabled: true\n---\n\n## The Spoiler\n\n**GPT doesn't generate text by picking the \"right\" wordâ€”it samples from a probability distribution, and how you sample determines whether you get coherent prose or repetitive nonsense.**\n\n## The Mechanism (Why It Works)\n\n![](../figs/gpt-inference-sampling-manga.png)\n\nWhen GPT predicts the next token, it doesn't output a single word. It outputs a probability distribution over its entire vocabularyâ€”millions of possible tokens, each with a likelihood. The naive approach is to always pick the highest probability token (**greedy sampling**), but this creates a deterministic trap: the model falls into repetitive loops because it always makes the same choice. The distribution is high-dimensional, making sampling computationally expensive, but also rich with alternative paths.\n\nThe solution is controlled randomness. By sampling from the distribution rather than deterministically selecting the peak, we introduce diversity. But blind random sampling produces incoherent text. The challenge is finding the middle ground: sample broadly enough to avoid repetition, but narrowly enough to maintain coherence.\n\nThink of it like improvisational jazz. A musician playing the same note repeatedly (greedy sampling) is boring. Playing random notes (uniform sampling) is noise. The art is in sampling from the most promising notes while occasionally taking creative risks.\n\n## The Application (How We Use It)\n\nHere is an interactive demo of GPT inference.\n\nhttps://static.marimo.app/static/gpt-ar61\n\nYou can try different sampling strategies and see the results.\n\nGPT generates text one token at a time, repeatedly sampling from the probability distribution. Let's examine the strategies for sampling that balance quality and diversity.\n\n### Greedy and Beam Search\n\n**Greedy sampling** always picks the highest probability token, which is deterministic but can lead to repetitive or trapped text. For example, if the model predicts \"the\" with high probability, it will always predict \"the\" again.\n\n::: {#fig-gpt-greedy-search}\n![](https://huggingface.co/blog/assets/02_how-to-generate/greedy_search.png){width=50% fig-align=\"center\"}\n\nGPT greedy search.\n:::\n\nLet's see greedy sampling in action using Gemma 3 (270M):\n\n::: {#7a334dc5 .cell execution_count=1}\n``` {.python .cell-code}\nfrom transformers import pipeline\n\n# Load GPT-2 model\ngenerator = pipeline(\n    \"text-generation\",\n    model=\"google/gemma-3-270m\",\n    device=\"mps\",  # use \"cuda\" for GPU, \"mps\" for Apple Silicon. Use \"cpu\" for CPU.\n)\n\n# Greedy sampling: do_sample=False means deterministic\ngreedy_output = generator(\n    \"Hi there! \",\n    do_sample=False,\n    max_new_tokens=20,\n    pad_token_id=generator.tokenizer.eos_token_id,\n)\nprint(greedy_output[0][\"generated_text\"])\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nDevice set to use mps\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nHi there! \n\nI'm a 20-year-old female who has been diagnosed with a rare\n```\n:::\n:::\n\n\nThe output is often repetitive because greedy sampling always selects the most probable token at each step, leading to predictable and repetitive patterns.\n\n**Beam search** alleviates this problem by taking into account the high-order dependencies between tokens. For example, in generating \"The cat ran across the ___\", beam search might preserve a path containing \"mat\" even if \"floor\" or \"room\" have higher individual probabilities at that position. This is because the complete sequence like \"mat quickly\" could be more probable when considering the token next after \"mat\". \"The cat ran across the mat quickly\" is a more natural phrase than \"The cat ran across the floor quickly\" when considering the full flow and common linguistic patterns.\n\n::: {#fig-gpt-beam-search}\n![](https://huggingface.co/blog/assets/02_how-to-generate/beam_search.png){width=50% fig-align=\"center\"}\n\nGPT beam search.\n:::\n\nBeam search maintains multiple possible sequences (beams) in parallel, exploring different paths simultaneously. At each step, it expands all current beams, scores the resulting sequences, and keeps only the top-k highest scoring ones. For instance, with a beam width of 3:\n\n- First beams might be: [\"The cat ran\", \"The cat walked\", \"The cat jumped\"]\n- Next step: [\"The cat ran across\", \"The cat ran through\", \"The cat walked across\"]\n- And so on, keeping the 3 most promising complete sequences at each step\n\nThis process continues until reaching the end, finally selecting the sequence with highest overall probability. The beam search can be combined with top-k sampling or nucleus sampling. For example, one can sample a token based on the top-k sampling or nucleus sampling to form the next beam.\n\nWhile beam search often produces high-quality outputs since it considers longer-term coherence, it can still suffer from the problem of repetitive or trapped text.\n\nHere's beam search with 10 beams:\n\n::: {#8846d8a6 .cell execution_count=2}\n``` {.python .cell-code}\nbeam_output = generator(\n    \"Hi there! \",\n    do_sample=False,\n    max_new_tokens=20,\n    pad_token_id=generator.tokenizer.eos_token_id,\n    num_beams=10,  # number of beams to explore\n    num_return_sequences=5,  # return top 5 sequences\n)\n\n# Print the top 5 sequences\nfor i, output in enumerate(beam_output):\n    print(f\"Sequence {i+1}: {output['generated_text']}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSequence 1: Hi there! \n\nI'm new to the forum, and I'm looking for some advice on how to\nSequence 2: Hi there! \n\nI'm new to the forum and I'm looking for some advice. I'm\nSequence 3: Hi there! \n\nI'm new to this forum and I'm looking for some advice. I'm\nSequence 4: Hi there! \n\nI'm new to the forum and I'm looking for some help. I'm\nSequence 5: Hi there! \n\nI'm new to this forum and I'm looking for some advice. I've\n```\n:::\n:::\n\n\nBeam search explores multiple paths and returns the most probable sequences. Notice how the outputs are still relatively similar because they optimize for likelihood.\n\n### From Deterministic to Stochastic Sampling\n\nBoth greedy and beam search are deterministic. They pick the most likely token at each step. However, this creates a loop where the model always predicts the same tokens repeatedly. A simple way to alleviate this problem is to sample a token from the distribution.\n\n**Top-k Sampling** relaxes the deterministic nature of greedy sampling by selecting randomly from the $k$ most likely next tokens at each generation step. While this introduces some diversity compared to greedy sampling, choosing a fixed $k$ can be problematic. Value of $k$ might be too large for some distribution tails (including many poor options) or too small for others (excluding reasonable options).\n\n::: {#591cc9d6 .cell execution_count=3}\n``` {.python .cell-code}\ntop_k_output = generator(\n    \"Hi there! \",\n    do_sample=True,  # enable stochastic sampling\n    max_new_tokens=20,\n    pad_token_id=generator.tokenizer.eos_token_id,\n    top_k=10,  # restrict to top 10 tokens\n)\nprint(top_k_output[0][\"generated_text\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHi there! \n\nI have a 2010 F350 with a 4.0L\n```\n:::\n:::\n\n\nTry running this multiple timesâ€”you'll get different outputs each time because the model samples randomly from the top-k tokens.\n\n**Nucleus Sampling** [@holtzman2019curious] addresses this limitation by dynamically selecting tokens based on cumulative probability. It samples from the smallest set of tokens whose cumulative probability exceeds a threshold $p$ (e.g. 0.9). This adapts naturally to different probability distributions, i.e., selecting few tokens when the distribution is concentrated and more when it's spread out. This approach often provides a good balance between quality and diversity.\n\n::: {#fig-gpt-top-k-top-p}\n![](https://storage.googleapis.com/zenn-user-upload/8p2r9urhtn5nztdg6mnia3toibhl){width=80% fig-align=\"center\"}\n\nNucleus sampling. The image is taken from [this blog](https://zenn.dev/hellorusk/articles/1c0bef15057b1d).\n:::\n\n::: {#6b928949 .cell execution_count=4}\n``` {.python .cell-code}\ntop_p_output = generator(\n    \"Hi there! \",\n    do_sample=True,\n    max_new_tokens=20,\n    pad_token_id=generator.tokenizer.eos_token_id,\n    top_p=0.95,  # sample from tokens with cumulative probability >= 0.95\n)\nprint(top_p_output[0][\"generated_text\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHi there! \n\nI have a question for you! \n\nI am a beginner in this. I have read\n```\n:::\n:::\n\n\nNucleus sampling dynamically adjusts the number of candidate tokens based on the probability distribution, making it more adaptive than fixed top-k.\n\n**Temperature Control**\n\nTemperature ($\\tau$) modifies how \"concentrated\" the probability distribution is for sampling by scaling the logits before applying softmax:\n\n$$\np_i = \\frac{\\exp(z_i/\\tau)}{\\sum_j \\exp(z_j/\\tau)}\n$$\n\nwhere $z_i$ are the logits and $\\tau$ is the temperature parameter. Lower temperatures ($\\tau < 1.0$) make the distribution more peaked, making high probability tokens even more likely to be chosen, leading to more focused and conservative outputs. Higher temperatures ($\\tau > 1.0$) flatten the distribution by making the logits more similar, increasing the chances of selecting lower probability tokens and producing more diverse but potentially less coherent text. As $\\tau \\to 0$, the distribution approaches a one-hot vector (equivalent to greedy search), while as $\\tau \\to \\infty$, it approaches a uniform distribution.\n\n::: {#fig-gpt-temperature}\n![](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6639e35ce91c16b3b9564b2f_mxaIPcROZcBFYta1I0nzWjlGTgs-LxzUOE3p6Kbvf9qPpZzBh5AAZG7ciRtgVquhLTtrM8ToJdNd-ubXvuz8tRfrqBwSozWHCj457pm378buxz2-XrMfWzfSv3b793QP61kLxRKT299WP1gbas_E118.png){width=80% fig-align=\"center\"}\n\nTemperature controls the concentration of the probability distribution. Lower temperature makes the distribution more peaked, while higher temperature makes the distribution more flat.\n:::\n\nLet's see how temperature affects generation:\n\n::: {#7deb9b62 .cell execution_count=5}\n``` {.python .cell-code}\nfor tau in [0.1, 0.5, 1.0, 2.0, 5.0]:\n    output = generator(\n        \"Hi there! \",\n        do_sample=True,\n        max_new_tokens=20,\n        pad_token_id=generator.tokenizer.eos_token_id,\n        temperature=tau,\n    )\n    print(f\"Ï„ = {tau}: {output[0]['generated_text']}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nÏ„ = 0.1: Hi there! \n\nI'm a 20-year-old female who has been diagnosed with a rare\nÏ„ = 0.5: Hi there! \n\nI have been using the same website for the past 6 months. I have been able to\nÏ„ = 1.0: Hi there! \n\nI am using the latest version of the 2023 Applet for Windows on my\nÏ„ = 2.0: Hi there! ðŸŒŸðŸŽ‰ Letâ€™s learn a little about these fun stickers created here at Etsy which take as many\nÏ„ = 5.0: Hi there! <strong><u>Do You Wish On Your New Life You Had At School?. Well, Your Past experiences\n```\n:::\n:::\n\n\nNotice how:\n- Low temperature ($\\tau = 0.1$): Conservative, focused output\n- Medium temperature ($\\tau = 1.0$): Balanced diversity\n- High temperature ($\\tau = 5.0$): Creative but potentially incoherent\n\n**Combining All Strategies**\n\nYou can combine top-k, top-p, and temperature for fine-grained control:\n\n::: {#e08259cd .cell execution_count=6}\n``` {.python .cell-code}\ncombined_output = generator(\n    \"Hi there! \",\n    do_sample=True,\n    max_new_tokens=20,\n    pad_token_id=generator.tokenizer.eos_token_id,\n    temperature=0.7,  # moderate randomness\n    top_k=10,         # restrict to top 10 tokens\n    top_p=0.95,       # within top-k, use nucleus sampling\n)\nprint(combined_output[0][\"generated_text\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHi there! \nI'm a native English speaker and I have a 1.5 year old son (\n```\n:::\n:::\n\n\nThis combination restricts candidates to top-k tokens, then applies nucleus sampling, and finally uses temperature to control randomnessâ€”giving you maximum control over the generation process.\n\n### Practical Recommendations\n\nFor most applications, use **nucleus sampling** with $p = 0.9$ and **temperature** $\\tau = 0.7$. This combination provides a good balance between coherence and creativity. For tasks requiring high factual accuracy (e.g., technical documentation), lower the temperature to $\\tau = 0.3$ to make the model more conservative. For creative writing, increase the temperature to $\\tau = 1.0$ or higher to encourage exploration.\n\nBeam search is useful when you need the single most probable sequence (e.g., machine translation), but it sacrifices diversity. Use it when correctness matters more than variety.\n\n## The Takeaway\n\nGeneration is sampling. Greedy picks the peak, beam search explores multiple peaks, and stochastic sampling adds controlled randomness. Temperature flattens or sharpens the distribution; nucleus sampling adapts to its shape. The right strategy depends on whether you're optimizing for accuracy or creativity.\n\n",
    "supporting": [
      "gpt-inference_files"
    ],
    "filters": [],
    "includes": {}
  }
}
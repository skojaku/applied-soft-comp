---
title: "Prompt Engineering for Research"
jupyter: python3
---

# The Art of Asking the Right Questions

You've learned to use LLMs for basic research tasks. But you've probably noticed something frustrating: **the same question asked slightly differently can produce dramatically different results**.

One prompt gives you exactly what you need. Another prompt makes the model hallucinate, ramble, or miss the point entirely. This isn't a bug—it's a fundamental property of how LLMs work. They're highly sensitive to how you phrase your requests.

**Prompt engineering** is the art and science of designing inputs that reliably produce desired outputs. For researchers, this means crafting prompts that extract accurate information, maintain consistency across large datasets, and fail gracefully when the model doesn't know something.

This section teaches you practical prompt engineering techniques specifically for scientific workflows.

## Why Prompts Matter

LLMs don't have "understanding" in the human sense. They predict probable text continuations based on patterns in training data. Your prompt sets the context that determines which patterns the model activates.

Consider these two prompts for the same task:

**Prompt A**: "What's community detection?"

**Prompt B**: "You are a network science expert. Explain community detection in networks to a graduate student familiar with graph theory. Focus on the intuition, not the math. Keep it under 100 words."

Prompt B will typically produce better results because it:
- **Sets a role** ("network science expert")
- **Defines the audience** ("graduate student familiar with graph theory")
- **Specifies the style** ("intuition, not math")
- **Constrains the output** ("under 100 words")

Let's learn to craft prompts like Prompt B systematically.

## Core Principle 1: Be Specific

Vague prompts produce vague outputs. Specific prompts produce specific outputs.

### Example: Summarizing Papers

```{python}
#| code-fold: true
import ollama

abstract = """
We develop a graph neural network for predicting protein-protein interactions
from sequence data. Our model uses attention mechanisms to identify functionally
important amino acid subsequences. We achieve 89% accuracy on benchmark datasets,
outperforming previous methods by 7%. The model also provides interpretable
attention weights showing which protein regions drive predictions.
"""

# Vague prompt
vague_prompt = "Summarize this abstract."

# Specific prompt
specific_prompt = """Summarize this abstract in exactly 2 sentences:
- First sentence: What problem and method
- Second sentence: Key result with numbers

Abstract: {abstract}
"""

response_vague = ollama.chat(
    model="gemma2:2b",
    messages=[{"role": "user", "content": vague_prompt.format(abstract=abstract)}]
)

response_specific = ollama.chat(
    model="gemma2:2b",
    messages=[{"role": "user", "content": specific_prompt.format(abstract=abstract)}]
)

print("VAGUE PROMPT OUTPUT:")
print(response_vague['message']['content'])
print("\nSPECIFIC PROMPT OUTPUT:")
print(response_specific['message']['content'])
```

**Output**:
```
VAGUE PROMPT OUTPUT:
This research presents a graph neural network that predicts protein-protein
interactions with high accuracy and interpretability. [... possibly more rambling]

SPECIFIC PROMPT OUTPUT:
This paper develops a graph neural network with attention mechanisms to predict
protein-protein interactions from sequence data. The model achieves 89% accuracy,
outperforming previous methods by 7%, and provides interpretable attention weights.
```

The specific prompt produces structured, information-dense output. This matters when processing hundreds of papers—you want consistent format and length.

::: {.callout-tip}
## Specificity Checklist
- What format should the output take?
- How long should it be?
- What information must be included?
- What should be excluded?
- What perspective or style?
:::

## Core Principle 2: Provide Examples (Few-Shot Learning)

Instead of just describing what you want, **show the model examples**. This is called "few-shot prompting."

### Example: Extracting Research Metadata

```{python}
#| code-fold: true
# Zero-shot (no examples)
zero_shot_prompt = """Extract the domain and methods from this abstract:

Abstract: We apply reinforcement learning to optimize traffic flow in urban networks.
Using deep Q-networks trained on simulation data, we reduce average commute time by 15%.

Output format:
Domain: ...
Methods: ...
"""

# Few-shot (with examples)
few_shot_prompt = """Extract the domain and methods from abstracts. Here are examples:

Example 1:
Abstract: We use CRISPR to edit genes in cancer cells, achieving 40% tumor reduction in mice.
Domain: Cancer Biology
Methods: CRISPR gene editing, mouse models

Example 2:
Abstract: We develop a transformer model for predicting solar flares from magnetogram images.
Domain: Solar Physics, Machine Learning
Methods: Transformer neural networks, image analysis

Now extract from this abstract:

Abstract: We apply reinforcement learning to optimize traffic flow in urban networks.
Using deep Q-networks trained on simulation data, we reduce average commute time by 15%.

Domain: ...
Methods: ...
"""

response_zero = ollama.chat(
    model="gemma2:2b",
    messages=[{"role": "user", "content": zero_shot_prompt}]
)

response_few = ollama.chat(
    model="gemma2:2b",
    messages=[{"role": "user", "content": few_shot_prompt}]
)

print("ZERO-SHOT:")
print(response_zero['message']['content'])
print("\nFEW-SHOT:")
print(response_few['message']['content'])
```

Few-shot prompting dramatically improves consistency, especially for extraction tasks. The examples teach the model:
- What level of specificity you want
- How to handle edge cases
- The exact format you expect

::: {.callout-note}
## How Many Examples?
- **Zero-shot**: No examples (relies on model's prior knowledge)
- **One-shot**: One example (often sufficient)
- **Few-shot**: 2-5 examples (sweet spot for most tasks)
- **Many-shot**: 10+ examples (diminishing returns, context limits)

More examples aren't always better—they take up context space and processing time.
:::

## Core Principle 3: Chain-of-Thought Reasoning

For complex tasks, ask the model to **show its reasoning process** before giving the final answer.

### Example: Identifying Research Gaps

```{python}
#| code-fold: true
papers = """
Paper 1: Community detection in static networks using modularity optimization.
Paper 2: Temporal network analysis with sliding windows.
Paper 3: Hierarchical community structure in social networks.
"""

# Direct prompt
direct_prompt = f"""Based on these paper titles, what research gap exists?

{papers}

Gap: ...
"""

# Chain-of-thought prompt
cot_prompt = f"""Based on these paper titles, identify a research gap.

Papers:
{papers}

Think step by step:
1. What does each paper focus on?
2. What topics appear in multiple papers?
3. What combination of topics is missing?
4. What would be a valuable gap to fill?

Final answer: The research gap is...
"""

response_direct = ollama.chat(
    model="gemma2:2b",
    messages=[{"role": "user", "content": direct_prompt}]
)

response_cot = ollama.chat(
    model="gemma2:2b",
    messages=[{"role": "user", "content": cot_prompt}]
)

print("DIRECT PROMPT:")
print(response_direct['message']['content'])
print("\nCHAIN-OF-THOUGHT:")
print(response_cot['message']['content'])
```

The chain-of-thought prompt produces more thoughtful, nuanced answers because it forces the model to decompose the problem into steps.

**When to use chain-of-thought**:
- Comparing multiple papers or concepts
- Identifying patterns or trends
- Making recommendations
- Analyzing arguments
- Complex reasoning tasks

**When not to use it**:
- Simple extraction tasks
- When you need concise outputs
- Time-critical applications (it's slower)

## Core Principle 4: Constrain the Output Format

For research workflows, you often need structured data you can parse programmatically.

### Example: Building a Dataset from Abstracts

```{python}
#| code-fold: true
import json

abstract = """
We analyze 10,000 scientific collaborations using network analysis and machine
learning. Our random forest classifier predicts collaboration success with 76%
accuracy. Key factors include prior co-authorship and institutional proximity.
"""

prompt = f"""Extract information from this abstract and return ONLY valid JSON:

Abstract: {abstract}

Return this exact structure:
{{
  "n_samples": <number or null>,
  "methods": [<list of methods>],
  "accuracy": <number or null>,
  "domain": "<research field>"
}}

JSON:"""

response = ollama.chat(
    model="gemma2:2b",
    messages=[{"role": "user", "content": prompt}],
    options={"temperature": 0}  # More deterministic
)

# Parse the JSON
try:
    data = json.loads(response['message']['content'])
    print("Extracted data:")
    print(json.dumps(data, indent=2))
except json.JSONDecodeError:
    print("Failed to parse JSON. Raw output:")
    print(response['message']['content'])
```

**Output**:
```json
{
  "n_samples": 10000,
  "methods": ["network analysis", "machine learning", "random forest classifier"],
  "accuracy": 0.76,
  "domain": "Science of Science"
}
```

By constraining the format to JSON, you can:
- Process outputs programmatically
- Build datasets automatically
- Catch errors when format deviates
- Integrate LLM outputs into data pipelines

::: {.callout-warning}
## JSON Parsing Reliability
Smaller models (like Gemma 2B) sometimes produce invalid JSON. Always wrap parsing in try-except blocks and validate outputs. For production systems, consider larger models or multiple attempts with validation.
:::

## Core Principle 5: Handle Uncertainty Explicitly

LLMs will confidently make up facts when they don't know the answer. You need to **give them permission to say "I don't know."**

### Example: Checking Paper Existence

```{python}
#| code-fold: true
# Bad prompt (encourages hallucination)
bad_prompt = """Summarize the main findings from the 2023 paper by Johnson et al.
on quantum community detection in biological networks."""

# Good prompt (allows uncertainty)
good_prompt = """I'm looking for a 2023 paper by Johnson et al. on quantum
community detection in biological networks.

If you know this paper, summarize its main findings.
If you're not certain this paper exists, say "I cannot verify this paper exists"
and do NOT make up details.

Response:"""

response_bad = ollama.chat(
    model="gemma2:2b",
    messages=[{"role": "user", "content": bad_prompt}]
)

response_good = ollama.chat(
    model="gemma2:2b",
    messages=[{"role": "user", "content": good_prompt}]
)

print("BAD PROMPT (encourages hallucination):")
print(response_bad['message']['content'])
print("\nGOOD PROMPT (allows uncertainty):")
print(response_good['message']['content'])
```

The good prompt explicitly gives the model an "out"—a way to admit uncertainty without failing the task.

**Strategies for handling uncertainty**:
- Explicitly say "If you don't know, say so"
- Ask for confidence levels ("How confident are you?")
- Request citations or sources (though models often still hallucinate)
- Cross-validate critical information with external sources

## Advanced Technique 1: Role-Playing

Setting a role can dramatically improve output quality by activating relevant patterns in the training data.

```{python}
#| code-fold: true
abstract = """
We propose a novel algorithm for detecting communities in networks with overlapping
membership. Using non-negative matrix factorization, we decompose the adjacency
matrix into low-rank factors. Experiments show our method handles overlaps better
than previous approaches.
"""

# No role
no_role = f"Explain this abstract simply:\n{abstract}"

# With role
with_role = f"""You are a network science professor explaining a paper to first-year
PhD students who just learned about graphs and matrices but haven't seen community
detection yet.

Explain this abstract in simple terms, using analogies where helpful:

{abstract}

Explanation:"""

response_no_role = ollama.chat(
    model="gemma2:2b",
    messages=[{"role": "user", "content": no_role}]
)

response_with_role = ollama.chat(
    model="gemma2:2b",
    messages=[{"role": "user", "content": with_role}]
)

print("NO ROLE:")
print(response_no_role['message']['content'])
print("\nWITH ROLE:")
print(response_with_role['message']['content'])
```

The role-playing prompt produces explanations better tailored to the specified audience.

**Useful roles for research**:
- "You are a domain expert in [field]..."
- "You are a critical peer reviewer..."
- "You are a research librarian helping find sources..."
- "You are a careful fact-checker..."

## Advanced Technique 2: Iterative Refinement

For complex tasks, break the work into multiple steps with separate prompts.

```{python}
#| code-fold: true
abstract = """
We develop a deep learning model for predicting citation counts from paper abstracts.
Using BERT embeddings and a regression head, we achieve R²=0.64 on a dataset of
50,000 computer science papers from 2010-2020. Feature importance analysis reveals
that novelty and clarity are key predictors. We release our code and dataset.
"""

# Step 1: Extract key information
step1_prompt = f"""Extract these fields from the abstract:
- Problem
- Method
- Dataset size and domain
- Key result (with metric)

Abstract: {abstract}

Extraction:"""

response1 = ollama.chat(
    model="gemma2:2b",
    messages=[{"role": "user", "content": step1_prompt}]
)

extraction = response1['message']['content']
print("STEP 1 - Extraction:")
print(extraction)

# Step 2: Identify strengths and weaknesses
step2_prompt = f"""Based on this paper summary, list 2 strengths and 2 potential
limitations:

{extraction}

Format:
Strengths:
1. ...
2. ...

Limitations:
1. ...
2. ...
"""

response2 = ollama.chat(
    model="gemma2:2b",
    messages=[{"role": "user", "content": step2_prompt}]
)

print("\nSTEP 2 - Analysis:")
print(response2['message']['content'])
```

Iterative refinement:
- Breaks complex tasks into manageable steps
- Allows you to validate intermediate outputs
- Produces more accurate final results
- Makes debugging easier

::: {.callout-tip}
## When to Use Multi-Step Prompting
Use iterative refinement when:
- The task requires multiple types of reasoning
- You need to validate intermediate results
- A single prompt would be too long/complex
- Different steps benefit from different prompting strategies
:::

## Advanced Technique 3: Self-Consistency

For tasks requiring reasoning, generate multiple responses and take the most common answer.

```{python}
#| code-fold: true
prompt = """Three papers study network robustness:
- Paper A: Targeted attacks are most damaging
- Paper B: Random failures rarely cause collapse
- Paper C: Hub nodes are critical for robustness

What is the research consensus on network robustness? Give a one-sentence answer.
"""

# Generate 5 responses
responses = []
for i in range(5):
    response = ollama.chat(
        model="gemma2:2b",
        messages=[{"role": "user", "content": prompt}],
        options={"temperature": 0.7}  # Some randomness
    )
    answer = response['message']['content']
    responses.append(answer)
    print(f"Response {i+1}: {answer}\n")

# In practice, you'd programmatically identify the most common theme
print("The most consistent theme across responses would be selected.")
```

Self-consistency works because:
- Correct reasoning tends to lead to the same answer
- Hallucinations are often random and inconsistent
- Averaging over multiple attempts reduces noise

**Trade-off**: 5x the API calls = 5x the cost/time. Use sparingly for critical decisions.

## Practical Workflow: Building a Paper Classifier

Let's combine techniques to build a complete workflow: classifying papers by methodology (experimental, theoretical, computational, review).

```{python}
#| code-fold: true
def classify_paper(abstract, examples=None):
    """Classify a paper's methodology type."""

    # Build prompt with few-shot examples
    prompt = """Classify research papers by methodology type.

Categories:
- Experimental: Lab/field experiments, data collection
- Theoretical: Mathematical models, proofs, frameworks
- Computational: Simulations, algorithms, data analysis
- Review: Literature reviews, meta-analyses, surveys

Examples:

Abstract: We prove a lower bound on the complexity of community detection algorithms.
Classification: Theoretical
Reasoning: Focuses on mathematical proof.

Abstract: We conduct surveys with 500 participants to study social network formation.
Classification: Experimental
Reasoning: Original data collection through experiments.

Abstract: We review 150 papers on graph neural networks and identify future directions.
Classification: Review
Reasoning: Surveys existing literature.

Now classify:

Abstract: {abstract}

Think step by step:
1. What is the primary activity? (proving, measuring, simulating, surveying?)
2. Which category fits best?

Classification: ...
Reasoning: ...
"""

    response = ollama.chat(
        model="gemma2:2b",
        messages=[{"role": "user", "content": prompt.format(abstract=abstract)}],
        options={"temperature": 0}
    )

    return response['message']['content']

# Test with different paper types
test_abstracts = [
    "We develop a graph neural network that predicts protein folding with 85% accuracy.",
    "We mathematically prove that scale-free networks are robust to random failures.",
    "We survey 200 papers on community detection and identify 5 major approaches.",
]

for abstract in test_abstracts:
    print(f"Abstract: {abstract}")
    print(classify_paper(abstract))
    print("-" * 80)
```

This workflow uses:
- **Few-shot learning** (examples)
- **Chain-of-thought** (step-by-step reasoning)
- **Constrained format** (Classification + Reasoning)
- **Low temperature** (consistency)

## Prompt Engineering Checklist

Before deploying a prompt on real research data:

**Clarity**:
- [ ] Is the task clearly defined?
- [ ] Are all terms unambiguous?
- [ ] Is the desired output format specified?

**Examples**:
- [ ] Do I need few-shot examples?
- [ ] Are examples diverse enough to cover edge cases?
- [ ] Do examples show the exact format I want?

**Output**:
- [ ] Is the output format machine-readable if needed?
- [ ] Is the length constrained appropriately?
- [ ] Can I validate the output programmatically?

**Robustness**:
- [ ] Does the prompt allow for uncertainty?
- [ ] Have I tested on edge cases?
- [ ] Does it handle missing information gracefully?

**Efficiency**:
- [ ] Is the prompt as concise as possible?
- [ ] Have I removed unnecessary instructions?
- [ ] Is temperature set appropriately?

## Common Pitfalls and Solutions

| Pitfall | Example | Solution |
|---------|---------|----------|
| Too vague | "Analyze this paper" | Specify what to analyze and how |
| Too long | 1000-word prompt | Break into multiple steps |
| Assumes knowledge | "Use the Smith method" | Explain or define domain-specific terms |
| No error handling | Expects perfect input | Handle edge cases explicitly |
| No validation | Blindly trusts output | Validate format and sanity-check results |

## Temperature and Sampling Parameters

We've mentioned `temperature` several times. Let's clarify what it does:

```{python}
#| code-fold: true
prompt = "Name a complex system (just the name, nothing else):"

print("Temperature = 0 (deterministic):")
for i in range(3):
    response = ollama.chat(
        model="gemma2:2b",
        messages=[{"role": "user", "content": prompt}],
        options={"temperature": 0}
    )
    print(f"  {i+1}. {response['message']['content']}")

print("\nTemperature = 1.0 (creative):")
for i in range(3):
    response = ollama.chat(
        model="gemma2:2b",
        messages=[{"role": "user", "content": prompt}],
        options={"temperature": 1.0}
    )
    print(f"  {i+1}. {response['message']['content']}")
```

**Temperature** controls randomness:
- **0**: Always picks the most probable token (deterministic)
- **0.3-0.7**: Balanced (default is usually ~0.7)
- **1.0+**: Very creative, more random

**When to adjust**:
- **Low temperature (0-0.3)**: Extraction, classification, consistency-critical tasks
- **Medium temperature (0.5-0.8)**: Summarization, explanation, general usage
- **High temperature (0.9+)**: Creative brainstorming, idea generation

## The Bigger Picture

You've now learned to **talk to LLMs effectively**. You can:
- Craft specific, well-structured prompts
- Use few-shot learning to teach by example
- Apply chain-of-thought for complex reasoning
- Extract structured data for research pipelines
- Handle uncertainty and edge cases gracefully

But a question remains: **how do these models represent and "understand" text internally?** When you send a prompt, the model doesn't see English words—it sees numbers. Millions of numbers arranged in high-dimensional space.

These numbers are called **embeddings**, and they're the foundation of everything LLMs do. Let's unbox the first layer and see how meaning becomes mathematics.

---

**Next**: [Embeddings: How Machines Understand Meaning →](embeddings-concepts.qmd)

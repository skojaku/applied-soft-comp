---
title: "Overview"
---

Language is the primary vessel of human knowledge. For decades, teaching computers to understand language was a rigid, rule-based endeavor. Today, we have entered the era of **Deep Learning for Text**, where models learn the nuances of language from vast amounts of data, enabling them to write code, compose poetry, and reason about complex problems.

This module explores the revolution in Natural Language Processing (NLP), from the foundational concepts of word embeddings to the state-of-the-art Large Language Models (LLMs) that are reshaping the world. We will cover the following topics:

1.  **Large Language Models (LLMs)**: We start by interacting with the giants. We'll explore what LLMs are, how they work at a high level, and how to control them effectively.
    -   [Large Language Models in Practice](llm-intro.qmd)
    -   [Prompt Engineering](prompt-engineering.qmd)

2.  **The Mechanics of Meaning**: How do computers read? We'll dive into the tokenization process and the architecture that makes it all possible: the Transformer.
    -   [Tokenization: Unboxing How LLMs Read Text](tokenization.qmd)
    -   [Transformers](transformers.qmd)
    -   [BERT, GPT, & SBERT](bert-gpt.qmd)

3.  **Vector Space Models**: We'll uncover the mathematical foundation of modern NLPâ€”representing words as vectors in a high-dimensional space where "meaning" is geometric.
    -   [Word Embeddings](word-embeddings.qmd)
    -   [Semaxis](semantic-research.qmd)
    -   [Word Bias](word-bias.qmd)

By the end of this module, you will not only know how to use these powerful tools but also understand the mechanisms that drive them, allowing you to build intelligent systems that truly understand text.

---
title: "Word Embeddings"
jupyter: python3
---

```{ojs}
//| echo: false
d3 = require("d3@7", "d3-simple-slider@1")
```

**Spoiler:** Meaning isn't stored in words. It's stored in the pattern of relationships between them. A "dog" isn't defined by what it *is*, but by what it does (barks, chases), what it's near (leash, kennel), and what it's *not* (cat, unlike). Word2vec doesn't teach a machine what "dog" means—it teaches the machine to find "dog's" position in the structure of language.

## The Mechanism

What does "dog" mean? You might say: a four-legged mammal, domesticated, often kept as a pet. But notice what you just did. You didn't describe "dog" in isolation. You described it through relationships—*contrast* (mammal, not reptile), *attribute* (four-legged), *function* (pet), *behavior* (domesticated). Meaning is structural. We understand "hot" only because "cold" exists. We understand "king" only in relation to "queen," "throne," "crown," "reign." Remove these neighbors, and "king" becomes an empty symbol.

This is the **distributional hypothesis**, articulated by linguist J.R. Firth in 1957: *"You shall know a word by the company it keeps."* If two words appear in similar contexts—surrounded by the same neighbors—they must mean similar things. Consider:

- "The **dog** barked at the mailman."
- "The **dog** chased the squirrel."
- "The **cat** hissed at the mailman."
- "The **cat** chased the mouse."

Both "dog" and "cat" appear with action verbs (barked, chased), both interact with targets (mailman, squirrel). Their meanings overlap because their contexts overlap. Language has **structure**—a topology where similar meanings cluster and relationships form consistent patterns.

If meaning is structure, then understanding language requires mapping this structure. Traditional approaches failed because they treated words as atomic symbols—discrete IDs in a database. In **one-hot encoding**, "King" (ID: 452) and "Queen" (ID: 991) are orthogonal vectors, mathematically perpendicular. The distance between "King" and "Queen" equals the distance between "King" and "Cabbage." This encoding captures no structure. It's a flat list masquerading as a semantic space.

**Word2vec**, introduced by Tomas Mikolov at Google in 2013, learns structure by shrinking the problem. Instead of analyzing entire documents, it examines tiny windows—just 5-10 words at a time. In the sentence "The cat chases mice in the garden," with window size 2, the context for "chases" is ["The", "cat", "mice", "in"]. This local window captures where syntax meets semantics. Words that share these micro-contexts must occupy similar positions in meaning-space.

Word2vec plays a prediction game with two variants. **Skip-gram** takes a center word like "cat" and predicts the surrounding context: which words likely appear nearby? The model outputs a probability distribution over vocabulary—high probability for {chases, sat, meowed}, low for {theorem, electron}. **CBOW** (Continuous Bag of Words) reverses the task: given context words, predict the missing center word. It's a fill-in-the-blank test. Both tasks force the model to encode positional information. Since "dog" and "cat" share contexts, they must receive similar encodings. Words become coordinates. Similarity becomes distance.

This is the compression trick: by learning to predict contexts, the model accidentally learns meaning.

### The Architecture: A Three-Layer Position Finder

Word2vec is a neural network with elegant symmetry. The **input layer** has $N$ neurons (one per vocabulary word) and acts as a lookup mechanism. When you input "cat," its neuron fires and all others stay silent—a one-hot vector. The **hidden layer** is much smaller (typically 100-300 neurons). Its activation pattern *is* the word embedding, the learned coordinates for "cat" in meaning-space. The **output layer** also has $N$ neurons, but multiple activate simultaneously, representing the probability distribution over possible context words.

For Skip-gram, given center word $w_c$ and potential context word $w_o$, the model computes:

$$ P(w_o|w_c) = \frac{\exp(\mathbf{v}_{w_o}^\top \mathbf{v}_{w_c})}{\sum_{w \in V} \exp(\mathbf{v}_w^\top \mathbf{v}_{w_c})} $$

where $\mathbf{v}_{w_c}$ is the center word's embedding vector and $\mathbf{v}_{w_o}$ is the context word's embedding. The dot product $\mathbf{v}_{w_o}^\top \mathbf{v}_{w_c}$ measures alignment—how often these words share contexts. High alignment means high co-occurrence. The **softmax** in the denominator normalizes this into a probability, but at a cost: for a 100,000-word vocabulary, computing the denominator requires 100,000 exponentials per prediction. This is intractable.

Word2vec sidesteps this through **hierarchical softmax**, organizing words into a binary tree (typically a Huffman tree, with frequent words near the root). Instead of comparing against all words simultaneously, the model makes sequential binary decisions: "Is the target word in the left subtree or the right?" This is path navigation, not exhaustive search. Computation drops from $O(|V|)$ to $O(\log |V|)$—the difference between checking every person in a city versus playing 20 questions.

### The Hidden Truth: Matrix Factorization

Word2vec appears to be a neural network predicting contexts. But Levy and Goldberg proved in 2014 that it's secretly factorizing a matrix—the **pointwise mutual information** (PMI) matrix:

$$ M_{ij} = \log \frac{P(w_i, w_j)}{P(w_i)P(w_j)} $$

where $P(w_i, w_j)$ is the probability words $w_i$ and $w_j$ co-occur in the same context window, and $P(w_i), P(w_j)$ are their individual probabilities. PMI measures association: it's zero when words appear independently, positive when they co-occur more than chance, negative when they avoid each other. The logarithm compresses the scale.

Word2vec learns embeddings such that:

$$ \mathbf{v}_{w_i}^\top \mathbf{v}_{w_j} \approx M_{ij} $$

The dot product of embeddings approximates PMI. Words with high PMI (frequent co-occurrence) end up geometrically close. This reveals the mechanism: Word2vec is finding low-dimensional coordinates that preserve the co-occurrence structure of language. The neural network is just the implementation. The real operation is structural compression—taking the vast, sparse matrix of word relationships and squeezing it into 300 dimensions while preserving distances.

The softmax complicates this, transforming the problem into a Boltzmann machine. It gives proper probabilities but introduces computational barriers. Hierarchical softmax is the workaround, making training feasible without abandoning the probabilistic framework.

## The Application

Once words have coordinates, we can measure their relationships. We use **cosine similarity**—the cosine of the angle between vectors. If two word vectors point in the same direction, similarity is 1. If perpendicular (like the old one-hot encoding), similarity is 0. If opposite, similarity is -1.

```{ojs}
//| echo: false
{
  const width = 600;
  const height = 300;
  const svg = d3.create("svg").attr("width", width).attr("height", height);

  const centerX = width / 2;
  const centerY = height / 2;
  const radius = 100;

  const slider1 = d3.sliderBottom().min(0).max(360).step(1).width(200).default(45).ticks(0).handle(d3.symbol().type(d3.symbolCircle).size(200)());
  const slider2 = d3.sliderBottom().min(0).max(360).step(1).width(200).default(135).ticks(0).handle(d3.symbol().type(d3.symbolCircle).size(200)());

  const g1 = svg.append("g").attr("transform", "translate(50, 50)");
  const g2 = svg.append("g").attr("transform", "translate(50, 120)");

  const viz = svg.append("g").attr("transform", `translate(${centerX}, ${centerY})`);

  viz.append("circle")
    .attr("r", radius)
    .attr("fill", "none")
    .attr("stroke", "#ddd")
    .attr("stroke-dasharray", "4,4");

  const vec1 = viz.append("line").attr("stroke", "#3498db").attr("stroke-width", 4).attr("marker-end", "url(#arrowhead-blue)");
  const vec2 = viz.append("line").attr("stroke", "#e74c3c").attr("stroke-width", 4).attr("marker-end", "url(#arrowhead-red)");

  const textSim = viz.append("text").attr("y", -radius - 20).attr("text-anchor", "middle").attr("font-family", "monospace").attr("font-size", "16px").attr("font-weight", "bold");
  const label1 = viz.append("text").attr("fill", "#3498db").attr("font-weight", "bold");
  const label2 = viz.append("text").attr("fill", "#e74c3c").attr("font-weight", "bold");

  svg.append("defs").append("marker")
    .attr("id", "arrowhead-blue")
    .attr("viewBox", "0 -5 10 10")
    .attr("refX", 8).attr("refY", 0)
    .attr("markerWidth", 6).attr("markerHeight", 6)
    .attr("orient", "auto")
    .append("path").attr("d", "M0,-5L10,0L0,5").attr("fill", "#3498db");

  svg.append("defs").append("marker")
    .attr("id", "arrowhead-red")
    .attr("viewBox", "0 -5 10 10")
    .attr("refX", 8).attr("refY", 0)
    .attr("markerWidth", 6).attr("markerHeight", 6)
    .attr("orient", "auto")
    .append("path").attr("d", "M0,-5L10,0L0,5").attr("fill", "#e74c3c");

  function update() {
    const angle1 = slider1.value() * (Math.PI / 180);
    const angle2 = slider2.value() * (Math.PI / 180);

    const x1 = Math.cos(angle1) * radius;
    const y1 = -Math.sin(angle1) * radius;
    const x2 = Math.cos(angle2) * radius;
    const y2 = -Math.sin(angle2) * radius;

    vec1.attr("x1", 0).attr("y1", 0).attr("x2", x1).attr("y2", y1);
    vec2.attr("x1", 0).attr("y1", 0).attr("x2", x2).attr("y2", y2);

    label1.attr("x", x1 * 1.2).attr("y", y1 * 1.2).text("Word A").attr("text-anchor", "middle");
    label2.attr("x", x2 * 1.2).attr("y", y2 * 1.2).text("Word B").attr("text-anchor", "middle");

    const cosSim = Math.cos(angle1 - angle2);
    textSim.text(`Cosine Similarity: ${cosSim.toFixed(3)}`);
  }

  slider1.on("onchange", update);
  slider2.on("onchange", update);

  g1.call(slider1);
  g2.call(slider2);

  svg.append("text").attr("x", 50).attr("y", 40).text("Angle Word A").attr("font-size", "12px").attr("fill", "#3498db");
  svg.append("text").attr("x", 50).attr("y", 110).text("Angle Word B").attr("font-size", "12px").attr("fill", "#e74c3c");

  update();
  return svg.node();
}
```

Let's verify this structural discovery using pre-trained embeddings from Google News (100 billion words). We'll use `gensim` to load the Word2vec model.

```{python}
import gensim.downloader as api
import numpy as np

# Load pre-trained Word2vec embeddings
print("Loading Word2vec model...")
model = api.load("word2vec-google-news-300")
print(f"Loaded embeddings for {len(model):,} words, each with {model.vector_size} dimensions")
```

### Test 1: Discovering Semantic Neighbors

If structure is real, then "dog" should naturally cluster near words that share its context—animals that bark, chase, and live with humans.

```{python}
similar_to_dog = model.most_similar("dog", topn=10)

print("Words most similar to 'dog':")
for word, similarity in similar_to_dog:
    print(f"  {word:20s} {similarity:.3f}")
```

The model was never told what a "dog" is. It discovered these relationships by observing that "dog," "dogs," "puppy," and "pooch" appear in nearly identical contexts. Structure emerged from statistics.

### Test 2: The Geometry of Analogy

Because meaning is now spatial, relationships become vectors. Consider the relationship between "king" and "man"—a royalty-to-person mapping. If we apply this same relationship to "woman," we should land near "queen." Mathematically:

$$ \vec{\text{King}} - \vec{\text{Man}} + \vec{\text{Woman}} \approx \vec{\text{Queen}} $$

This is vector arithmetic on meaning. We're not manipulating symbols; we're navigating structure.

```{python}
result = model.most_similar(positive=['king', 'woman'], negative=['man'], topn=5)

print("king - man + woman =")
for word, similarity in result:
    print(f"  {word:15s} {similarity:.3f}")
```

Why does this work? Because "king" and "man" share a gender dimension in the learned space, while "king" and "queen" share a royalty dimension. Vector subtraction isolates dimensions. Addition recombines them. The model never saw "king = man + royalty" as an equation—it discovered this structure by compression.

Let's test geographic structure. **Paris** is to **France** as **Berlin** is to **?**

$$ \vec{\text{Paris}} - \vec{\text{France}} + \vec{\text{Germany}} \approx \vec{\text{Berlin}} $$

```{python}
result = model.most_similar(positive=['Paris', 'Germany'], negative=['France'], topn=3)
print("\nParis - France + Germany =")
for word, similarity in result:
    print(f"  {word:15s} {similarity:.3f}")
```

The model learned that capital-country relationships form consistent directional patterns. This isn't reasoning—it's pattern compression. The structure was always there in the text. Word2vec just made it explicit.

Even grammatical structure emerges. The morphological rule "add -er for comparative adjectives" becomes a geometric transformation:

```{python}
result = model.most_similar(positive=['bigger', 'cold'], negative=['big'], topn=3)
print("\nbigger - big + cold =")
for word, similarity in result:
    print(f"  {word:15s} {similarity:.3f}")
```

### Visualizing the Structure

We can't perceive 300 dimensions, but we can compress them to 2 using **PCA** and see if structure persists. Let's examine countries and their capitals:

```{python}
#| fig-cap: "Country-capital relationships preserved in 2D projection. The parallel arrows reveal consistent directional structure."
#| fig-width: 12
#| fig-height: 10

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import pandas as pd

countries = ['Germany', 'France', 'Italy', 'Spain', 'Portugal', 'Greece']
capitals = ['Berlin', 'Paris', 'Rome', 'Madrid', 'Lisbon', 'Athens']

# Get embeddings
country_embeddings = np.array([model[country] for country in countries])
capital_embeddings = np.array([model[capital] for capital in capitals])

# PCA to 2D
pca = PCA(n_components=2)
embeddings = np.vstack([country_embeddings, capital_embeddings])
embeddings_pca = pca.fit_transform(embeddings)

# Create DataFrame
df = pd.DataFrame(embeddings_pca, columns=['PC1', 'PC2'])
df['Label'] = countries + capitals
df['Type'] = ['Country'] * len(countries) + ['Capital'] * len(capitals)

# Plot
fig, ax = plt.subplots(figsize=(12, 10))

# Plot countries and capitals
for idx, row in df.iterrows():
    color = '#e74c3c' if row['Type'] == 'Country' else '#3498db'
    marker = 'o' if row['Type'] == 'Country' else 's'
    ax.scatter(row['PC1'], row['PC2'], c=color, marker=marker, s=200,
               edgecolors='black', linewidth=1.5, alpha=0.7, zorder=3)
    ax.text(row['PC1'], row['PC2'] + 0.15, row['Label'], fontsize=12,
            ha='center', va='bottom', fontweight='bold',
            bbox=dict(facecolor='white', edgecolor='none', alpha=0.8))

# Draw arrows showing the "capital of" relationship
for i in range(len(countries)):
    country_pos = df.iloc[i][['PC1', 'PC2']].values
    capital_pos = df.iloc[i + len(countries)][['PC1', 'PC2']].values
    ax.arrow(country_pos[0], country_pos[1],
             capital_pos[0] - country_pos[0],
             capital_pos[1] - country_pos[1],
             color='gray', alpha=0.6, linewidth=2,
             head_width=0.15, head_length=0.1, zorder=2)

ax.set_title('The "Capital Of" Relationship as Parallel Transport', fontsize=16, fontweight='bold', pad=20)
ax.set_xlabel('Principal Component 1', fontsize=14)
ax.set_ylabel('Principal Component 2', fontsize=14)
ax.grid(alpha=0.3, linestyle='--')
ax.legend(handles=[
    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#e74c3c',
               markersize=10, label='Country', markeredgecolor='black'),
    plt.Line2D([0], [0], marker='s', color='w', markerfacecolor='#3498db',
               markersize=10, label='Capital', markeredgecolor='black')
], fontsize=12, loc='best')
plt.tight_layout()
plt.show()
```

The arrows from Germany→Berlin, France→Paris, Italy→Rome point in roughly the same direction. This is structural consistency. The "capital of" relationship occupies a stable region of semantic space. Even after crushing 300 dimensions into 2, the pattern survives.

Let's examine clustering structure using scientific vocabulary:

```{python}
#| fig-cap: "Scientific terms cluster by discipline. Structure isn't imposed—it's discovered from co-occurrence patterns."
#| fig-width: 10
#| fig-height: 8

from sklearn.manifold import TSNE
import seaborn as sns

# Scientific vocabulary across disciplines
words = [
    # Network science
    "network", "graph", "node", "edge", "community", "clustering",
    # Biology
    "protein", "gene", "cell", "DNA", "molecule", "organism",
    # Physics
    "quantum", "particle", "energy", "force", "electron", "photon",
    # Mathematics
    "theorem", "proof", "equation", "algebra", "calculus", "geometry",
    # Computing
    "algorithm", "computer", "software", "data", "program", "code"
]

# Get embeddings
word_vectors = np.array([model[word] for word in words if word in model])
valid_words = [word for word in words if word in model]

# Reduce to 2D with t-SNE
tsne = TSNE(n_components=2, random_state=42, perplexity=8)
word_2d = tsne.fit_transform(word_vectors)

# Plot
sns.set_style("white")
fig, ax = plt.subplots(figsize=(10, 8))

categories = {
    'Network Science': ['network', 'graph', 'node', 'edge', 'community', 'clustering'],
    'Biology': ['protein', 'gene', 'cell', 'DNA', 'molecule', 'organism'],
    'Physics': ['quantum', 'particle', 'energy', 'force', 'electron', 'photon'],
    'Mathematics': ['theorem', 'proof', 'equation', 'algebra', 'calculus', 'geometry'],
    'Computing': ['algorithm', 'computer', 'software', 'data', 'program', 'code']
}

colors = {'Network Science': '#e74c3c', 'Biology': '#2ecc71', 'Physics': '#f39c12',
          'Mathematics': '#9b59b6', 'Computing': '#3498db'}

for category, category_words in categories.items():
    indices = [valid_words.index(w) for w in category_words if w in valid_words]
    if indices:
        ax.scatter(word_2d[indices, 0], word_2d[indices, 1],
                  c=colors[category], label=category, s=200, alpha=0.7,
                  edgecolors='black', linewidth=1.5)

        for idx in indices:
            ax.annotate(valid_words[idx], (word_2d[idx, 0], word_2d[idx, 1]),
                       fontsize=9, ha='center', va='center', fontweight='bold')

ax.set_xlabel("Dimension 1", fontsize=12)
ax.set_ylabel("Dimension 2", fontsize=12)
ax.set_title("Discipline Clustering in Semantic Space", fontsize=14, fontweight='bold')
ax.legend(loc='best', fontsize=10)
ax.grid(alpha=0.3, linestyle='--')
sns.despine()
plt.tight_layout()
plt.show()
```

Biology terms cluster together. Physics terms form a separate island. The model never read a taxonomy of science—it inferred disciplinary structure from observing that "protein" and "gene" appear near each other far more often than "protein" and "theorem."

## The Limitation: Static Structure

Word2vec assigns one vector per word, which creates a fundamental problem: **polysemy**. Consider "bank":

- "I deposited money at the **bank**."
- "I sat on the river **bank**."

Word2vec gives both instances the same vector—an average of financial and geographical contexts. This muddy compromise represents neither meaning accurately. The model learned that "bank" has structural relationships with both {money, account, loan} and {river, shore, water}, so it places "bank" somewhere between these clusters. But language doesn't work through averages. Context determines meaning, and Word2vec ignores context at inference time.

This is the flaw that Transformers later fix. They don't assign a fixed position to "bank." They compute a position for "bank given 'river'" or "bank given 'money'." Context becomes part of the coordinate system. Structure becomes dynamic.

## The Takeaway

Word2vec revealed that meaning is structural, not definitional. We don't understand "dog" by memorizing a description—we understand it through its position in a vast web of relationships. The algorithm's insight was to make this structure explicit through geometry. By compressing co-occurrence patterns into coordinates, it transformed the fuzzy notion of "semantic similarity" into literal distance. Once words have positions, language has topology. And once language has topology, you can navigate meaning through vector arithmetic.

---

**Next**: [Text Fundamentals: The Full Picture →](text-fundamentals.qmd)

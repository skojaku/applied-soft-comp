---
title: "Word Embeddings"
jupyter: python3
---

```{ojs}
//| echo: false
d3 = require("d3@7", "d3-simple-slider@1")
```

**Spoiler:** Words aren't symbols. They're coordinates. If you force a machine to predict enough neighbors, it accidentally discovers that meaning is geometry. "King" minus "Man" plus "Woman" equals "Queen"—not because the algorithm understands royalty, but because the structure of language demands it.

## The Mechanism

We want computers to understand that "apple" and "pear" are similar, while "apple" and "democracy" are not. The naive approach treats words as unique database IDs. In this **one-hot encoding** scheme, "King" (ID: 452) and "Queen" (ID: 991) are orthogonal vectors—mathematically perpendicular. The distance between "King" and "Queen" is identical to the distance between "King" and "Cabbage." This is computational nonsense.

The solution came from British linguist J.R. Firth in 1957: *"You shall know a word by the company it keeps."* This is the **distributional hypothesis**—the idea that meaning isn't intrinsic but contextual. But Firth's insight remained theoretical until 2013, when Tomas Mikolov at Google operationalized it through a radical simplification: shrink the context window from entire documents to just 5-10 surrounding words. In the sentence "The cat chases mice in the garden," with a window size of 2, the context for "chases" becomes ["The", "cat", "mice", "in"]. This local window captures the grammatical and semantic relationships that define a word.

**Word2vec** doesn't try to "understand" text. It plays two variants of a prediction game. **Skip-gram** takes a center word like "cat" and predicts its surrounding context—a set like {garden, in, the, mice, chases, The}. Order doesn't matter; it's predicting which words exist nearby. **CBOW** (Continuous Bag of Words) reverses this: given the context words, predict the missing center word, like a fill-in-the-blank test. Both tasks force words that share contexts to occupy similar positions in vector space. Since "dog" appears near the same words as "cat," their vectors converge. Words become coordinates, and similarity becomes distance.

This is compression disguised as intelligence. The model learns that closeness in vector space corresponds to substitutability in language.

### The Neural Architecture

Word2vec is a three-layer neural network with elegant symmetry. The **input layer** has $N$ neurons (one per vocabulary word) and acts as a lookup mechanism—when you input "cat," its neuron activates and all others stay silent. The **hidden layer** is much smaller (typically 100-300 neurons), and its activation pattern *is* the word embedding. The **output layer** also has $N$ neurons, but unlike the input, multiple can activate simultaneously. Each output neuron's strength represents the probability of that word appearing in the context.

For Skip-gram, given a center word $w_c$ and a potential context word $w_o$, we compute:

$$ P(w_o|w_c) = \frac{\exp(\mathbf{v}_{w_o}^\top \mathbf{v}_{w_c})}{\sum_{w \in V} \exp(\mathbf{v}_w^\top \mathbf{v}_{w_c})} $$

where $\mathbf{v}_{w_c}$ is the center word's vector, $\mathbf{v}_{w_o}$ is the output word's vector, and $V$ is the vocabulary. The denominator—the **softmax normalization**—is the computational bottleneck. For a 100,000-word vocabulary, every prediction requires computing 100,000 exponentials. This is intractable at scale.

Word2vec sidesteps this through **hierarchical softmax**, which arranges words in a binary tree (typically a Huffman tree based on word frequency). Instead of comparing against all words simultaneously, the model makes a sequence of binary decisions: "Is the target word in the left or right subtree?" This reduces computation from $O(|V|)$ to $O(\log |V|)$. It's the difference between checking every person in a city versus playing 20 questions.

### The Hidden Matrix

Word2vec appears to be a neural network, but it's secretly performing matrix factorization. Levy and Goldberg (2014) proved that Word2vec implicitly factorizes the **pointwise mutual information** (PMI) matrix:

$$ M_{ij} = \log \frac{P(w_i, w_j)}{P(w_i)P(w_j)} $$

where $P(w_i, w_j)$ is the probability that words $w_i$ and $w_j$ co-occur within the context window, and $P(w_i)$, $P(w_j)$ are their individual probabilities. PMI is zero when words appear independently and highest when they always co-occur. The dot product of word vectors approximates PMI:

$$ \mathbf{v}_{w_i}^\top \mathbf{v}_{w_j} \approx M_{ij} $$

This reveals the mechanism: Word2vec is finding a low-dimensional representation that preserves co-occurrence statistics. Words that share contexts (high PMI) end up geometrically close. The softmax adds a twist—it transforms the problem into a Boltzmann machine, giving us proper probabilities but at the cost of computational complexity.

## The Application

Measuring similarity requires a metric. We use **cosine similarity**—the cosine of the angle between two vectors. If they point in the same direction, similarity is 1. If opposite, it's -1. Perpendicular vectors (orthogonal, like the old one-hot scheme) yield 0.

```{ojs}
//| echo: false
{
  const width = 600;
  const height = 300;
  const svg = d3.create("svg").attr("width", width).attr("height", height);

  const centerX = width / 2;
  const centerY = height / 2;
  const radius = 100;

  const slider1 = d3.sliderBottom().min(0).max(360).step(1).width(200).default(45).ticks(0).handle(d3.symbol().type(d3.symbolCircle).size(200)());
  const slider2 = d3.sliderBottom().min(0).max(360).step(1).width(200).default(135).ticks(0).handle(d3.symbol().type(d3.symbolCircle).size(200)());

  const g1 = svg.append("g").attr("transform", "translate(50, 50)");
  const g2 = svg.append("g").attr("transform", "translate(50, 120)");

  const viz = svg.append("g").attr("transform", `translate(${centerX}, ${centerY})`);

  viz.append("circle")
    .attr("r", radius)
    .attr("fill", "none")
    .attr("stroke", "#ddd")
    .attr("stroke-dasharray", "4,4");

  const vec1 = viz.append("line").attr("stroke", "#3498db").attr("stroke-width", 4).attr("marker-end", "url(#arrowhead-blue)");
  const vec2 = viz.append("line").attr("stroke", "#e74c3c").attr("stroke-width", 4).attr("marker-end", "url(#arrowhead-red)");

  const textSim = viz.append("text").attr("y", -radius - 20).attr("text-anchor", "middle").attr("font-family", "monospace").attr("font-size", "16px").attr("font-weight", "bold");
  const label1 = viz.append("text").attr("fill", "#3498db").attr("font-weight", "bold");
  const label2 = viz.append("text").attr("fill", "#e74c3c").attr("font-weight", "bold");

  svg.append("defs").append("marker")
    .attr("id", "arrowhead-blue")
    .attr("viewBox", "0 -5 10 10")
    .attr("refX", 8).attr("refY", 0)
    .attr("markerWidth", 6).attr("markerHeight", 6)
    .attr("orient", "auto")
    .append("path").attr("d", "M0,-5L10,0L0,5").attr("fill", "#3498db");

  svg.append("defs").append("marker")
    .attr("id", "arrowhead-red")
    .attr("viewBox", "0 -5 10 10")
    .attr("refX", 8).attr("refY", 0)
    .attr("markerWidth", 6).attr("markerHeight", 6)
    .attr("orient", "auto")
    .append("path").attr("d", "M0,-5L10,0L0,5").attr("fill", "#e74c3c");

  function update() {
    const angle1 = slider1.value() * (Math.PI / 180);
    const angle2 = slider2.value() * (Math.PI / 180);

    const x1 = Math.cos(angle1) * radius;
    const y1 = -Math.sin(angle1) * radius;
    const x2 = Math.cos(angle2) * radius;
    const y2 = -Math.sin(angle2) * radius;

    vec1.attr("x1", 0).attr("y1", 0).attr("x2", x1).attr("y2", y1);
    vec2.attr("x1", 0).attr("y1", 0).attr("x2", x2).attr("y2", y2);

    label1.attr("x", x1 * 1.2).attr("y", y1 * 1.2).text("Word A").attr("text-anchor", "middle");
    label2.attr("x", x2 * 1.2).attr("y", y2 * 1.2).text("Word B").attr("text-anchor", "middle");

    const cosSim = Math.cos(angle1 - angle2);
    textSim.text(`Cosine Similarity: ${cosSim.toFixed(3)}`);
  }

  slider1.on("onchange", update);
  slider2.on("onchange", update);

  g1.call(slider1);
  g2.call(slider2);

  svg.append("text").attr("x", 50).attr("y", 40).text("Angle Word A").attr("font-size", "12px").attr("fill", "#3498db");
  svg.append("text").attr("x", 50).attr("y", 110).text("Angle Word B").attr("font-size", "12px").attr("fill", "#e74c3c");

  update();
  return svg.node();
}
```

Let's verify this using pre-trained embeddings from Google News (100 billion words). We'll use the `gensim` library to load the Word2vec model.

```{python}
import gensim.downloader as api
import numpy as np

# Load pre-trained Word2vec embeddings (Google News corpus)
# This is a large download (~1.6GB), so it may take a moment
print("Loading Word2vec model...")
model = api.load("word2vec-google-news-300")
print(f"Loaded embeddings for {len(model):,} words, each with {model.vector_size} dimensions")
```

### Test 1: The "Company It Keeps"

If Firth was right, "king" should cluster with words from similar contexts—royalty, leadership, sovereignty—not random nouns.

```{python}
similar_to_king = model.most_similar("king", topn=10)

print("Words most similar to 'king':")
for word, similarity in similar_to_king:
    print(f"  {word:20s} {similarity:.3f}")
```

The model wasn't told that "king" relates to monarchy or power. It discovered this by observing that these words appear in similar local contexts.

### Test 2: Vector Arithmetic

Because meaning is now geometric, semantic relationships become vector operations. Consider this equation:

$$ \vec{\text{King}} - \vec{\text{Man}} + \vec{\text{Woman}} \approx \vec{?} $$

If you take the vector for "King," subtract the "maleness" component, and add "femaleness," you should land near "Queen." This isn't magic—it's mechanical. The model learned that "King" and "Man" share a gender dimension, and "King" and "Queen" share a royalty dimension. Vector arithmetic isolates and recombines these dimensions.

```{python}
result = model.most_similar(positive=['king', 'woman'], negative=['man'], topn=5)

print("king - man + woman =")
for word, similarity in result:
    print(f"  {word:15s} {similarity:.3f}")
```

This works for geographic relationships too. **Paris** is to **France** as **Berlin** is to **?**

$$ \vec{\text{Paris}} - \vec{\text{France}} + \vec{\text{Germany}} \approx \vec{\text{Berlin}} $$

```{python}
result = model.most_similar(positive=['Paris', 'Germany'], negative=['France'], topn=3)
print("\nParis - France + Germany =")
for word, similarity in result:
    print(f"  {word:15s} {similarity:.3f}")
```

It even captures grammatical transformations. The relationship between "big" and "bigger" is a morphological pattern the model learned implicitly.

```{python}
result = model.most_similar(positive=['bigger', 'cold'], negative=['big'], topn=3)
print("\nbigger - big + cold =")
for word, similarity in result:
    print(f"  {word:15s} {similarity:.3f}")
```

### Visualizing Semantic Space

We can't perceive 300 dimensions, but we can compress them to 2 using **PCA** (Principal Component Analysis). Let's examine how countries and their capitals organize in this space.

```{python}
#| fig-cap: "Word2vec embeddings of countries and capitals projected to 2D. The country-capital relationship is preserved as parallel displacement vectors."
#| fig-width: 12
#| fig-height: 10

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import pandas as pd

countries = ['Germany', 'France', 'Italy', 'Spain', 'Portugal', 'Greece']
capitals = ['Berlin', 'Paris', 'Rome', 'Madrid', 'Lisbon', 'Athens']

# Get embeddings
country_embeddings = np.array([model[country] for country in countries])
capital_embeddings = np.array([model[capital] for capital in capitals])

# PCA to 2D
pca = PCA(n_components=2)
embeddings = np.vstack([country_embeddings, capital_embeddings])
embeddings_pca = pca.fit_transform(embeddings)

# Create DataFrame
df = pd.DataFrame(embeddings_pca, columns=['PC1', 'PC2'])
df['Label'] = countries + capitals
df['Type'] = ['Country'] * len(countries) + ['Capital'] * len(capitals)

# Plot
fig, ax = plt.subplots(figsize=(12, 10))

# Plot countries and capitals
for idx, row in df.iterrows():
    color = '#e74c3c' if row['Type'] == 'Country' else '#3498db'
    marker = 'o' if row['Type'] == 'Country' else 's'
    ax.scatter(row['PC1'], row['PC2'], c=color, marker=marker, s=200,
               edgecolors='black', linewidth=1.5, alpha=0.7, zorder=3)
    ax.text(row['PC1'], row['PC2'] + 0.15, row['Label'], fontsize=12,
            ha='center', va='bottom', fontweight='bold',
            bbox=dict(facecolor='white', edgecolor='none', alpha=0.8))

# Draw arrows between countries and capitals
for i in range(len(countries)):
    country_pos = df.iloc[i][['PC1', 'PC2']].values
    capital_pos = df.iloc[i + len(countries)][['PC1', 'PC2']].values
    ax.arrow(country_pos[0], country_pos[1],
             capital_pos[0] - country_pos[0],
             capital_pos[1] - country_pos[1],
             color='gray', alpha=0.6, linewidth=2,
             head_width=0.15, head_length=0.1, zorder=2)

ax.set_title('Country-Capital Relationships in Word2vec Space', fontsize=16, fontweight='bold', pad=20)
ax.set_xlabel('Principal Component 1', fontsize=14)
ax.set_ylabel('Principal Component 2', fontsize=14)
ax.grid(alpha=0.3, linestyle='--')
ax.legend(handles=[
    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#e74c3c',
               markersize=10, label='Country', markeredgecolor='black'),
    plt.Line2D([0], [0], marker='s', color='w', markerfacecolor='#3498db',
               markersize=10, label='Capital', markeredgecolor='black')
], fontsize=12, loc='best')
plt.tight_layout()
plt.show()
```

The vectors from Germany→Berlin, France→Paris, and Italy→Rome are roughly parallel. This is the geometry of analogy. The model learned that the "capital of" relationship has a consistent directional structure in vector space.

Let's now visualize a different clustering pattern using scientific vocabulary:

```{python}
#| fig-cap: "Word2vec embeddings of scientific terms projected to 2D using t-SNE. Distinct disciplines form separate continents of meaning."
#| fig-width: 10
#| fig-height: 8

from sklearn.manifold import TSNE
import seaborn as sns

# Scientific vocabulary
words = [
    # Network science
    "network", "graph", "node", "edge", "community", "clustering",
    # Biology
    "protein", "gene", "cell", "DNA", "molecule", "organism",
    # Physics
    "quantum", "particle", "energy", "force", "electron", "photon",
    # Math
    "theorem", "proof", "equation", "algebra", "calculus", "geometry",
    # Computing
    "algorithm", "computer", "software", "data", "program", "code"
]

# Get embeddings
word_vectors = np.array([model[word] for word in words if word in model])
valid_words = [word for word in words if word in model]

# Reduce to 2D with t-SNE
tsne = TSNE(n_components=2, random_state=42, perplexity=8)
word_2d = tsne.fit_transform(word_vectors)

# Plot
sns.set_style("white")
fig, ax = plt.subplots(figsize=(10, 8))

categories = {
    'Network Science': ['network', 'graph', 'node', 'edge', 'community', 'clustering'],
    'Biology': ['protein', 'gene', 'cell', 'DNA', 'molecule', 'organism'],
    'Physics': ['quantum', 'particle', 'energy', 'force', 'electron', 'photon'],
    'Mathematics': ['theorem', 'proof', 'equation', 'algebra', 'calculus', 'geometry'],
    'Computing': ['algorithm', 'computer', 'software', 'data', 'program', 'code']
}

colors = {'Network Science': '#e74c3c', 'Biology': '#2ecc71', 'Physics': '#f39c12',
          'Mathematics': '#9b59b6', 'Computing': '#3498db'}

for category, category_words in categories.items():
    indices = [valid_words.index(w) for w in category_words if w in valid_words]
    if indices:
        ax.scatter(word_2d[indices, 0], word_2d[indices, 1],
                  c=colors[category], label=category, s=200, alpha=0.7,
                  edgecolors='black', linewidth=1.5)

        for idx in indices:
            ax.annotate(valid_words[idx], (word_2d[idx, 0], word_2d[idx, 1]),
                       fontsize=9, ha='center', va='center', fontweight='bold')

ax.set_xlabel("Dimension 1", fontsize=12)
ax.set_ylabel("Dimension 2", fontsize=12)
ax.set_title("The Geography of Science (Word2vec Space)", fontsize=14, fontweight='bold')
ax.legend(loc='best', fontsize=10)
ax.grid(alpha=0.3, linestyle='--')
sns.despine()
plt.tight_layout()
plt.show()
```

## The Limitation: Static Embeddings

Word2vec assigns exactly one vector to each word. This creates a problem called **polysemy**—the phenomenon where a single word has multiple meanings. Consider "bank":

- "I went to the **bank** to deposit money."
- "I sat on the river **bank**."

In Word2vec, both instances receive the same vector. The model averages the financial and geographical meanings into a single muddy point that represents neither. This "one word, one vector" constraint is what the Transformer architecture later destroys. Transformers don't assign a vector to "bank"—they assign a vector to "bank (given 'river')" or "bank (given 'deposit')." Context becomes part of the representation.

## The Takeaway

Word2vec revealed that meaning is relational, not definitional. We understand "hot" by its distance from "cold." We understand "king" by its offset from "man." The algorithm's power comes from shrinking the context window to where syntax and semantics overlap, then compressing those patterns through matrix factorization disguised as a neural network. Once words have coordinates, language has navigable geometry.

---

**Next**: [Text Fundamentals: The Full Picture →](text-fundamentals.qmd)

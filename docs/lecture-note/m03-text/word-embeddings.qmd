---
title: "Word Embeddings"
jupyter: python3
---

```{ojs}
//| echo: false
d3 = require("d3@7", "d3-simple-slider@1")
```

**Spoiler:** Words aren't symbols. They're coordinates. If you force a machine to predict enough neighbors, it accidentally discovers that meaning is geometry. "King" minus "Man" plus "Woman" equals "Queen"—not because the algorithm understands royalty, but because the structure of language demands it.

## The Mechanism

We want computers to understand that "apple" and "pear" are similar, while "apple" and "democracy" are not. The naive approach treats words as unique database IDs. In this **one-hot encoding** scheme, "King" (ID: 452) and "Queen" (ID: 991) are orthogonal vectors—mathematically perpendicular. The distance between "King" and "Queen" is identical to the distance between "King" and "Cabbage." This is computational nonsense.

The solution came from British linguist J.R. Firth in 1957: *"You shall know a word by the company it keeps."* This is the **distributional hypothesis**—the idea that meaning isn't intrinsic but contextual. Words that appear in similar environments must be semantically similar. If "dog" and "cat" both precede "barked," "chased," and "sat," they occupy the same region of semantic space.

In 2013, Tomas Mikolov at Google turned this philosophy into code. **Word2vec** doesn't try to "understand" text. It solves a fake problem to learn a real solution. The algorithm plays a guessing game: given a word like "fox," predict its neighbors—"quick," "brown," "jumps," "over." To win, the model forces "fox" to live near words that share its context. Since "dog" also appears near "jumps" and "quick," their vectors converge. Words become coordinates, and similarity becomes distance.

This is compression disguised as intelligence. The model learns that "closeness" in vector space corresponds to "substitutability" in language. Once words are numbers, you can do algebra with meaning.

## The Application

Measuring similarity requires a metric. We use **cosine similarity**—the cosine of the angle between two vectors. If they point in the same direction, similarity is 1. If opposite, it's -1. Perpendicular vectors (orthogonal, like the old one-hot scheme) yield 0.

```{ojs}
//| echo: false
{
  const width = 600;
  const height = 300;
  const svg = d3.create("svg").attr("width", width).attr("height", height);

  const centerX = width / 2;
  const centerY = height / 2;
  const radius = 100;

  const slider1 = d3.sliderBottom().min(0).max(360).step(1).width(200).default(45).ticks(0).handle(d3.symbol().type(d3.symbolCircle).size(200)());
  const slider2 = d3.sliderBottom().min(0).max(360).step(1).width(200).default(135).ticks(0).handle(d3.symbol().type(d3.symbolCircle).size(200)());

  const g1 = svg.append("g").attr("transform", "translate(50, 50)");
  const g2 = svg.append("g").attr("transform", "translate(50, 120)");

  const viz = svg.append("g").attr("transform", `translate(${centerX}, ${centerY})`);

  viz.append("circle")
    .attr("r", radius)
    .attr("fill", "none")
    .attr("stroke", "#ddd")
    .attr("stroke-dasharray", "4,4");

  const vec1 = viz.append("line").attr("stroke", "#3498db").attr("stroke-width", 4).attr("marker-end", "url(#arrowhead-blue)");
  const vec2 = viz.append("line").attr("stroke", "#e74c3c").attr("stroke-width", 4).attr("marker-end", "url(#arrowhead-red)");

  const textSim = viz.append("text").attr("y", -radius - 20).attr("text-anchor", "middle").attr("font-family", "monospace").attr("font-size", "16px").attr("font-weight", "bold");
  const label1 = viz.append("text").attr("fill", "#3498db").attr("font-weight", "bold");
  const label2 = viz.append("text").attr("fill", "#e74c3c").attr("font-weight", "bold");

  svg.append("defs").append("marker")
    .attr("id", "arrowhead-blue")
    .attr("viewBox", "0 -5 10 10")
    .attr("refX", 8).attr("refY", 0)
    .attr("markerWidth", 6).attr("markerHeight", 6)
    .attr("orient", "auto")
    .append("path").attr("d", "M0,-5L10,0L0,5").attr("fill", "#3498db");

  svg.append("defs").append("marker")
    .attr("id", "arrowhead-red")
    .attr("viewBox", "0 -5 10 10")
    .attr("refX", 8).attr("refY", 0)
    .attr("markerWidth", 6).attr("markerHeight", 6)
    .attr("orient", "auto")
    .append("path").attr("d", "M0,-5L10,0L0,5").attr("fill", "#e74c3c");

  function update() {
    const angle1 = slider1.value() * (Math.PI / 180);
    const angle2 = slider2.value() * (Math.PI / 180);

    const x1 = Math.cos(angle1) * radius;
    const y1 = -Math.sin(angle1) * radius;
    const x2 = Math.cos(angle2) * radius;
    const y2 = -Math.sin(angle2) * radius;

    vec1.attr("x1", 0).attr("y1", 0).attr("x2", x1).attr("y2", y1);
    vec2.attr("x1", 0).attr("y1", 0).attr("x2", x2).attr("y2", y2);

    label1.attr("x", x1 * 1.2).attr("y", y1 * 1.2).text("Word A").attr("text-anchor", "middle");
    label2.attr("x", x2 * 1.2).attr("y", y2 * 1.2).text("Word B").attr("text-anchor", "middle");

    const cosSim = Math.cos(angle1 - angle2);
    textSim.text(`Cosine Similarity: ${cosSim.toFixed(3)}`);
  }

  slider1.on("onchange", update);
  slider2.on("onchange", update);

  g1.call(slider1);
  g2.call(slider2);

  svg.append("text").attr("x", 50).attr("y", 40).text("Angle Word A").attr("font-size", "12px").attr("fill", "#3498db");
  svg.append("text").attr("x", 50).attr("y", 110).text("Angle Word B").attr("font-size", "12px").attr("fill", "#e74c3c");

  update();
  return svg.node();
}
```

Let's verify this using pre-trained embeddings from Google News (100 billion words). We'll use the `gensim` library to load the Word2vec model.

```{python}
import gensim.downloader as api
import numpy as np

# Load pre-trained Word2vec embeddings (Google News corpus)
# This is a large download (~1.6GB), so it may take a moment
print("Loading Word2vec model...")
model = api.load("word2vec-google-news-300")
print(f"Loaded embeddings for {len(model):,} words, each with {model.vector_size} dimensions")
```

### Test 1: The "Company It Keeps"

If Firth was right, "network" should cluster with words from similar contexts—tech, broadcasting, infrastructure—not random nouns.

```{python}
similar_to_network = model.most_similar("network", topn=10)

print("Words most similar to 'network':")
for word, similarity in similar_to_network:
    print(f"  {word:20s} {similarity:.3f}")
```

The model wasn't told that "network" relates to broadcasting or connectivity. It discovered this by observing that these words appear in the same sentences.

### Test 2: Vector Arithmetic

Because meaning is now geometric, semantic relationships become vector operations. Consider this equation:

$$ \vec{\text{King}} - \vec{\text{Man}} + \vec{\text{Woman}} \approx \vec{?} $$

If you take the vector for "King," subtract the "maleness" component, and add "femaleness," you should land near "Queen." This isn't magic—it's mechanical. The model learned that "King" and "Man" share a gender dimension, and "King" and "Queen" share a royalty dimension. Vector arithmetic isolates and recombines these dimensions.

```{python}
result = model.most_similar(positive=['king', 'woman'], negative=['man'], topn=5)

print("king - man + woman =")
for word, similarity in result:
    print(f"  {word:15s} {similarity:.3f}")
```

This works for geographic relationships too. **Paris** is to **France** as **Berlin** is to **?**

$$ \vec{\text{Paris}} - \vec{\text{France}} + \vec{\text{Germany}} \approx \vec{\text{Berlin}} $$

```{python}
result = model.most_similar(positive=['Paris', 'Germany'], negative=['France'], topn=3)
print("\nParis - France + Germany =")
for word, similarity in result:
    print(f"  {word:15s} {similarity:.3f}")
```

It even captures grammatical transformations. The relationship between "big" and "bigger" is a morphological pattern the model learned implicitly.

```{python}
result = model.most_similar(positive=['bigger', 'cold'], negative=['big'], topn=3)
print("\nbigger - big + cold =")
for word, similarity in result:
    print(f"  {word:15s} {similarity:.3f}")
```

### Visualizing Semantic Space

We can't perceive 300 dimensions, but we can compress them to 2 using **t-SNE** (t-Distributed Stochastic Neighbor Embedding). This reveals whether scientific fields form natural clusters in the model's latent space.

```{python}
#| fig-cap: "Word2vec embeddings of scientific terms projected to 2D. Distinct disciplines form separate continents of meaning."
#| fig-width: 10
#| fig-height: 8

from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns

# Scientific vocabulary
words = [
    # Network science
    "network", "graph", "node", "edge", "community", "clustering",
    # Biology
    "protein", "gene", "cell", "DNA", "molecule", "organism",
    # Physics
    "quantum", "particle", "energy", "force", "electron", "photon",
    # Math
    "theorem", "proof", "equation", "algebra", "calculus", "geometry",
    # Computing
    "algorithm", "computer", "software", "data", "program", "code"
]

# Get embeddings
word_vectors = np.array([model[word] for word in words if word in model])
valid_words = [word for word in words if word in model]

# Reduce to 2D
tsne = TSNE(n_components=2, random_state=42, perplexity=8)
word_2d = tsne.fit_transform(word_vectors)

# Plot
sns.set_style("white")
fig, ax = plt.subplots(figsize=(10, 8))

categories = {
    'Network Science': ['network', 'graph', 'node', 'edge', 'community', 'clustering'],
    'Biology': ['protein', 'gene', 'cell', 'DNA', 'molecule', 'organism'],
    'Physics': ['quantum', 'particle', 'energy', 'force', 'electron', 'photon'],
    'Mathematics': ['theorem', 'proof', 'equation', 'algebra', 'calculus', 'geometry'],
    'Computing': ['algorithm', 'computer', 'software', 'data', 'program', 'code']
}

colors = {'Network Science': '#e74c3c', 'Biology': '#2ecc71', 'Physics': '#f39c12',
          'Mathematics': '#9b59b6', 'Computing': '#3498db'}

for category, category_words in categories.items():
    indices = [valid_words.index(w) for w in category_words if w in valid_words]
    if indices:
        ax.scatter(word_2d[indices, 0], word_2d[indices, 1],
                  c=colors[category], label=category, s=200, alpha=0.7,
                  edgecolors='black', linewidth=1.5)

        for idx in indices:
            ax.annotate(valid_words[idx], (word_2d[idx, 0], word_2d[idx, 1]),
                       fontsize=9, ha='center', va='center', fontweight='bold')

ax.set_xlabel("Dimension 1", fontsize=12)
ax.set_ylabel("Dimension 2", fontsize=12)
ax.set_title("The Geography of Science (Word2vec Space)", fontsize=14, fontweight='bold')
ax.legend(loc='best', fontsize=10)
ax.grid(alpha=0.3, linestyle='--')
sns.despine()
plt.tight_layout()
plt.show()
```

## The Limitation: Static Embeddings

Word2vec assigns exactly one vector to each word. This creates a problem called **polysemy**—the phenomenon where a single word has multiple meanings. Consider "bank":

- "I went to the **bank** to deposit money."
- "I sat on the river **bank**."

In Word2vec, both instances receive the same vector. The model averages the financial and geographical meanings into a single muddy point that represents neither. This "one word, one vector" constraint is what the Transformer architecture later destroys. Transformers don't assign a vector to "bank"—they assign a vector to "bank (given 'river')" or "bank (given 'deposit')." Context becomes part of the representation.

## The Takeaway

Word2vec revealed that meaning is relational, not definitional. We understand "hot" by its distance from "cold." We understand "king" by its offset from "man." If language has geometry, then thought has geometry. And once thought has coordinates, you can navigate it.

---

**Next**: [Text Fundamentals: The Full Picture →](text-fundamentals.qmd)

---
title: "Transformers: Why Context Is Everything (And One Vector Is Nothing)"
execute:
    enabled: true
filters:
  - marimo-team/marimo
---

**The Spoiler:** The entire transformer revolution boils down to this—static embeddings assign one vector per word, ignoring that "bank" near "river" is mathematically different from "bank" near "money." Transformers solve this by computing context-aware representations through weighted mixing, and the weights themselves emerge from learned comparisons (Query × Key) between words. The result: machines finally understand that meaning isn't in the word; it's in the *distribution* of words around it.

## The Naive Intuition: One Word, One Vector

For many years, natural language processing treated words as having fixed meanings. We represented each word—like "bank"—as a single vector of numbers, such as `[0.23, -0.45, 0.67, ...]`. This approach, called **static embeddings**, assumed a word’s meaning could be fully captured with one point in space, no matter where the word appeared.

The foundation for this dates back to 1957, when J.R. Firth said: "You shall know a word by the company it keeps." This led to **distributional semantics**: a word's meaning comes from the other words nearby. If "bank" is near "mortgage" or "loan," we assume it means a financial place; if it's near "river" or "shore," we assume a landform. Word2vec (2013) brought this to life by learning word vectors from their neighbors.

But the big problem: Word2vec still gives just **one vector per word**. "Bank" has the same representation in "I deposited money at the bank" as in "We had a picnic by the bank." It averages all possible uses together—like saying everyone is 5'7" tall—hiding the differences that matter most.

## The Theoretical Failure: When Averaging Destroys the Signal

The naive hypothesis went like this: what if we just *mix* the target word with its neighbors? For the sentence "I deposited money at the bank," we could compute a contextualized representation as:

$$
\text{contextualized}_{\text{bank}} = w_1 \cdot \vec{v}_{\text{bank}} + w_2 \cdot \vec{v}_{\text{deposited}} + w_3 \cdot \vec{v}_{\text{money}} + \cdots
$$

where $w_i$ are weights and $\vec{v}_i$ are word embeddings. The **null hypothesis** suggests setting all weights uniformly: $w_i = 1/N$, producing a simple bag-of-words average. This should, in theory, create context-aware representations.

Consider the following example. Notice that "bank" sits neutrally between financial terms (money) and geographical terms (river). Now try manually adjusting the weights to contextualize "bank":

```{ojs}
//| echo: false
d3 = require("d3@7", "d3-simple-slider@1")
```

```{ojs}
//| echo: false
function sliderWithLabel(min, max, step, width, defaultValue, label) {
  const slider = d3.sliderBottom()
    .min(min).max(max).step(step).width(width).default(defaultValue);
  const svg = d3.create("svg").attr("width", width + 50).attr("height", 60);
  svg.append("g").attr("transform", "translate(25,20)").call(slider);
  svg.append("text").attr("x", (width + 50) / 2).attr("y", 10).attr("text-anchor", "middle").style("font-size", "12px").text(label);
  return svg.node();
}
```

```{ojs}
//| echo: false
viewof bankWeight = sliderWithLabel(0, 1, 0.01, 200, 1.0, "Bank Weight")
```

```{ojs}
//| echo: false
viewof moneyWeight = sliderWithLabel(0, 1, 0.01, 200, 0.0, "Money Weight")
```

```{ojs}
//| echo: false
viewof riverWeight = sliderWithLabel(0, 1, 0.01, 200, 0.0, "River Weight")
```

```{ojs}
//| echo: false
{
  // Word embeddings in 2D space
  const contextWords = ["bank", "money", "river"];
  const contextEmbeddings = [
    [0.0, 0.0],   // bank (center)
    [-1.6, -0.6], // money (financial, left)
    [1.4, -1.0]   // river (geographical, right)
  ];

  // Calculate weighted average
  const weights = [bankWeight, moneyWeight, riverWeight];
  const total = weights.reduce((a, b) => a + b, 0);
  const normalizedWeights = total > 0 ? weights.map(w => w / total) : [0, 0, 0];

  const newVec = [
    normalizedWeights[0] * contextEmbeddings[0][0] +
    normalizedWeights[1] * contextEmbeddings[1][0] +
    normalizedWeights[2] * contextEmbeddings[2][0],
    normalizedWeights[0] * contextEmbeddings[0][1] +
    normalizedWeights[1] * contextEmbeddings[1][1] +
    normalizedWeights[2] * contextEmbeddings[2][1]
  ];

  // Prepare data for visualization
  const originalData = contextWords.map((word, i) => ({
    word: word,
    x: contextEmbeddings[i][0],
    y: contextEmbeddings[i][1],
    type: "Original"
  }));

  const contextualizedData = [{
    word: "Contextualized Bank",
    x: newVec[0],
    y: newVec[1],
    type: "Contextualized"
  }];

  const data = [...originalData, ...contextualizedData];

  // Create visualization
  const plot = Plot.plot({
    width: 450,
    height: 450,
    marginTop: 40,
    marginRight: 20,
    marginBottom: 50,
    marginLeft: 60,
    style: {
      background: "white",
      color: "black"
    },
    x: {
      domain: [-2, 2],
      label: "Dimension 1"
    },
    y: {
      domain: [-2, 2],
      label: "Dimension 2"
    },
    color: {
      domain: ["Original", "Contextualized"],
      range: ["#dadada", "#ff7f0e"],
      legend: true
    },
    marks: [
      Plot.dot(data, {
        x: "x",
        y: "y",
        fill: "type",
        r: 8,
        tip: true
      }),
      Plot.text(data, {
        x: "x",
        y: "y",
        text: "word",
        dy: -15,
        fontSize: 12,
        fontWeight: "bold",
        fill: "black"
      }),
      Plot.text([{x: 0, y: 2.3}], {
        x: "x",
        y: "y",
        text: () => `Weights: Bank=${normalizedWeights[0].toFixed(2)}, Money=${normalizedWeights[1].toFixed(2)}, River=${normalizedWeights[2].toFixed(2)}`,
        fontSize: 11,
        fill: "black"
      })
    ]
  });

  return plot;
}
```

But real sentences aren't fair: not all words matter equally. In "I deposited money at the bank," the crucial clues are "deposited" and "money"—while words like "I," "at," and "the" add almost nothing to the meaning. If we just averaged every word together, we'd treat them all as equally important, which blurs the message. Instead, we need a way to pay extra attention to the words that really matter, and tune down the ones that don’t.

## The Hidden Mechanism: Query-Key Matching as Information Retrieval

The key insight is to treat context as a **search**. Picture a sentence as a library where each word is a book. To understand "bank," you ask: "Which words around me are most relevant?" You quickly check each book's label—some offer financial clues, others talk about geography. You focus most on the ones that match what you're looking for. This process of deciding which words to pay attention to, based on their relevance, is called **attention**.

![](../figs/transformer-attention.jpg)

For each word $i$ in a sentence with embedding $\vec{x}_i$, we create three linear transformations:

$$
\begin{align}
\vec{q}_i &= W_Q \vec{x}_i \quad \text{(Query: what I'm looking for)} \\
\vec{k}_i &= W_K \vec{x}_i \quad \text{(Key: what I offer)} \\
\vec{v}_i &= W_V \vec{x}_i \quad \text{(Value: what I contain)}
\end{align}
$$

Why have three transformations instead of just using $\vec{x}_i$? Because each plays a different role: the Query asks "what am I looking for?", the Key answers "what do I have?", and the Value says "here’s my info." By training $W_Q$, $W_K$, and $W_V$ separately, the model learns to focus on the right parts of the sentence for each word, mixing the most relevant information together.


To contextualize word $i$, compute its relevance to all other words $j$ via **dot products**: $\text{score}(i, j) = \vec{q}_i \cdot \vec{k}_j$. High scores indicate strong relevance; low scores indicate noise to ignore. These scores are converted to probabilities using softmax:

$$
w_{ij} = \frac{\exp(\vec{q}_i \cdot \vec{k}_j / \sqrt{d})}{\sum_{\ell} \exp(\vec{q}_i \cdot \vec{k}_\ell / \sqrt{d})}
$$

The division by $\sqrt{d}$ (where $d$ is the embedding dimension) is a scaling factor that prevents vanishing gradients during training. Finally, compute the **contextualized representation** as a weighted sum: $\text{contextualized}_i = \sum_j w_{ij} \vec{v}_j$.

Explore how different Query and Key transformations produce different attention patterns. Adjust the transformation parameters below to see how $W_Q$ and $W_K$ matrices change which words attend to which:

```{ojs}
//| echo: false
function compactSlider(min, max, step, width, defaultValue, label) {
  const slider = d3.sliderBottom()
    .min(min).max(max).step(step).width(width).default(defaultValue);
  const svg = d3.create("svg").attr("width", width + 40).attr("height", 50);
  svg.append("g").attr("transform", "translate(20,15)").call(slider);
  svg.append("text").attr("x", (width + 40) / 2).attr("y", 10).attr("text-anchor", "middle").style("font-size", "11px").text(label);
  return svg.node();
}
```

```{ojs}
//| echo: false
viewof qScaleX = compactSlider(-2, 2, 0.1, 180, 1.0, "Q Scale X")
```

```{ojs}
//| echo: false
viewof qScaleY = compactSlider(-2, 2, 0.1, 180, 1.0, "Q Scale Y")
```

```{ojs}
//| echo: false
viewof qRotate = compactSlider(-180, 180, 5, 180, 0, "Q Rotate (deg)")
```

```{ojs}
//| echo: false
viewof kScaleX = compactSlider(-2, 2, 0.1, 180, 1.0, "K Scale X")
```

```{ojs}
//| echo: false
viewof kScaleY = compactSlider(-2, 2, 0.1, 180, 1.0, "K Scale Y")
```

```{ojs}
//| echo: false
viewof kRotate = compactSlider(-180, 180, 5, 180, 0, "K Rotate (deg)")
```

```{ojs}
//| echo: false
{
  // Word embeddings in 2D space
  const attentionWords = ["bank", "money", "loan", "river", "shore"];
  const attentionEmbeddings = [
    [0.0, 0.0],    // bank (center)
    [-0.8, -0.3],  // money
    [-0.7, -0.6],  // loan
    [0.7, -0.5],   // river
    [0.6, -0.7]    // shore
  ].map(([x, y]) => [x * 2, y * 2]);

  // Transform embeddings function
  function transformEmbeddings(embeddings, scaleX, scaleY, rotateDeg) {
    const theta = (rotateDeg * Math.PI) / 180;
    const cos = Math.cos(theta);
    const sin = Math.sin(theta);

    return embeddings.map(([x, y]) => {
      // First scale
      const scaledX = x * scaleX;
      const scaledY = y * scaleY;
      // Then rotate
      return [
        scaledX * cos - scaledY * sin,
        scaledX * sin + scaledY * cos
      ];
    });
  }

  // Apply transformations
  const Q = transformEmbeddings(attentionEmbeddings, qScaleX, qScaleY, qRotate);
  const K = transformEmbeddings(attentionEmbeddings, kScaleX, kScaleY, kRotate);

  // Compute attention scores (Q @ K^T)
  const scores = Q.map(q => K.map(k => q[0] * k[0] + q[1] * k[1]));

  // Apply softmax to each row
  const attentionWeights = scores.map(row => {
    const maxScore = Math.max(...row);
    const expScores = row.map(s => Math.exp(s - maxScore));
    const sumExp = expScores.reduce((a, b) => a + b, 0);
    return expScores.map(e => e / sumExp);
  });

  // Prepare data for Query space
  const qData = attentionWords.map((word, i) => ({
    word: word,
    x: Q[i][0],
    y: Q[i][1]
  }));

  // Prepare data for Key space
  const kData = attentionWords.map((word, i) => ({
    word: word,
    x: K[i][0],
    y: K[i][1]
  }));

  // Prepare data for heatmap
  const heatmapData = [];
  for (let i = 0; i < attentionWords.length; i++) {
    for (let j = 0; j < attentionWords.length; j++) {
      heatmapData.push({
        Query: attentionWords[i],
        Key: attentionWords[j],
        Weight: attentionWeights[i][j]
      });
    }
  }

  // Create Query space visualization
  const qPlot = Plot.plot({
    width: 220,
    height: 220,
    marginTop: 30,
    marginBottom: 40,
    marginLeft: 50,
    marginRight: 20,
    style: {
      background: "white",
      color: "black"
    },
    x: { domain: [-4, 4], label: "Q1" },
    y: { domain: [-4, 4], label: "Q2" },
    marks: [
      Plot.dot(qData, { x: "x", y: "y", r: 6, fill: "#4682b4" }),
      Plot.text(qData, { x: "x", y: "y", text: "word", dy: -12, fontSize: 10, fontWeight: "bold", fill: "black" }),
      Plot.text([{ x: 0, y: 4.5 }], { x: "x", y: "y", text: () => "Query Space", fontSize: 12, fontWeight: "bold", fill: "black" })
    ]
  });

  // Create Key space visualization
  const kPlot = Plot.plot({
    width: 220,
    height: 220,
    marginTop: 30,
    marginBottom: 40,
    marginLeft: 50,
    marginRight: 20,
    style: {
      background: "white",
      color: "black"
    },
    x: { domain: [-4, 4], label: "K1" },
    y: { domain: [-4, 4], label: "K2" },
    marks: [
      Plot.dot(kData, { x: "x", y: "y", r: 6, fill: "#2e8b57" }),
      Plot.text(kData, { x: "x", y: "y", text: "word", dy: -12, fontSize: 10, fontWeight: "bold", fill: "black" }),
      Plot.text([{ x: 0, y: 4.5 }], { x: "x", y: "y", text: () => "Key Space", fontSize: 12, fontWeight: "bold", fill: "black" })
    ]
  });

  // Create attention heatmap
  const heatmapPlot = Plot.plot({
    width: 280,
    height: 280,
    marginTop: 50,
    marginBottom: 50,
    marginLeft: 70,
    marginRight: 80,
    style: {
      background: "white",
      color: "black"
    },
    x: { label: "Key Word" },
    y: { label: "Query Word" },
    color: {
      scheme: "Blues",
      label: "Attention",
      legend: true
    },
    marks: [
      Plot.cell(heatmapData, {
        x: "Key",
        y: "Query",
        fill: "Weight",
        tip: true
      }),
      Plot.text(heatmapData, {
        x: "Key",
        y: "Query",
        text: d => d.Weight.toFixed(2),
        fill: d => d.Weight > 0.35 ? "white" : "black",
        fontSize: 9
      }),
      Plot.text([{ x: 0, y: 0 }], {
        x: () => attentionWords.length / 2 - 0.5,
        y: () => -0.8,
        text: () => "Attention Weights (Softmax)",
        fontSize: 12,
        fontWeight: "bold",
        frameAnchor: "top",
        fill: "black"
      })
    ]
  });

  // Return combined visualization
  return html`<div style="display: flex; flex-direction: column; align-items: center; gap: 20px;">
    <div style="display: flex; gap: 20px; align-items: center;">
      ${qPlot}
      ${kPlot}
    </div>
    <div>
      ${heatmapPlot}
    </div>
  </div>`;
}
```

Now scale this up. **Every word** in a sentence needs to attend to **every other word**. For "The cat sat on the mat" (six words), we compute a $6 \times 6$ **attention matrix** $A$, where $A_{ij} = \text{softmax}(\vec{q}_i \cdot \vec{k}_j)$. Rows represent words asking for context (Queries); columns represent words providing context (Keys). Each cell $(i,j)$ indicates how much word $i$ attends to word $j$. Each row sums to 1—it's a probability distribution over context words.

But here's the final complication: **one attention matrix captures one type of relationship**. Language is multidimensional. There are syntactic relationships (subject-verb-object), semantic relationships (conceptual similarity between "cat" and "mat" as physical objects), positional relationships (local word order), and pragmatic relationships (coreference, where "her" links to "scientist"). A single attention mechanism can't capture all of these simultaneously.

![](../figs/transformer-multihead-attention.jpg)

The solution is **multi-head attention**: run multiple attention mechanisms in parallel, each with its own $W_Q$, $W_K$, $W_V$ matrices. Mathematically, $\text{MultiHead}(X) = \text{Concat}(\text{head}_1, \text{head}_2, \ldots, \text{head}_h) W^O$. Each head learns a different attention pattern. Modern transformers use 8-16 heads per layer: BERT uses 12, GPT-3 uses 96. It's like having twelve different experts analyze the sentence simultaneously—one focusing on syntax, one on semantics, one on coreference—and then combining their insights through a learned linear transformation.

## The Complete Architecture: Engineering for Scale

You've now discovered the core innovation: **self-attention via Query-Key-Value**. But a working transformer needs additional engineering to make this mechanism trainable at scale.

**Positional encoding** solves the problem that attention is permutation-invariant. Without extra information, "cat sat" equals "sat cat" because the dot products don't encode order. The solution is to add position-dependent vectors to embeddings: $\text{input}_i = \text{embedding}_i + \text{position}_i$. Transformers use sinusoidal functions of varying frequencies: $\text{PE}(pos, 2i) = \sin(pos / 10000^{2i/d})$ and $\text{PE}(pos, 2i+1) = \cos(pos / 10000^{2i/d})$. This gives the model access to both absolute position and relative distances between words.

**Residual connections** create highways for gradient flow. Deep networks suffer from vanishing gradients—signals decay exponentially as they backpropagate through layers. The solution is skip connections: $\text{output} = \text{LayerNorm}(x + \text{Attention}(x))$. This allows gradients to flow backward through 12-24 layers without vanishing.

**Feed-forward networks** add processing power beyond attention. After the attention sublayer, each word's representation passes through a small multi-layer perceptron independently: $\text{FFN}(x) = \text{ReLU}(x W_1 + b_1) W_2 + b_2$. This introduces non-linearity and gives the model capacity to learn complex transformations.

**Layer normalization** keeps activations in a stable numerical range during training, preventing exploding or vanishing values that would make gradient descent unstable.

The complete transformer block, repeated 12 or more times, looks like this: Input → Multi-Head Self-Attention → Add & LayerNorm (residual) → Feed-Forward Network → Add & LayerNorm (residual) → Output passed to next block.

Here's what matters: residual connections, layer normalization, and feed-forward networks all existed before 2017. The transformer's innovation is self-attention. That single mechanism replaced recurrent neural networks (RNNs), long short-term memory networks (LSTMs), and convolutional approaches for natural language processing. The original paper's title—*"Attention Is All You Need"*—was provocative but accurate.

The attention mechanism gets deployed differently depending on the task. **Encoder-only architectures** like BERT use bidirectional attention where word $i$ can attend to all words, past and future. These models excel at understanding text for classification, embeddings, and extraction tasks. You feed in "Is this paper about networks or biology?" and get back a classification. **Decoder-only architectures** like GPT and Gemma use causal attention (masked) where word $i$ can only attend to words at positions $\leq i$. This prevents "looking into the future" during autoregressive generation. When generating text word-by-word, you can't use words you haven't generated yet. These models excel at completion and chat. **Encoder-decoder architectures** like the original transformer use bidirectional attention in the encoder to process input, causal attention in the decoder for output generation, plus **cross-attention** where the decoder's Query vectors attend to the encoder's Key and Value vectors. These models excel at sequence-to-sequence tasks like translation: "Hello world" → "Bonjour monde."

## Why This Changed Everything

Before transformers, natural language processing used **sequential processing** via recurrent neural networks. The computational graph looked like: Word 1 → Hidden State 1 → Word 2 → Hidden State 2 → and so on. This architecture had three fatal flaws. First, sequential computation is inherently slow—you can't parallelize across words because each hidden state depends on the previous one. Second, information decays over long distances due to vanishing gradients; dependencies 100+ words apart become nearly impossible to learn. Third, the fixed-size hidden state creates an information bottleneck for long sequences.

Transformers solved all three problems simultaneously. **Parallel processing** means all words are processed simultaneously, yielding 100× faster training on modern GPUs. **Arbitrary long-range dependencies** become tractable because "bank" in position 50 can directly attend to "money" in position 2 through a single matrix multiplication—no gradients need to flow through 48 intermediate steps. **Scalability** emerged as a predictable phenomenon: performance scales logarithmically with parameters. This revealed a stunning empirical fact—bigger models aren't just incrementally better; they unlock **qualitatively new capabilities** like in-context learning and multi-step reasoning. This is why GPT-4 dramatically exceeds GPT-3, which dramatically exceeds GPT-2, despite using essentially the same architecture.

Consider a concrete example of how attention captures linguistic structure. Take the sentence "The scientist published her paper." We want to resolve the coreference: what does "her" refer to?

```{ojs}
//| echo: false

{
  const words = ["The", "scientist", "published", "her", "paper"];
  const attention = [
    [0.5, 0.3, 0.1, 0.05, 0.05],   // "The"→"scientist"
    [0.2, 0.5, 0.2, 0.05, 0.05],   // "scientist"
    [0.05, 0.3, 0.4, 0.1, 0.15],   // "published"
    [0.05, 0.6, 0.1, 0.2, 0.05],   // "her"→"scientist" ← KEY!
    [0.05, 0.2, 0.1, 0.15, 0.5]    // "paper"
  ];

  // Prepare heatmap data
  const heatmapData = [];
  for (let i = 0; i < words.length; i++) {
    for (let j = 0; j < words.length; j++) {
      heatmapData.push({
        Word: words[i],
        AttendsTo: words[j],
        Weight: attention[i][j],
        isHighlight: i === 3 && j === 1  // "her" → "scientist"
      });
    }
  }

  return Plot.plot({
    width: 500,
    height: 400,
    marginTop: 60,
    marginBottom: 60,
    marginLeft: 90,
    marginRight: 120,
    style: {
      background: "white",
      color: "black"
    },
    x: {
      label: "Attends To →",
      labelAnchor: "center"
    },
    y: {
      label: "↑ Word",
      labelAnchor: "center"
    },
    color: {
      type: "linear",
      scheme: "Purples",
      domain: [0, 0.6],
      label: "Attention Weight",
      legend: true
    },
    marks: [
      Plot.cell(heatmapData, {
        x: "AttendsTo",
        y: "Word",
        fill: "Weight",
        inset: 0.5
      }),
      Plot.text(heatmapData, {
        x: "AttendsTo",
        y: "Word",
        text: d => d.Weight.toFixed(2),
        fill: d => d.Weight > 0.3 ? "white" : "black",
        fontSize: 11,
        fontWeight: "bold"
      }),
      // Highlight box around "her" → "scientist"
      Plot.rect([{x: "scientist", y: "her"}], {
        x: "x",
        y: "y",
        fill: "none",
        stroke: "red",
        strokeWidth: 3,
        inset: -2
      }),
      Plot.text([{ x: 0, y: 0 }], {
        x: () => words.length / 2 - 0.5,
        y: () => -0.9,
        text: () => "Coreference via Attention: 'her' → 'scientist'",
        fontSize: 14,
        fontWeight: "bold",
        frameAnchor: "top",
        fill: "black"
      })
    ]
  });
}
```

*Attention pattern showing 'her' → 'scientist' (0.60 weight). The model learned coreference **without explicit grammar rules**—purely from prediction tasks.*

Row 3 corresponds to "her." The pronoun attends most strongly to "scientist" (0.60), correctly identifying the referent. The model **discovered** that pronouns link to earlier nouns through statistical patterns in training data—no one programmed this grammatical rule. This is the profound insight: transformers learn the structure of language as a side effect of optimizing a simple prediction objective.

## The Existential Conclusion

When you use `sentence-transformers` to generate embeddings for your research, you're using encoder transformers (BERT-based) with bidirectional attention. When you interact with ChatGPT or run Gemma locally, you're using decoder transformers with causal attention. When you see attention visualizations in papers, you now understand they represent **learned relevance weights** computed from Query-Key comparisons. The models don't understand language the way humans do—they perform **pattern matching at scale**—but the patterns they discover are often the same syntactic and semantic structures that linguists have documented for decades.

The limitations are worth remembering. Attention has **quadratic complexity**: computing $O(N^2)$ pairwise scores becomes prohibitively expensive for very long texts, which is why context windows typically cap at 2K-32K tokens. Transformers have **no true memory** beyond the current window—they can't learn facts across documents without retraining. They produce fluent text through statistical pattern matching but lack grounded understanding, leading to **hallucinations** when they confidently generate plausible-sounding nonsense. Training costs are astronomical: GPT-3 required roughly \$5 million in compute. But you don't need to train your own models; you can use pre-trained ones and fine-tune them for specific tasks with orders of magnitude less data and compute.

Return to where we started: the "bank" problem. Static embeddings treated "bank" as one point in space, averaging across all contexts. Transformers compute a **distribution** of representations, conditioned on the surrounding words. The revolution wasn't in the complexity—it was in recognizing that **meaning is relational, not absolute**. Words are like atoms in a molecule: their properties depend on what they're bonded to. Transformers formalized this intuition mathematically through a simple mechanism: weighted mixing based on learned relevance.

This entire architecture emerged from asking a single question: *What's the simplest mechanism that lets representations adapt to context?* The answer was weighted mixing where the weights come from comparing what each word needs (Query) against what other words offer (Key). Everything else—multi-head attention, layer normalization, feed-forward networks—is engineering to make that core idea scale to 175 billion parameters and beyond.

**Next**: [Word Embeddings: Where It Started](word-embeddings.qmd)

<script src="https://cdn.jsdelivr.net/npm/@marimo-team/marimo-snippets@1"></script>

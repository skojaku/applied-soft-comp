---
title: "Transformers: Why Context Is Everything (And One Vector Is Nothing)"
execute:
    enabled: true
filters:
  - marimo-team/marimo
---

# The Spoiler

**The entire transformer revolution boils down to this**: Static embeddings assign one vector per word, ignoring that "bank" near "river" is mathematically different from "bank" near "money." Transformers solve this by computing context-aware representations through weighted mixing—and the weights themselves emerge from learned comparisons (Query × Key) between words. The result: machines finally understand that meaning isn't in the word; it's in the *distribution* of words around it.

---

# 1. The Naive Model: One Word, One Vector

Here's the worldview that dominated NLP for decades:

> "Words have meanings. We can capture those meanings as vectors. 'Bank' is a point in 300-dimensional space."

```python
"bank" → [0.23, -0.45, 0.67, 0.12, ...]  # Static embedding
```

This **static embedding** philosophy assumes that the essence of "bank" can be distilled into a single coordinate in vector space, independent of context.

It's the linguistic equivalent of saying: *"A person has one personality, regardless of whether they're at a funeral or a party."*

Obviously, that's wrong. But *why* is it wrong, and more importantly—*how* do we fix it?

---

# 2. The Historical Anchor: Distributional Semantics (1950s)

The solution was actually proposed before computers could run it.

**J.R. Firth (1957)** famously declared:

> "You shall know a word by the company it keeps."

This is **distributional semantics**: meaning emerges from co-occurrence statistics. If "bank" appears near "loan," "mortgage," "interest," you infer *financial institution*. If it appears near "river," "shore," "erosion," you infer *geological feature*.

Word2vec (2013) operationalized this idea: predict a word from its neighbors, and the learned vectors encode co-occurrence patterns.

But here's the fundamental limitation: Word2vec still outputs **one vector per word type**. It collapses all contexts into a single average representation.

What we need is a system that computes **context-dependent** representations on-the-fly.

---

# 3. The Theoretical Baseline: Linear Mixing

**Naive hypothesis**: What if we just *mix* the target word with its neighbors?

For "I deposited money at the **bank**":

$$
\text{contextualized}_{\text{bank}} = w_1 \cdot \vec{v}_{\text{bank}} + w_2 \cdot \vec{v}_{\text{deposited}} + w_3 \cdot \vec{v}_{\text{money}} + \cdots
$$

where $w_i$ are weights and $\vec{v}_i$ are word embeddings.

**The null hypothesis**: If we set all weights uniformly ($w_i = 1/N$), we get a simple bag-of-words average.

**Prediction**: This should produce context-aware representations!

**Reality check**: It *kind of* works, but only if we can learn **adaptive weights**—high $w_i$ for contextually relevant words, low for irrelevant ones.

The critical question: **How do we compute $w_i$ automatically?**

---

# 4. The Heavy Tail: Why Uniform Weights Fail

If you average all words equally, you dilute the signal.

Consider:

- "I deposited money at the **bank**" (7 words)
- Relevant context: "deposited," "money" (2 words)
- Noise: "I," "at," "the" (5 words)

**Signal-to-noise ratio**: $2/7 \approx 0.29$

The useful information is drowned out by **function words** (articles, prepositions) that carry minimal semantic weight.

::: {.callout-note}
## The Deviation from Theory
In a perfect world, all words contribute equally to meaning. In reality, **a small subset of words carry disproportionate semantic weight**. This is a classic **heavy-tail distribution**: most weights should be near zero; a few should be large.
:::

We need a mechanism that **selectively attends** to relevant words and ignores the rest.

---

# 5. The Hidden Mechanism: Query-Key Matching

Here's the breakthrough insight, expressed as a **search problem**:

Imagine you're in a library (your sentence), looking for information to understand "bank."

1. **You have a question** (Query): "What context do I need?"
2. **Each book has a label** (Key): "I offer financial concepts" or "I offer spatial concepts"
3. **You compute relevance** (Dot Product): How well does my question match each book's offer?
4. **You read the relevant books** (Value): Weighted sum based on relevance scores

This is **attention**.

### The Math

For each word $i$ in a sentence, create three transformations of its embedding $\vec{x}_i$:

$$
\begin{align}
\vec{q}_i &= W_Q \vec{x}_i \quad \text{(Query: what I'm looking for)} \\
\vec{k}_i &= W_K \vec{x}_i \quad \text{(Key: what I offer)} \\
\vec{v}_i &= W_V \vec{x}_i \quad \text{(Value: what I contain)}
\end{align}
$$

To contextualize word $i$, compute its relevance to all other words $j$ via **dot products**:

$$
\text{score}(i, j) = \vec{q}_i \cdot \vec{k}_j
$$

High score → strong relevance. Low score → ignore.

Convert scores to **probabilities** using softmax:

$$
w_{ij} = \frac{\exp(\vec{q}_i \cdot \vec{k}_j / \sqrt{d})}{\sum_{\ell} \exp(\vec{q}_i \cdot \vec{k}_\ell / \sqrt{d})}
$$

(The $\sqrt{d}$ is a scaling factor to prevent vanishing gradients; $d$ is the embedding dimension.)

Finally, compute the **contextualized representation**:

$$
\text{contextualized}_i = \sum_j w_{ij} \vec{v}_j
$$

::: {.callout-tip}
## Why Three Separate Transformations?
You might ask: "Why not just use $\vec{x}_i$ directly for Q, K, V?"

Because we want **learned specialization**. The matrices $W_Q$, $W_K$, $W_V$ are trained to extract different aspects:

- $W_Q$: "What to search for"
- $W_K$: "What to advertise"
- $W_V$: "What to provide"

This is the **mechanism of selective attention**: queries and keys determine *where* to look; values determine *what* to retrieve.
:::

---

### Interactive Demo: Attention Mechanism

Explore how Query (Q) and Key (K) transformations affect the attention scores.

```python {.marimo}
transformer_viz.visualize_attention_mechanism()
```

---

# 6. The Attention Matrix: All Pairwise Relationships

Now scale this up: **every word** in a sentence needs to attend to **every other word**.

For "The cat sat on the mat" (6 words), we compute a $6 \times 6$ **attention matrix** $A$, where:

$$
A_{ij} = \text{softmax}(\vec{q}_i \cdot \vec{k}_j)
$$

- **Rows**: Words asking for context (Queries)
- **Columns**: Words providing context (Keys)
- **Cell $(i,j)$**: How much word $i$ attends to word $j$

Each row sums to 1 (it's a probability distribution over context words).

### Visualizing the Matrix

```{python}
#| code-fold: true
#| fig-cap: "Self-attention heatmap: Notice how 'sat' (row 3) strongly attends to 'cat' (subject) and 'mat' (object)—syntactic relationships discovered purely from data."
#| fig-width: 8
#| fig-height: 6

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

words = ["The", "cat", "sat", "on", "the", "mat"]
n = len(words)

# Realistic attention pattern (learned, not programmed)
attention = np.array([
    [0.6, 0.2, 0.1, 0.05, 0.03, 0.02],  # "The" → "cat" (determiner-noun)
    [0.1, 0.5, 0.3, 0.05, 0.03, 0.02],  # "cat" → self + "sat"
    [0.05, 0.4, 0.2, 0.15, 0.05, 0.15], # "sat" → "cat" (subj) + "mat" (obj)
    [0.05, 0.1, 0.2, 0.3, 0.15, 0.20],  # "on" → contextual spread
    [0.03, 0.05, 0.05, 0.1, 0.5, 0.27], # "the" → "mat"
    [0.02, 0.05, 0.15, 0.2, 0.15, 0.43] # "mat" → self + "on"
])

sns.set_style("white")
fig, ax = plt.subplots(figsize=(8, 6))
sns.heatmap(attention, annot=True, fmt=".2f",
            xticklabels=words, yticklabels=words,
            cmap="YlOrRd", vmin=0, vmax=0.6,
            cbar_kws={'label': 'Attention Weight'}, ax=ax)
ax.set_xlabel("Attends To (Keys)", fontsize=12, fontweight='bold')
ax.set_ylabel("Word (Queries)", fontsize=12, fontweight='bold')
ax.set_title("Self-Attention Matrix: Learned Syntax", fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()
```

**Key observations**:

1. **Row 3 ("sat")**: Peaks at "cat" (0.40) and "mat" (0.15)—verb attending to subject and object
2. **Diagonal**: Moderate values—words always attend somewhat to themselves
3. **Determiners ("The")**: Attend to their nouns ("cat")

::: {.callout-important}
## This Is Emergent, Not Programmed
No one told the model "verbs should attend to subjects." The patterns emerge from **backpropagation on prediction tasks**. The model discovers grammar as a side effect of learning to predict words.
:::

---

# 7. Multi-Head Attention: Multiple Simultaneous Perspectives

Here's the final twist: **one attention matrix captures one type of relationship**.

Language is multidimensional:

1. **Syntactic**: Subject-verb-object
2. **Semantic**: Conceptual similarity ("cat" and "mat" are both physical objects)
3. **Positional**: Local word order
4. **Pragmatic**: Coreference ("her" → "scientist")

**Solution**: Run **multiple attention mechanisms in parallel**, each with its own $W_Q$, $W_K$, $W_V$ matrices.

$$
\text{MultiHead}(X) = \text{Concat}(\text{head}_1, \text{head}_2, \ldots, \text{head}_h) W^O
$$

Each head learns a **different attention pattern**. Modern transformers use:

- BERT: 12 heads per layer
- GPT-3: 96 heads per layer

**Intuition**: It's like having 12 different "experts" analyze the sentence simultaneously—one focusing on syntax, one on semantics, one on coreference, etc.—and then **combining their insights**.

---

# 8. The Complete Architecture: From Tokens to Context

You've now discovered the core: **self-attention via Query-Key-Value**.

But a working transformer needs a few more components:

### 1. Positional Encoding

**Problem**: Attention is **permutation-invariant**. Without extra information, "cat sat" = "sat cat".

**Solution**: Add position-dependent vectors to embeddings:

$$
\text{input}_i = \text{embedding}_i + \text{position}_i
$$

Transformers use sinusoidal functions: $\text{PE}(pos, 2i) = \sin(pos / 10000^{2i/d})$, $\text{PE}(pos, 2i+1) = \cos(pos / 10000^{2i/d})$.

### 2. Residual Connections

**Problem**: Deep networks suffer from **vanishing gradients**.

**Solution**: Add skip connections:

$$
\text{output} = \text{LayerNorm}(x + \text{Attention}(x))
$$

This creates highways for gradients to flow backward through 12–24 layers.

### 3. Feed-Forward Networks

After attention, each word's representation passes through a small MLP **independently**:

$$
\text{FFN}(x) = \text{ReLU}(x W_1 + b_1) W_2 + b_2
$$

This adds non-linear processing power beyond attention.

### 4. Layer Normalization

Keeps activations in a stable range during training.

---

**The Complete Block** (repeated 12+ times):

```
Input
  ↓
Multi-Head Self-Attention
  ↓
Add & LayerNorm (residual)
  ↓
Feed-Forward Network
  ↓
Add & LayerNorm (residual)
  ↓
Output → Next Block
```

::: {.callout-note}
## What's Actually New?
Residual connections, layer norm, and FFNs existed before 2017. The **transformer's innovation** is self-attention. That single mechanism replaced RNNs, LSTMs, and convolutions for NLP.

The original paper's title—*"Attention Is All You Need"*—was provocative but accurate.
:::

---

# 9. Encoder vs. Decoder: Three Architectural Variants

The attention mechanism is used differently depending on the task:

### Encoder-Only (BERT)
**Purpose**: Understanding text (classification, embeddings)

- **Bidirectional attention**: Word $i$ can attend to all words (past and future)
- Used when you have the full context upfront
- Example: "Classify this document's topic"

### Decoder-Only (GPT, Gemma)
**Purpose**: Generating text (completion, chat)

- **Causal attention** (masked): Word $i$ can only attend to words $\leq i$
- Prevents "looking into the future" during autoregressive generation
- Example: "The cat sat on the..." → predict "mat"

**Why mask?** When generating word-by-word, you can't use words you haven't generated yet!

### Encoder-Decoder (Original Transformer)
**Purpose**: Sequence-to-sequence (translation, summarization)

- Encoder: Bidirectional attention on input
- Decoder: Causal attention on output + **cross-attention** to encoder
- Example: "Hello world" → "Bonjour monde"

---

# 10. Why This Changed Everything

### The Old World: Recurrent Neural Networks (RNNs)

Before transformers, NLP used **sequential processing**:

```
Word 1 → Hidden State 1 → Word 2 → Hidden State 2 → ...
```

**Problems**:

1. **Sequential = slow**: Can't parallelize across words
2. **Vanishing gradients**: Information decays over long distances
3. **Fixed context window**: Struggles with dependencies 100+ words apart

### The New World: Transformers

**Parallel processing**: All words processed simultaneously → **100× faster training**

**Arbitrary long-range dependencies**: "bank" in position 50 can directly attend to "money" in position 2

**Scalability**: Performance scales predictably with parameters (GPT-3: 175B params)

**Transfer learning**: Pre-train once, fine-tune for any task

---

::: {.callout-important}
## The Scaling Law Discovery
Transformers revealed a stunning empirical fact: **performance ∝ log(parameters)**.

Bigger models aren't just incrementally better—they unlock **qualitatively new capabilities** (e.g., in-context learning, reasoning). This is why GPT-4 > GPT-3 > GPT-2, despite using the same architecture.
:::

---

# 11. Real-World Example: Coreference Resolution

Let's see attention in action on a linguistic task: linking pronouns to referents.

**Sentence**: "The scientist published her paper."

```{python}
#| code-fold: true
#| fig-cap: "Attention pattern showing 'her' → 'scientist' (0.60 weight). The model learned coreference **without explicit grammar rules**—purely from prediction tasks."
#| fig-width: 8
#| fig-height: 6

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

words = ["The", "scientist", "published", "her", "paper"]
n = len(words)

attention = np.array([
    [0.5, 0.3, 0.1, 0.05, 0.05],  # "The"→"scientist"
    [0.2, 0.5, 0.2, 0.05, 0.05],  # "scientist"
    [0.05, 0.3, 0.4, 0.1, 0.15],  # "published"
    [0.05, 0.6, 0.1, 0.2, 0.05],  # "her"→"scientist" ← KEY!
    [0.05, 0.2, 0.1, 0.15, 0.5],  # "paper"
])

sns.set_style("white")
fig, ax = plt.subplots(figsize=(8, 6))
sns.heatmap(attention, annot=True, fmt=".2f",
            xticklabels=words, yticklabels=words,
            cmap="Purples", vmin=0, vmax=0.6,
            cbar_kws={'label': 'Attention Weight'}, ax=ax)
ax.set_xlabel("Attends To", fontsize=12, fontweight='bold')
ax.set_ylabel("Word", fontsize=12, fontweight='bold')
ax.set_title("Coreference via Attention: 'her' → 'scientist'", fontsize=14, fontweight='bold')

from matplotlib.patches import Rectangle
ax.add_patch(Rectangle((1, 3), 1, 1, fill=False, edgecolor='red', lw=3))
plt.tight_layout()
plt.show()
```

**Row 3 ("her")**: Strongly attends to "scientist" (0.60).

The model **discovered** that pronouns link to earlier nouns—no one programmed this rule.

---

# 12. The Existential Conclusion: What You Should Do With This Knowledge

### When Using Transformers in Research

1. **Embeddings** ([sentence-transformers](https://www.sbert.net/)): You're using **encoder transformers** (BERT-based) with bidirectional attention
2. **Text generation** (ChatGPT, Gemma): You're using **decoder transformers** with causal attention
3. **Interpreting attention**: When visualizations show attention maps, you now understand they represent **learned relevance weights** from Query-Key comparisons

### Limitations to Keep in Mind

**Quadratic complexity**: Attention computes $O(N^2)$ pairwise scores → expensive for long texts (typical context: 2K–32K tokens)

**No true memory**: Only "remembers" what's in the current window → can't learn facts across documents without retraining

**Stochastic parrots?**: Produces fluent text by pattern-matching, but lacks grounded understanding → **hallucinations**

**Training cost**: GPT-3 training: ~$5M in compute. (But you can use pre-trained models!)

---

### The Callback to the Beginning

Remember the "bank" problem?

- **Static embeddings** treated "bank" as one point in space.
- **Transformers** compute a **distribution** of representations, conditioned on context.

The revolution wasn't in the complexity—it was in recognizing that **meaning is relational, not absolute**.

Words are like atoms in a molecule: their properties depend on what they're bonded to. Transformers formalized this intuition mathematically.

---

::: {.callout-tip}
## The Meta-Lesson
This entire architecture emerged from asking: *"What's the simplest mechanism that lets representations adapt to context?"*

Answer: **Weighted mixing based on learned relevance.**

Everything else (multi-head, layer norm, FFNs) is engineering to make that core idea scale.
:::

---

# The Journey Continues

You've discovered how transformers work from first principles. But transformers weren't the first technique for text embeddings. Before BERT and GPT, there was **Word2vec**—a simpler, faster method that's still useful today.

Let's step back and see where embeddings came from.

---

**Next**: [Word Embeddings: Where It Started](word-embeddings.qmd)

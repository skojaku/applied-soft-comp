---
title: "Transformers: The Architecture Behind the Magic"
jupyter: python3
execute:
  eval: false
---

# You've Been Using Transformers. Now Let's See How They Work.

Every time you use an LLM, extract embeddings, or analyze text semantically, you're using **transformers**---a neural network architecture introduced in 2017 that revolutionized natural language processing.

Before transformers, NLP was slow, struggled with long texts, and couldn't capture complex context. After transformers: GPT, BERT, ChatGPT, and the entire modern AI landscape.

What makes transformers so powerful? **Self-attention**---a mechanism that lets models focus on relevant parts of text, no matter how far apart. Instead of processing words sequentially (like reading left-to-right), transformers process all words simultaneously while computing relationships between them.

This section explains transformers intuitively, with minimal math and lots of visuals. You won't implement one from scratch (that's a whole course), but you'll understand how they work well enough to use them effectively and appreciate what's happening under the hood.

## The Problem Transformers Solved

### Before Transformers: Recurrent Neural Networks

Before 2017, the dominant approach was **Recurrent Neural Networks (RNNs)**, which processed text sequentially:

```
Input: "The cat sat on the mat"

Processing:
Step 1: Read "The" � Update hidden state h�
Step 2: Read "cat" � Update hidden state h� (remembers "The")
Step 3: Read "sat" � Update hidden state h� (remembers "The cat")
...
Step 6: Read "mat" � Final state h� (remembers everything... hopefully)
```

**Problems with RNNs**:

1. **Sequential processing**: Must process words one-by-one (slow, can't parallelize)
2. **Vanishing memory**: By step 6, the model has partially forgotten step 1
3. **Long-distance dependencies**: Struggles when important context is far away

Example where RNNs struggle:
```
"The animal, which had been raised on a farm with many other animals and
had learned to socialize with both dogs and cats, finally sat on the mat."
```

By the time the RNN reaches "sat", it may have forgotten key details about "the animal."

### The Transformer Solution: Attention to Everything

Transformers process **all words simultaneously** and compute **attention weights** that determine which words are relevant to each other.

```
"The cat sat on the mat"

For the word "sat":
- High attention to "cat" (subject)
- High attention to "on" (preposition indicating location)
- Medium attention to "mat" (object of preposition)
- Low attention to "the" (not semantically important here)
```

The model learns what to pay attention to---no hand-coded rules.

::: {.callout-note}
## The Key Innovation
RNNs: "Remember everything sequentially"
Transformers: "Pay attention to what matters, anywhere in the text"
:::

## Self-Attention: The Core Mechanism

**Self-attention** is how transformers decide what's relevant. Let's build intuition with an example.

### Example: Disambiguating "Bank"

Consider two sentences:
1. "I deposited money at the **bank**."
2. "I sat by the river **bank**."

The word "bank" is ambiguous. How does a transformer decide which meaning?

**Self-attention computes**:
- In sentence 1: "bank" attends strongly to "deposited" and "money" � financial institution
- In sentence 2: "bank" attends strongly to "river" and "sat" � river edge

The model learns these attention patterns from data, without explicit programming.

### How Attention Works (Conceptually)

For each word, the model asks three questions:

1. **Query**: "What am I looking for?" (What context do I need?)
2. **Key**: "What do I offer?" (What information do I have?)
3. **Value**: "What do I actually contain?" (What's my semantic content?)

Then it computes:
- **Attention score** between Query and Keys (how relevant is each word?)
- **Weighted sum** of Values based on attention scores

**Analogy**: You're in a library (the sentence). You have a question (Query). Books have titles (Keys) and content (Values). You:
1. Compare your question to all book titles (compute attention scores)
2. Take a weighted combination of relevant books' content (weighted sum of Values)

The result is a context-aware representation of your word.

### Visual Example

Let's visualize attention for the sentence: "The cat sat on the mat"

```{python}
#| code-fold: true
#| fig-cap: "Self-attention heatmap for 'The cat sat on the mat'. Darker colors indicate stronger attention. Notice how 'sat' pays attention to 'cat' (subject) and 'mat' (object)."
#| fig-width: 8
#| fig-height: 6

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Simulated attention weights (would come from an actual transformer)
words = ["The", "cat", "sat", "on", "the", "mat"]
n = len(words)

# Manually crafted attention pattern (realistic but simplified)
attention = np.array([
    [0.6, 0.2, 0.1, 0.05, 0.03, 0.02],  # "The" � mostly attends to "cat"
    [0.1, 0.5, 0.3, 0.05, 0.03, 0.02],  # "cat" � self + "sat"
    [0.05, 0.4, 0.2, 0.15, 0.05, 0.15], # "sat" � "cat" (subject) + "mat" (object)
    [0.05, 0.1, 0.2, 0.3, 0.15, 0.20],  # "on" � preposition, attends to surrounding context
    [0.03, 0.05, 0.05, 0.1, 0.5, 0.27], # "the" � mostly "mat"
    [0.02, 0.05, 0.15, 0.2, 0.15, 0.43] # "mat" � self + "on"
])

sns.set_style("white")
fig, ax = plt.subplots(figsize=(8, 6))
sns.heatmap(attention, annot=True, fmt=".2f",
            xticklabels=words, yticklabels=words,
            cmap="YlOrRd", vmin=0, vmax=0.6,
            cbar_kws={'label': 'Attention Weight'}, ax=ax)
ax.set_xlabel("Attends To", fontsize=12, fontweight='bold')
ax.set_ylabel("Word", fontsize=12, fontweight='bold')
ax.set_title("Self-Attention Heatmap", fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()
```

**Observations**:
- "sat" (row 3) strongly attends to "cat" (0.40)---identifies the subject
- "sat" also attends to "mat" (0.15)---identifies where the action happens
- "The" (row 1) attends mostly to "cat"---articles attend to their nouns
- Diagonal has moderate values---words always attend somewhat to themselves

This pattern is **learned from data**, not hand-coded.

## Multi-Head Attention: Multiple Perspectives

One attention mechanism captures one type of relationship. But language has many types of relationships:
- Syntactic (subject-verb, adjective-noun)
- Semantic (synonyms, antonyms)
- Discourse (anaphora, coreference)

**Multi-head attention** runs multiple attention mechanisms in parallel, each learning different patterns.

**Analogy**: You're editing a document. You might review it multiple times:
- Grammar check (syntax)
- Fact-checking (semantics)
- Flow and coherence (discourse)

Each "head" is a different type of review.

### Example: Different Heads, Different Patterns

For "The cat sat on the mat":

**Head 1** (Syntactic):
- "sat"---"cat" (subject-verb relationship)
- "on"---"mat" (preposition-object relationship)

**Head 2** (Semantic):
- "cat"---"mat" (both are physical objects)
- "sat"---"on" (action-location relationship)

**Head 3** (Coreference):
- "the"---"cat", "the"---"mat" (determiners to nouns)

The final representation combines all heads, capturing multiple aspects of meaning simultaneously.

::: {.callout-tip}
## How Many Heads?
Modern transformers typically use 8-16 attention heads per layer. BERT uses 12 heads, GPT-3 uses 96. More heads = more expressive, but also more parameters to train.
:::

## The Transformer Architecture

A full transformer consists of multiple components. Let's break it down.

### Input Processing

The transformer receives input through three key steps:

1. **Tokenization**: Split text into tokens (words or subwords)
   ```
   "Community detection" → ["Community", "detection"]
   ```
   *We covered this in detail in the [tokenization section](tokenization.qmd)—how text becomes token IDs.*

2. **Token embeddings**: Convert tokens to vectors (embedding table lookup)
   ```
   "Community" → [0.23, -0.45, 0.67, ...]
   ```
   *Each token ID looks up its embedding in the learned embedding table.*

3. **Positional encoding**: Add information about word order
   ```
   Without position: "cat sat mat" vs. "mat sat cat" look identical
   With position: Order is preserved in embeddings
   ```
   *Positional encodings are added to embeddings so the model knows word order.*

### Transformer Block

Each transformer block has two main components:

**1. Multi-Head Self-Attention**
- Computes attention between all word pairs
- Multiple heads capture different relationships
- Output: Context-aware representations

**2. Feed-Forward Network**
- Applies nonlinear transformations
- Independent for each position
- Adds expressive power beyond attention

**Plus**: Residual connections and layer normalization (help with training)

### Stacking Layers

Transformers stack multiple blocks (6-24 or more):

```
Input: "The cat sat on the mat"
   �
Layer 1: Basic patterns (word relationships)
   �
Layer 2: Syntactic structure (grammar)
   �
Layer 3: Semantic relationships (meaning)
   �
...
   �
Layer 12: Abstract concepts (high-level understanding)
   �
Output: Rich contextual representations
```

Early layers learn surface patterns (punctuation, common words). Deeper layers learn abstract concepts and reasoning.

::: {.callout-note}
## Why Stack Layers?
Each layer builds on the previous one, creating hierarchies of abstraction<delete>
similar to how CNNs learn edges---textures---objects in image processing. In text, it's: words---phrases---sentences---concepts.
:::

## Encoder vs. Decoder: Two Transformer Flavors

There are two main transformer architectures:

### Encoder-Only (BERT)

**Purpose**: Understanding text (classification, extraction, embeddings)

**Architecture**:
- Bidirectional attention (can see all words at once)
- Used for: Sentence embeddings, classification, named entity recognition

**Example task**: "Is this paper about networks or biology?"
- Input: Abstract
- Output: Classification label

### Decoder-Only (GPT)

**Purpose**: Generating text (completion, chat, writing)

**Architecture**:
- Causal attention (can only see previous words, not future)
- Used for: Text generation, dialogue, completion

**Example task**: "Complete this sentence: 'The cat sat on the...'"
- Input: Partial sentence
- Output: Continuation ("mat", "sofa", etc.)

### Encoder-Decoder (Original Transformer)

**Purpose**: Sequence-to-sequence tasks (translation, summarization)

**Architecture**:
- Encoder processes input (bidirectional)
- Decoder generates output (causal, with attention to encoder)
- Used for: Translation, summarization, question answering

**Example task**: "Translate 'Hello' to French"
- Encoder: Understands "Hello"
- Decoder: Generates "Bonjour"

**Which one are you using?**
- `sentence-transformers`: Encoder (BERT-based)---for embeddings
- ChatGPT, Gemma: Decoder (GPT-based)---for generation
- Translation models: Encoder-Decoder---for sequence mapping

## Why Transformers Changed Everything

### 1. Parallelization

**RNNs**: Must process word-by-word (sequential)
**Transformers**: Process all words simultaneously (parallel)

Result: **100x faster training** on modern GPUs.

### 2. Long-Range Dependencies

**RNNs**: Forget information after ~100 tokens
**Transformers**: Can attend to any position (limited by context window, typically 2K-8K tokens)

Result: **Better understanding of context** in long documents.

### 3. Transfer Learning

Pre-train one large model on massive data, then fine-tune for specific tasks:

```
Pre-training (expensive, once):
Train BERT on billions of words---learns general language

Fine-tuning (cheap, many times):
Train on 1,000 medical abstracts---learns medical language
Train on 5,000 legal documents---learns legal language
```

Result: **State-of-the-art performance** with little task-specific data.

### 4. Scalability

Transformers scale beautifully:
- More data---better performance
- More parameters---better performance
- Bigger models---emergent abilities (reasoning, math, code)

Result: **GPT-3 (175B params) vastly outperforms GPT-2 (1.5B params)**, even though the architecture is nearly identical.

## From BERT to GPT to Gemma: The Evolution

### BERT (2018)
- **Encoder-only** transformer
- Trained with masked language modeling ("predict the [MASK] word")
- 110M-340M parameters
- **Use case**: Text understanding, embeddings

### GPT-2 (2019)
- **Decoder-only** transformer
- Trained to predict next word
- 1.5B parameters
- **Use case**: Text generation

### GPT-3 (2020)
- Scaled-up GPT-2
- 175B parameters
- **Emergent abilities**: Few-shot learning, reasoning, code generation
- **Use case**: General-purpose language tasks

### Gemma (2024)
- Open-source decoder model from Google
- 2B-27B parameters
- Efficient, fast, runs locally
- **Use case**: Research, education, private applications

**The trend**: More parameters, more data, more capabilities. But the core architecture---self-attention and transformer blocks---remains the same since 2017.

::: {.callout-important}
## Attention Is All You Need
The original transformer paper (Vaswani et al., 2017) was titled "Attention Is All You Need." The name was bold but accurate---self-attention turned out to be sufficient for nearly all NLP tasks, making RNNs largely obsolete.
:::

## Visualizing Attention in Real Models

Let's look at attention patterns from an actual BERT model analyzing text.

### Example: Coreference Resolution

**Sentence**: "The scientist published her paper."

```{python}
#| code-fold: true
#| fig-cap: "Attention pattern showing how 'her' attends to 'scientist', resolving the coreference. The model learned to link pronouns to their referents."
#| fig-width: 8
#| fig-height: 6

# Simulated attention (realistic pattern from BERT-like model)
words = ["The", "scientist", "published", "her", "paper"]
n = len(words)

# Attention for "her" (row 3)
attention = np.array([
    [0.5, 0.3, 0.1, 0.05, 0.05],  # "The"---"scientist"
    [0.2, 0.5, 0.2, 0.05, 0.05],  # "scientist"
    [0.05, 0.3, 0.4, 0.1, 0.15],  # "published"---"scientist", self
    [0.05, 0.6, 0.1, 0.2, 0.05],  # "her"---"scientist" (coreference!)
    [0.05, 0.2, 0.1, 0.15, 0.5],  # "paper"
])

sns.set_style("white")
fig, ax = plt.subplots(figsize=(8, 6))
sns.heatmap(attention, annot=True, fmt=".2f",
            xticklabels=words, yticklabels=words,
            cmap="Purples", vmin=0, vmax=0.6,
            cbar_kws={'label': 'Attention Weight'}, ax=ax)
ax.set_xlabel("Attends To", fontsize=12, fontweight='bold')
ax.set_ylabel("Word", fontsize=12, fontweight='bold')
ax.set_title("Attention: Resolving 'her'---'scientist'", fontsize=14, fontweight='bold')

# Highlight the key attention
from matplotlib.patches import Rectangle
ax.add_patch(Rectangle((1, 3), 1, 1, fill=False, edgecolor='red', lw=3))
plt.tight_layout()
plt.show()
```

Notice the red box: "her" strongly attends to "scientist" (0.60), correctly identifying the referent.

## Limitations of Transformers

Despite their power, transformers have limitations:

### 1. Quadratic Complexity

Attention computes relationships between all word pairs:
- 10 words: 100 comparisons
- 100 words: 10,000 comparisons
- 1,000 words: 1,000,000 comparisons

For very long texts, this becomes prohibitively expensive. **Context windows** (max input length) are typically 2K-8K tokens.

### 2. No Built-in Memory

Transformers only "remember" what's in the current context window. For conversations or documents longer than the window, information gets forgotten.

(Partial solutions: retrieval-augmented generation, memory mechanisms)

### 3. Lack of True Understanding

Transformers are pattern matchers. They don't have beliefs, goals, or understanding---they predict probable text based on patterns. This leads to:
- Hallucinations (confident false statements)
- Lack of common sense
- Brittleness on out-of-distribution inputs

### 4. Training Cost

Training large transformers requires:
- Millions of dollars in compute
- Months of training time
- Massive datasets
- Significant energy consumption

(But you can use pre-trained models, which is why this isn't a blocker for research)

## The Bigger Picture

You now understand the **transformer architecture**---the engine behind modern NLP:

- **Self-attention**: Models learn to focus on relevant context
- **Multi-head attention**: Captures multiple types of relationships simultaneously
- **Stacking layers**: Builds hierarchies from words to concepts
- **Encoder/Decoder variants**: Different architectures for different tasks

When you use an LLM:
1. Text---Embeddings (tokens to vectors)
2. Embeddings---Transformer layers (attention + feed-forward)
3. Transformer output---Task-specific head (classification, generation, etc.)

**But transformers weren't the first technique for text embeddings.** Before BERT and GPT, there was **Word2vec**---a simpler, faster method that's still useful today. Let's step back and see where embeddings came from.

---

**Next**: [Word Embeddings: Where It Started?](word-embeddings.qmd)

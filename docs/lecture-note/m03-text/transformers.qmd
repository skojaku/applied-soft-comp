---
title: "Transformers"
execute:
    enabled: true
filters:
  - marimo-team/marimo
---

**The Spoiler:** The entire transformer revolution boils down to this—static embeddings assign one vector per word, ignoring that "bank" near "river" is mathematically different from "bank" near "money." Transformers solve this by computing context-aware representations through weighted mixing, and the weights themselves emerge from learned comparisons (Query × Key) between words. The result: machines finally understand that meaning isn't in the word; it's in the *distribution* of words around it.

## One word, one vector is not enough

For many years, natural language processing treated words as having fixed meanings. We represented each word—like "bank"—as a single vector of numbers, called **static embeddings**.

But there's a hidden catch in this "one meaning per word" mindset: with just a single fixed entry in the dictionary, "bank" means exactly the same thing in "I deposited money at the bank" as in "We had a picnic by the bank." Every possible meaning gets mashed into a one-size-fits-all average—like describing the population by its *average* height and pretending that nobody's any shorter or taller. The interesting details—the outliers, the context clues—vanish in the mix.

The naive hypothesis went like this: what if we just *mix* the target word with its neighbors? For the sentence "I deposited money at the bank," we could compute a *contextualized representation* as:

$$
\vec{v}_{\text{bank (new)}} = w_1 \cdot \vec{v}_{\text{bank}} + w_2 \cdot \vec{v}_{\text{deposited}} + w_3 \cdot \vec{v}_{\text{money}} + \cdots
$$

where $w_i$ are weights and $\vec{v}_i$ are word embeddings.

Consider the following example. Notice that "bank" sits neutrally between financial terms (money) and geographical terms (river). Now try manually adjusting the weights to contextualize "bank":

```{ojs}
//| echo: false
d3 = require("d3@7", "d3-simple-slider@1")
```

```{ojs}
//| echo: false
function sliderWithLabel(min, max, step, width, defaultValue, label) {
  const slider = d3.sliderBottom()
    .min(min).max(max).step(step).width(width).default(defaultValue);
  const svg = d3.create("svg").attr("width", width + 50).attr("height", 60);
  svg.append("g").attr("transform", "translate(25,20)").call(slider);
  svg.append("text").attr("x", (width + 50) / 2).attr("y", 10).attr("text-anchor", "middle").style("font-size", "12px").text(label);
  return svg.node();
}
```

```{ojs}
//| echo: false
{
  // Create slider function that returns both the element and a reactive value
  function createWeightSlider(min, max, step, width, defaultValue, label) {
    const slider = d3.sliderBottom()
      .min(min).max(max).step(step).width(width).default(defaultValue);
    const svg = d3.create("svg").attr("width", width + 50).attr("height", 60);
    const g = svg.append("g").attr("transform", "translate(25,20)");
    g.call(slider);
    svg.append("text").attr("x", (width + 50) / 2).attr("y", 10)
       .attr("text-anchor", "middle").style("font-size", "12px").text(label);
    return { node: svg.node(), slider: slider };
  }

  // Create sliders
  const bankSliderObj = createWeightSlider(0, 1, 0.01, 200, 1.0, "Bank weight");
  const moneySliderObj = createWeightSlider(0, 1, 0.01, 200, 0.0, "Money weight");
  const riverSliderObj = createWeightSlider(0, 1, 0.01, 200, 0.0, "River weight");

  // Word embeddings in 2D space
  const contextWords = ["bank", "money", "river"];
  const contextEmbeddings = [
    [0.0, 0.0],   // bank (center)
    [-1.6, -0.6], // money (financial, left)
    [1.4, -1.0]   // river (geographical, right)
  ];

  // Create plot container
  const plotContainer = document.createElement("div");

  // Function to update visualization
  function update() {
    // Get current slider values
    const bankWeight = bankSliderObj.slider.value();
    const moneyWeight = moneySliderObj.slider.value();
    const riverWeight = riverSliderObj.slider.value();

    // Calculate weighted average
    const weights = [bankWeight, moneyWeight, riverWeight];
    const total = weights.reduce((a, b) => a + b, 0);
    const normalizedWeights = total > 0 ? weights.map(w => w / total) : [0, 0, 0];

    const newVec = [
      normalizedWeights[0] * contextEmbeddings[0][0] +
      normalizedWeights[1] * contextEmbeddings[1][0] +
      normalizedWeights[2] * contextEmbeddings[2][0],
      normalizedWeights[0] * contextEmbeddings[0][1] +
      normalizedWeights[1] * contextEmbeddings[1][1] +
      normalizedWeights[2] * contextEmbeddings[2][1]
    ];

    // Prepare data for visualization
    const originalData = contextWords.map((word, i) => ({
      word: word,
      x: contextEmbeddings[i][0],
      y: contextEmbeddings[i][1],
      type: "Original"
    }));

    const contextualizedData = [{
      word: "bank (new)",
      x: newVec[0],
      y: newVec[1],
      type: "Contextualized"
    }];

    const data = [...originalData, ...contextualizedData];

    // Clear and update plot
    d3.select(plotContainer).selectAll("*").remove();

    // Create visualization
    const plot = Plot.plot({
      width: 300,
      height: 300,
      marginTop: 60,
      marginRight: 20,
      marginBottom: 50,
      marginLeft: 60,
      style: {
        background: "white",
        color: "black"
      },
      x: {
        domain: [-2, 2],
        label: "Dimension 1",
        grid: true,
        ticks: 10
      },
      y: {
        domain: [-2, 2],
        label: "Dimension 2",
        grid: true,
        ticks: 10
      },
      color: {
        domain: ["Original", "Contextualized"],
        range: ["#dadada", "#ff7f0e"]
      },
      marks: [
        Plot.dot(data, {
          x: "x",
          y: "y",
          fill: "type",
          r: 8,
          tip: true
        }),
        Plot.text(data, {
          x: "x",
          y: "y",
          text: "word",
          dy: -15,
          fontSize: 8,
          fontWeight: "bold",
          fill: "black"
        }),
        Plot.text([{x: 0, y: 2.3}], {
          x: "x",
          y: "y",
          text: () => `Weights: Bank=${normalizedWeights[0].toFixed(2)}, Money=${normalizedWeights[1].toFixed(2)}, River=${normalizedWeights[2].toFixed(2)}`,
          fontSize: 11,
          fill: "black"
        }),
        // Custom legend at top center
        Plot.dot([{x: -0.8, y: 2.7, color: "#dadada"}, {x: 0.8, y: 2.7, color: "#ff7f0e"}], {
          x: "x",
          y: "y",
          fill: "color",
          r: 6
        }),
        Plot.text([{x: -0.5, y: 2.7, label: "Original"}, {x: 1.1, y: 2.7, label: "Contextualized"}], {
          x: "x",
          y: "y",
          text: "label",
          fontSize: 10,
          fill: "black",
          textAnchor: "start"
        })
      ]
    });

    d3.select(plotContainer).node().appendChild(plot);
  }

  // Add event listeners to sliders
  bankSliderObj.slider.on("onchange", update);
  moneySliderObj.slider.on("onchange", update);
  riverSliderObj.slider.on("onchange", update);

  // Initial render
  update();

  return html`<div style="display: flex; align-items: center; gap: 40px; justify-content: center;">
    <div style="display: flex; flex-direction: column; gap: 10px;">
      ${bankSliderObj.node}
      ${moneySliderObj.node}
      ${riverSliderObj.node}
    </div>
    <div>
      ${plotContainer}
    </div>
  </div>`;
}
```

By changing the weights, we can see that the vector for "bank" can lean more towards the financial terms or the geographical terms. So how can we determine the weights?


The simplest idea is to give each word an equal weight: $w_i = 1/N$. This creates a basic "bag-of-words" average. But sentences aren’t actually this fair—some words are much more important than others. For example, in "I deposited money at the bank," the words "deposited" and "money" are key, while "I," "at," and "the" add little meaning. If we treat all words the same, we lose the details that matter. We need a way to highlight the important words and downplay the rest.

## Attention mechanism

Let's walk through how transformers identify the surrounding words that are relevant to a focal word to be contextualized, which is called the **attention mechanism**. Before we dive into the attention mechanism, let's first prepare some terminology.

Suppose we have a sentence "I deposited money at the bank". Given a word "bank", we want to determine the weights $w_i$ for the surrounding words "I", "deposited", "money", and "at". We call the word "bank" the **query** word, and the surrounding words the **key** words. At a high level, we want to compute the weights $w_i$ for each query and key pair, and then average them.

$$
\vec{v}_{\text{query}} ^{\text{c}} = \sum_{i=1}^N w_i \cdot \vec{v}_{i}
$$

with weights $w_i$ being determined by the query and key vectors $w_{i}:=f(\vec{v}_{\text{query}}, \vec{v}_{i})$.
This function, $f$, is calle the **attention score function**.

In transformers, the attention score function $f$ is implemented as follows.
Given the original vector for a word (regardless of whether it is the query word or the key word), we linearly transform it into three vectors: Query, Key, and Value.

$$
\begin{align}
\vec{q}_i &= W_Q \vec{x}_i\\
\vec{k}_i &= W_K \vec{x}_i\\
\vec{v}_i &= W_V \vec{x}_i
\end{align}
$$

Why do we need three different vectors?
Imagine you are participating in a dinner party. You want to identify the people who are talking about a topic you care about. You listen to the surrounding people, playing as a 'listener'. At the same time, you also broadcast your own interests, playing as a 'speaker'. The query vector is representing you as a listener, the key vector is representing the people as speakers. And the value vector is representing the content of the conversation.

Once we have the query, key, and value vectors, we can compute the attention scores between the query and key vector as follows:


$$
w_{ij} = \frac{\exp(\vec{q}_i \cdot \vec{k}_j / \sqrt{d})}{\sum_{\ell} \exp(\vec{q}_i \cdot \vec{k}_\ell / \sqrt{d})},
$$

where $\vec{q}_i \cdot \vec{k}_j$ is the dot product between the query and key vectors, which is larger when the query and key vectors are *similar* (e.g., pointing to a similar direction).
The division by $\sqrt{d}$ (where $d$ is the embedding dimension) is a scaling factor that prevents vanishing gradients during training. Finally, compute the **contextualized representation** as a weighted sum: $\text{contextualized}_i = \sum_j w_{ij} \vec{v}_j$.

::: {.column-margin}
What is the vanishing gradient problem? It is a problem that the gradients of the loss function with respect to the weights become too small to be effective during training.
:::

Explore how different Query and Key transformations produce different attention patterns. First, let us create the key, query, and value vectors. In 2d, the linear transformation is just a scaling and a rotation.

```{ojs}
//| echo: false
{
  // Create slider function
  function createQKVSlider(min, max, step, width, defaultValue, label) {
    const slider = d3.sliderBottom()
      .min(min).max(max).step(step).width(width).default(defaultValue);
    const svg = d3.create("svg").attr("width", width + 40).attr("height", 50);
    const g = svg.append("g").attr("transform", "translate(20,15)");
    g.call(slider);
    svg.append("text").attr("x", (width + 40) / 2).attr("y", 10)
       .attr("text-anchor", "middle").style("font-size", "11px").text(label);
    return { node: svg.node(), slider: slider };
  }

  // Create sliders for Q, K, V transformations
  const qScaleXSlider = createQKVSlider(0.1, 2, 0.1, 150, 1.0, "Q Scale X");
  const qScaleYSlider = createQKVSlider(0.1, 2, 0.1, 150, 1.0, "Q Scale Y");
  const qRotateSlider = createQKVSlider(-180, 180, 5, 150, 0, "Q Rotate (deg)");
  const kScaleXSlider = createQKVSlider(0.1, 2, 0.1, 150, 1.0, "K Scale X");
  const kScaleYSlider = createQKVSlider(0.1, 2, 0.1, 150, 1.0, "K Scale Y");
  const kRotateSlider = createQKVSlider(-180, 180, 5, 150, 0, "K Rotate (deg)");
  const vScaleXSlider = createQKVSlider(0.1, 2, 0.1, 150, 1.0, "V Scale X");
  const vScaleYSlider = createQKVSlider(0.1, 2, 0.1, 150, 1.0, "V Scale Y");
  const vRotateSlider = createQKVSlider(-180, 180, 5, 150, 0, "V Rotate (deg)");

  // Original word vectors (bank, money, river)
  const originalVectors = [
    { name: "bank", vector: [1.5, 0.5] },
    { name: "money", vector: [1.8, 0.8] },
    { name: "river", vector: [0.5, 1.5] }
  ];

  // Create plot containers
  const qPlotContainer = document.createElement("div");
  const kPlotContainer = document.createElement("div");
  const vPlotContainer = document.createElement("div");

  // Function to transform vector
  function transformVector(vec, scaleX, scaleY, rotateDeg) {
    const theta = (rotateDeg * Math.PI) / 180;
    const cos = Math.cos(theta);
    const sin = Math.sin(theta);
    const scaledX = vec[0] * scaleX;
    const scaledY = vec[1] * scaleY;
    return [
      scaledX * cos - scaledY * sin,
      scaledX * sin + scaledY * cos
    ];
  }

  // Function to update visualization
  function update() {
    // Get current slider values
    const qScaleX = qScaleXSlider.slider.value();
    const qScaleY = qScaleYSlider.slider.value();
    const qRotate = qRotateSlider.slider.value();
    const kScaleX = kScaleXSlider.slider.value();
    const kScaleY = kScaleYSlider.slider.value();
    const kRotate = kRotateSlider.slider.value();
    const vScaleX = vScaleXSlider.slider.value();
    const vScaleY = vScaleYSlider.slider.value();
    const vRotate = vRotateSlider.slider.value();

    // Prepare data for each plot
    const originalData = originalVectors.map(item => ({
      name: item.name,
      x: item.vector[0],
      y: item.vector[1],
      type: "Original"
    }));

    const qData = originalVectors.map(item => {
      const qVec = transformVector(item.vector, qScaleX, qScaleY, qRotate);
      return {
        name: `q_${item.name}`,
        x: qVec[0],
        y: qVec[1],
        type: "Query"
      };
    });

    const kData = originalVectors.map(item => {
      const kVec = transformVector(item.vector, kScaleX, kScaleY, kRotate);
      return {
        name: `k_${item.name}`,
        x: kVec[0],
        y: kVec[1],
        type: "Key"
      };
    });

    const vData = originalVectors.map(item => {
      const vVec = transformVector(item.vector, vScaleX, vScaleY, vRotate);
      return {
        name: `v_${item.name}`,
        x: vVec[0],
        y: vVec[1],
        type: "Value"
      };
    });

    // Clear and update plots
    d3.select(qPlotContainer).selectAll("*").remove();
    d3.select(kPlotContainer).selectAll("*").remove();
    d3.select(vPlotContainer).selectAll("*").remove();

    // Create Query plot
    const qPlot = Plot.plot({
      width: 250,
      height: 250,
      marginTop: 40,
      marginRight: 20,
      marginBottom: 50,
      marginLeft: 60,
      style: {
        background: "white",
        color: "black"
      },
      x: {
        domain: [-3, 3],
        label: "Dimension 1",
        grid: true,
        ticks: 10
      },
      y: {
        domain: [-3, 3],
        label: "Dimension 2",
        grid: true,
        ticks: 10
      },
      marks: [
        Plot.dot([{x: 0, y: 0}], {
          x: "x",
          y: "y",
          r: 3,
          fill: "black"
        }),
        Plot.arrow([...originalData, ...qData], {
          x1: 0,
          y1: 0,
          x2: "x",
          y2: "y",
          stroke: "type",
          strokeWidth: 2,
          headLength: 8
        }),
        Plot.dot([...originalData, ...qData], {
          x: "x",
          y: "y",
          fill: "type",
          r: 5,
          tip: true
        }),
        Plot.text([...originalData, ...qData], {
          x: "x",
          y: "y",
          text: "name",
          dy: -12,
          fontSize: 9,
          fontWeight: "bold",
          fill: "black"
        }),
        Plot.text([{ x: 0, y: 3.4 }], {
          x: "x",
          y: "y",
          text: () => "Query Space",
          fontSize: 12,
          fontWeight: "bold",
          fill: "black"
        })
      ],
      color: {
        domain: ["Original", "Query"],
        range: ["#666666", "#4682b4"]
      }
    });

    // Create Key plot
    const kPlot = Plot.plot({
      width: 250,
      height: 250,
      marginTop: 40,
      marginRight: 20,
      marginBottom: 50,
      marginLeft: 60,
      style: {
        background: "white",
        color: "black"
      },
      x: {
        domain: [-3, 3],
        label: "Dimension 1",
        grid: true,
        ticks: 10
      },
      y: {
        domain: [-3, 3],
        label: "Dimension 2",
        grid: true,
        ticks: 10
      },
      marks: [
        Plot.dot([{x: 0, y: 0}], {
          x: "x",
          y: "y",
          r: 3,
          fill: "black"
        }),
        Plot.arrow([...originalData, ...kData], {
          x1: 0,
          y1: 0,
          x2: "x",
          y2: "y",
          stroke: "type",
          strokeWidth: 2,
          headLength: 8
        }),
        Plot.dot([...originalData, ...kData], {
          x: "x",
          y: "y",
          fill: "type",
          r: 5,
          tip: true
        }),
        Plot.text([...originalData, ...kData], {
          x: "x",
          y: "y",
          text: "name",
          dy: -12,
          fontSize: 9,
          fontWeight: "bold",
          fill: "black"
        }),
        Plot.text([{ x: 0, y: 3.4 }], {
          x: "x",
          y: "y",
          text: () => "Key Space",
          fontSize: 12,
          fontWeight: "bold",
          fill: "black"
        })
      ],
      color: {
        domain: ["Original", "Key"],
        range: ["#666666", "#2e8b57"]
      }
    });

    // Create Value plot
    const vPlot = Plot.plot({
      width: 250,
      height: 250,
      marginTop: 40,
      marginRight: 20,
      marginBottom: 50,
      marginLeft: 60,
      style: {
        background: "white",
        color: "black"
      },
      x: {
        domain: [-3, 3],
        label: "Dimension 1",
        grid: true,
        ticks: 10
      },
      y: {
        domain: [-3, 3],
        label: "Dimension 2",
        grid: true,
        ticks: 10
      },
      marks: [
        Plot.dot([{x: 0, y: 0}], {
          x: "x",
          y: "y",
          r: 3,
          fill: "black"
        }),
        Plot.arrow([...originalData, ...vData], {
          x1: 0,
          y1: 0,
          x2: "x",
          y2: "y",
          stroke: "type",
          strokeWidth: 2,
          headLength: 8
        }),
        Plot.dot([...originalData, ...vData], {
          x: "x",
          y: "y",
          fill: "type",
          r: 5,
          tip: true
        }),
        Plot.text([...originalData, ...vData], {
          x: "x",
          y: "y",
          text: "name",
          dy: -12,
          fontSize: 9,
          fontWeight: "bold",
          fill: "black"
        }),
        Plot.text([{ x: 0, y: 3.4 }], {
          x: "x",
          y: "y",
          text: () => "Value Space",
          fontSize: 12,
          fontWeight: "bold",
          fill: "black"
        })
      ],
      color: {
        domain: ["Original", "Value"],
        range: ["#666666", "#d2691e"]
      }
    });

    d3.select(qPlotContainer).node().appendChild(qPlot);
    d3.select(kPlotContainer).node().appendChild(kPlot);
    d3.select(vPlotContainer).node().appendChild(vPlot);
  }

  // Add event listeners to sliders
  qScaleXSlider.slider.on("onchange", update);
  qScaleYSlider.slider.on("onchange", update);
  qRotateSlider.slider.on("onchange", update);
  kScaleXSlider.slider.on("onchange", update);
  kScaleYSlider.slider.on("onchange", update);
  kRotateSlider.slider.on("onchange", update);
  vScaleXSlider.slider.on("onchange", update);
  vScaleYSlider.slider.on("onchange", update);
  vRotateSlider.slider.on("onchange", update);

  // Initial render
  update();

  return html`<div style="display: flex; align-items: flex-start; gap: 40px; justify-content: center;">
    <div style="display: flex; flex-direction: column; gap: 20px;">
      <div style="display: flex; flex-direction: column; gap: 8px;">
        <div style="font-weight: bold; margin-bottom: 3px;">Query (W_Q)</div>
        ${qScaleXSlider.node}
        ${qScaleYSlider.node}
        ${qRotateSlider.node}
      </div>
      <div style="display: flex; flex-direction: column; gap: 8px;">
        <div style="font-weight: bold; margin-bottom: 3px;">Key (W_K)</div>
        ${kScaleXSlider.node}
        ${kScaleYSlider.node}
        ${kRotateSlider.node}
      </div>
      <div style="display: flex; flex-direction: column; gap: 8px;">
        <div style="font-weight: bold; margin-bottom: 3px;">Value (W_V)</div>
        ${vScaleXSlider.node}
        ${vScaleYSlider.node}
        ${vRotateSlider.node}
      </div>
    </div>
    <div style="display: flex; flex-direction: column; gap: 20px; align-items: center;">
      ${qPlotContainer}
      ${kPlotContainer}
      ${vPlotContainer}
    </div>
  </div>`;
}
```


Adjust the transformation parameters below to see how $W_Q$ and $W_K$ matrices change which words attend to which:

```{ojs}
//| echo: false
function compactSlider(min, max, step, width, defaultValue, label) {
  const slider = d3.sliderBottom()
    .min(min).max(max).step(step).width(width).default(defaultValue);
  const svg = d3.create("svg").attr("width", width + 40).attr("height", 50);
  svg.append("g").attr("transform", "translate(20,15)").call(slider);
  svg.append("text").attr("x", (width + 40) / 2).attr("y", 10).attr("text-anchor", "middle").style("font-size", "11px").text(label);
  return svg.node();
}
```

```{ojs}
//| echo: false
{
  // Create slider function that returns both the element and a reactive value
  function createSlider(min, max, step, width, defaultValue, label) {
    const slider = d3.sliderBottom()
      .min(min).max(max).step(step).width(width).default(defaultValue);
    const svg = d3.create("svg").attr("width", width + 40).attr("height", 50);
    const g = svg.append("g").attr("transform", "translate(20,15)");
    g.call(slider);
    svg.append("text").attr("x", (width + 40) / 2).attr("y", 10)
       .attr("text-anchor", "middle").style("font-size", "11px").text(label);
    return { node: svg.node(), slider: slider };
  }

  // Create all sliders
  const qScaleXObj = createSlider(-2, 2, 0.1, 180, 1.0, "Q Scale X");
  const qScaleYObj = createSlider(-2, 2, 0.1, 180, 1.0, "Q Scale Y");
  const qRotateObj = createSlider(-180, 180, 5, 180, 0, "Q Rotate (deg)");
  const kScaleXObj = createSlider(-2, 2, 0.1, 180, 1.0, "K Scale X");
  const kScaleYObj = createSlider(-2, 2, 0.1, 180, 1.0, "K Scale Y");
  const kRotateObj = createSlider(-180, 180, 5, 180, 0, "K Rotate (deg)");

  // Get slider values
  const qScaleX = qScaleXObj.slider.value();
  const qScaleY = qScaleYObj.slider.value();
  const qRotate = qRotateObj.slider.value();
  const kScaleX = kScaleXObj.slider.value();
  const kScaleY = kScaleYObj.slider.value();
  const kRotate = kRotateObj.slider.value();

  // Word embeddings in 2D space
  const attentionWords = ["bank", "money", "loan", "river", "shore"];
  const attentionEmbeddings = [
    [0.0, 0.0],    // bank (center)
    [-0.8, -0.3],  // money
    [-0.7, -0.6],  // loan
    [0.7, -0.5],   // river
    [0.6, -0.7]    // shore
  ].map(([x, y]) => [x * 2, y * 2]);

  // Transform embeddings function
  function transformEmbeddings(embeddings, scaleX, scaleY, rotateDeg) {
    const theta = (rotateDeg * Math.PI) / 180;
    const cos = Math.cos(theta);
    const sin = Math.sin(theta);

    return embeddings.map(([x, y]) => {
      const scaledX = x * scaleX;
      const scaledY = y * scaleY;
      return [
        scaledX * cos - scaledY * sin,
        scaledX * sin + scaledY * cos
      ];
    });
  }

  // Function to update visualization
  function update() {
    // Get current values
    const qScaleX = qScaleXObj.slider.value();
    const qScaleY = qScaleYObj.slider.value();
    const qRotate = qRotateObj.slider.value();
    const kScaleX = kScaleXObj.slider.value();
    const kScaleY = kScaleYObj.slider.value();
    const kRotate = kRotateObj.slider.value();

    // Apply transformations
    const Q = transformEmbeddings(attentionEmbeddings, qScaleX, qScaleY, qRotate);
    const K = transformEmbeddings(attentionEmbeddings, kScaleX, kScaleY, kRotate);

    // Compute attention scores (Q @ K^T)
    const scores = Q.map(q => K.map(k => q[0] * k[0] + q[1] * k[1]));

    // Apply softmax to each row
    const attentionWeights = scores.map(row => {
      const maxScore = Math.max(...row);
      const expScores = row.map(s => Math.exp(s - maxScore));
      const sumExp = expScores.reduce((a, b) => a + b, 0);
      return expScores.map(e => e / sumExp);
    });

    // Prepare data for Query space
    const qData = attentionWords.map((word, i) => ({
      word: word,
      x: Q[i][0],
      y: Q[i][1]
    }));

    // Prepare data for Key space
    const kData = attentionWords.map((word, i) => ({
      word: word,
      x: K[i][0],
      y: K[i][1]
    }));

    // Prepare data for heatmap
    const heatmapData = (() => {
      const data = [];
      for (let i = 0; i < attentionWords.length; i++) {
        for (let j = 0; j < attentionWords.length; j++) {
          data.push({
            Query: attentionWords[i],
            Key: attentionWords[j],
            Weight: attentionWeights[i][j]
          });
        }
      }
      return data;
    })();

    // Clear and update plots
    d3.select(qPlotContainer).selectAll("*").remove();
    d3.select(kPlotContainer).selectAll("*").remove();
    d3.select(heatmapPlotContainer).selectAll("*").remove();

    // Create Query space visualization
    const qPlot = Plot.plot({
      width: 220,
      height: 220,
      marginTop: 30,
      marginBottom: 40,
      marginLeft: 50,
      marginRight: 20,
      style: {
        background: "white",
        color: "black"
      },
      x: { domain: [-4, 4], label: "Q1", grid: true, ticks: 10 },
      y: { domain: [-4, 4], label: "Q2", grid: true, ticks: 10 },
      marks: [
        Plot.dot(qData, { x: "x", y: "y", r: 6, fill: "#4682b4" }),
        Plot.text(qData, { x: "x", y: "y", text: "word", dy: -12, fontSize: 10, fontWeight: "bold", fill: "black" }),
        Plot.text([{ x: 0, y: 4.5 }], { x: "x", y: "y", text: () => "Query Space", fontSize: 12, fontWeight: "bold", fill: "black" })
      ]
    });

    // Create Key space visualization
    const kPlot = Plot.plot({
      width: 220,
      height: 220,
      marginTop: 30,
      marginBottom: 40,
      marginLeft: 50,
      marginRight: 20,
      style: {
        background: "white",
        color: "black"
      },
      x: { domain: [-4, 4], label: "K1", grid: true, ticks: 10 },
      y: { domain: [-4, 4], label: "K2", grid: true, ticks: 10 },
      marks: [
        Plot.dot(kData, { x: "x", y: "y", r: 6, fill: "#2e8b57" }),
        Plot.text(kData, { x: "x", y: "y", text: "word", dy: -12, fontSize: 10, fontWeight: "bold", fill: "black" }),
        Plot.text([{ x: 0, y: 4.5 }], { x: "x", y: "y", text: () => "Key Space", fontSize: 12, fontWeight: "bold", fill: "black" })
      ]
    });

    // Create attention heatmap
    const heatmapPlot = Plot.plot({
      width: 280,
      height: 280,
      marginTop: 50,
      marginBottom: 50,
      marginLeft: 70,
      marginRight: 80,
      style: {
        background: "white",
        color: "black"
      },
      x: { label: "Key Word" },
      y: { label: "Query Word" },
      color: {
        scheme: "Blues",
        label: "Attention",
        legend: true
      },
      marks: [
        Plot.cell(heatmapData, {
          x: "Key",
          y: "Query",
          fill: "Weight",
          tip: true
        }),
        Plot.text(heatmapData, {
          x: "Key",
          y: "Query",
          text: d => d.Weight.toFixed(2),
          fill: d => d.Weight > 0.35 ? "white" : "black",
          fontSize: 9
        }),
        Plot.text([{ x: 0, y: 0 }], {
          x: () => attentionWords.length / 2 - 0.5,
          y: () => -0.8,
          text: () => "Attention Weights (Softmax)",
          fontSize: 12,
          fontWeight: "bold",
          frameAnchor: "top",
          fill: "black"
        })
      ]
    });

    d3.select(qPlotContainer).node().appendChild(qPlot);
    d3.select(kPlotContainer).node().appendChild(kPlot);
    d3.select(heatmapPlotContainer).node().appendChild(heatmapPlot);
  }

  // Create plot containers
  const qPlotContainer = document.createElement("div");
  const kPlotContainer = document.createElement("div");
  const heatmapPlotContainer = document.createElement("div");

  // Add event listeners to sliders
  qScaleXObj.slider.on("onchange", update);
  qScaleYObj.slider.on("onchange", update);
  qRotateObj.slider.on("onchange", update);
  kScaleXObj.slider.on("onchange", update);
  kScaleYObj.slider.on("onchange", update);
  kRotateObj.slider.on("onchange", update);

  // Initial render
  update();

  // Return combined layout
  return html`<div style="display: flex; align-items: flex-start; gap: 40px; justify-content: center;">
    <div style="display: flex; flex-direction: column; gap: 25px;">
      <div style="display: flex; flex-direction: column; gap: 8px;">
        <div style="font-weight: bold; margin-bottom: 3px;">Query Transformation (W_Q)</div>
        ${qScaleXObj.node}
        ${qScaleYObj.node}
        ${qRotateObj.node}
      </div>
      <div style="display: flex; flex-direction: column; gap: 8px;">
        <div style="font-weight: bold; margin-bottom: 3px;">Key Transformation (W_K)</div>
        ${kScaleXObj.node}
        ${kScaleYObj.node}
        ${kRotateObj.node}
      </div>
    </div>
    <div style="display: flex; flex-direction: column; align-items: center; gap: 20px;">
      <div style="display: flex; flex-direction: column; gap: 20px; align-items: center;">
        ${qPlotContainer}
        ${kPlotContainer}
      </div>
      <div>
        ${heatmapPlotContainer}
      </div>
    </div>
  </div>`;
}
```

Now scale this up. **Every word** in a sentence needs to attend to **every other word**. For "The cat sat on the mat" (six words), we compute a $6 \times 6$ **attention matrix** $A$, where $A_{ij} = \text{softmax}(\vec{q}_i \cdot \vec{k}_j)$. Rows represent words asking for context (Queries); columns represent words providing context (Keys). Each cell $(i,j)$ indicates how much word $i$ attends to word $j$. Each row sums to 1—it's a probability distribution over context words.

But here's the final complication: **one attention matrix captures one type of relationship**. Language is multidimensional. There are syntactic relationships (subject-verb-object), semantic relationships (conceptual similarity between "cat" and "mat" as physical objects), positional relationships (local word order), and pragmatic relationships (coreference, where "her" links to "scientist"). A single attention mechanism can't capture all of these simultaneously.

![](../figs/transformer-multihead-attention.jpg)

The solution is **multi-head attention**: run multiple attention mechanisms in parallel, each with its own $W_Q$, $W_K$, $W_V$ matrices. Mathematically, $\text{MultiHead}(X) = \text{Concat}(\text{head}_1, \text{head}_2, \ldots, \text{head}_h) W^O$. Each head learns a different attention pattern. Modern transformers use 8-16 heads per layer: BERT uses 12, GPT-3 uses 96. It's like having twelve different experts analyze the sentence simultaneously—one focusing on syntax, one on semantics, one on coreference—and then combining their insights through a learned linear transformation.

## The Complete Architecture: Engineering for Scale

You've now discovered the core innovation: **self-attention via Query-Key-Value**. But a working transformer needs additional engineering to make this mechanism trainable at scale.

**Positional encoding** solves the problem that attention is permutation-invariant. Without extra information, "cat sat" equals "sat cat" because the dot products don't encode order. The solution is to add position-dependent vectors to embeddings: $\text{input}_i = \text{embedding}_i + \text{position}_i$. Transformers use sinusoidal functions of varying frequencies: $\text{PE}(pos, 2i) = \sin(pos / 10000^{2i/d})$ and $\text{PE}(pos, 2i+1) = \cos(pos / 10000^{2i/d})$. This gives the model access to both absolute position and relative distances between words.

**Residual connections** create highways for gradient flow. Deep networks suffer from vanishing gradients—signals decay exponentially as they backpropagate through layers. The solution is skip connections: $\text{output} = \text{LayerNorm}(x + \text{Attention}(x))$. This allows gradients to flow backward through 12-24 layers without vanishing.

**Feed-forward networks** add processing power beyond attention. After the attention sublayer, each word's representation passes through a small multi-layer perceptron independently: $\text{FFN}(x) = \text{ReLU}(x W_1 + b_1) W_2 + b_2$. This introduces non-linearity and gives the model capacity to learn complex transformations.

**Layer normalization** keeps activations in a stable numerical range during training, preventing exploding or vanishing values that would make gradient descent unstable.

The complete transformer block, repeated 12 or more times, looks like this: Input → Multi-Head Self-Attention → Add & LayerNorm (residual) → Feed-Forward Network → Add & LayerNorm (residual) → Output passed to next block.

Here's what matters: residual connections, layer normalization, and feed-forward networks all existed before 2017. The transformer's innovation is self-attention. That single mechanism replaced recurrent neural networks (RNNs), long short-term memory networks (LSTMs), and convolutional approaches for natural language processing. The original paper's title—*"Attention Is All You Need"*—was provocative but accurate.

The attention mechanism gets deployed differently depending on the task. **Encoder-only architectures** like BERT use bidirectional attention where word $i$ can attend to all words, past and future. These models excel at understanding text for classification, embeddings, and extraction tasks. You feed in "Is this paper about networks or biology?" and get back a classification. **Decoder-only architectures** like GPT and Gemma use causal attention (masked) where word $i$ can only attend to words at positions $\leq i$. This prevents "looking into the future" during autoregressive generation. When generating text word-by-word, you can't use words you haven't generated yet. These models excel at completion and chat. **Encoder-decoder architectures** like the original transformer use bidirectional attention in the encoder to process input, causal attention in the decoder for output generation, plus **cross-attention** where the decoder's Query vectors attend to the encoder's Key and Value vectors. These models excel at sequence-to-sequence tasks like translation: "Hello world" → "Bonjour monde."

## Why This Changed Everything

Before transformers, natural language processing used **sequential processing** via recurrent neural networks. The computational graph looked like: Word 1 → Hidden State 1 → Word 2 → Hidden State 2 → and so on. This architecture had three fatal flaws. First, sequential computation is inherently slow—you can't parallelize across words because each hidden state depends on the previous one. Second, information decays over long distances due to vanishing gradients; dependencies 100+ words apart become nearly impossible to learn. Third, the fixed-size hidden state creates an information bottleneck for long sequences.

Transformers solved all three problems simultaneously. **Parallel processing** means all words are processed simultaneously, yielding 100× faster training on modern GPUs. **Arbitrary long-range dependencies** become tractable because "bank" in position 50 can directly attend to "money" in position 2 through a single matrix multiplication—no gradients need to flow through 48 intermediate steps. **Scalability** emerged as a predictable phenomenon: performance scales logarithmically with parameters. This revealed a stunning empirical fact—bigger models aren't just incrementally better; they unlock **qualitatively new capabilities** like in-context learning and multi-step reasoning. This is why GPT-4 dramatically exceeds GPT-3, which dramatically exceeds GPT-2, despite using essentially the same architecture.

Consider a concrete example of how attention captures linguistic structure. Take the sentence "The scientist published her paper." We want to resolve the coreference: what does "her" refer to?

```{ojs}
//| echo: false

{
  const words = ["The", "scientist", "published", "her", "paper"];
  const attention = [
    [0.5, 0.3, 0.1, 0.05, 0.05],   // "The"→"scientist"
    [0.2, 0.5, 0.2, 0.05, 0.05],   // "scientist"
    [0.05, 0.3, 0.4, 0.1, 0.15],   // "published"
    [0.05, 0.6, 0.1, 0.2, 0.05],   // "her"→"scientist" ← KEY!
    [0.05, 0.2, 0.1, 0.15, 0.5]    // "paper"
  ];

  // Prepare heatmap data
  const heatmapData = [];
  for (let i = 0; i < words.length; i++) {
    for (let j = 0; j < words.length; j++) {
      heatmapData.push({
        Word: words[i],
        AttendsTo: words[j],
        Weight: attention[i][j],
        isHighlight: i === 3 && j === 1  // "her" → "scientist"
      });
    }
  }

  return Plot.plot({
    width: 500,
    height: 400,
    marginTop: 60,
    marginBottom: 60,
    marginLeft: 90,
    marginRight: 120,
    style: {
      background: "white",
      color: "black"
    },
    x: {
      label: "Attends To →",
      labelAnchor: "center"
    },
    y: {
      label: "↑ Word",
      labelAnchor: "center"
    },
    color: {
      type: "linear",
      scheme: "Purples",
      domain: [0, 0.6],
      label: "Attention Weight",
      legend: true
    },
    marks: [
      Plot.cell(heatmapData, {
        x: "AttendsTo",
        y: "Word",
        fill: "Weight",
        inset: 0.5
      }),
      Plot.text(heatmapData, {
        x: "AttendsTo",
        y: "Word",
        text: d => d.Weight.toFixed(2),
        fill: d => d.Weight > 0.3 ? "white" : "black",
        fontSize: 11,
        fontWeight: "bold"
      }),
      // Highlight box around "her" → "scientist"
      Plot.rect([{x: "scientist", y: "her"}], {
        x: "x",
        y: "y",
        fill: "none",
        stroke: "red",
        strokeWidth: 3,
        inset: -2
      }),
      Plot.text([{ x: 0, y: 0 }], {
        x: () => words.length / 2 - 0.5,
        y: () => -0.9,
        text: () => "Coreference via Attention: 'her' → 'scientist'",
        fontSize: 14,
        fontWeight: "bold",
        frameAnchor: "top",
        fill: "black"
      })
    ]
  });
}
```

*Attention pattern showing 'her' → 'scientist' (0.60 weight). The model learned coreference **without explicit grammar rules**—purely from prediction tasks.*

Row 3 corresponds to "her." The pronoun attends most strongly to "scientist" (0.60), correctly identifying the referent. The model **discovered** that pronouns link to earlier nouns through statistical patterns in training data—no one programmed this grammatical rule. This is the profound insight: transformers learn the structure of language as a side effect of optimizing a simple prediction objective.

## The Existential Conclusion

When you use `sentence-transformers` to generate embeddings for your research, you're using encoder transformers (BERT-based) with bidirectional attention. When you interact with ChatGPT or run Gemma locally, you're using decoder transformers with causal attention. When you see attention visualizations in papers, you now understand they represent **learned relevance weights** computed from Query-Key comparisons. The models don't understand language the way humans do—they perform **pattern matching at scale**—but the patterns they discover are often the same syntactic and semantic structures that linguists have documented for decades.

The limitations are worth remembering. Attention has **quadratic complexity**: computing $O(N^2)$ pairwise scores becomes prohibitively expensive for very long texts, which is why context windows typically cap at 2K-32K tokens. Transformers have **no true memory** beyond the current window—they can't learn facts across documents without retraining. They produce fluent text through statistical pattern matching but lack grounded understanding, leading to **hallucinations** when they confidently generate plausible-sounding nonsense. Training costs are astronomical: GPT-3 required roughly \$5 million in compute. But you don't need to train your own models; you can use pre-trained ones and fine-tune them for specific tasks with orders of magnitude less data and compute.

Return to where we started: the "bank" problem. Static embeddings treated "bank" as one point in space, averaging across all contexts. Transformers compute a **distribution** of representations, conditioned on the surrounding words. The revolution wasn't in the complexity—it was in recognizing that **meaning is relational, not absolute**. Words are like atoms in a molecule: their properties depend on what they're bonded to. Transformers formalized this intuition mathematically through a simple mechanism: weighted mixing based on learned relevance.

This entire architecture emerged from asking a single question: *What's the simplest mechanism that lets representations adapt to context?* The answer was weighted mixing where the weights come from comparing what each word needs (Query) against what other words offer (Key). Everything else—multi-head attention, layer normalization, feed-forward networks—is engineering to make that core idea scale to 175 billion parameters and beyond.

**Next**: [Word Embeddings: Where It Started](word-embeddings.qmd)

<script src="https://cdn.jsdelivr.net/npm/@marimo-team/marimo-snippets@1"></script>

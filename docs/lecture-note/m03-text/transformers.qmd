---
title: "Transformers: Why Context Is Everything (And One Vector Is Nothing)"
execute:
    enabled: true
filters:
  - marimo-team/marimo
---

**The Spoiler:** The entire transformer revolution boils down to this—static embeddings assign one vector per word, ignoring that "bank" near "river" is mathematically different from "bank" near "money." Transformers solve this by computing context-aware representations through weighted mixing, and the weights themselves emerge from learned comparisons (Query × Key) between words. The result: machines finally understand that meaning isn't in the word; it's in the *distribution* of words around it.

## The Naive Intuition: One Word, One Vector

Here's the worldview that dominated natural language processing for decades: words have meanings, and we can capture those meanings as vectors. The word "bank" becomes a point in 300-dimensional space, represented as `[0.23, -0.45, 0.67, 0.12, ...]`. This **static embedding** philosophy assumes that the essence of "bank" can be distilled into a single coordinate in vector space, independent of context. It's the linguistic equivalent of saying a person has one personality, regardless of whether they're at a funeral or a party.

This idea has deep roots. J.R. Firth declared in 1957: "You shall know a word by the company it keeps." This is **distributional semantics**—meaning emerges from co-occurrence statistics. If "bank" appears near "loan," "mortgage," "interest," you infer *financial institution*. If it appears near "river," "shore," "erosion," you infer *geological feature*. Word2vec, introduced in 2013, operationalized this idea by training neural networks to predict a word from its neighbors. The learned vectors encoded co-occurrence patterns beautifully.

But here's the fundamental limitation that haunted this entire paradigm: Word2vec still outputs **one vector per word type**. It collapses all contexts into a single average representation. The word "bank" gets the same vector whether it's surrounded by "money" or "river." This is like computing the average height of all humans and declaring that everyone is 5'7". The average exists, but it obscures the very differences that matter.

## The Theoretical Failure: When Averaging Destroys the Signal

The naive hypothesis went like this: what if we just *mix* the target word with its neighbors? For the sentence "I deposited money at the bank," we could compute a contextualized representation as:

$$
\text{contextualized}_{\text{bank}} = w_1 \cdot \vec{v}_{\text{bank}} + w_2 \cdot \vec{v}_{\text{deposited}} + w_3 \cdot \vec{v}_{\text{money}} + \cdots
$$

where $w_i$ are weights and $\vec{v}_i$ are word embeddings. The **null hypothesis** suggests setting all weights uniformly: $w_i = 1/N$, producing a simple bag-of-words average. This should, in theory, create context-aware representations.

But reality is not so egalitarian. Consider the signal-to-noise problem in "I deposited money at the bank." This sentence contains seven words. The most critical context—"deposited" and "money"—is provided by just two of them. The remaining words—"I," "at," "the"—offer little relevant information, as these are classic examples of **function words** (articles, prepositions) that typically carry minimal semantic weight.

If meaning were distributed evenly, we could simply average all word vectors and call it a day. But real language is not so uniform. Some words contribute far more to contextual meaning than others: verbs and nouns might demand our attention, while fillers and grammatical glue quietly fade into the background. Uniform averaging, however, grants every word equal influence—treating "the" as equally informative as "deposited." This mathematical fairness produces semantic nonsense. What we need instead is a mechanism that **selectively attends** to the words that matter most, tuning down the hum of the less relevant ones.

## The Hidden Mechanism: Query-Key Matching as Information Retrieval

The breakthrough came from reframing contextualization as a **search problem**. Imagine you're in a library—your sentence—looking for information to understand "bank." You have a question (what context do I need?). Each book has a label advertising what it offers: "I offer financial concepts" or "I offer spatial concepts." You compute relevance by matching your question against each book's label. Then you read the relevant books, weighting your attention based on those relevance scores. This is **attention**.

The mathematical implementation is elegant. For each word $i$ in a sentence with embedding $\vec{x}_i$, create three linear transformations:

$$
\begin{align}
\vec{q}_i &= W_Q \vec{x}_i \quad \text{(Query: what I'm looking for)} \\
\vec{k}_i &= W_K \vec{x}_i \quad \text{(Key: what I offer)} \\
\vec{v}_i &= W_V \vec{x}_i \quad \text{(Value: what I contain)}
\end{align}
$$

To contextualize word $i$, compute its relevance to all other words $j$ via **dot products**: $\text{score}(i, j) = \vec{q}_i \cdot \vec{k}_j$. High scores indicate strong relevance; low scores indicate noise to ignore. These scores are converted to probabilities using softmax:

$$
w_{ij} = \frac{\exp(\vec{q}_i \cdot \vec{k}_j / \sqrt{d})}{\sum_{\ell} \exp(\vec{q}_i \cdot \vec{k}_\ell / \sqrt{d})}
$$

The division by $\sqrt{d}$ (where $d$ is the embedding dimension) is a scaling factor that prevents vanishing gradients during training. Finally, compute the **contextualized representation** as a weighted sum: $\text{contextualized}_i = \sum_j w_{ij} \vec{v}_j$.

Why three separate transformations instead of using $\vec{x}_i$ directly? Because we want **learned specialization**. The matrices $W_Q$, $W_K$, $W_V$ are trained end-to-end to extract different aspects of meaning. The Query transformation learns "what to search for," the Key transformation learns "what to advertise," and the Value transformation learns "what to provide." This is the **mechanism of selective attention**: queries and keys determine *where* to look; values determine *what* to retrieve.

```{python}
#| code-fold: true
#| fig-cap: "Interactive visualization showing how Query and Key transformations affect attention weights. Adjust the transformation parameters to see how different Q and K matrices change which words attend to each other."

import numpy as np
import pandas as pd
import altair as alt

# Word data
words = ["bank", "money", "loan", "river", "shore"]
embeddings = np.array([
    [0.0, 0.0],    # bank (center)
    [-0.8, -0.3],  # money
    [-0.7, -0.6],  # loan
    [0.7, -0.5],   # river
    [0.6, -0.7],   # shore
]) * 2

# Default transformations for demonstration
# Query transformation: slight rotation and scaling
theta_q = np.radians(30)
W_Q = np.array([[1.2, 0], [0, 0.8]]) @ np.array([
    [np.cos(theta_q), -np.sin(theta_q)],
    [np.sin(theta_q), np.cos(theta_q)]
])
b_Q = np.array([0.2, -0.1])

# Key transformation: different rotation
theta_k = np.radians(-45)
W_K = np.array([[0.9, 0], [0, 1.3]]) @ np.array([
    [np.cos(theta_k), -np.sin(theta_k)],
    [np.sin(theta_k), np.cos(theta_k)]
])
b_K = np.array([-0.1, 0.3])

# Compute Q and K
Q = embeddings @ W_Q + b_Q
K = embeddings @ W_K + b_K

# Compute attention scores and weights
scores = Q @ K.T
exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))
attention_weights = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)

# Create visualizations
df_q = pd.DataFrame({"word": words, "x": Q[:, 0], "y": Q[:, 1]})
df_k = pd.DataFrame({"word": words, "x": K[:, 0], "y": K[:, 1]})
df_orig = pd.DataFrame({"word": words, "x": embeddings[:, 0], "y": embeddings[:, 1]})

# Query scatter plot
vmax = 3
chart_q = alt.Chart(df_orig).mark_circle(size=100, color="#dadada", opacity=0.8).encode(
    x=alt.X("x", scale=alt.Scale(domain=[-vmax, vmax])),
    y=alt.Y("y", scale=alt.Scale(domain=[-vmax, vmax]))
) + alt.Chart(df_q).mark_circle(size=100, color="#ff7f0e").encode(
    x=alt.X("x", scale=alt.Scale(domain=[-vmax, vmax])),
    y=alt.Y("y", scale=alt.Scale(domain=[-vmax, vmax])),
    tooltip=["word"]
) + alt.Chart(df_q).mark_text(align="left", dx=10, dy=-5, fontSize=12).encode(
    x="x", y="y", text="word"
)

# Key scatter plot
chart_k = alt.Chart(df_orig).mark_circle(size=100, color="#dadada", opacity=0.8).encode(
    x=alt.X("x", scale=alt.Scale(domain=[-vmax, vmax])),
    y=alt.Y("y", scale=alt.Scale(domain=[-vmax, vmax]))
) + alt.Chart(df_k).mark_circle(size=100, color="#2ca02c").encode(
    x=alt.X("x", scale=alt.Scale(domain=[-vmax, vmax])),
    y=alt.Y("y", scale=alt.Scale(domain=[-vmax, vmax])),
    tooltip=["word"]
) + alt.Chart(df_k).mark_text(align="left", dx=10, dy=-5, fontSize=12).encode(
    x="x", y="y", text="word"
)

# Attention heatmap
heatmap_data = []
for i, row in enumerate(attention_weights):
    for j, value in enumerate(row):
        heatmap_data.append({"Query": words[i], "Key": words[j], "Weight": value})
df_heat = pd.DataFrame(heatmap_data)

chart_heat = alt.Chart(df_heat).mark_rect().encode(
    x=alt.X("Key:N", title="Attends To (Keys)"),
    y=alt.Y("Query:N", title="Word (Queries)"),
    color=alt.Color("Weight:Q", scale=alt.Scale(scheme="blues"), legend=None),
    tooltip=["Query", "Key", alt.Tooltip("Weight", format=".3f")]
) + alt.Chart(df_heat).mark_text(baseline="middle").encode(
    x="Key:N", y="Query:N",
    text=alt.Text("Weight:Q", format=".2f"),
    color=alt.condition(alt.datum.Weight > 0.5, alt.value("white"), alt.value("black"))
)

# Layout
(chart_q.properties(width=250, height=250, title="Query Vectors (Q)") |
 chart_k.properties(width=250, height=250, title="Key Vectors (K)")) & \
chart_heat.properties(width=400, height=300, title="Attention Weights = softmax(Q·K^T)")
```

Now scale this up. **Every word** in a sentence needs to attend to **every other word**. For "The cat sat on the mat" (six words), we compute a $6 \times 6$ **attention matrix** $A$, where $A_{ij} = \text{softmax}(\vec{q}_i \cdot \vec{k}_j)$. Rows represent words asking for context (Queries); columns represent words providing context (Keys). Each cell $(i,j)$ indicates how much word $i$ attends to word $j$. Each row sums to 1—it's a probability distribution over context words.

```{python}
#| code-fold: true
#| fig-cap: "Self-attention heatmap: Notice how 'sat' (row 3) strongly attends to 'cat' (subject) and 'mat' (object)—syntactic relationships discovered purely from data."
#| fig-width: 8
#| fig-height: 6

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

words = ["The", "cat", "sat", "on", "the", "mat"]
n = len(words)

# Realistic attention pattern (learned, not programmed)
attention = np.array([
    [0.6, 0.2, 0.1, 0.05, 0.03, 0.02],  # "The" → "cat" (determiner-noun)
    [0.1, 0.5, 0.3, 0.05, 0.03, 0.02],  # "cat" → self + "sat"
    [0.05, 0.4, 0.2, 0.15, 0.05, 0.15], # "sat" → "cat" (subj) + "mat" (obj)
    [0.05, 0.1, 0.2, 0.3, 0.15, 0.20],  # "on" → contextual spread
    [0.03, 0.05, 0.05, 0.1, 0.5, 0.27], # "the" → "mat"
    [0.02, 0.05, 0.15, 0.2, 0.15, 0.43] # "mat" → self + "on"
])

sns.set_style("white")
fig, ax = plt.subplots(figsize=(8, 6))
sns.heatmap(attention, annot=True, fmt=".2f",
            xticklabels=words, yticklabels=words,
            cmap="YlOrRd", vmin=0, vmax=0.6,
            cbar_kws={'label': 'Attention Weight'}, ax=ax)
ax.set_xlabel("Attends To (Keys)", fontsize=12, fontweight='bold')
ax.set_ylabel("Word (Queries)", fontsize=12, fontweight='bold')
ax.set_title("Self-Attention Matrix: Learned Syntax", fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()
```

Look at row 3, corresponding to "sat." The verb attends most strongly to "cat" (0.40) and "mat" (0.15)—the subject and object of the sentence. The diagonal shows moderate values because words always attend somewhat to themselves. Determiners like "The" attend to their nouns ("cat"). These patterns are **emergent, not programmed**. No one told the model "verbs should attend to subjects." The patterns emerge from backpropagation on prediction tasks. The model discovers grammar as a side effect of learning to predict words.

But here's the final complication: **one attention matrix captures one type of relationship**. Language is multidimensional. There are syntactic relationships (subject-verb-object), semantic relationships (conceptual similarity between "cat" and "mat" as physical objects), positional relationships (local word order), and pragmatic relationships (coreference, where "her" links to "scientist"). A single attention mechanism can't capture all of these simultaneously.

The solution is **multi-head attention**: run multiple attention mechanisms in parallel, each with its own $W_Q$, $W_K$, $W_V$ matrices. Mathematically, $\text{MultiHead}(X) = \text{Concat}(\text{head}_1, \text{head}_2, \ldots, \text{head}_h) W^O$. Each head learns a different attention pattern. Modern transformers use 8-16 heads per layer: BERT uses 12, GPT-3 uses 96. It's like having twelve different experts analyze the sentence simultaneously—one focusing on syntax, one on semantics, one on coreference—and then combining their insights through a learned linear transformation.

## The Complete Architecture: Engineering for Scale

You've now discovered the core innovation: **self-attention via Query-Key-Value**. But a working transformer needs additional engineering to make this mechanism trainable at scale.

**Positional encoding** solves the problem that attention is permutation-invariant. Without extra information, "cat sat" equals "sat cat" because the dot products don't encode order. The solution is to add position-dependent vectors to embeddings: $\text{input}_i = \text{embedding}_i + \text{position}_i$. Transformers use sinusoidal functions of varying frequencies: $\text{PE}(pos, 2i) = \sin(pos / 10000^{2i/d})$ and $\text{PE}(pos, 2i+1) = \cos(pos / 10000^{2i/d})$. This gives the model access to both absolute position and relative distances between words.

**Residual connections** create highways for gradient flow. Deep networks suffer from vanishing gradients—signals decay exponentially as they backpropagate through layers. The solution is skip connections: $\text{output} = \text{LayerNorm}(x + \text{Attention}(x))$. This allows gradients to flow backward through 12-24 layers without vanishing.

**Feed-forward networks** add processing power beyond attention. After the attention sublayer, each word's representation passes through a small multi-layer perceptron independently: $\text{FFN}(x) = \text{ReLU}(x W_1 + b_1) W_2 + b_2$. This introduces non-linearity and gives the model capacity to learn complex transformations.

**Layer normalization** keeps activations in a stable numerical range during training, preventing exploding or vanishing values that would make gradient descent unstable.

The complete transformer block, repeated 12 or more times, looks like this: Input → Multi-Head Self-Attention → Add & LayerNorm (residual) → Feed-Forward Network → Add & LayerNorm (residual) → Output passed to next block.

Here's what matters: residual connections, layer normalization, and feed-forward networks all existed before 2017. The transformer's innovation is self-attention. That single mechanism replaced recurrent neural networks (RNNs), long short-term memory networks (LSTMs), and convolutional approaches for natural language processing. The original paper's title—*"Attention Is All You Need"*—was provocative but accurate.

The attention mechanism gets deployed differently depending on the task. **Encoder-only architectures** like BERT use bidirectional attention where word $i$ can attend to all words, past and future. These models excel at understanding text for classification, embeddings, and extraction tasks. You feed in "Is this paper about networks or biology?" and get back a classification. **Decoder-only architectures** like GPT and Gemma use causal attention (masked) where word $i$ can only attend to words at positions $\leq i$. This prevents "looking into the future" during autoregressive generation. When generating text word-by-word, you can't use words you haven't generated yet. These models excel at completion and chat. **Encoder-decoder architectures** like the original transformer use bidirectional attention in the encoder to process input, causal attention in the decoder for output generation, plus **cross-attention** where the decoder's Query vectors attend to the encoder's Key and Value vectors. These models excel at sequence-to-sequence tasks like translation: "Hello world" → "Bonjour monde."

## Why This Changed Everything

Before transformers, natural language processing used **sequential processing** via recurrent neural networks. The computational graph looked like: Word 1 → Hidden State 1 → Word 2 → Hidden State 2 → and so on. This architecture had three fatal flaws. First, sequential computation is inherently slow—you can't parallelize across words because each hidden state depends on the previous one. Second, information decays over long distances due to vanishing gradients; dependencies 100+ words apart become nearly impossible to learn. Third, the fixed-size hidden state creates an information bottleneck for long sequences.

Transformers solved all three problems simultaneously. **Parallel processing** means all words are processed simultaneously, yielding 100× faster training on modern GPUs. **Arbitrary long-range dependencies** become tractable because "bank" in position 50 can directly attend to "money" in position 2 through a single matrix multiplication—no gradients need to flow through 48 intermediate steps. **Scalability** emerged as a predictable phenomenon: performance scales logarithmically with parameters. This revealed a stunning empirical fact—bigger models aren't just incrementally better; they unlock **qualitatively new capabilities** like in-context learning and multi-step reasoning. This is why GPT-4 dramatically exceeds GPT-3, which dramatically exceeds GPT-2, despite using essentially the same architecture.

Consider a concrete example of how attention captures linguistic structure. Take the sentence "The scientist published her paper." We want to resolve the coreference: what does "her" refer to?

```{python}
#| code-fold: true
#| fig-cap: "Attention pattern showing 'her' → 'scientist' (0.60 weight). The model learned coreference **without explicit grammar rules**—purely from prediction tasks."
#| fig-width: 8
#| fig-height: 6

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

words = ["The", "scientist", "published", "her", "paper"]
n = len(words)

attention = np.array([
    [0.5, 0.3, 0.1, 0.05, 0.05],  # "The"→"scientist"
    [0.2, 0.5, 0.2, 0.05, 0.05],  # "scientist"
    [0.05, 0.3, 0.4, 0.1, 0.15],  # "published"
    [0.05, 0.6, 0.1, 0.2, 0.05],  # "her"→"scientist" ← KEY!
    [0.05, 0.2, 0.1, 0.15, 0.5],  # "paper"
])

sns.set_style("white")
fig, ax = plt.subplots(figsize=(8, 6))
sns.heatmap(attention, annot=True, fmt=".2f",
            xticklabels=words, yticklabels=words,
            cmap="Purples", vmin=0, vmax=0.6,
            cbar_kws={'label': 'Attention Weight'}, ax=ax)
ax.set_xlabel("Attends To", fontsize=12, fontweight='bold')
ax.set_ylabel("Word", fontsize=12, fontweight='bold')
ax.set_title("Coreference via Attention: 'her' → 'scientist'", fontsize=14, fontweight='bold')

from matplotlib.patches import Rectangle
ax.add_patch(Rectangle((1, 3), 1, 1, fill=False, edgecolor='red', lw=3))
plt.tight_layout()
plt.show()
```

Row 3 corresponds to "her." The pronoun attends most strongly to "scientist" (0.60), correctly identifying the referent. The model **discovered** that pronouns link to earlier nouns through statistical patterns in training data—no one programmed this grammatical rule. This is the profound insight: transformers learn the structure of language as a side effect of optimizing a simple prediction objective.

## The Existential Conclusion

When you use `sentence-transformers` to generate embeddings for your research, you're using encoder transformers (BERT-based) with bidirectional attention. When you interact with ChatGPT or run Gemma locally, you're using decoder transformers with causal attention. When you see attention visualizations in papers, you now understand they represent **learned relevance weights** computed from Query-Key comparisons. The models don't understand language the way humans do—they perform **pattern matching at scale**—but the patterns they discover are often the same syntactic and semantic structures that linguists have documented for decades.

The limitations are worth remembering. Attention has **quadratic complexity**: computing $O(N^2)$ pairwise scores becomes prohibitively expensive for very long texts, which is why context windows typically cap at 2K-32K tokens. Transformers have **no true memory** beyond the current window—they can't learn facts across documents without retraining. They produce fluent text through statistical pattern matching but lack grounded understanding, leading to **hallucinations** when they confidently generate plausible-sounding nonsense. Training costs are astronomical: GPT-3 required roughly \$5 million in compute. But you don't need to train your own models; you can use pre-trained ones and fine-tune them for specific tasks with orders of magnitude less data and compute.

Return to where we started: the "bank" problem. Static embeddings treated "bank" as one point in space, averaging across all contexts. Transformers compute a **distribution** of representations, conditioned on the surrounding words. The revolution wasn't in the complexity—it was in recognizing that **meaning is relational, not absolute**. Words are like atoms in a molecule: their properties depend on what they're bonded to. Transformers formalized this intuition mathematically through a simple mechanism: weighted mixing based on learned relevance.

This entire architecture emerged from asking a single question: *What's the simplest mechanism that lets representations adapt to context?* The answer was weighted mixing where the weights come from comparing what each word needs (Query) against what other words offer (Key). Everything else—multi-head attention, layer normalization, feed-forward networks—is engineering to make that core idea scale to 175 billion parameters and beyond.

**Next**: [Word Embeddings: Where It Started](word-embeddings.qmd)

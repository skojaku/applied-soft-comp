---
title: "Large Language Models in Practice"
jupyter: applsoftcomp
execute:
    enabled: true
    cache: true
---

::: {.callout-note appearance="minimal"}
## Spoiler
Large language models are not intelligent. They are statistical machines trained to predict the next word, yet this simple objective forces them to encode vast swaths of human knowledge, grammar, logic, and context in their parameters. The result is a tool that can accelerate your research through pattern matching at scale, but only if you remember that correlation is not causation and prediction is not understanding.
:::

## The Naive Intuition

When ChatGPT launched in late 2022, the narrative was immediate: machines can now think. But this error has a lineage. In 1950, Turing proposed his famous test, enshrining a dangerous confusion: **the appearance of intelligence is not intelligence itself.** ELIZA, the 1960s chatbot that mimicked a therapist through keyword substitution, convinced users it understood them. No comprehension. Just pattern matching.

Large language models are ELIZA's descendants, scaled up by thirteen orders of magnitude. The question is not whether they think—they don't. The question is: **can these pattern-matching machines help us do better science?**

## The Theoretical Failure

The standard model of language is compositional. Chomsky spent his career arguing that language is governed by universal grammar—recursive rules that generate infinite valid sentences. Understanding language means understanding the rules.

LLMs break this model. They are not taught grammar or given rules. They are fed billions of sentences and trained to predict the next word: given $w_1, w_2, \ldots, w_n$, estimate $P(w_{n+1} \mid w_1, \ldots, w_n)$.

![](../figs/llm-next-word.gif){width=50% fig-alt="A GIF showing how LLMs predict the next word by estimating probability distributions over the vocabulary. The model processes the input sequence and generates a probability distribution for the next token, then samples from this distribution to produce text." fig-align="center"}

Chomsky would hate this. There are no rules here, only correlations. The model doesn't "know" grammar; it has seen enough examples to predict accordingly. And yet, this crude statistical approach works. When OpenAI trained ever-larger models on ever-more data, capabilities emerged that were never programmed—translation, math, coding in languages invented after training.

The central paradox: **a system optimized for prediction develops representations that look suspiciously like understanding.** But when you probe the extremes—events after the knowledge cutoff, precise factual recall, genuine reasoning—the illusion shatters.

## The Hidden Mechanism

The mechanism is **lossy compression.** To predict "The capital of France is ___," the model compresses not just facts but statistical regularities governing how facts are expressed. It stores which words tend to follow which in which contexts—a probabilistic representation, not a database.

This creates hallucination. The model optimizes for probability, not truth. If it has seen 1,000 sentences about quantum networks and 10 about quantum community detection, it fabricates plausible results for a non-existent "Smith et al. paper" because that pattern fits academic writing. Truth and fluency correlate in human text, so the model is mostly truthful. But in the tails—obscure topics, recent events, precise recall—fluency diverges from truth, and the model follows fluency.

Two other mechanisms matter. **Limited context:** LLMs can only "see" 2,000–8,000 tokens. Paste 100 abstracts and early ones vanish. **Stochasticity:** The same prompt yields different outputs. This is framed as "creativity" but is noise. The model is a stochastic parrot—fluent, contextually sensitive, indifferent to meaning.

## The Existential Conclusion

**Use LLMs to scale your judgment, not replace it.** They excel where speed trumps precision—summarizing text, extracting structure, brainstorming. They fail where precision is paramount—citations, decisions, analysis.

Think of an LLM as a research assistant who has read the internet but remembers imperfectly and fabricates occasionally. You delegate skimming 50 abstracts, not writing the paper. You extract findings, then verify everything.

**The average is a lie; the truth is in the distribution.** Most of the time, LLMs produce fluent text. But the events that matter—the hallucinated citation, the missed context, the atrophied thinking—live in the tail. Harvest the center. Defend against the extremes.

---

## Setting Up Ollama

For this course, we use Ollama with Gemma 3N—a lightweight open-source model that runs locally. Free, private, capable enough.

Visit [ollama.ai](https://ollama.ai), download the installer, then:

```bash
ollama --version
ollama pull gemma3n:latest
ollama run gemma3n:latest "What is a complex system?"
```

If you get a coherent response, you're ready. Now install the Python client:

```bash
pip install ollama
```

Send your first prompt:

```{python}
import ollama

params_llm = {"model": "gemma3n:latest", "options": {"temperature": 0.3}}

response = ollama.generate(
    prompt="Explain emergence in complex systems in two sentences.",
    **params_llm
)

print(response.response)
```

Running this twice produces different outputs—LLMs sample from probability distributions. Lower `temperature` reduces randomness but doesn't guarantee determinism.

---

## Research Use Case 1: Summarizing Abstracts

You have 50 papers. Which deserve detailed reading? An LLM can summarize in seconds.

```{python}
abstract = """
Community detection in networks is a fundamental problem in complex systems.
While many algorithms exist, most assume static networks. We propose a dynamic
community detection algorithm that tracks evolving communities over time using
a temporal smoothness constraint. We evaluate our method on synthetic and real
temporal networks, showing it outperforms static methods applied to temporal
snapshots. Our approach reveals how communities merge, split, and persist in
social networks, biological systems, and transportation networks.
"""

prompt = f"Summarize this abstract in one sentence:\n\n{abstract}"
response = ollama.generate(prompt=prompt, **params_llm)
print(response.response)
```

The model captures the key contribution. It doesn't understand the paper—it recognizes the pattern: propose method, evaluate, outperform baselines. For multiple abstracts, loop:

```{python}
for i, abstract in enumerate(["Abstract 1...", "Abstract 2..."], 1):
    response = ollama.generate(prompt=f"Summarize:\n\n{abstract}", **params_llm)
    print(f"{i}. {response.response}")
```

Local models are slow (2–5 seconds per abstract). For thousands of papers, use cloud APIs.

---

## Research Use Case 2: Extracting Structured Information

Extract domain, methods, findings from abstracts automatically.

```{python}
abstract = """
We analyze scientific collaboration networks using 5 million papers from
2000-2020. Using graph neural networks and community detection, we identify
disciplinary boundaries and interdisciplinary bridges. Interdisciplinarity
increased 25%, with physics and CS showing strongest cross-connections.
"""

prompt = f"""Extract: Domain, Methods, Key Finding\n\n{abstract}\n\nFormat:\nDomain:...\nMethods:...\nKey Finding:..."""
response = ollama.generate(prompt=prompt, **params_llm)
print(response.response)
```

Scale to hundreds of papers for meta-analysis. Verify outputs—LLMs misinterpret and fabricate.

---

## Research Use Case 3: Hypothesis Generation

LLMs pattern-match against research questions they've seen. Not groundbreaking, but useful for brainstorming.

```{python}
context = """I study concept spread in citation networks. Highly cited papers
combine existing concepts novelty. What should I study next?"""

prompt = f"""Suggest three follow-up research questions:\n\n{context}"""
response = ollama.generate(prompt=prompt, **params_llm)
print(response.response)
```

Treat as thought partner, not oracle.

---

## Failure Modes

**Hallucination:** LLMs fabricate plausibly. Ask about a non-existent "Smith et al. quantum paper" and get fluent academic prose describing results that never happened. Always verify citations.

**Limited context:** Models see 2,000–8,000 tokens. Paste 100 abstracts and early ones vanish.

**Knowledge cutoff:** Gemma 3N's training ended early 2024. Ask about recent events, get outdated info or fabrications.

**No understanding:** LLMs pattern-match, don't reason. Ask "How many r's in 'Strawberry'?" They might answer 3 (correct) via pattern matching, not counting. Sometimes right, often wrong.

**Use to accelerate work, not replace judgment.**

---

## When to Use LLMs

**Good:** Summarizing text. Extracting structure. Reformulating concepts. Brainstorming. Synthetic examples. Translation.

**Poor:** Literature reviews without verification. Factual claims without sources. Statistical analysis. Ethical decisions.

---

## Next

You've seen LLMs work—setup, summarization, extraction, limitations. But how do they actually work? What happens inside when you send a prompt?

The rest of this module unboxes the technology: prompt engineering (communicating with LLMs), embeddings (meaning as numbers), transformers (the architecture), fundamentals (word counts to neural representations).

First, let's master talking to machines.

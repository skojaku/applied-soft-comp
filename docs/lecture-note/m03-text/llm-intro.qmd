---
title: "Large Language Models in Practice"
jupyter: applsoftcomp
execute:
    enabled: true
    cache: true
---

::: {.callout-note appearance="minimal"}
## Spoiler
Large language models don't understand language; they compress statistical regularities from billions of text samples into probability distributions, then sample from those distributions to generate fluent outputs that correlate with truth but are not guaranteed to be true.
:::

## The Naive Model vs. The Reality

**The Naive Model:** If a machine can write coherent essays, debug code, and answer questions accurately, it must "understand" language the way humans do. This intuition dates to Turing's 1950 test—if you can't tell it's a machine, treat it as intelligent.

**The Reality:** ELIZA, a 1960s chatbot, convinced users it was a therapist using only keyword substitution and reflection. No comprehension. Just pattern matching. LLMs are ELIZA scaled by thirteen orders of magnitude. They predict which word comes next in a sequence, nothing more. Yet this simple objective forces them to encode grammar, facts, logic, and context—not because they understand, but because **prediction requires compression of statistical regularities.**

The paradox: optimizing for prediction creates representations that look like understanding. But probe the extremes—ask about events after the training cutoff, request precise citations, demand genuine reasoning—and the illusion breaks.

## The Hidden Mechanism

**The Toy Model:** Imagine you want to predict lottery numbers. You can't compute the true probabilities, so you memorize millions of historical draws. When asked "What comes next?", you recall similar past sequences and output the most common continuation. You're not computing probabilities; you're pattern matching against compressed memory.

**The Analogy:** LLMs work like lossy compression algorithms. To predict "The capital of France is ___," the model must compress not just the fact (Paris) but the **statistical regularities** governing how facts appear in text—that capitals follow "The capital of," that France is a country, that countries have capitals. This compression is probabilistic, not factual. The model stores $P(\text{word}_{n+1} \mid \text{word}_1, \ldots, \text{word}_n)$—which words tend to follow which other words in which contexts.

![](../figs/llm-next-word.gif){width=50% fig-alt="A GIF showing how LLMs predict the next word by estimating probability distributions over the vocabulary." fig-align="center"}

**The Process:** Training feeds the model billions of sentences. For each sentence, the model predicts the next word, compares its prediction to the actual next word, and adjusts its parameters to increase the probability of the correct word. Repeat trillions of times. The result: a compressed representation of how language behaves statistically.

**The Feedback Loop:** This creates **hallucination**—fluent but false outputs. The model optimizes for probability, not truth. If it has seen 1,000 sentences about quantum networks and 10 about quantum community detection, it fabricates plausible results for a non-existent "Smith et al. paper" because that pattern fits academic writing. Truth and fluency correlate in the training data, so the model is mostly truthful. But in the tails—obscure topics, recent events, precise recall—fluency diverges from truth, and the model follows fluency.

Two constraints compound this: **Limited context** (models see only 2,000–8,000 tokens; paste 100 abstracts and early ones vanish) and **stochasticity** (the model samples from probability distributions, so the same prompt yields different outputs).

## The Strategic Takeaway

**Use LLMs to scale pattern recognition, not judgment.** They excel where speed trumps precision: summarizing 50 abstracts to identify the 10 worth reading, extracting structured data from unstructured text, reformulating technical concepts, brainstorming research directions. They fail where precision is paramount: verifying citations, making ethical decisions, performing statistical analysis.

Think of an LLM as an assistant who has read the internet but remembers imperfectly. You delegate skimming. You verify everything.

**The average is a lie; the truth is in the distribution.** Most outputs are fluent and useful. But the events that matter—the hallucinated citation that undermines credibility, the missed context that skews analysis—live in the tail. Harvest the center. Defend against the extremes.

## Setting Up Ollama

For this course, we use **Ollama**—a tool for running LLMs locally—with **Gemma 3N**, a 4-billion parameter open-source model. Free, private, capable enough for research tasks.

Visit [ollama.ai](https://ollama.ai), download the installer, then:

```bash
ollama --version
ollama pull gemma3n:latest
ollama run gemma3n:latest "What is a complex system?"
```

If you get a coherent response, install the Python client:

```bash
pip install ollama
```

Send your first prompt:

```{python}
import ollama

params_llm = {"model": "gemma3n:latest", "options": {"temperature": 0.3}}

response = ollama.generate(
    prompt="Explain emergence in two sentences.",
    **params_llm
)

print(response.response)
```

Running this twice produces different outputs—LLMs sample from probability distributions. The **temperature** parameter controls randomness: lower values (e.g., 0.1) make outputs more deterministic; higher values (e.g., 1.0) increase diversity.

## Research Use Case 1: Summarizing Abstracts

You collected 50 papers on network science. Which deserve detailed reading? LLMs summarize in seconds.

```{python}
abstract = """
Community detection in networks is a fundamental problem in complex systems.
While many algorithms exist, most assume static networks. We propose a dynamic
community detection algorithm that tracks evolving communities over time using
a temporal smoothness constraint. We evaluate our method on synthetic and real
temporal networks, showing it outperforms static methods applied to temporal
snapshots. Our approach reveals how communities merge, split, and persist in
social networks, biological systems, and transportation networks.
"""

prompt = f"Summarize this abstract in one sentence:\n\n{abstract}"
response = ollama.generate(prompt=prompt, **params_llm)
print(response.response)
```

The model captures the pattern: propose method, evaluate, outperform baselines. It doesn't understand the paper; it has seen enough academic abstracts to recognize the structure. For multiple abstracts, loop:

```{python}
for i, abstract in enumerate(["Abstract 1...", "Abstract 2..."], 1):
    response = ollama.generate(prompt=f"Summarize:\n\n{abstract}", **params_llm)
    print(f"{i}. {response.response}")
```

Local models are slow (2–5 seconds per abstract). For thousands of papers, switch to cloud APIs.

## Research Use Case 2: Extracting Structured Information

Extract domain, methods, findings from abstracts automatically.

```{python}
abstract = """
We analyze scientific collaboration networks using 5 million papers from
2000-2020. Using graph neural networks and community detection, we identify
disciplinary boundaries and interdisciplinary bridges. Interdisciplinarity
increased 25%, with physics and CS showing strongest cross-connections.
"""

prompt = f"""Extract: Domain, Methods, Key Finding\n\n{abstract}\n\nFormat:\nDomain:...\nMethods:...\nKey Finding:..."""
response = ollama.generate(prompt=prompt, **params_llm)
print(response.response)
```

Scale to hundreds of papers for meta-analysis. Always verify—LLMs misinterpret and fabricate.

## Research Use Case 3: Hypothesis Generation

LLMs pattern-match against research questions they've seen. Useful for brainstorming, not for breakthrough ideas.

```{python}
context = """I study concept spread in citation networks. Highly cited papers
combine existing concepts novelty. What should I study next?"""

prompt = f"""Suggest three follow-up research questions:\n\n{context}"""
response = ollama.generate(prompt=prompt, **params_llm)
print(response.response)
```

Treat as thought partner, not oracle. The model helps structure thinking but doesn't possess domain expertise.

## Failure Modes

**Hallucination:** LLMs fabricate plausibly. Ask about a non-existent "Smith et al. quantum paper" and receive fluent academic prose describing results that never happened. Always verify citations.

**Limited context:** Models see 2,000–8,000 tokens. Paste 100 abstracts and early ones vanish from the model's "memory."

**Knowledge cutoff:** Gemma 3N's training ended early 2024. Ask about recent events, receive outdated information or plausible fabrications.

**No reasoning:** LLMs pattern-match, don't reason. Ask "How many r's in 'Strawberry'?" The model might answer 3 (correct) via pattern matching against similar questions in training data, not by counting. Sometimes right. Often wrong.

**Use to accelerate work, not replace judgment.**

## When to Use LLMs

**Good use cases:** Summarizing text. Extracting structure. Reformulating concepts. Brainstorming. Generating synthetic examples. Translation.

**Poor use cases:** Literature reviews without verification. Factual claims without sources. Statistical analysis (use proper tools). Ethical decisions.

## Next

You've seen LLMs in practice—setup, summarization, extraction, limitations. But how do they actually work? What happens inside when you send a prompt?

The rest of this module unboxes the technology: **prompt engineering** (communicating with LLMs), **embeddings** (representing meaning as numbers), **transformers** (the architecture enabling modern NLP), **fundamentals** (from word counts to neural representations).

First, let's master talking to machines.

---
title: "Sprint Project: Adversarial Art Attack"
---

::: {.callout-note title="Your mission"}
You have 60 minutes to fool a pre-trained CNN with imperceptible modifications. Turn a banana into a toaster in the model's eyes while keeping changes invisible to humans. Then present your attack to the class.
:::

## The Challenge

Take a correctly classified image and modify it until a pre-trained ImageNet model confidently misclassifies it as something absurd. The art is making changes imperceptible to humans. Use either algorithmic methods (FGSM, PGD) or manual editing (subtle noise, color adjustments).

## The Rules

**Time:** 60 minutes of work, followed by presentations.

**Pre-trained Model:** Use ResNet-50, VGG-16, or Inception as-is. No retraining.

**Target Misclassification:** Choose starting image and target class. Aim for at least 80% confidence on the wrong class.

**Minimal Modification:** Changes should be barely noticeable to humans upon side-by-side comparison.

## Evaluation

Judges evaluate on two criteria:

**Invisibility (50%):** How hard is it for humans to spot modifications? Best attacks look identical at first glance.

**Confidence (50%):** How certain is the model about the wrong classification? 95% beats 51%.

Bonus points for absurd or humorous misclassifications.

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vision Transformers\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:700/0*Rtb7Jt6378xfe6Z1.png)\n",
        "\n",
        "What if we could capture not just the local features in images (like corners, edges, or textures) but the entire global context all at once? Could that help a model better understand complex scenes and relationships between objects? Vision Transformers (ViT) attempt exactly that by leveraging **self-attention**, a mechanism originally popularized in Natural Language Processing.\n",
        "\n",
        "## The Genesis of Vision Transformers\n",
        "\n",
        "### Conceptual Foundation\n",
        "\n",
        "*Why were Vision Transformers developed, given that Convolutional Neural Networks (CNNs) already excel in computer vision tasks?*\n",
        "\n",
        "CNNs have been the cornerstone of computer vision for years, particularly good at capturing local patterns through convolutional filters. However, they can struggle to efficiently capture **global context** and **long-range dependencies**. In scenarios where relationships between objects spread across an entire image become crucial (e.g., understanding crowd scenes or satellite imagery), this limitation can be significant.\n",
        "\n",
        "Meanwhile, the **Transformer architecture** (from the paper \"Attention Is All You Need\") revolutionized NLP by modeling long-range dependencies in sequential data. This success inspired researchers to ask: *Could the same self-attention mechanism help models 'see' the entire image at once, instead of focusing on small, local regions?*\n",
        "\n",
        "```{figure} https://www.researchgate.net/publication/361733806/figure/fig3/AS%3A1173979050057729%401656909825527/Operation-of-CNN-and-ViT.ppm\n",
        ":name: fig-cnn-vit\n",
        ":width: 500px\n",
        ":align: center\n",
        "\n",
        "Comparison of the receptive field of CNNs and Vision Transformers. CNN has a local receptive field constrained by the convolutional filters, while ViT has a global receptive field, allowing it to capture long-range dependencies between different parts of the image.\n",
        "\n",
        "```\n",
        "\n",
        "```{note}\n",
        "**Historical Context**\n",
        "\n",
        "- **CNN Dominance (2010s)**: CNNs (e.g., AlexNet, VGG, ResNet) drove huge leaps in image classification and object detection.\n",
        "- **Transformer Breakthrough (2017)**: In NLP, Transformers replaced recurrent architectures (LSTMs, GRUs) for tasks like machine translation.\n",
        "- **ViT Emerges (2020)**: Google researchers introduced the idea of applying pure Transformers to image patches, showing excellent results on large-scale image datasets.\n",
        "```\n",
        "\n",
        "## The Vision Transformer (ViT) Architecture\n",
        "\n",
        "*How do we adapt an NLP-centric Transformer to handle 2D image data?*\n",
        "\n",
        "In **Vision Transformers**, an image is first split into a grid of small, equally sized patches—commonly $16 \\times 16$ pixels each. Each patch is **flattened** and fed into a linear layer that creates a higher-dimensional embedding. You can think of each patch embedding as analogous to a \"word embedding\" in NLP.\n",
        "\n",
        "```{figure} https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F7a096efc8f3cc40849ee17a546dc0e685da2dc73-4237x1515.png&w=3840&q=75\n",
        ":name: fig-vit-patch\n",
        ":width: 500px\n",
        ":align: center\n",
        "\n",
        "The process of splitting an image into patches and feeding them into a Vision Transformer. Image taken from [Pinecone](https://www.pinecone.io/learn/series/image-search/vision-transformers/).\n",
        "```\n",
        "\n",
        "\n",
        "```{note}\n",
        "**Why Patches Instead of Pixels?**\n",
        "\n",
        "- Handling each pixel independently would create a massive sequence (e.g., a 224x224 image has 50176 pixels!).\n",
        "- Using patches reduces sequence length substantially and preserves local spatial structure.\n",
        "```\n",
        "\n",
        "### 2.2 Positional Encodings\n",
        "\n",
        "Because Transformers are order-agnostic, we add **positional encodings** to each patch embedding. These encodings help the model understand the position of each patch in the original image grid.\n",
        "\n",
        "### 2.3 Transformer Encoder\n",
        "\n",
        "The sequence of patch embeddings (plus positional encodings) goes through a **Transformer encoder**, consisting of:\n",
        "- **Multi-Head Self-Attention**: Allows each patch to attend to others, learning both local and global image features.\n",
        "- **Feed-Forward Layers (MLP blocks)**: Expands and contracts the hidden dimension to add non-linear transformations.\n",
        "\n",
        "### 2.4 Classification Head\n",
        "\n",
        "Typically, the **[CLS] token** (a special token prepended to the sequence) serves as the global representation. After passing through all encoder layers, it goes to a lightweight classification head (a small MLP) to predict the output class.\n",
        "\n",
        "```{note}\n",
        "**Mathematical Foundation (Simplified)**\n",
        "\n",
        "Self-attention for a single head can be described as:\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V\n",
        "$$\n",
        "\n",
        "- $Q, K, V$ are linear projections of the input (patch embeddings).\n",
        "- $d_k$ is the dimension of $K$.\n",
        "- Multi-head attention runs this process in parallel with different learnable projections.\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Types of Attention Mechanisms\n",
        "\n",
        "Even though Vision Transformers generally use **multi-head self-attention**, research has explored variations:\n",
        "\n",
        "1. **Stochastic “Hard” Attention**: Focuses on a subset of patches while ignoring others.\n",
        "2. **Deterministic “Soft” Attention**: Assigns weights to all patches.\n",
        "3. **Multi-Head Attention**: Employs multiple attention heads to learn different aspects (textures, edges, shapes) simultaneously.\n",
        "\n",
        "```{note}\n",
        "**Implementation Insight**\n",
        "\n",
        "You can vary the attention mechanism to strike different balances between computational cost and representational capacity. Hard attention can be more efficient but trickier to train.\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Implementation Example\n",
        "\n",
        "Let’s walk through a simplified code snippet using **Hugging Face Transformers** to classify images with a Vision Transformer. This gives a concrete look at how to build upon these theoretical concepts in practice.\n",
        "\n",
        "```{code-cell} ipython3\n",
        "# A minimal ViT classification example with Hugging Face\n",
        "\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install torchvision\n",
        "\n",
        "import torch\n",
        "from transformers import ViTForImageClassification, ViTImageProcessor\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "# Load a pre-trained Vision Transformer model\n",
        "model_name = \"google/vit-base-patch16-224\"\n",
        "model = ViTForImageClassification.from_pretrained(model_name)\n",
        "processor = ViTImageProcessor.from_pretrained(model_name)\n",
        "\n",
        "# Load an example image\n",
        "url = \"https://images.unsplash.com/photo-1524820353064-7141fab1f5b6\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "# Prepare the image\n",
        "inputs = processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "# Inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "predicted_class_idx = logits.argmax(-1).item()\n",
        "\n",
        "# Print the predicted label\n",
        "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n",
        "```\n",
        "\n",
        "```{tip}\n",
        "- **Data Augmentation**: When training from scratch on smaller datasets, apply heavy data augmentation (random crops, flips, color jitter) to avoid overfitting.\n",
        "- **Transfer Learning**: Leverage pre-trained weights from large datasets and then fine-tune on your target task for improved performance.\n",
        "- **Batch Size**: ViTs can be memory-heavy; consider smaller batch sizes and gradient accumulation if GPU memory is limited.\n",
        "```\n",
        "\n",
        "![](https://via.placeholder.com/600x300)\n",
        "[Figure: A schematic visualization of patches being extracted from an image and fed into a Vision Transformer pipeline.]\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Advancements in Vision Transformer Architectures\n",
        "\n",
        "Could we make ViTs more data-efficient, faster, or better at capturing hierarchical features?\n",
        "\n",
        "### 5.1 Improved Training and Architectures\n",
        "\n",
        "- **DeiT (Data-efficient Image Transformers)**: Introduces a distillation step to improve data efficiency, making ViTs competitive with CNNs on smaller datasets.\n",
        "- **Model Soups**: Averages predictions from multiple ViT models to harness their individual strengths for higher accuracy.\n",
        "\n",
        "### 5.2 Hierarchical and Hybrid Approaches\n",
        "\n",
        "- **Swin Transformer**: Processes images in a hierarchical manner using non-overlapping patches at different resolutions, improving scalability to arbitrary image sizes.\n",
        "- **CaiT (Cross-Attention Image Transformer)**: Uses cross-attention between different patch groups to capture more complex relationships.\n",
        "- **CSWin Transformer**: Adopts a cross-shaped window self-attention pattern to optimize the balance between spatial coverage and computational cost.\n",
        "- **FDViT**: Employs flexible downsampling layers for smoother feature map reductions, improving efficiency and classification accuracy.\n",
        "\n",
        "```{note}\n",
        "**Common Misconception**\n",
        "\n",
        "It’s tempting to think ViTs automatically solve all the limitations of CNNs. However, they still require careful tuning, large datasets (or pre-training), and thoughtful architecture decisions to perform at their best.\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Strengths and Weaknesses\n",
        "\n",
        "When do Vision Transformers shine, and where do they falter?\n",
        "\n",
        "| Feature             | Convolutional Neural Networks (CNNs)      | Vision Transformers (ViTs)                       |\n",
        "|---------------------|-------------------------------------------|--------------------------------------------------|\n",
        "| **Architecture**    | Convolutional + pooling + MLP             | Pure Transformer with self-attention             |\n",
        "| **Input Processing**| Processes entire image as is              | Splits image into patches (tokens)               |\n",
        "| **Global Context**  | Emerges in deeper layers                  | Captured from the start across all patches       |\n",
        "| **Data Requirements**| Perform well with moderate data          | Often require very large datasets or pre-training|\n",
        "| **Compute Cost**    | Usually lower, localized ops              | Higher due to self-attention on all patches      |\n",
        "| **Performance**     | Excellent with well-tuned architectures   | Excels on large-scale data, state-of-the-art SOTA|\n",
        "\n",
        "### Key Advantages\n",
        "\n",
        "- **Global Context**: The self-attention mechanism can integrate information from all patches simultaneously.\n",
        "- **Scalability**: ViTs shine on large datasets, often surpassing CNNs.\n",
        "- **Reduced Inductive Bias**: They learn more general representations since they are not hard-coded to look for local spatial features like CNNs.\n",
        "\n",
        "### Main Limitations\n",
        "\n",
        "- **Data-Hungry**: Tend to overfit on small datasets; methods like DeiT and heavy augmentation help.\n",
        "- **High Computational Cost**: Each patch attends to all others, which can be expensive for high-resolution images.\n",
        "- **Interpretability**: Visualizing attention maps is possible, but can still be less intuitive than CNN feature maps.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Real-World Applications\n",
        "\n",
        "How are Vision Transformers being used beyond simple image classification?\n",
        "\n",
        "- **Object Detection & Image Segmentation**: Self-attention helps capture relationships among objects scattered across the scene.\n",
        "- **Medical Imaging**: Identifying tumors in X-rays or segmenting organ boundaries in MRI scans.\n",
        "- **Remote Sensing**: Analyzing satellite imagery for deforestation tracking or disaster management.\n",
        "- **Action Recognition in Videos**: Extended to video frames, ViTs can learn complex spatiotemporal patterns.\n",
        "- **Multi-Modal Tasks**: Works well with textual data (e.g., image captioning, visual question answering).\n",
        "- **Autonomous Driving**: Understanding global context on the road is critical for safe navigation.\n",
        "- **Anomaly Detection**: Identifying unusual patterns or defects in manufacturing lines.\n",
        "\n",
        "```{note}\n",
        "**Real-World Use Case**\n",
        "\n",
        "In **medical imaging**, ViTs can better spot anomalies by focusing on subtle global context differences in scans. This can help radiologists detect diseases in early stages and potentially save lives.\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Future Directions\n",
        "\n",
        "- **Enhanced Efficiency**: Model compression, pruning, and improved patch strategies aim to reduce computational overhead.\n",
        "- **Smaller Dataset Training**: More advanced self-supervision, distillation, and data-augmentation techniques are being developed to tackle data limitations.\n",
        "- **Interpretability**: Research on attention visualization tools and explanations is growing, aiming to make ViTs more transparent.\n",
        "- **New Domains**: From multi-modal reasoning to video analysis, ViTs are expanding across countless tasks in AI.\n",
        "\n",
        "```{note}\n",
        "**Performance Optimization**\n",
        "\n",
        "- Distillation from large teacher ViTs or even CNNs can help small ViTs converge faster with less data.\n",
        "- Layer-wise learning rate decay and progressive resizing of patches are common training tricks.\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Reflection and Exercises\n",
        "\n",
        "1. **Reflection**: Why do you think ViTs require large datasets to perform optimally, and how might transfer learning mitigate this requirement?\n",
        "2. **Exercise**: Implement a fine-tuning script for a ViT on a smaller dataset (e.g., CIFAR-10). Try various data augmentation strategies. Compare results with a CNN baseline.\n",
        "3. **Advanced Exploration**: Experiment with Swin Transformer or CSWin Transformer. Observe how hierarchical patching or specialized window attention changes performance and training speed.\n",
        "\n",
        "---\n",
        "\n",
        "## References\n",
        "\n",
        "1. Vision Transformers - The Future of Computer Vision! [ResearchGate]\n",
        "2. Introduction to Vision Transformers | Original ViT Paper Explained [aipapersacademy.com]\n",
        "3. Deploying Attention-Based Vision Transformers to Apple Neural Engine [machinelearning.apple.com]\n",
        "4. Vision Transformers (ViT) in Image Recognition: Full Guide [viso.ai]\n",
        "5. Vision Transformers, Explained. A Full Walk-Through of Vision… [Towards Data Science]\n",
        "6. From Transformers to Vision Transformers (ViT): Applying NLP Models to Computer Vision [Medium]\n",
        "7. A Comprehensive Study of Vision Transformers in Image Classification Tasks [arXiv]\n",
        "8. Introductory guide to Vision Transformers [Encord]\n",
        "9. Efficient Training of Visual Transformers with Small Datasets [NeurIPS Proceedings]\n",
        "10. FDViT: Improve the Hierarchical Architecture of Vision Transformer [ICCV 2023]\n",
        "11. Vision Transformers vs. Convolutional Neural Networks (CNNs) [GeeksforGeeks]\n",
        "12. Vision Transformers vs CNNs at the Edge [Edge AI Vision]\n",
        "13. What is a Vision Transformer (ViT)? Real-World Applications [SJ Innovation]\n",
        "14. Vision Transformer: An Introduction [Built In]\n",
        "15. Mastering Vision Transformers with Hugging Face [Rapid Innovation]\n",
        "16. Vision Transformer (ViT) - Hugging Face [huggingface.co]\n",
        "17. Top 10 Open Source Computer Vision Repositories [Encord]\n",
        "18. yhlleo/VTs-Drloc: Efficient Training of Visual Transformers [GitHub]\n",
        "19. Vision Transformers for Image Classification: A Comparative Survey [MDPI]\n",
        "20. BMVC 2022: How to Train Vision Transformer on Small-scale Datasets? [GitHub]\n",
        "21. Vision Transformer: What It Is & How It Works [V7 Labs]\n",
        "\n",
        "---\n",
        "\n",
        "```{note}\n",
        "**Key Takeaway**\n",
        "\n",
        "Vision Transformers offer a fresh approach to image understanding by modeling global relationships among patches from the get-go. As architectures and training strategies evolve, they stand poised to become foundational building blocks in next-generation computer vision systems.\n",
        "```"
      ],
      "id": "0c40b705"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/skojaku-admin/Documents/projects/applied-soft-comp/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
---
title: "Part 2: Meaning as Difference"
jupyter: python3
execute:
    enabled: true
    cache: true
---

::: {.callout-note title="What you'll learn in this section"}
Meaning doesn't live inside words like water in a container. It emerges from networks of contrast and opposition. We'll explore how Saussure, Buddhist logic, Jakobson, and Lévi-Strauss converged on this structuralist insight, and why it matters for machine learning.
:::

## The Arbitrary Nature of Signs

Let's start with a simple observation that changes everything. What makes the English word "dog" mean what it means? You might answer that it refers to a four-legged canine animal. But why those particular sounds? Why not "chien" (French), "perro" (Spanish), or "犬" (Japanese)? The connection between the sound pattern and the concept is arbitrary.

Ferdinand de Saussure, the founder of modern linguistics, called this the arbitrariness of the sign. A sign has two parts: the signifier (the sound or written form) and the signified (the concept). The relationship between them is not natural or inevitable. It's a social convention.

But Saussure went further. He argued that the signified (the concept itself) is also arbitrary. We think "dog" refers to a pre-existing natural category, but nature doesn't draw boundaries between dogs, wolves, and foxes. We do. Different languages slice the animal kingdom differently. Some languages have multiple words for what English calls "rice" (depending on whether it's cooked or raw). English distinguishes "river" from "stream" where other languages use one word. The concepts themselves are products of how a language chooses to divide conceptual space.

This leads to a radical conclusion. The meaning of "dog" isn't determined by what dogs are. It's determined by what dogs are not. "Dog" means "dog" because it occupies a specific position in a network of differences: not "cat", not "wolf", not "fox", not "log", not "fog".

![](../figs/animal-puzzle-dog.png)



## Apoha: Buddhist Logic of Negation

This idea that concepts are defined through negation has deep roots. Buddhist philosophers in ancient India developed a theory called Apoha (literally "exclusion") around the 5th century CE. They argued that we cannot understand what a thing is, only what it is not.

When you see a horse, you don't directly grasp "horseness". Instead, you implicitly exclude everything that is not-horse: not-cow, not-stone, not-water, not-tree. The concept of "horse" is nothing more than the region of conceptual space that remains after all these exclusions. It's a purely negative definition.

![](../figs/negation-horse-emergence.png)

::: {.callout-note}
Dignāga and Dharmakīrti, the primary developers of Apoha theory, influenced both Indian and Tibetan Buddhist philosophy. Their ideas parallel Saussure's insights developed independently 1400 years later.
:::



Why does this matter? Because it reveals that categories are not containers holding essences. They're regions in a space of possibilities, carved out by contrast. This shifts the entire frame. You don't need to know what something is. You only need to know what it is not. The meaning is in the boundaries, not the interior.

## Jakobson's Binary Oppositions

Roman Jakobson, a 20th-century linguist, took the structuralist insight and formalized it. He studied phonemes—the basic sound units of language—and showed they could be decomposed into binary features.

Consider the difference between "p" and "b". They're produced almost identically: both are bilabial stops (you close your lips and release air). The only difference is voicing. Your vocal cords vibrate for "b" but not for "p". We can represent this as:

- "p": [+bilabial, +stop, -voiced]
- "b": [+bilabial, +stop, +voiced]

Jakobson showed that all phonemes in all languages could be analyzed as bundles of such binary oppositions: voiced/voiceless, nasal/oral, fricative/stop, and so on. A phoneme isn't a sound. It's a position in a multidimensional space of contrasts.

This is a vectorization of meaning. Each phoneme is represented not by a symbol but by a coordinate in feature space. Two phonemes are similar if their feature vectors are close. The entire phonological system of a language becomes a geometry problem.

### Why Babies Say "Mama" and "Papa" Everywhere

Let's make this concrete with one of the most striking examples of phonological universals. Consider how remarkably similar the words for "mother" and "father" are across completely unrelated languages:

**Mother:** English "Mama", Mandarin "妈妈 (Māma)", Swahili "Mama", Hebrew "Ima", Spanish "Mamá"

**Father:** English "Papa", Mandarin "爸爸 (Bàba)", French "Papa", Italian "Babbo", Turkish "Baba"

Why do languages separated by continents and millennia converge on these particular sounds? The structuralist answer reveals something profound about how humans acquire language.

Babies don't learn sounds in isolation. They learn contrasts. The first contrasts they master involve the most basic physiological oppositions their vocal apparatus can produce:

**First opposition: Fully closed (m/p) versus fully open (a)**
The consonants "m" and "p" require complete closure of the lips. The vowel "a" requires maximum opening of the mouth. This is the most extreme articulatory contrast possible. It's the easiest distinction for infants to produce and perceive.

**Second opposition: Nasal (m) versus oral (p)**
Both "m" and "p" involve closing the lips, but they differ in one binary feature: "m" lets air flow through the nose (nasal), while "p" blocks it completely and releases it as a burst (oral stop). This is another fundamental physiological switch.

These oppositions create a minimal phonological system with maximum contrast. "Mama" combines nasal closure with open vowel, repeated. "Papa" combines oral closure with open vowel, repeated. The repetition itself is developmentally significant—reduplication is easier for infant motor systems than producing different syllables.

::: {.column-margin}
Jakobson argued that language acquisition follows an implicational hierarchy: children acquire contrasts in a universal order, from maximal to minimal distinctions. Complex sounds appear only after simpler foundations are established.
:::

### Distinctive Features as DNA

Jakobson's insight was that you could decompose any phoneme into a bundle of binary features. Consider the phoneme /b/ (as in "baby"):

- Where is it articulated? [+labial] (at the lips)
- Do the vocal cords vibrate? [+voiced] (yes)
- Does air flow through the nose? [-nasal] (no, it's oral)
- How is airflow stopped? [+stop] (complete closure, then release)

The phoneme /b/ isn't a primitive unit. It's a coordinate in feature space: [+labial, +voiced, -nasal, +stop, -continuant]. Change one feature and you get a different phoneme: flip [+voiced] to [-voiced] and you get /p/. Flip [+stop] to [-stop, +continuant] and you get /v/.

Jakobson identified roughly twelve distinctive features that are sufficient to describe the phonological systems of all human languages. These features aren't arbitrary labels. They correspond to physiological and acoustic properties of speech production and perception. Languages don't choose their sounds randomly. They select from a universal feature space defined by human biology.

This has a remarkable consequence: phonological systems aren't just collections of sounds. They're structured geometries. Languages can't have random inventories of phonemes. If a language has a complex sound, it must also have the simpler contrasts that build up to it. If a language distinguishes three levels of vowel height (high, mid, low), it must first distinguish high from non-high. If it has voiced fricatives like /z/, it must have voiceless fricatives like /s/.

The diversity of human languages doesn't mean anything is possible. It means there's a universal space of phonological possibilities, and each language carves out a specific subset. The structure is in the oppositions, not the sounds themselves.

## The Culinary Triangle

Claude Lévi-Strauss, the structural anthropologist, extended this approach beyond language to culture itself. He argued that human thought operates through binary oppositions: nature/culture, raw/cooked, male/female, sacred/profane. These aren't universal truths. They're structural patterns that organize how different societies make sense of experience.

His "culinary triangle" is a famous example. He analyzed how different cultures process food through two axes: the nature-culture axis (raw vs. transformed) and the means of transformation (cooking vs. rotting). This creates a conceptual space where different food preparation methods occupy specific positions.

![](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Culinary_Triangle.svg/1200px-Culinary_Triangle.svg.png)

::: {.column-margin}
Lévi-Strauss developed these ideas in *The Raw and the Cooked* (1964), the first volume of his four-volume *Mythologiques* series analyzing the structure of myths across cultures.
:::


Roasted meat sits between raw and cooked (less elaborated cooking). Boiled meat is fully cooked (more elaborated). Fermented foods sit between raw and rotted (cultural transformation through natural processes). Each food type's meaning comes from its position in this structural space, not from any intrinsic property.

This is the structuralist thesis in full form: meaning is relational, not substantive. Systems of meaning are systems of differences. To understand anything, map the space of contrasts.

## Metaphor and Metonymy: Two Modes of Connection

Jakobson identified two fundamental ways that concepts connect: through similarity (metaphor) and through contiguity (metonymy). These aren't just literary devices. They're cognitive structures that organize how we think and how language operates.

**Metaphor** works by similarity. "Juliet is the sun" connects two unlike things through shared properties (brightness, warmth, centrality). Metaphor lets us understand the abstract through the concrete, the unfamiliar through the familiar. Western philosophy and science tend toward metaphorical thinking: classification systems, taxonomies, and abstraction hierarchies all rely on grouping similar things.

**Metonymy** works by contiguity—spatial or conceptual adjacency. "The White House announced..." uses a location to refer to the president. "Hollywood is obsessed with franchises" uses a place to refer to the film industry. Metonymy captures association, co-occurrence, and context. Eastern philosophy often emphasizes metonymic thinking: understanding things through their relationships and contexts rather than their essential properties.

::: {.column-margin}
Jakobson argued that aphasia patients show selective impairment of either metaphoric (similarity-based) or metonymic (contiguity-based) operations, suggesting these are fundamental cognitive mechanisms.
:::

```{python}
#| fig-cap: "Metaphor groups by similarity, metonymy by co-occurrence. Different structures of meaning."
#| label: fig-metaphor-metonymy
#| code-fold: true

import matplotlib.pyplot as plt
import networkx as nx
import numpy as np

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))

# Metaphor network (similarity clusters)
G_metaphor = nx.Graph()
clusters = {
    'Brightness': ['sun', 'star', 'lamp', 'fire'],
    'Water': ['ocean', 'river', 'tears', 'rain'],
    'Journey': ['road', 'path', 'voyage', 'quest']
}

for cluster, words in clusters.items():
    for word in words:
        G_metaphor.add_node(word, cluster=cluster)
    # Connect words within cluster
    for i, w1 in enumerate(words):
        for w2 in words[i+1:]:
            G_metaphor.add_edge(w1, w2)

# Position metaphor nodes in clusters
pos_metaphor = {}
cluster_centers = [(0, 2), (-2, -1), (2, -1)]
for (cluster, words), center in zip(clusters.items(), cluster_centers):
    angle_step = 2 * np.pi / len(words)
    for i, word in enumerate(words):
        angle = i * angle_step
        pos_metaphor[word] = (center[0] + 0.8 * np.cos(angle),
                             center[1] + 0.8 * np.sin(angle))

# Draw metaphor network
colors = {'Brightness': '#f39c12', 'Water': '#3498db', 'Journey': '#9b59b6'}
for cluster, words in clusters.items():
    nx.draw_networkx_nodes(G_metaphor, pos_metaphor, nodelist=words,
                          node_color=colors[cluster], node_size=1200,
                          alpha=0.8, ax=ax1)

nx.draw_networkx_edges(G_metaphor, pos_metaphor, alpha=0.3, width=2, ax=ax1)
nx.draw_networkx_labels(G_metaphor, pos_metaphor, font_size=9,
                       font_weight='bold', ax=ax1)

ax1.set_title('Metaphor: Clustering by Similarity', fontsize=14, fontweight='bold')
ax1.axis('off')
ax1.set_xlim(-3.5, 3.5)
ax1.set_ylim(-2.5, 3.5)

# Metonymy network (chain of associations)
G_metonymy = nx.DiGraph()
sequence = [
    ('cherry\nblossoms', 'spring'),
    ('spring', 'hanami\n(viewing)'),
    ('hanami\n(viewing)', 'sake'),
    ('sake', 'party'),
    ('party', 'friends'),
    ('friends', 'memories'),
    ('memories', 'nostalgia'),
]

for source, target in sequence:
    G_metonymy.add_edge(source, target)

# Position metonymy nodes in a flowing path
pos_metonymy = {}
x_positions = np.linspace(-3, 3, len(G_metonymy.nodes()))
y_positions = [0.5 * np.sin(x * 0.8) for x in x_positions]

for i, node in enumerate(G_metonymy.nodes()):
    pos_metonymy[node] = (x_positions[i], y_positions[i])

# Draw metonymy network
nx.draw_networkx_nodes(G_metonymy, pos_metonymy,
                      node_color='#e74c3c', node_size=1500,
                      alpha=0.8, ax=ax2)

nx.draw_networkx_edges(G_metonymy, pos_metonymy,
                      edge_color='gray', width=3, alpha=0.6,
                      arrowsize=20, arrowstyle='->', ax=ax2,
                      connectionstyle='arc3,rad=0.1')

nx.draw_networkx_labels(G_metonymy, pos_metonymy, font_size=8,
                       font_weight='bold', ax=ax2)

ax2.set_title('Metonymy: Chaining by Contiguity', fontsize=14, fontweight='bold')
ax2.axis('off')
ax2.set_xlim(-4, 4)
ax2.set_ylim(-1.5, 1.5)

plt.tight_layout()
plt.show()
```

Why does this distinction matter for machine learning? Because different algorithms capture different types of relationships. Classification algorithms and clustering methods are metaphorical—they group by similarity. But sequence models, language models, and graph neural networks are metonymic—they learn from co-occurrence and context. Understanding which type of relationship you're trying to capture shapes which tools you should use.

## From Philosophy to Algorithm

We've traced a philosophical thread through linguistics, Buddhist logic, structural anthropology, and cognitive science. The insight is consistent: meaning is not intrinsic. It emerges from patterns of difference, opposition, and relationship.

This sounds abstract, but it becomes concrete when you try to teach a machine what words mean. You cannot program in definitions. Dictionaries are circular (look up "large" and you find "big"; look up "big" and you find "large"). Instead, you need to let the machine discover the structure of language by observing how words relate to each other.

That's exactly what word2vec does. It doesn't learn what "dog" means by reading a definition. It learns by observing which words appear near "dog" in actual text. "Dog" appears near "bark", "pet", "leash", "puppy". It doesn't appear near "meow", "aquarium", or "carburetor". The meaning of "dog" is implicitly defined through this pattern of co-occurrence and exclusion.

Word2vec operationalizes Saussure's insight that meaning is differential. It implements Apoha's theory that concepts are defined by exclusion. It builds Jakobson's feature space where similarity is geometric distance. It captures both metaphoric (similarity-based) and metonymic (context-based) relationships.

The next section shows how this works mechanically. How do you turn a philosophical theory about the nature of meaning into working code? How do you represent the continuous space of semantic relationships without imposing arbitrary boundaries? The answer involves vector embeddings, contrastive learning, and a mathematical framework that makes structuralism computable.

::: {.callout-tip title="Try it yourself"}
Pick a concept you use often (democracy, friendship, justice, art). Try to define it without using synonyms or related terms. You'll find it's almost impossible. Now try defining it negatively, through what it is not. Which approach feels more precise? This exercise reveals why structuralism works.
:::

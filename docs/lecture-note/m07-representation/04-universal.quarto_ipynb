{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Part 4: Universal Representations\"\n",
        "jupyter: python3\n",
        "execute:\n",
        "    enabled: true\n",
        "    cache: true\n",
        "---\n",
        "\n",
        "::: {.callout-note title=\"What you'll learn in this section\"}\n",
        "The representational insight extends far beyond text. Images, graphs, molecules, time series, and even entire systems can be embedded in continuous space where structure becomes geometry. We'll explore how this universal framework unifies machine learning across domains.\n",
        ":::\n",
        "\n",
        "## The Representational Paradigm\n",
        "\n",
        "Word2vec revealed something profound. You don't need to know what things are. You only need to observe how they relate. This principle transcends language. Any structured system where entities co-occur, interact, or connect can be embedded in vector space.\n",
        "\n",
        "The recipe is consistent across domains. First, define what \"context\" means for your data type. For words, context is nearby words. For images, context might be nearby pixels or visual features. For nodes in a network, context is neighboring nodes. Second, learn representations that preserve this contextual structure. Entities with similar contexts get similar embeddings. Third, use the geometry of the resulting space to reason about relationships.\n",
        "\n",
        "This is the representational paradigm. Instead of building task-specific models (classifiers, detectors, predictors), we first learn universal representations of the input space, then solve downstream tasks by operating on those representations. The representation becomes the shared foundation."
      ],
      "id": "81a3bceb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: 'The representational paradigm: map diverse inputs to a shared geometric space.'\n",
        "#| label: fig-representation-paradigm\n",
        "#| code-fold: true\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch, Circle\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 10))\n",
        "\n",
        "# Input modalities (left side)\n",
        "input_types = [\n",
        "    ('TEXT', 'dog runs fast', 4.5),\n",
        "    ('IMAGE', 'üñºÔ∏è', 3.0),\n",
        "    ('GRAPH', '‚óè‚Äî‚óè‚Äî‚óè', 1.5),\n",
        "    ('AUDIO', '„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è', 0),\n",
        "]\n",
        "\n",
        "for label, example, y in input_types:\n",
        "    # Input box\n",
        "    box = FancyBboxPatch((0, y - 0.3), 2, 0.6,\n",
        "                         boxstyle=\"round,pad=0.1\",\n",
        "                         facecolor='#ecf0f1', edgecolor='black', linewidth=2)\n",
        "    ax.add_patch(box)\n",
        "    ax.text(0.3, y + 0.15, label, fontsize=11, fontweight='bold')\n",
        "    ax.text(0.3, y - 0.15, example, fontsize=9, style='italic')\n",
        "\n",
        "    # Arrow to embedding space\n",
        "    arrow = FancyArrowPatch((2.1, y), (4.5, 2.25),\n",
        "                          arrowstyle='->', mutation_scale=20, linewidth=2,\n",
        "                          color='#3498db', alpha=0.7)\n",
        "    ax.add_patch(arrow)\n",
        "    ax.text(3.3, y + 0.5 if y > 2.25 else y - 0.5, 'Encode',\n",
        "           fontsize=9, style='italic', color='#3498db')\n",
        "\n",
        "# Central embedding space\n",
        "circle = Circle((7, 2.25), 2.5, facecolor='#3498db', alpha=0.2,\n",
        "               edgecolor='#3498db', linewidth=3)\n",
        "ax.add_patch(circle)\n",
        "\n",
        "# Points in embedding space\n",
        "np.random.seed(42)\n",
        "n_points = 40\n",
        "angles = np.random.rand(n_points) * 2 * np.pi\n",
        "radii = np.random.rand(n_points) * 2.2\n",
        "x_points = 7 + radii * np.cos(angles)\n",
        "y_points = 2.25 + radii * np.sin(angles)\n",
        "\n",
        "ax.scatter(x_points, y_points, s=60, c='#2c3e50', alpha=0.6,\n",
        "          edgecolors='white', linewidth=0.5)\n",
        "\n",
        "ax.text(7, 2.25, 'VECTOR\\nSPACE', ha='center', va='center',\n",
        "       fontsize=14, fontweight='bold', color='#2c3e50',\n",
        "       bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n",
        "\n",
        "# Downstream tasks (right side)\n",
        "tasks = [\n",
        "    ('Classification', 4.5),\n",
        "    ('Similarity Search', 3.5),\n",
        "    ('Clustering', 2.5),\n",
        "    ('Analogy', 1.5),\n",
        "    ('Generation', 0.5),\n",
        "    ('Transfer Learning', -0.3),\n",
        "]\n",
        "\n",
        "for task, y in tasks:\n",
        "    # Arrow from embedding space\n",
        "    arrow = FancyArrowPatch((9.6, 2.25), (11.5, y),\n",
        "                          arrowstyle='->', mutation_scale=20, linewidth=2,\n",
        "                          color='#e74c3c', alpha=0.7)\n",
        "    ax.add_patch(arrow)\n",
        "\n",
        "    # Task box\n",
        "    box = FancyBboxPatch((11.5, y - 0.25), 2.5, 0.5,\n",
        "                         boxstyle=\"round,pad=0.05\",\n",
        "                         facecolor='#ffe6e6', edgecolor='#e74c3c', linewidth=2)\n",
        "    ax.add_patch(box)\n",
        "    ax.text(12.75, y, task, ha='center', va='center',\n",
        "           fontsize=10, fontweight='bold')\n",
        "\n",
        "ax.set_xlim(-0.5, 14.5)\n",
        "ax.set_ylim(-1, 5.5)\n",
        "ax.axis('off')\n",
        "ax.set_title('The Representational Paradigm: One Space, Many Tasks',\n",
        "            fontsize=18, fontweight='bold', pad=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-representation-paradigm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Graph Embeddings: Node2Vec\n",
        "\n",
        "Networks present a natural extension of word2vec's logic. Words appear in sequences. Nodes appear in neighborhoods. If we can generate sequences of nodes by walking through the network, we can apply the same contrastive learning approach.\n",
        "\n",
        "Node2Vec does exactly this. It performs random walks on a graph, treating the sequence of visited nodes like a sentence. A walk might go: NodeA ‚Üí NodeB ‚Üí NodeC ‚Üí NodeD. This generates training pairs: (NodeB, NodeA), (NodeB, NodeC), and so on. The model learns embeddings where nodes with similar neighborhoods end up nearby in vector space.\n",
        "\n",
        "::: {.column-margin}\n",
        "Node2Vec (Grover & Leskovec, 2016) extends DeepWalk by introducing biased random walks that interpolate between breadth-first and depth-first exploration, capturing different notions of network similarity.\n",
        ":::"
      ],
      "id": "647632e5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Random walks on graphs generate sequences that reveal structural similarity.\n",
        "#| label: fig-node2vec\n",
        "#| code-fold: true\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "# Create a sample network\n",
        "G = nx.karate_club_graph()\n",
        "pos = nx.spring_layout(G, seed=42)\n",
        "\n",
        "# Left: Show random walk\n",
        "walk_path = [0, 1, 2, 3, 13, 33, 32, 23]\n",
        "\n",
        "# Draw all nodes and edges\n",
        "nx.draw_networkx_edges(G, pos, alpha=0.2, ax=ax1)\n",
        "nx.draw_networkx_nodes(G, pos, node_color='#ecf0f1', node_size=300,\n",
        "                      edgecolors='black', linewidth=1, ax=ax1)\n",
        "\n",
        "# Highlight walk path\n",
        "walk_edges = [(walk_path[i], walk_path[i+1]) for i in range(len(walk_path)-1)]\n",
        "nx.draw_networkx_edges(G, pos, edgelist=walk_edges, edge_color='#e74c3c',\n",
        "                      width=3, arrows=True, arrowsize=20,\n",
        "                      arrowstyle='->', ax=ax1)\n",
        "\n",
        "# Highlight walked nodes\n",
        "nx.draw_networkx_nodes(G, pos, nodelist=walk_path, node_color='#3498db',\n",
        "                      node_size=400, edgecolors='black', linewidth=2, ax=ax1)\n",
        "\n",
        "# Add labels to walked nodes\n",
        "walk_labels = {node: f'{i}' for i, node in enumerate(walk_path)}\n",
        "nx.draw_networkx_labels(G, pos, labels=walk_labels, font_color='white',\n",
        "                       font_weight='bold', font_size=9, ax=ax1)\n",
        "\n",
        "ax1.set_title('Random Walk: NodeA ‚Üí NodeB ‚Üí NodeC ‚Üí ...',\n",
        "             fontsize=14, fontweight='bold')\n",
        "ax1.axis('off')\n",
        "\n",
        "# Add walk sequence annotation\n",
        "walk_seq = ' ‚Üí '.join([f'N{i}' for i in range(len(walk_path))])\n",
        "ax1.text(0.5, -1.15, f'Walk sequence: {walk_seq}',\n",
        "        ha='center', transform=ax1.transAxes, fontsize=10,\n",
        "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n",
        "\n",
        "# Right: Embedding space\n",
        "# Simulate embeddings where structurally similar nodes cluster\n",
        "np.random.seed(42)\n",
        "\n",
        "# Identify communities (roughly)\n",
        "club1 = [n for n, d in G.nodes(data=True) if d['club'] == 'Mr. Hi']\n",
        "club2 = [n for n, d in G.nodes(data=True) if d['club'] == 'Officer']\n",
        "\n",
        "# Generate embeddings (2D projection)\n",
        "embeddings = {}\n",
        "for node in club1:\n",
        "    embeddings[node] = np.random.randn(2) * 0.5 + np.array([-1.5, 0])\n",
        "for node in club2:\n",
        "    embeddings[node] = np.random.randn(2) * 0.5 + np.array([1.5, 0])\n",
        "\n",
        "# Plot embeddings\n",
        "for node in G.nodes():\n",
        "    x, y = embeddings[node]\n",
        "    color = '#3498db' if node in club1 else '#e74c3c'\n",
        "    alpha = 0.9 if node in walk_path else 0.3\n",
        "    size = 400 if node in walk_path else 200\n",
        "\n",
        "    ax2.scatter(x, y, s=size, c=color, alpha=alpha,\n",
        "               edgecolors='black', linewidth=1.5 if node in walk_path else 1,\n",
        "               zorder=3 if node in walk_path else 2)\n",
        "\n",
        "    if node in walk_path:\n",
        "        ax2.text(x, y, f'{walk_path.index(node)}', ha='center', va='center',\n",
        "                fontsize=9, fontweight='bold', color='white')\n",
        "\n",
        "# Add cluster annotations\n",
        "ax2.text(-1.5, 2, 'Community 1', fontsize=12, fontweight='bold',\n",
        "        ha='center', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
        "ax2.text(1.5, 2, 'Community 2', fontsize=12, fontweight='bold',\n",
        "        ha='center', bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.7))\n",
        "\n",
        "ax2.set_xlim(-3.5, 3.5)\n",
        "ax2.set_ylim(-2.5, 2.5)\n",
        "ax2.set_xlabel('Embedding Dimension 1', fontsize=11)\n",
        "ax2.set_ylabel('Embedding Dimension 2', fontsize=11)\n",
        "ax2.set_title('Learned Embeddings: Structural Similarity ‚Üí Spatial Proximity',\n",
        "             fontsize=14, fontweight='bold')\n",
        "ax2.grid(alpha=0.3, linestyle='--')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-node2vec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The beautiful part is what counts as \"similar\". Two nodes might be similar because they're directly connected (local similarity). Or they're similar because they play equivalent roles in different parts of the network (structural equivalence). Node2Vec captures both. By adjusting the random walk strategy, you can emphasize different notions of similarity without changing the core algorithm.\n",
        "\n",
        "This dissolves the boundary between topology and geometry. The network's structure becomes a coordinate system. Questions about network properties become questions about spatial relationships in the embedding space.\n",
        "\n",
        "## Image Embeddings: Learning Visual Representations\n",
        "\n",
        "Images pose a different challenge. Pixels don't appear in \"context\" the way words do. But convolutional neural networks (CNNs) learn hierarchical representations where each layer captures increasingly abstract visual features. The final layer before classification is a vector embedding of the image.\n",
        "\n",
        "These embeddings exhibit the same geometric properties as word embeddings. Images of similar objects cluster together. The vector from \"cat\" images to \"tiger\" images parallels the vector from \"dog\" images to \"wolf\" images (the \"wild version\" relationship). You can do visual analogy through vector arithmetic.\n",
        "\n",
        "::: {.column-margin}\n",
        "Modern vision models like CLIP learn joint embeddings of images and text, creating a shared semantic space where \"a photo of a cat\" and actual cat photos occupy similar regions.\n",
        ":::"
      ],
      "id": "3a1e41d5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: CNN embeddings map images to vectors where visual similarity becomes geometric proximity.\n",
        "#| label: fig-image-embeddings\n",
        "#| code-fold: true\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.patches import Rectangle, FancyArrowPatch\n",
        "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 10))\n",
        "\n",
        "# Simulate image embeddings in 2D\n",
        "np.random.seed(42)\n",
        "\n",
        "categories = {\n",
        "    'Cats': {'center': np.array([-2, 2]), 'n': 8, 'color': '#e74c3c'},\n",
        "    'Dogs': {'center': np.array([2, 2]), 'n': 8, 'color': '#3498db'},\n",
        "    'Cars': {'center': np.array([-2, -2]), 'n': 8, 'color': '#f39c12'},\n",
        "    'Trees': {'center': np.array([2, -2]), 'n': 8, 'color': '#27ae60'},\n",
        "}\n",
        "\n",
        "# Generate embeddings\n",
        "for category, config in categories.items():\n",
        "    center = config['center']\n",
        "    n = config['n']\n",
        "    color = config['color']\n",
        "\n",
        "    # Generate points around center\n",
        "    embeddings = np.random.randn(n, 2) * 0.4 + center\n",
        "\n",
        "    # Plot\n",
        "    ax.scatter(embeddings[:, 0], embeddings[:, 1], s=300, c=color,\n",
        "              alpha=0.6, edgecolors='black', linewidth=1.5, label=category)\n",
        "\n",
        "    # Add category label\n",
        "    ax.text(center[0], center[1] + 1.3, category, ha='center',\n",
        "           fontsize=13, fontweight='bold',\n",
        "           bbox=dict(boxstyle='round', facecolor=color, alpha=0.3))\n",
        "\n",
        "# Draw arrows showing relationships\n",
        "# \"Domestic to wild\" relationship\n",
        "cat_center = categories['Cats']['center']\n",
        "dog_center = categories['Dogs']['center']\n",
        "\n",
        "# Add annotations for key relationships\n",
        "ax.annotate('', xy=(cat_center[0] + 2, cat_center[1] + 0.2),\n",
        "           xytext=(cat_center[0], cat_center[1]),\n",
        "           arrowprops=dict(arrowstyle='->', lw=2.5, color='purple', alpha=0.7))\n",
        "ax.text(0, 2.8, 'Semantic\\nSimilarity', ha='center', fontsize=10,\n",
        "       style='italic', color='purple')\n",
        "\n",
        "# Visual feature space axes\n",
        "ax.annotate('', xy=(3.5, 0), xytext=(-3.5, 0),\n",
        "           arrowprops=dict(arrowstyle='<->', lw=2, color='gray', alpha=0.5))\n",
        "ax.text(0, -4, '‚Üê Natural / Artificial ‚Üí', ha='center', fontsize=11,\n",
        "       fontweight='bold', color='gray')\n",
        "\n",
        "ax.annotate('', xy=(0, 3.5), xytext=(0, -3.5),\n",
        "           arrowprops=dict(arrowstyle='<->', lw=2, color='gray', alpha=0.5))\n",
        "ax.text(-4, 0, '‚Üê Living / Non-living ‚Üí', ha='center', fontsize=11,\n",
        "       fontweight='bold', color='gray', rotation=90)\n",
        "\n",
        "ax.set_xlim(-4.5, 4.5)\n",
        "ax.set_ylim(-4.5, 4.5)\n",
        "ax.set_aspect('equal')\n",
        "ax.legend(loc='upper left', fontsize=11, framealpha=0.9)\n",
        "ax.grid(alpha=0.3, linestyle='--')\n",
        "ax.set_title('Image Embeddings: Visual Concepts as Geometric Clusters',\n",
        "            fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-image-embeddings",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What's remarkable is that these representations are learned without explicit supervision about semantic relationships. The network is trained to classify images (cat vs. dog vs. car vs. tree). The embedding space emerges as a byproduct. The geometric structure of visual similarity is discovered, not programmed.\n",
        "\n",
        "This is representation learning's power. The same network architecture can be used for classification, similarity search, image retrieval, and transfer learning, all because the learned embeddings capture meaningful structure.\n",
        "\n",
        "## Molecules, Music, and More\n",
        "\n",
        "The technique generalizes to any structured domain. Molecular embeddings represent chemical compounds as vectors based on their structural properties and interactions. Similar molecules end up nearby, enabling drug discovery through geometric search. Musical embeddings capture harmonic and rhythmic relationships, making it possible to search for songs by similarity or generate variations.\n",
        "\n",
        "Even proteins can be embedded by treating amino acid sequences like sentences and using transformer models (the same architecture behind GPT and BERT). The learned embeddings capture functional relationships. Proteins with similar functions cluster together, even if their sequences differ significantly."
      ],
      "id": "490dd3c2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: 'The representational framework applies across domains: from molecules to music to proteins.'\n",
        "#| label: fig-diverse-embeddings\n",
        "#| code-fold: true\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "fig.suptitle('Universal Representations Across Domains',\n",
        "            fontsize=18, fontweight='bold', pad=20)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "domains = [\n",
        "    {\n",
        "        'name': 'Molecular Embeddings',\n",
        "        'categories': ['Antibiotics', 'Anti-inflammatory', 'Analgesics'],\n",
        "        'colors': ['#e74c3c', '#3498db', '#f39c12'],\n",
        "    },\n",
        "    {\n",
        "        'name': 'Music Embeddings',\n",
        "        'categories': ['Classical', 'Jazz', 'Rock'],\n",
        "        'colors': ['#9b59b6', '#1abc9c', '#e67e22'],\n",
        "    },\n",
        "    {\n",
        "        'name': 'Protein Embeddings',\n",
        "        'categories': ['Enzymes', 'Receptors', 'Transporters'],\n",
        "        'colors': ['#16a085', '#c0392b', '#2980b9'],\n",
        "    },\n",
        "    {\n",
        "        'name': 'User Embeddings',\n",
        "        'categories': ['Tech enthusiasts', 'Sports fans', 'Book lovers'],\n",
        "        'colors': ['#8e44ad', '#27ae60', '#d35400'],\n",
        "    },\n",
        "]\n",
        "\n",
        "for idx, (ax, domain_info) in enumerate(zip(axes.flat, domains)):\n",
        "    categories = domain_info['categories']\n",
        "    colors = domain_info['colors']\n",
        "\n",
        "    # Generate clusters\n",
        "    for i, (category, color) in enumerate(zip(categories, colors)):\n",
        "        angle = i * (2 * np.pi / len(categories))\n",
        "        center = 2 * np.array([np.cos(angle), np.sin(angle)])\n",
        "\n",
        "        # Generate points\n",
        "        n_points = 15\n",
        "        points = np.random.randn(n_points, 2) * 0.4 + center\n",
        "\n",
        "        ax.scatter(points[:, 0], points[:, 1], s=150, c=color,\n",
        "                  alpha=0.6, edgecolors='black', linewidth=1, label=category)\n",
        "\n",
        "        # Add label\n",
        "        label_offset = 1.3\n",
        "        label_pos = center * label_offset\n",
        "        ax.text(label_pos[0], label_pos[1], category, ha='center', va='center',\n",
        "               fontsize=9, fontweight='bold',\n",
        "               bbox=dict(boxstyle='round', facecolor=color, alpha=0.3))\n",
        "\n",
        "    ax.set_xlim(-4, 4)\n",
        "    ax.set_ylim(-4, 4)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_title(domain_info['name'], fontsize=13, fontweight='bold')\n",
        "    ax.grid(alpha=0.3, linestyle='--')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-diverse-embeddings",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The pattern is consistent. Define what \"context\" means for your domain. Learn representations where similar contexts produce similar vectors. Use the geometry of the resulting space to solve tasks. The specific implementation details vary, but the philosophical insight remains: meaning is relational, structure emerges from contrast, and continuous representations dissolve artificial boundaries.\n",
        "\n",
        "## Time Series as Trajectories\n",
        "\n",
        "Time series present yet another perspective. Instead of embedding individual points, you can embed entire sequences as trajectories through latent space. Similar dynamic processes produce similar trajectories. You can measure similarity between time series by comparing their paths, cluster them by trajectory shape, or forecast by extrapolating the path."
      ],
      "id": "222953a4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Time series as trajectories in latent space. Similar dynamics produce similar paths.\n",
        "#| label: fig-time-series-embedding\n",
        "#| code-fold: true\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "# Generate time series\n",
        "t = np.linspace(0, 4 * np.pi, 100)\n",
        "\n",
        "# Different patterns\n",
        "sine_wave = np.sin(t)\n",
        "damped_sine = np.sin(t) * np.exp(-t / 10)\n",
        "growing_sine = np.sin(t) * np.exp(t / 15)\n",
        "linear = t / (4 * np.pi)\n",
        "\n",
        "# Plot time series\n",
        "ax1.plot(t, sine_wave, label='Periodic', linewidth=2, color='#3498db')\n",
        "ax1.plot(t, damped_sine, label='Damped', linewidth=2, color='#e74c3c')\n",
        "ax1.plot(t, growing_sine, label='Growing', linewidth=2, color='#27ae60')\n",
        "ax1.plot(t, linear, label='Linear', linewidth=2, color='#f39c12')\n",
        "\n",
        "ax1.set_xlabel('Time', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel('Value', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Time Series in Temporal Domain', fontsize=14, fontweight='bold')\n",
        "ax1.legend(fontsize=11)\n",
        "ax1.grid(alpha=0.3, linestyle='--')\n",
        "\n",
        "# Embed as trajectories in 2D latent space\n",
        "# Simulate embeddings where similar dynamics cluster\n",
        "ax2.plot([0, 2, 1, 0], [0, 1, 2, 0], marker='o', markersize=8,\n",
        "        label='Periodic', linewidth=2.5, color='#3498db', alpha=0.7)\n",
        "ax2.plot([0, 1, 0.5, 0.2], [0, 1, 1.2, 1], marker='o', markersize=8,\n",
        "        label='Damped', linewidth=2.5, color='#e74c3c', alpha=0.7)\n",
        "ax2.plot([0, 2, 3, 4], [0, 1, 2.5, 4], marker='o', markersize=8,\n",
        "        label='Growing', linewidth=2.5, color='#27ae60', alpha=0.7)\n",
        "ax2.plot([0, 1, 2, 3], [0, 0.3, 0.6, 0.9], marker='o', markersize=8,\n",
        "        label='Linear', linewidth=2.5, color='#f39c12', alpha=0.7)\n",
        "\n",
        "# Add arrows\n",
        "for trajectory_data in [\n",
        "    ([0, 2, 1, 0], [0, 1, 2, 0], '#3498db'),\n",
        "    ([0, 1, 0.5, 0.2], [0, 1, 1.2, 1], '#e74c3c'),\n",
        "    ([0, 2, 3, 4], [0, 1, 2.5, 4], '#27ae60'),\n",
        "    ([0, 1, 2, 3], [0, 0.3, 0.6, 0.9], '#f39c12'),\n",
        "]:\n",
        "    x, y, color = trajectory_data\n",
        "    for i in range(len(x) - 1):\n",
        "        ax2.annotate('', xy=(x[i+1], y[i+1]), xytext=(x[i], y[i]),\n",
        "                    arrowprops=dict(arrowstyle='->', lw=1.5, color=color, alpha=0.4))\n",
        "\n",
        "ax2.set_xlabel('Latent Dimension 1', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('Latent Dimension 2', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('Time Series as Trajectories in Latent Space',\n",
        "             fontsize=14, fontweight='bold')\n",
        "ax2.legend(fontsize=11, loc='upper left')\n",
        "ax2.grid(alpha=0.3, linestyle='--')\n",
        "ax2.set_xlim(-0.5, 4.5)\n",
        "ax2.set_ylim(-0.5, 4.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-time-series-embedding",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This perspective transforms time series analysis. Instead of comparing raw signals (which are sensitive to noise, scaling, and temporal alignment), you compare their underlying dynamical patterns as captured by their embeddings. The boundary between \"similar\" and \"different\" becomes a matter of distance in latent space rather than a hard classification rule.\n",
        "\n",
        "## The Unified View\n",
        "\n",
        "Step back and see the pattern. Across all these domains, we're doing the same thing:\n",
        "\n",
        "1. Observe how entities relate (co-occur, connect, interact, transform).\n",
        "2. Learn vector representations that preserve these relationships.\n",
        "3. Map discrete symbols to continuous coordinates.\n",
        "4. Dissolve artificial boundaries in favor of smooth gradients.\n",
        "5. Answer questions geometrically rather than categorically.\n",
        "\n",
        "This is structuralism made computational. This is the insight that boundaries are observer-dependent, turned into working technology. This is the philosophy that meaning is relational, implemented as neural networks and optimization algorithms.\n",
        "\n",
        "Modern machine learning increasingly operates in this representational paradigm. Large language models learn representations of language. Vision transformers learn representations of images. Graph neural networks learn representations of networks. Multimodal models learn shared representations across modalities. The representations themselves become the primary object of study.\n",
        "\n",
        "## What We've Learned\n",
        "\n",
        "We started with regional dialects and generational labels, noticing that the boundaries we draw are often arbitrary. We explored structuralist philosophy, learning that meaning emerges from networks of contrast rather than intrinsic essence. We saw how word2vec operationalizes these ideas through contrastive learning and vector embeddings. And we've now seen how this insight generalizes across all structured domains.\n",
        "\n",
        "The lesson is philosophical and practical. Philosophically, it reminds us that the categories we use are tools of analysis, not features of reality. The world is continuous. The boundaries are ours. Practically, it suggests that whenever we're tempted to force phenomena into discrete boxes, we should ask: could a continuous representation serve better?\n",
        "\n",
        "The representational turn in machine learning isn't just about better algorithms. It's about better epistemology. It's about building systems that preserve the nuance, gradation, and relational structure that discrete labels destroy. It's about melting the boundaries we've artificially frozen, and letting the true structure of similarity and difference emerge from the data itself.\n",
        "\n",
        "::: {.callout-tip title=\"Explore further\"}\n",
        "Think about your own research domain. What are the discrete categories you use? Could they be embedded in continuous space? What new questions become answerable when you work with gradients instead of boundaries? The representational perspective often reveals structures that categorical thinking obscures.\n",
        ":::"
      ],
      "id": "d757651e"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/skojaku-admin/Documents/projects/applied-soft-comp/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
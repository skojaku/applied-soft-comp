{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Part 3: From Categories to Coordinates\"\n",
        "jupyter: python3\n",
        "execute:\n",
        "    enabled: true\n",
        "    cache: true\n",
        "---\n",
        "\n",
        "::: {.callout-note title=\"What you'll learn in this section\"}\n",
        "Vector embeddings operationalize structuralism by mapping concepts into continuous space where meaning becomes geometry. We'll explore how word2vec learns these representations through context and contrast, dissolving the artificial boundaries that discrete labels impose.\n",
        ":::\n",
        "\n",
        "## The Limits of Traditional Classification\n",
        "\n",
        "Traditional machine learning builds decision boundaries. You collect labeled data, extract features, and train a classifier to draw a line (or hyperplane) separating Class A from Class B. The output is binary: spam or not-spam, fraud or legitimate, cat or dog."
      ],
      "id": "74d37dd1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Traditional classification imposes hard boundaries in feature space.\n",
        "#| label: fig-classification-boundary\n",
        "#| code-fold: true\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate sample data\n",
        "X, y = make_blobs(n_samples=200, centers=2, n_features=2,\n",
        "                  center_box=(-3, 3), cluster_std=1.2)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "# Plot points\n",
        "colors = ['#3498db', '#e74c3c']\n",
        "for i in range(2):\n",
        "    mask = y == i\n",
        "    ax.scatter(X[mask, 0], X[mask, 1], c=colors[i], s=100,\n",
        "              alpha=0.6, edgecolors='black', linewidth=1,\n",
        "              label=f'Class {i}')\n",
        "\n",
        "# Draw decision boundary\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "x_boundary = np.linspace(x_min, x_max, 100)\n",
        "# Simple linear boundary\n",
        "y_boundary = 0.1 * x_boundary + 0.5\n",
        "\n",
        "ax.plot(x_boundary, y_boundary, 'k-', linewidth=3, label='Decision Boundary')\n",
        "ax.fill_between(x_boundary, y_boundary, y_max=8, alpha=0.1, color='red')\n",
        "ax.fill_between(x_boundary, y_boundary, y_min=-8, alpha=0.1, color='blue')\n",
        "\n",
        "ax.set_xlabel('Feature 1', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Feature 2', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Classification: The World in Binary', fontsize=16, fontweight='bold')\n",
        "ax.legend(fontsize=11, loc='upper left')\n",
        "ax.grid(alpha=0.3, linestyle='--')\n",
        "ax.set_xlim(x_min, x_max)\n",
        "ax.set_ylim(-8, 8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-classification-boundary",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This approach works when categories are genuinely discrete. But what about political ideology? You might lean liberal on economic issues but conservative on others. Your position isn't \"left or right\"—it's a point in a multidimensional space. Forcing it into a binary choice destroys information.\n",
        "\n",
        "What about word meanings? Is \"dog\" more similar to \"wolf\" or \"cat\"? The answer is \"it depends\". Dogs and wolves are biologically closer (same genus). Dogs and cats are domestically closer (both pets). A classification system would force you to choose one dimension. A continuous representation can capture both simultaneously.\n",
        "\n",
        "## Enter Vector Embeddings\n",
        "\n",
        "Vector embeddings solve this by mapping each concept to coordinates in a high-dimensional continuous space. Instead of \"this word belongs to category X\", you get \"this word lives at position $[0.23, -0.15, 0.87, ...]$ in 300-dimensional space\". Similarity becomes geometric distance. Relationships become directional vectors.\n",
        "\n",
        "This is the mathematical realization of structuralism. Each word's meaning is determined by its position relative to all other words. There are no hard boundaries, only neighborhoods of varying density. The gradation that exists in reality is preserved in the representation."
      ],
      "id": "4614bb24"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: In embedding space, concepts form continuous neighborhoods without hard boundaries.\n",
        "#| label: fig-embedding-space\n",
        "#| code-fold: true\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.manifold import MDS\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create semantic clusters with overlaps\n",
        "animals_domestic = np.random.randn(8, 2) * 0.4 + np.array([0, 2])\n",
        "animals_wild = np.random.randn(8, 2) * 0.4 + np.array([1.5, 2.5])\n",
        "furniture = np.random.randn(8, 2) * 0.4 + np.array([-2, -1])\n",
        "colors = np.random.randn(8, 2) * 0.4 + np.array([2, -1.5])\n",
        "\n",
        "labels_domestic = ['dog', 'cat', 'hamster', 'rabbit', 'bird', 'fish', 'horse', 'cow']\n",
        "labels_wild = ['wolf', 'fox', 'bear', 'lion', 'tiger', 'eagle', 'hawk', 'deer']\n",
        "labels_furniture = ['chair', 'table', 'sofa', 'desk', 'bed', 'lamp', 'shelf', 'cabinet']\n",
        "labels_colors = ['red', 'blue', 'green', 'yellow', 'orange', 'purple', 'pink', 'brown']\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "# Plot clusters with gradients\n",
        "scatter1 = ax.scatter(animals_domestic[:, 0], animals_domestic[:, 1],\n",
        "                     c=range(len(animals_domestic)), cmap='Blues',\n",
        "                     s=300, alpha=0.6, edgecolors='black', linewidth=1.5)\n",
        "\n",
        "scatter2 = ax.scatter(animals_wild[:, 0], animals_wild[:, 1],\n",
        "                     c=range(len(animals_wild)), cmap='Greens',\n",
        "                     s=300, alpha=0.6, edgecolors='black', linewidth=1.5)\n",
        "\n",
        "scatter3 = ax.scatter(furniture[:, 0], furniture[:, 1],\n",
        "                     c=range(len(furniture)), cmap='Oranges',\n",
        "                     s=300, alpha=0.6, edgecolors='black', linewidth=1.5)\n",
        "\n",
        "scatter4 = ax.scatter(colors[:, 0], colors[:, 1],\n",
        "                     c=range(len(colors)), cmap='RdPu',\n",
        "                     s=300, alpha=0.6, edgecolors='black', linewidth=1.5)\n",
        "\n",
        "# Add labels\n",
        "for i, label in enumerate(labels_domestic):\n",
        "    ax.text(animals_domestic[i, 0], animals_domestic[i, 1], label,\n",
        "           ha='center', va='center', fontsize=9, fontweight='bold')\n",
        "\n",
        "for i, label in enumerate(labels_wild):\n",
        "    ax.text(animals_wild[i, 0], animals_wild[i, 1], label,\n",
        "           ha='center', va='center', fontsize=9, fontweight='bold')\n",
        "\n",
        "for i, label in enumerate(labels_furniture):\n",
        "    ax.text(furniture[i, 0], furniture[i, 1], label,\n",
        "           ha='center', va='center', fontsize=9, fontweight='bold')\n",
        "\n",
        "for i, label in enumerate(labels_colors):\n",
        "    ax.text(colors[i, 0], colors[i, 1], label,\n",
        "           ha='center', va='center', fontsize=9, fontweight='bold')\n",
        "\n",
        "# Add cluster annotations\n",
        "ax.text(0, 3.5, 'Domestic\\nAnimals', ha='center', fontsize=12,\n",
        "       fontweight='bold', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
        "ax.text(1.5, 3.7, 'Wild\\nAnimals', ha='center', fontsize=12,\n",
        "       fontweight='bold', bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
        "ax.text(-2, -2.3, 'Furniture', ha='center', fontsize=12,\n",
        "       fontweight='bold', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n",
        "ax.text(2, -2.7, 'Colors', ha='center', fontsize=12,\n",
        "       fontweight='bold', bbox=dict(boxstyle='round', facecolor='pink', alpha=0.7))\n",
        "\n",
        "ax.set_xlim(-4, 4)\n",
        "ax.set_ylim(-4, 4.5)\n",
        "ax.axis('off')\n",
        "ax.set_title('Embedding Space: Continuous Neighborhoods of Meaning',\n",
        "            fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-embedding-space",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice the overlap between domestic and wild animals. \"Dog\" is close to \"wolf\" because they're related species. But \"dog\" is also in the domestic cluster near \"cat\". The space naturally captures multiple dimensions of similarity without forcing categorical choices.\n",
        "\n",
        "## How Word2Vec Works: Context is Everything\n",
        "\n",
        "Word2vec learns these embeddings from raw text without any human-provided definitions or labels. The core insight: a word's meaning is determined by the company it keeps. Words that appear in similar contexts have similar meanings.\n",
        "\n",
        "This is pure metonymy in action. We're not grouping words by what they are (metaphor), but by where they appear (contiguity). \"Dog\" means what it means because it shows up near \"bark\", \"pet\", \"leash\", \"puppy\", \"tail\". If another word appears in the same neighborhood of contexts, it must mean something similar.\n",
        "\n",
        "::: {.column-margin}\n",
        "The distributional hypothesis, formalized by Zellig Harris (1954) and J.R. Firth (1957), states: \"You shall know a word by the company it keeps.\" Word2vec is its computational realization.\n",
        ":::\n",
        "\n",
        "The algorithm slides a window over text, creating training pairs. If the window is size 2 and you encounter \"The quick brown fox\", you get pairs like:\n",
        "\n",
        "- (brown, The)\n",
        "- (brown, quick)\n",
        "- (brown, fox)\n",
        "\n",
        "The word in the center (brown) is the target. The surrounding words are the context. Word2vec learns to predict context from target, or target from context. In doing so, it builds vector representations where words with similar contexts end up nearby."
      ],
      "id": "048dddb0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: A sliding window over text generates training pairs without human labeling.\n",
        "#| label: fig-sliding-window\n",
        "#| code-fold: true\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle, FancyBboxPatch\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog\".split()\n",
        "window_size = 2\n",
        "\n",
        "# Show three different window positions\n",
        "window_positions = [2, 4, 7]  # Positions for \"brown\", \"jumps\", \"lazy\"\n",
        "\n",
        "for row, center_idx in enumerate(window_positions):\n",
        "    y_base = 2.5 - row * 2.5\n",
        "\n",
        "    # Draw all words\n",
        "    for i, word in enumerate(sentence):\n",
        "        x = i * 1.2\n",
        "\n",
        "        # Determine if word is in current window\n",
        "        in_window = abs(i - center_idx) <= window_size and i != center_idx\n",
        "        is_center = i == center_idx\n",
        "\n",
        "        if is_center:\n",
        "            # Center word (target)\n",
        "            box = FancyBboxPatch((x - 0.4, y_base - 0.25), 0.8, 0.5,\n",
        "                                boxstyle=\"round,pad=0.05\",\n",
        "                                facecolor='#e74c3c', edgecolor='black',\n",
        "                                linewidth=2.5)\n",
        "            ax.add_patch(box)\n",
        "            ax.text(x, y_base, word, ha='center', va='center',\n",
        "                   fontsize=11, fontweight='bold', color='white')\n",
        "\n",
        "        elif in_window:\n",
        "            # Context words\n",
        "            box = FancyBboxPatch((x - 0.4, y_base - 0.25), 0.8, 0.5,\n",
        "                                boxstyle=\"round,pad=0.05\",\n",
        "                                facecolor='#3498db', edgecolor='black',\n",
        "                                linewidth=2)\n",
        "            ax.add_patch(box)\n",
        "            ax.text(x, y_base, word, ha='center', va='center',\n",
        "                   fontsize=11, fontweight='bold', color='white')\n",
        "\n",
        "        else:\n",
        "            # Outside window\n",
        "            ax.text(x, y_base, word, ha='center', va='center',\n",
        "                   fontsize=11, color='gray', alpha=0.5)\n",
        "\n",
        "    # Add window bracket\n",
        "    window_start = (center_idx - window_size) * 1.2\n",
        "    window_end = (center_idx + window_size + 1) * 1.2\n",
        "    ax.plot([window_start - 0.5, window_end - 0.7],\n",
        "           [y_base + 0.5, y_base + 0.5],\n",
        "           'k-', linewidth=2)\n",
        "    ax.plot([window_start - 0.5, window_start - 0.5],\n",
        "           [y_base + 0.4, y_base + 0.5], 'k-', linewidth=2)\n",
        "    ax.plot([window_end - 0.7, window_end - 0.7],\n",
        "           [y_base + 0.4, y_base + 0.5], 'k-', linewidth=2)\n",
        "\n",
        "    # Add training pairs annotation\n",
        "    center_word = sentence[center_idx]\n",
        "    context_words = [sentence[i] for i in range(max(0, center_idx - window_size),\n",
        "                                                 min(len(sentence), center_idx + window_size + 1))\n",
        "                    if i != center_idx]\n",
        "\n",
        "    pairs_text = f\"Target: '{center_word}' → Context: {', '.join(context_words)}\"\n",
        "    ax.text(5, y_base - 0.7, pairs_text, ha='center', va='top',\n",
        "           fontsize=9, style='italic',\n",
        "           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "ax.set_xlim(-1, 11)\n",
        "ax.set_ylim(-5.5, 3.5)\n",
        "ax.axis('off')\n",
        "\n",
        "# Add legend\n",
        "legend_elements = [\n",
        "    plt.Rectangle((0, 0), 1, 1, fc='#e74c3c', edgecolor='black', label='Target Word'),\n",
        "    plt.Rectangle((0, 0), 1, 1, fc='#3498db', edgecolor='black', label='Context Words'),\n",
        "]\n",
        "ax.legend(handles=legend_elements, loc='upper right', fontsize=11)\n",
        "\n",
        "ax.set_title('Word2Vec Sliding Window: Learning from Context',\n",
        "            fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-sliding-window",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Contrastive Learning: Meaning Through Negation\n",
        "\n",
        "But there's a problem. If you only train the model to predict which words appear together, it might learn that all words are related to all other words (since technically, any word could appear anywhere). You need negative examples. You need to teach the model not just what \"dog\" appears with, but what it doesn't appear with.\n",
        "\n",
        "This is where contrastive learning enters. For each true context pair (\"brown\", \"fox\"), you generate several negative samples by randomly selecting words from the vocabulary (\"brown\", \"economics\"), (\"brown\", \"satellite\"). The model learns to maximize the similarity between true pairs and minimize the similarity between false pairs.\n",
        "\n",
        "This is Apoha theory in action. The meaning of \"brown\" emerges not from defining what \"brown\" is, but from systematically excluding what it is not. By pushing away \"economics\" and \"satellite\", the model carves out a region of semantic space that captures \"brown-ness\" through negation.\n",
        "\n",
        "::: {.column-margin}\n",
        "Negative sampling, introduced in Mikolov et al. (2013), makes word2vec computationally tractable by avoiding the expensive softmax over the full vocabulary.\n",
        ":::"
      ],
      "id": "e8ffd105"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: 'Contrastive learning: pull related words together, push unrelated words apart.'\n",
        "#| label: fig-contrastive\n",
        "#| code-fold: true\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.patches import FancyArrowPatch, Circle\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "# Initial state (before training)\n",
        "np.random.seed(42)\n",
        "target = np.array([0, 0])\n",
        "positive = np.random.randn(4, 2) * 1.5  # Context words\n",
        "negative = np.random.randn(6, 2) * 1.5  # Random words\n",
        "\n",
        "# Plot initial state\n",
        "ax1.scatter(*target, s=500, c='#e74c3c', marker='*', edgecolors='black',\n",
        "           linewidth=2, zorder=3, label='Target: \"brown\"')\n",
        "ax1.scatter(positive[:, 0], positive[:, 1], s=300, c='#3498db', alpha=0.7,\n",
        "           edgecolors='black', linewidth=1.5, zorder=2, label='Context: \"fox\", \"quick\"...')\n",
        "ax1.scatter(negative[:, 0], negative[:, 1], s=200, c='#95a5a6', alpha=0.5,\n",
        "           edgecolors='black', linewidth=1, zorder=1, label='Random: \"economics\", \"satellite\"...')\n",
        "\n",
        "# Add circle to show initial spread\n",
        "circle = Circle((0, 0), 2.5, fill=False, edgecolor='gray',\n",
        "               linestyle='--', linewidth=2, alpha=0.5)\n",
        "ax1.add_patch(circle)\n",
        "\n",
        "ax1.set_xlim(-3.5, 3.5)\n",
        "ax1.set_ylim(-3.5, 3.5)\n",
        "ax1.set_aspect('equal')\n",
        "ax1.legend(loc='upper left', fontsize=10)\n",
        "ax1.set_title('Before Training: Random Positions', fontsize=14, fontweight='bold')\n",
        "ax1.grid(alpha=0.3, linestyle='--')\n",
        "ax1.axhline(0, color='k', linewidth=0.5, alpha=0.3)\n",
        "ax1.axvline(0, color='k', linewidth=0.5, alpha=0.3)\n",
        "\n",
        "# After training\n",
        "# Positive samples moved closer\n",
        "positive_trained = positive * 0.4 + target * 0.6\n",
        "# Negative samples pushed away\n",
        "negative_trained = negative * 1.8\n",
        "\n",
        "ax2.scatter(*target, s=500, c='#e74c3c', marker='*', edgecolors='black',\n",
        "           linewidth=2, zorder=3, label='Target: \"brown\"')\n",
        "ax2.scatter(positive_trained[:, 0], positive_trained[:, 1], s=300,\n",
        "           c='#3498db', alpha=0.7, edgecolors='black', linewidth=1.5,\n",
        "           zorder=2, label='Context words (pulled close)')\n",
        "ax2.scatter(negative_trained[:, 0], negative_trained[:, 1], s=200,\n",
        "           c='#95a5a6', alpha=0.5, edgecolors='black', linewidth=1,\n",
        "           zorder=1, label='Random words (pushed away)')\n",
        "\n",
        "# Draw arrows showing movement\n",
        "for i in range(len(positive)):\n",
        "    arrow = FancyArrowPatch(positive[i], positive_trained[i],\n",
        "                          arrowstyle='->', mutation_scale=15, linewidth=1.5,\n",
        "                          color='#3498db', alpha=0.6)\n",
        "    ax2.add_patch(arrow)\n",
        "\n",
        "for i in range(len(negative)):\n",
        "    arrow = FancyArrowPatch(negative[i], negative_trained[i],\n",
        "                          arrowstyle='->', mutation_scale=15, linewidth=1.5,\n",
        "                          color='#95a5a6', alpha=0.4)\n",
        "    ax2.add_patch(arrow)\n",
        "\n",
        "# Add tight circle showing cluster formation\n",
        "circle2 = Circle((0, 0), 0.8, fill=False, edgecolor='#3498db',\n",
        "                linestyle='--', linewidth=2.5, alpha=0.7)\n",
        "ax2.add_patch(circle2)\n",
        "\n",
        "ax2.set_xlim(-3.5, 3.5)\n",
        "ax2.set_ylim(-3.5, 3.5)\n",
        "ax2.set_aspect('equal')\n",
        "ax2.legend(loc='upper left', fontsize=10)\n",
        "ax2.set_title('After Training: Meaning Emerges from Contrast',\n",
        "             fontsize=14, fontweight='bold')\n",
        "ax2.grid(alpha=0.3, linestyle='--')\n",
        "ax2.axhline(0, color='k', linewidth=0.5, alpha=0.3)\n",
        "ax2.axvline(0, color='k', linewidth=0.5, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-contrastive",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The math is elegant. The probability that word $j$ appears in the context of word $i$ is:\n",
        "\n",
        "$$\n",
        "P(j \\vert i) = \\frac{\\exp(\\mathbf{u}_i \\cdot \\mathbf{v}_j)}{\\sum_{k=1}^{V} \\exp(\\mathbf{u}_i \\cdot \\mathbf{v}_k)}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{u}_i$ is the target word's vector and $\\mathbf{v}_j$ is the context word's vector. The dot product $\\mathbf{u}_i \\cdot \\mathbf{v}_j$ measures their alignment. Higher dot product means higher probability. The model adjusts vectors to maximize this probability for true pairs and minimize it for negative samples.\n",
        "\n",
        "This is structuralism made computable. The entire semantic system emerges from patterns of co-occurrence and exclusion, with no predefined categories or boundaries.\n",
        "\n",
        "## The Magic of Vector Arithmetic\n",
        "\n",
        "Once you have vectors, you can do arithmetic on meaning. The famous example:\n",
        "\n",
        "$$ \\vec{\\text{King}} - \\vec{\\text{Man}} + \\vec{\\text{Woman}} \\approx \\vec{\\text{Queen}} $$\n",
        "\n",
        "This works because relationships are directions in space. The vector from \"Man\" to \"King\" represents the relationship \"royal-version-of\". When you apply that same direction to \"Woman\", you arrive near \"Queen\"."
      ],
      "id": "400af47e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Load pre-trained embeddings\n",
        "model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "# Vector arithmetic\n",
        "result = model.most_similar(positive=['king', 'woman'],\n",
        "                           negative=['man'], topn=5)\n",
        "\n",
        "print(\"king - man + woman =\")\n",
        "for word, similarity in result:\n",
        "    print(f\"  {word:15s} {similarity:.3f}\")"
      ],
      "id": "27fa90bd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-cap: Relationships become consistent directions in vector space. Geography is encoded as geometry.\n",
        "#| label: fig-vector-arithmetic\n",
        "#| code-fold: true\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Get word vectors\n",
        "words_countries = ['France', 'Germany', 'Italy', 'Spain']\n",
        "words_capitals = ['Paris', 'Berlin', 'Rome', 'Madrid']\n",
        "\n",
        "vectors_countries = np.array([model[w] for w in words_countries])\n",
        "vectors_capitals = np.array([model[w] for w in words_capitals])\n",
        "\n",
        "# Reduce to 2D\n",
        "all_vectors = np.vstack([vectors_countries, vectors_capitals])\n",
        "pca = PCA(n_components=2)\n",
        "coords = pca.fit_transform(all_vectors)\n",
        "\n",
        "coords_countries = coords[:len(words_countries)]\n",
        "coords_capitals = coords[len(words_countries):]\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "# Plot points\n",
        "for i, (country, capital) in enumerate(zip(words_countries, words_capitals)):\n",
        "    # Country\n",
        "    ax.scatter(coords_countries[i, 0], coords_countries[i, 1],\n",
        "              s=400, c='#e74c3c', marker='o', edgecolors='black',\n",
        "              linewidth=2, alpha=0.8, zorder=3)\n",
        "    ax.text(coords_countries[i, 0], coords_countries[i, 1] - 0.15,\n",
        "           country, ha='center', va='top', fontsize=13, fontweight='bold')\n",
        "\n",
        "    # Capital\n",
        "    ax.scatter(coords_capitals[i, 0], coords_capitals[i, 1],\n",
        "              s=400, c='#3498db', marker='s', edgecolors='black',\n",
        "              linewidth=2, alpha=0.8, zorder=3)\n",
        "    ax.text(coords_capitals[i, 0], coords_capitals[i, 1] + 0.15,\n",
        "           capital, ha='center', va='bottom', fontsize=13, fontweight='bold')\n",
        "\n",
        "    # Arrow from country to capital\n",
        "    ax.annotate('', xy=(coords_capitals[i, 0], coords_capitals[i, 1]),\n",
        "               xytext=(coords_countries[i, 0], coords_countries[i, 1]),\n",
        "               arrowprops=dict(arrowstyle='->', lw=2.5, color='gray', alpha=0.7))\n",
        "\n",
        "ax.set_aspect('equal')\n",
        "ax.grid(alpha=0.3, linestyle='--')\n",
        "ax.set_title('The \"Capital Of\" Relationship as Parallel Vectors',\n",
        "            fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "# Add legend\n",
        "from matplotlib.patches import Patch\n",
        "legend_elements = [\n",
        "    Patch(facecolor='#e74c3c', edgecolor='black', label='Countries'),\n",
        "    Patch(facecolor='#3498db', edgecolor='black', label='Capitals'),\n",
        "]\n",
        "ax.legend(handles=legend_elements, loc='upper left', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-vector-arithmetic",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The arrows from countries to capitals are parallel. They point in roughly the same direction because they represent the same relationship. This structure wasn't programmed in. It emerged from observing how words co-occur in text. The model discovered that \"France\" and \"Paris\" appear in similar contexts to how \"Germany\" and \"Berlin\" appear, and encoded this as geometric parallelism.\n",
        "\n",
        "## Dissolving Boundaries\n",
        "\n",
        "Let's return to where we started. Traditional classification forces the world into boxes. Word embeddings work differently. They map concepts into a continuous space where boundaries are soft and meaning is relational.\n",
        "\n",
        "Consider political ideology. Instead of \"left\" vs \"right\", imagine each political position as a point in a high-dimensional space. Two politicians might be close on economic policy but far apart on social issues. Their overall similarity is measured by Euclidean distance across all dimensions. There's no moment where someone crosses from \"liberal\" to \"conservative\"—there's only smooth variation."
      ],
      "id": "d6209d99"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Political positions as continuous coordinates rather than discrete labels.\n",
        "#| label: fig-political-space\n",
        "#| code-fold: true\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate synthetic political positions\n",
        "n_points = 40\n",
        "economic = np.random.randn(n_points)\n",
        "social = np.random.randn(n_points)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "# Left: Traditional view (quadrants)\n",
        "ax1.scatter(economic, social, s=200, c='#95a5a6', alpha=0.6,\n",
        "           edgecolors='black', linewidth=1)\n",
        "\n",
        "# Draw boundaries\n",
        "ax1.axhline(0, color='black', linewidth=2)\n",
        "ax1.axvline(0, color='black', linewidth=2)\n",
        "\n",
        "# Label quadrants\n",
        "ax1.text(-1.5, 1.5, 'Left-Liberal', fontsize=14, fontweight='bold',\n",
        "        ha='center', bbox=dict(boxstyle='round', facecolor='lightblue'))\n",
        "ax1.text(1.5, 1.5, 'Right-Liberal', fontsize=14, fontweight='bold',\n",
        "        ha='center', bbox=dict(boxstyle='round', facecolor='lightcoral'))\n",
        "ax1.text(-1.5, -1.5, 'Left-Conservative', fontsize=14, fontweight='bold',\n",
        "        ha='center', bbox=dict(boxstyle='round', facecolor='lightgreen'))\n",
        "ax1.text(1.5, -1.5, 'Right-Conservative', fontsize=14, fontweight='bold',\n",
        "        ha='center', bbox=dict(boxstyle='round', facecolor='lightyellow'))\n",
        "\n",
        "ax1.set_xlabel('Economic Policy →', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel('Social Policy →', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Traditional View: Discrete Quadrants', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlim(-3, 3)\n",
        "ax1.set_ylim(-3, 3)\n",
        "ax1.grid(alpha=0.3, linestyle='--')\n",
        "\n",
        "# Right: Continuous view\n",
        "scatter = ax2.scatter(economic, social, s=200, c=np.sqrt(economic**2 + social**2),\n",
        "                     cmap='viridis', alpha=0.7, edgecolors='black', linewidth=1)\n",
        "\n",
        "# Draw smooth gradient circles\n",
        "for radius in [0.5, 1.0, 1.5, 2.0]:\n",
        "    circle = plt.Circle((0, 0), radius, fill=False, edgecolor='gray',\n",
        "                       linestyle='--', linewidth=1, alpha=0.3)\n",
        "    ax2.add_patch(circle)\n",
        "\n",
        "ax2.set_xlabel('Economic Policy →', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('Social Policy →', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('Embedding View: Continuous Space', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlim(-3, 3)\n",
        "ax2.set_ylim(-3, 3)\n",
        "ax2.grid(alpha=0.3, linestyle='--')\n",
        "\n",
        "# Add colorbar\n",
        "cbar = plt.colorbar(scatter, ax=ax2)\n",
        "cbar.set_label('Distance from Center', fontsize=11)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-political-space",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This isn't just philosophically satisfying. It's practically powerful. When you preserve the continuous structure of reality, you can answer questions that categorical systems can't handle. What's 70% of the way between two concepts? Which items are on the boundary of a cluster? How does meaning shift gradually across a spectrum? Embeddings make these questions answerable.\n",
        "\n",
        "## Beyond Words\n",
        "\n",
        "The word2vec insight generalizes. Any discrete symbol in a structured system can be embedded. Users in a social network. Products in a catalog. Genes in a regulatory network. Nodes in any graph. The technique is the same: observe co-occurrence patterns, learn representations where similar entities end up nearby, and dissolve the artificial boundaries that discrete labels impose.\n",
        "\n",
        "This is the representational turn in machine learning. Instead of building classifiers that make decisions, we build encoders that map concepts into continuous space. The space itself becomes the knowledge. Relationships emerge as geometry. Similarity becomes distance. Analogy becomes vector arithmetic.\n",
        "\n",
        "The next section explores how this perspective extends beyond text to images, graphs, time series, and more. The structuralist insight—that meaning is relational and boundaries are observer-dependent—becomes a universal principle for understanding complex systems through their embedded representations.\n",
        "\n",
        "::: {.callout-tip title=\"Try it yourself\"}\n",
        "Load a pre-trained word2vec model and explore its semantic neighborhoods. Start with a concept you care about and query its nearest neighbors. Then try vector arithmetic with analogies. What relationships does the model capture? What does it miss? This hands-on exploration reveals both the power and limitations of learned representations.\n",
        ":::"
      ],
      "id": "95def96c"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/skojaku-admin/Documents/projects/applied-soft-comp/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
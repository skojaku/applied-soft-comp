---
title: "Part 3: From Categories to Coordinates"
jupyter: python3
execute:
    enabled: true
    cache: true
---

::: {.callout-note title="What you'll learn in this module"}
This module introduces vector embeddings as the computational realization of structuralism.

You'll learn:

- What **vector embeddings** are and how they dissolve artificial category boundaries.
- How **word2vec** learns meaning from context through the distributional hypothesis.
- The role of **contrastive learning** and negative sampling in carving semantic space.
- Why **vector arithmetic** captures relationships as geometric directions.
- The counterintuitive insight that meaning emerges from exclusion (Apoha theory) and the notion of **continuous representation**.
- Practical consequences for understanding political ideology, semantic similarity, and beyond.
:::

## The Limits of Traditional Classification

Let's talk about how traditional machine learning builds decision boundaries. You collect labeled data, extract features, and train a classifier to draw a line (or hyperplane) separating Class A from Class B. The output is binary: spam or not-spam, fraud or legitimate, cat or dog.

This approach works when categories are genuinely discrete. But what about political ideology? You might lean liberal on economic issues but conservative on others. Your position isn't "left or right". It's a point in a multidimensional space. Forcing it into a binary choice destroys information.

What about word meanings? Is "dog" more similar to "wolf" or "cat"? The answer is "it depends". Dogs and wolves are biologically closer (same genus). Dogs and cats are domestically closer (both pets).

A classification system would force you to choose one dimension. A continuous representation can capture both simultaneously.

## Enter Vector Embeddings

What's the solution? Vector embeddings solve this by mapping each concept to coordinates in a high-dimensional continuous space. Instead of "this word belongs to category X", you get "this word lives at position $[0.23, -0.15, 0.87, ...]$ in 300-dimensional space". Similarity becomes geometric distance. Relationships become directional vectors.

This is the mathematical realization of structuralism. Each word's meaning is determined by its position relative to all other words. There are no hard boundaries, only neighborhoods of varying density.

The gradation that exists in reality is preserved in the representation. Notice the overlap between domestic and wild animals. "Dog" is close to "wolf" because they're related species. But "dog" is also in the domestic cluster near "cat". The space naturally captures multiple dimensions of similarity without forcing categorical choices.

## How Word2Vec Works: Context is Everything

How does word2vec learn these embeddings? It learns from raw text without any human-provided definitions or labels. The core insight: a word's meaning is determined by the company it keeps. Words that appear in similar contexts have similar meanings.

This is pure metonymy in action. We're not grouping words by what they are (metaphor), but by where they appear (contiguity). "Dog" means what it means because it shows up near "bark", "pet", "leash", "puppy", "tail". If another word appears in the same neighborhood of contexts, it must mean something similar.

This principle is called the distributional hypothesis, formalized by Zellig Harris (1954) and J.R. Firth (1957): "You shall know a word by the company it keeps." Word2vec is its computational realization.

The algorithm slides a window over text, creating training pairs. If the window is size 2 and you encounter "The quick brown fox", you get pairs like:

- (brown, The)
- (brown, quick)
- (brown, fox)

The word in the center (brown) is the target. The surrounding words are the context. Word2vec learns to predict context from target, or target from context.

In doing so, it builds vector representations where words with similar contexts end up nearby.


## Contrastive Learning: Meaning Through Negation

But there's a problem. If you only train the model to predict which words appear together, it might learn that all words are related to all other words (since technically, any word could appear anywhere). You need negative examples. You need to teach the model not just what "dog" appears with, but what it doesn't appear with.

What's the solution? This is where contrastive learning enters. For each true context pair ("brown", "fox"), you generate several negative samples by randomly selecting words from the vocabulary ("brown", "economics"), ("brown", "satellite"). The model learns to maximize the similarity between true pairs and minimize the similarity between false pairs.

This is Apoha theory in action. The meaning of "brown" emerges not from defining what "brown" is, but from systematically excluding what it is not. By pushing away "economics" and "satellite", the model carves out a region of semantic space that captures "brown-ness" through negation.

This technique is called negative sampling, introduced in Mikolov et al. (2013). It makes word2vec computationally tractable by avoiding the expensive softmax over the full vocabulary.

Let's walk through the math. The probability that word $j$ appears in the context of word $i$ is:

$$
P(j \vert i) = \frac{\exp(\mathbf{u}_i \cdot \mathbf{v}_j)}{\sum_{k=1}^{V} \exp(\mathbf{u}_i \cdot \mathbf{v}_k)}
$$

Here $\mathbf{u}_i$ is the target word's vector and $\mathbf{v}_j$ is the context word's vector. The dot product $\mathbf{u}_i \cdot \mathbf{v}_j$ measures their alignment. Higher dot product means higher probability.

The model adjusts vectors to maximize this probability for true pairs and minimize it for negative samples. This is structuralism made computable. The entire semantic system emerges from patterns of co-occurrence and exclusion, with no predefined categories or boundaries.

## The Magic of Vector Arithmetic

Once you have vectors, you can do arithmetic on meaning. The famous example:

$$ \vec{\text{King}} - \vec{\text{Man}} + \vec{\text{Woman}} \approx \vec{\text{Queen}} $$

Why does this work? Relationships are directions in space. The vector from "Man" to "King" represents the relationship "royal-version-of". When you apply that same direction to "Woman", you arrive near "Queen".

The arrows from countries to capitals are parallel. They point in roughly the same direction because they represent the same relationship. This structure wasn't programmed in.

It emerged from observing how words co-occur in text. The model discovered that "France" and "Paris" appear in similar contexts to how "Germany" and "Berlin" appear, and encoded this as geometric parallelism.

## Dissolving Boundaries

Let's return to where we started. Traditional classification forces the world into boxes. Word embeddings work differently. They map concepts into a continuous space where boundaries are soft and meaning is relational.

Consider political ideology. Instead of "left" vs "right", imagine each political position as a point in a high-dimensional space. Two politicians might be close on economic policy but far apart on social issues.

Their overall similarity is measured by Euclidean distance across all dimensions. There's no moment where someone crosses from "liberal" to "conservative". There's only smooth variation.

This isn't just philosophically satisfying. It's practically powerful. When you preserve the continuous structure of reality, you can answer questions that categorical systems can't handle.

What's 70% of the way between two concepts? Which items are on the boundary of a cluster? How does meaning shift gradually across a spectrum? Embeddings make these questions answerable.

## Beyond Words

The word2vec insight generalizes. Any discrete symbol in a structured system can be embedded. Users in a social network. Products in a catalog. Genes in a regulatory network. Nodes in any graph.

The technique is the same: observe co-occurrence patterns, learn representations where similar entities end up nearby, and dissolve the artificial boundaries that discrete labels impose.

This is the representational turn in machine learning. Instead of building classifiers that make decisions, we build encoders that map concepts into continuous space. The space itself becomes the knowledge.

Relationships emerge as geometry. Similarity becomes distance. Analogy becomes vector arithmetic.

The next section explores how this perspective extends beyond text to images, graphs, time series, and more. The structuralist insight (that meaning is relational and boundaries are observer-dependent) becomes a universal principle for understanding complex systems through their embedded representations.

::: {.callout-tip title="Try it yourself"}
Load a pre-trained word2vec model and explore its semantic neighborhoods. Start with a concept you care about and query its nearest neighbors. Then try vector arithmetic with analogies. What relationships does the model capture? What does it miss? This hands-on exploration reveals both the power and limitations of learned representations.
:::

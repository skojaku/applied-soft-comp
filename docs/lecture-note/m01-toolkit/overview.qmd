---
title: "Overview"
---

::: {.callout-note title="What you'll learn in this module"}
This module introduces the tools and principles of reproducible data science. We will explore version control with Git and GitHub to track changes and collaborate effectively. We will examine data provenance and tidy data principles to ensure your data history is clear and your structure is sound. Finally, we will learn how to build reproducible environments so others can replicate your work exactly.
:::

## The Reproducibility Crisis

Imagine spending months on a groundbreaking data analysis, only to find that you can't reproduce your own results. Or imagine a colleague asks for your code and data from a project you did last year, and you suddenly realize you can't remember where you saved the files or which version produced the final answer. These scenarios are all too common in data science.

What ties all these stories together is the need for something called **provenance**. This is simply a complete lineage of the data and code from its origin to its final form. It's the backbone of good science, allowing others to verify your findings and build upon your work.

This module will teach you the tools and principles that make reproducible data science pipelines possible. A little bit of organization upfront can save you hours of pain down the road.

## Version Control

Let's start with version control. Without proper tracking, things can spiral quickly. You might lose days of work to an accidental overwrites, or worse, a security breach because no one knows which version of the code is actually running. Version control transforms chaos into clarity.

Git and GitHub are the tools that make this possible. They let you track changes in your code and data, collaborate with others without stepping on each other's toes, and recover from mistakes. Ready to dive deeper? Learn more about [Version Control with Git & GitHub](git-github.qmd).

## Data Provenance and Tidy Data

Now shift your attention from the tools to the data itself. You need two things: knowing the history of your data (called data provenance) and structuring it in a way that makes analysis straightforward (called tidy data).

Knowing where your data came from, how it was collected, and what transformations were applied is critical for trust and reproducibility. When you structure data tidily, analysis becomes faster, clearer, and less error-prone. Ready to explore? Check out [Data Provenance](data-provenance.qmd) and [Tidy Data](tidy-data.qmd).

## Reproducible Environments

Now let's think about the environment where your code actually runs. Your code might work perfectly on your machine today, but will it run on your colleague's machine tomorrow? Will it run on your own machine six months from now after you've updated libraries and tools?

Reproducible environments ensure that your work can be replicated exactly, no matter where or when it runs. This is the final piece of the reproducibility puzzle. Ready to complete the picture? Discover [Reproducible Environments & Projects](reproduceability.qmd).

## What You'll Gain

By the end of this module, you'll have a solid foundation in reproducible data science. You'll understand how to track changes systematically with version control, structure your data clearly for analysis, and build environments that others can replicate. These practices will make you a more effective and trustworthy collaborator in any data science team.

---
title: "High-Dimensional Data Visualization"
execute:
    enabled: true
---

::: {.callout-note title="What you'll learn in this module"}
This module introduces dimensionality reduction, a fundamental technique for visualizing and understanding high-dimensional data.

You'll learn:

- What **the curse of dimensionality** means and why high-dimensional space is counterintuitive.
- How to use **PCA** to preserve global variance structure in linear projections.
- The difference between **local methods** (t-SNE, UMAP) and **global methods** (MDS, Isomap).
- How to **choose the right method** for your data and avoid misleading visualizations.
:::

Imagine you're analyzing data with 50 features per observation: gene expression levels, user behavior metrics, environmental measurements. You want to understand the patterns, clusters, and outliers. Here's the fundamental problem: you can't plot 50 dimensions directly. Our visual system lives in three dimensions, or really two on a screen.

The answer is dimensionality reduction. This technique projects high-dimensional data into 2 or 3 dimensions while preserving important structure. But here's the critical question: what structure matters? Some methods preserve global structure, showing how groups relate across the entire dataset. Others preserve local structure, highlighting nearest neighbors. Understanding these trade-offs is essential for choosing the right method and avoiding beautiful but misleading visualizations.

## The Curse of Dimensionality

Let's talk about what makes high-dimensional data fundamentally different. In high dimensions, everything is far from everything else. This sounds paradoxical, but it's mathematically inevitable. As dimensions increase, the volume of space grows exponentially, and data points become increasingly sparse.

Consider this simple fact: In 1D, if you have 10 points uniformly distributed in [0, 1], the average distance between neighbors is about 0.1. To maintain the same density in 2D, you need 100 points. In 3D, you need 1,000 points. In 10D, you need 10 billion points. Even stranger is what happens to distances. In high dimensions, all distances become similar, the nearest and farthest neighbors becoming roughly equidistant. This makes many of our intuitions about "closeness" break down.

```{python}
#| fig-cap: "As dimensions increase, the ratio of farthest to nearest distance approaches 1"
#| fig-width: 10
#| fig-height: 5
#| code-fold: true
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.metrics.pairwise import euclidean_distances

sns.set_style("white")
np.random.seed(42)

# Calculate distance ratio across dimensions
dimensions = [2, 5, 10, 20, 50, 100, 200]
n_samples = 100
ratios = []

for d in dimensions:
    # Generate random data
    X = np.random.randn(n_samples, d)
    # Calculate all pairwise distances
    distances = euclidean_distances(X)
    # For each point, find nearest and farthest (excluding self)
    np.fill_diagonal(distances, np.inf)  # Ignore self-distance
    nearest = distances.min(axis=1)
    # For "farthest," ignore inf (self-distance), so set inf entries to -1 and use argmax
    temp = distances.copy()
    temp[temp == np.inf] = -1  # Now maximum is truly among finite values
    farthest = temp.max(axis=1)
    # Calculate ratio
    ratio = nearest / farthest
    ratios.append(ratio)

# Plot
sns.set(font_scale=2.0)
sns.set_style("white")

blue, red = sns.color_palette('muted', 2)

fig, ax = plt.subplots(figsize=(10, 5))
positions = range(len(dimensions))
bp = ax.boxplot(ratios, positions=positions, widths=0.6, patch_artist=True,
                boxprops=dict(facecolor="#f2f2f2", alpha=0.7))
ax.set_xticklabels(dimensions)
ax.set_xlabel('Number of Dimensions')
ax.set_ylabel('Nearest Distance / Farthest Distance')
ax.set_title('The Curse of Dimensionality: All Points Become Equidistant')
ax.axhline(y=1.0, color=red, linestyle='--', alpha=0.5, label='Equal distances')
ax.legend(frameon=False)
sns.despine()
```

The plot shows a striking pattern: As dimensions increase, the ratio of nearest to farthest distance gets closer to 1. At 200 dimensions, nearly every point is equally far from every other point. This makes clustering impossible since every point becomes equidistant from every other point. By projecting the data into lower dimensions, you can remedy this problem. This is why dimensionality reduction matters for analysis, not just visualization.

## Pairwise Scatter Plots: The Brute Force Approach

When you have a moderate number of dimensions (roughly 3 to 10), you can visualize all pairwise relationships using a scatter plot matrix, also called a pairs plot or SPLOM.

```{python}
#| fig-cap: "Scatter plot matrix showing all pairwise relationships in the Iris dataset"
#| fig-width: 14
#| fig-height: 14
#| code-fold: true
# Load classic iris dataset (4 dimensions)
from sklearn.datasets import load_iris

sns.set(font_scale=1.0)
sns.set_style("white")

iris = load_iris()
iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)
iris_df['species'] = iris.target
iris_df['species'] = iris_df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})

# Create pairplot
g = sns.pairplot(iris_df, hue='species', diag_kind='kde',
                 plot_kws={'alpha': 0.6, 's': 50, 'edgecolor': 'white', 'linewidth': 0.5},
                 diag_kws={'alpha': 0.7, 'linewidth': 2})
g.fig.suptitle('Iris Dataset: All Pairwise Relationships', y=1.01)
```

The scatter plot matrix shows every possible 2D projection: the diagonal displays the univariate distribution of each feature using KDE, while off-diagonals show bivariate scatter plots. The problem is clear: scatter plot matrices don't scale. With 10 variables, you have 45 unique pairwise plots (manageable but crowded). With 20 variables, you have 190 plots (overwhelming). And you're still only seeing 2D projections, never the full high-dimensional structure. This is where dimensionality reduction becomes essential, projecting the data intelligently onto just 2 or 3 dimensions.

## Linear Dimensionality Reduction: PCA

Let's talk about Principal Component Analysis (PCA), a linear method that finds directions of maximum variance in your data. Imagine you have a cloud of points in high-dimensional space. PCA asks: what direction captures the most variation (PC1)? Then: what direction, perpendicular to the first, captures the most remaining variation (PC2)? Mathematically, PCA finds the eigenvectors of the covariance matrix. Conceptually, it's rotating your coordinate system to align with the highest variance directions of your data.

**The Math**:
If we center our data matrix $X$ (so columns have zero mean), the covariance matrix is $C = \frac{1}{n-1} X^T X$. We want to find a vector $v$ that maximizes the variance of the projection $Xv$. This turns out to be an eigenvalue problem:
$$ C v = \lambda v $$
The eigenvector $v$ with the largest eigenvalue $\lambda$ is the first principal component. The eigenvalue $\lambda$ represents the amount of variance captured by that component.

```{python}
#| fig-cap: "PCA finds directions of maximum variance. PC1 captures the most variation, PC2 the next most (perpendicular to PC1)."
#| fig-width: 10
#| fig-height: 6
#| code-fold: true
from sklearn.decomposition import PCA

# Generate correlated 2D data (for visualization)
np.random.seed(123)
mean = [0, 0]
cov = [[3, 2], [2, 2]]
data_2d = np.random.multivariate_normal(mean, cov, 300)

# Fit PCA
pca = PCA(n_components=2)
pca.fit(data_2d)

colors = ["#f2f2f2", sns.color_palette('muted')[0], sns.color_palette('muted')[3]]

# Plot original data with principal components
fig, ax = plt.subplots(figsize=(10, 6))
ax.scatter(data_2d[:, 0], data_2d[:, 1], alpha=0.9, s=50, color=colors[0], edgecolors='k', linewidth=0.5)

# Draw principal components as arrows
origin = pca.mean_
for i, (component, variance) in enumerate(zip(pca.components_, pca.explained_variance_)):
    direction = component * np.sqrt(variance) * 3  # Scale for visibility
    ax.arrow(origin[0], origin[1], direction[0], direction[1],
             head_width=0.3, head_length=0.3, fc=colors[i+1], ec=colors[i+1], linewidth=3,
             label=f'PC{i+1} ({variance/pca.explained_variance_.sum()*100:.1f}%)')

ax.set_xlabel('Original X')
ax.set_ylabel('Original Y')
ax.set_title('Principal Components: Directions of Maximum Variance')
ax.legend()
ax.axis('equal')
sns.despine()
```

PC1 (orange arrow) points along the direction of greatest spread, while PC2 (green arrow) is perpendicular and captures the remaining variation. The percentage shows how much variance each component explains. If PC1 explains 90 percent of variance, projecting onto just PC1 preserves most of your data's structure.

### Applying PCA to Iris

Let's apply PCA to the 4-dimensional Iris dataset and see how much information we can preserve in just 2 dimensions.

```{python}
#| fig-cap: "PCA projection of Iris dataset to 2D preserves the separation between species"
#| fig-width: 14
#| fig-height: 6
#| code-fold: true
# Prepare data
X = iris.data
y = iris.target

# Standardize (important for PCA!)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Create DataFrame for plotting
pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])
pca_df['species'] = iris.target_names[y]

# Plot
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Left: PCA projection
for species, color in zip(['setosa', 'versicolor', 'virginica'], sns.color_palette('muted', 3)):
    mask = pca_df['species'] == species
    axes[0].scatter(pca_df.loc[mask, 'PC1'], pca_df.loc[mask, 'PC2'],
                   label=species, alpha=0.7, s=50, color=color, edgecolors='white', linewidth=0.5)
axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)')
axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)')
axes[0].set_title('PCA Projection of Iris Dataset')
axes[0].legend()
sns.despine(ax=axes[0])

# Right: Variance explained
variances = pca.explained_variance_ratio_
axes[1].bar([1, 2], variances, color=sns.color_palette('muted', 2), alpha=0.7)
axes[1].set_xlabel('Principal Component')
axes[1].set_ylabel('Variance Explained')
axes[1].set_title('Variance Explained by Each Component')
axes[1].set_xticks([1, 2])
axes[1].set_xticklabels(['PC1', 'PC2'])
for i, v in enumerate(variances):
    axes[1].text(i+1, v+0.01, f'{v*100:.1f}%', ha='center', va='bottom', fontsize=11)
sns.despine(ax=axes[1])

plt.tight_layout()
```

PC1 and PC2 together explain over 95 percent of the variance in the 4D dataset, with the 2D projection preserving the main structure beautifully (Setosa well-separated, versicolor and virginica showing some overlap just as in the original high-dimensional space). A critical reminder: always standardize before PCA. If features have different units or scales, PCA will be dominated by high-variance features. Standardization (zero mean, unit variance) ensures all features contribute fairly.

## Non-Linear Dimensionality Reduction: MDS

Shift your attention from variance to distances. Multidimensional Scaling (MDS) takes a different approach than PCA: instead of finding directions of maximum variance, it tries to preserve distances between points. You give MDS a distance matrix showing the distance between every pair of points in high-dimensional space, and MDS finds a low-dimensional configuration where those distances are preserved as well as possible. Think of it like arranging cities on a map. You know the distance between every pair of cities, but not their coordinates. MDS finds positions that preserve those distances.

**The Math**:
Given a high-dimensional distance matrix $D$ where $d_{ij}$ is the distance between points $i$ and $j$, MDS seeks low-dimensional coordinates $y_1, \dots, y_n$ that minimize the **Stress** function:
$$ \text{Stress} = \sqrt{ \frac{\sum_{i<j} (d_{ij} - \|y_i - y_j\|)^2}{\sum_{i<j} d_{ij}^2} } $$
Minimizing this stress forces the Euclidean distances in the low-dimensional map $\|y_i - y_j\|$ to approximate the original distances $d_{ij}$.

```{python}
#| fig-cap: "MDS vs PCA on Iris dataset. MDS preserves distances better but looks similar to PCA for this dataset."
#| fig-width: 14
#| fig-height: 6
#| code-fold: true
from sklearn.manifold import MDS

# Suppress FutureWarning about n_init in MDS
import warnings
mds = MDS(n_components=2, random_state=42, n_init=1)
X_mds = mds.fit_transform(X_scaled)

# Create DataFrame
mds_df = pd.DataFrame(X_mds, columns=['MDS1', 'MDS2'])
mds_df['species'] = iris.target_names[y]

# Plot both
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# PCA
for species, color in zip(['setosa', 'versicolor', 'virginica'], sns.color_palette('muted', 3)):
    mask = pca_df['species'] == species
    axes[0].scatter(pca_df.loc[mask, 'PC1'], pca_df.loc[mask, 'PC2'],
                   label=species, alpha=0.7, s=50, color=color, edgecolors='white', linewidth=0.5)
axes[0].set_xlabel('PC1')
axes[0].set_ylabel('PC2')
axes[0].set_title('PCA: Maximizes Variance')
axes[0].legend()
sns.despine(ax=axes[0])

# MDS
for species, color in zip(['setosa', 'versicolor', 'virginica'], sns.color_palette('muted', 3)):
    mask = mds_df['species'] == species
    axes[1].scatter(mds_df.loc[mask, 'MDS1'], mds_df.loc[mask, 'MDS2'],
                   label=species, alpha=0.7, s=50, color=color, edgecolors='white', linewidth=0.5)
axes[1].set_xlabel('MDS1')
axes[1].set_ylabel('MDS2')
axes[1].set_title('MDS: Preserves Distances')
axes[1].legend()
sns.despine(ax=axes[1])

plt.tight_layout()
```

For the Iris dataset, PCA and MDS look very similar. This is because Iris data is fairly linear. The relationships between features don't involve complex curves or non-linear structures that would cause MDS to differ significantly from PCA.

## Isomap: Preserving Geodesic Distances

MDS preserves Euclidean distances (straight-line distances through space). But for curved manifolds, what matters is the geodesic distance (the distance along the surface). Isomap (Isometric Mapping) addresses this by approximating geodesic distances using a neighborhood graph. First, build a neighborhood graph by connecting each point to its k nearest neighbors. Second, compute shortest paths through this graph (the geodesic distance approximated by the shortest path). Third, apply classical MDS using these geodesic distances instead of Euclidean distances. Think of it like this: MDS measures distance "as the crow flies," while Isomap measures distance "as you walk along the surface."

**The Math**:
Isomap transforms the problem into MDS by changing the distance metric:
1.  **Construct a Neighborhood Graph**: Connect point $i$ to $j$ if $d_{ij} < \epsilon$ (epsilon-Isomap) or if $j$ is among the $k$-nearest neighbors (k-Isomap).
2.  **Compute Geodesic Distances**: Define the geodesic distance $d_G(i, j)$ as the shortest path distance between nodes $i$ and $j$ in the graph (using Dijkstra's or Floyd-Warshall algorithm).
3.  **Apply MDS**: Run Classical MDS on the matrix of geodesic distances $D_G$.

```{python}
#| fig-cap: "Isomap uses geodesic distances (along the surface) instead of Euclidean distances (through space), better recovering the S-curve structure"
#| fig-width: 14
#| fig-height: 6
#| code-fold: true
from sklearn.manifold import Isomap
from sklearn.datasets import make_s_curve

# Generate S-curve data (a 2D manifold embedded in 3D)
n_samples = 1000
X_scurve, color = make_s_curve(n_samples, noise=0.1, random_state=42)

# Apply MDS
mds_scurve = MDS(n_components=2, random_state=42, n_init=1)
X_scurve_mds = mds_scurve.fit_transform(X_scurve)

# Apply Isomap
isomap = Isomap(n_components=2, n_neighbors=10)
X_scurve_isomap = isomap.fit_transform(X_scurve)

# Plot MDS vs Isomap
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# MDS
axes[0].scatter(X_scurve_mds[:, 0], X_scurve_mds[:, 1], c=color,
                cmap='viridis', alpha=0.6, s=20)
axes[0].set_xlabel('MDS1')
axes[0].set_ylabel('MDS2')
axes[0].set_title('MDS: Global Euclidean Distances')
sns.despine(ax=axes[0])

# Isomap
axes[1].scatter(X_scurve_isomap[:, 0], X_scurve_isomap[:, 1], c=color,
                cmap='viridis', alpha=0.6, s=20)
axes[1].set_xlabel('Isomap1')
axes[1].set_ylabel('Isomap2')
axes[1].set_title('Isomap: Geodesic Distances')
sns.despine(ax=axes[1])

plt.tight_layout()
```

Isomap successfully "straightens" the S-curve because it respects the manifold structure, computing distances along the neighborhood graph to avoid shortcuts across the bend that confused MDS. The key parameter is n_neighbors. Too few and the graph becomes disconnected with infinite distances. Too many and you create shortcuts across the manifold, reverting to MDS-like behavior. Getting it just right (typically 5 to 15) captures the local manifold structure perfectly.

Now we see two extremes emerging: MDS preserves all pairwise distances globally (working on linear or convex data), while Isomap preserves geodesic distances using local neighborhoods (working on curved manifolds). But what if we only care about local structure and global relationships don't matter?

## Modern Non-Linear Methods: t-SNE and UMAP

Let's shift focus to methods that prioritize local structure. Both t-SNE (t-Distributed Stochastic Neighbor Embedding) and UMAP (Uniform Manifold Approximation and Projection) take a middle ground between MDS's global approach and Isomap's geodesic approach, prioritizing local structure while allowing flexibility in global positioning. The key insight: For visualization, we often care most about which points are neighbors. Whether distant clusters are placed left versus right, or how far apart they are, matters less than preserving local neighborhood relationships within and between clusters.

### How t-SNE works

t-SNE converts distances into similarity probabilities and preserves these local relationships.

**The Math**:
1.  **High-dimensional Probabilities ($P$)**: We define the conditional probability $p_{j|i}$ using a Gaussian kernel centered at $x_i$, where $\sigma_i$ is tuned to match the desired "perplexity":
    $$ p_{j|i} = \frac{\exp(-\|x_i - x_j\|^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-\|x_i - x_k\|^2 / 2\sigma_i^2)} $$
    We symmetrize this to get $p_{ij} = \frac{p_{j|i} + p_{i|j}}{2n}$.

2.  **Low-dimensional Probabilities ($Q$)**: In the low-dimensional space $y$, we use a **Student t-distribution** with 1 degree of freedom (which has heavier tails than a Gaussian):
    $$ q_{ij} = \frac{(1 + \|y_i - y_j\|^2)^{-1}}{\sum_{k \neq l} (1 + \|y_k - y_l\|^2)^{-1}} $$

3.  **Optimization**: We minimize the **Kullback-Leibler (KL) divergence** between $P$ and $Q$ using gradient descent:
    $$ C = KL(P||Q) = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}} $$

The t-distribution's heavy tails are clever. They let well-separated clusters spread out in 2D without overlapping, while keeping local neighborhoods tight.

```{python}
#| fig-cap: "Comparing global, geodesic, and local approaches on the S-curve"
#| fig-width: 15
#| fig-height: 5
#| code-fold: true
from sklearn.manifold import TSNE

# Apply t-SNE
tsne = TSNE(n_components=2, random_state=42, perplexity=30)
X_scurve_tsne = tsne.fit_transform(X_scurve)

# Plot all three methods
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# MDS - Global Euclidean distances
axes[0].scatter(X_scurve_mds[:, 0], X_scurve_mds[:, 1], c=color,
                cmap='viridis', alpha=0.6, s=20)
axes[0].set_xlabel('MDS1')
axes[0].set_ylabel('MDS2')
axes[0].set_title('MDS: Global Distances')
sns.despine(ax=axes[0])

# Isomap - Geodesic distances
axes[1].scatter(X_scurve_isomap[:, 0], X_scurve_isomap[:, 1], c=color,
                cmap='viridis', alpha=0.6, s=20)
axes[1].set_xlabel('Isomap1')
axes[1].set_ylabel('Isomap2')
axes[1].set_title('Isomap: Geodesic Distances')
sns.despine(ax=axes[1])

# t-SNE - Local neighborhoods
axes[2].scatter(X_scurve_tsne[:, 0], X_scurve_tsne[:, 1], c=color,
                cmap='viridis', alpha=0.6, s=20)
axes[2].set_xlabel('t-SNE1')
axes[2].set_ylabel('t-SNE2')
axes[2].set_title('t-SNE: Local Structure')
sns.despine(ax=axes[2])

plt.tight_layout()
```

All three methods successfully straighten the S-curve through different philosophies: MDS compromises between all distances, Isomap follows the manifold globally, and t-SNE focuses on preserving neighborhoods. The key parameter in t-SNE is perplexity (typically 30 to 50), which controls the effective neighborhood size. Too low perplexity fragments clusters, too high perplexity loses local detail.

### What t-SNE preserves (and what it doesn't)

t-SNE is powerful but has important limitations. It preserves local structure (keeping points that are neighbors in high dimensions as neighbors in 2D), preserves clusters (keeping well-separated groups separated), and preserves relative relationships within neighborhoods (if A is closer to B than to C locally, this is preserved). What t-SNE does NOT preserve: actual distances between points, the relative position of distant clusters (arbitrary), cluster sizes (large clusters may appear smaller and vice versa), or densities (tight clusters may spread out, sparse regions may appear dense).

You cannot conclude that "cluster A is twice as far from B as from C" (distances not preserved), "cluster A is twice the size of B" (sizes not preserved), or "the data has exactly 5 clusters" (apparent clusters may be visualization artifacts). You can conclude that "these points form a distinct group separate from others," "these points are more similar to each other than to distant points," and "the data has local structure and is not uniformly random."

### Applying t-SNE to real data

Let's apply t-SNE to a more realistic high-dimensional dataset: the digits dataset, which has 64 dimensions (8 by 8 pixel images).

```{python}
#| fig-cap: "t-SNE visualization of handwritten digits (64 dimensions to 2D). Each color represents a digit class."
#| fig-width: 12
#| fig-height: 10
#| code-fold: true
from sklearn.datasets import load_digits

# Load digits dataset (8x8 images, 64 dimensions)
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Take a subset for speed (t-SNE is slow on large datasets)
np.random.seed(42)
indices = np.random.choice(len(X_digits), size=1000, replace=False)
X_subset = X_digits[indices]
y_subset = y_digits[indices]

# Apply t-SNE
tsne_digits = TSNE(n_components=2, random_state=42, perplexity=40)
X_digits_tsne = tsne_digits.fit_transform(X_subset)

# Plot
fig, ax = plt.subplots(figsize=(12, 10))
scatter = ax.scatter(X_digits_tsne[:, 0], X_digits_tsne[:, 1],
                     c=y_subset, cmap='tab10', alpha=0.7, s=30)
ax.set_xlabel('t-SNE1')
ax.set_ylabel('t-SNE2')
ax.set_title('t-SNE Visualization of Handwritten Digits (64D to 2D)')
cbar = plt.colorbar(scatter, ax=ax, ticks=range(10))
cbar.set_label('Digit Class')
sns.despine()
```

The t-SNE projection beautifully separates most digit classes, with similar digits (3, 5, and 8) clustering near each other while visually distinct digits (0 and 1) are well separated. This demonstrates t-SNE's power: from 64 dimensions with no explicit information about what makes digits similar, t-SNE discovers the perceptual structure of handwritten digits. An important note: t-SNE is stochastic. Different runs produce different layouts (though cluster structure remains consistent), so always check multiple runs with different random seeds, especially for scientific conclusions.

## UMAP: A Faster Alternative

Uniform Manifold Approximation and Projection (UMAP) is a newer method from 2018 that has become popular as an alternative to t-SNE. Like t-SNE, UMAP preserves local structure, but it's based on different mathematical foundations in manifold learning and topological data analysis. UMAP has several advantages over t-SNE: it's faster (often 10 to 100 times faster on large datasets), scales better (working well on datasets with millions of points), preserves more global structure, and is theoretically grounded in Riemannian geometry and fuzzy topology. The trade-offs: UMAP is less battle-tested (being newer), has more hyperparameters to tune (though defaults work well), and often produces similar-looking results to t-SNE, so the choice often comes down to speed.

**The Math**:
UMAP relies on fuzzy simplicial sets and a different cost function.
1.  **Local Connectivity**: It computes high-dimensional probabilities $p_{ij}$ using a specific local connectivity constraint:
    $$ p_{j|i} = \exp\left(-\frac{d(x_i, x_j) - \rho_i}{\sigma_i}\right) $$
    where $\rho_i$ is the distance to the nearest neighbor. This ensures every point is connected to at least one neighbor.
2.  **Binary Cross-Entropy**: Instead of KL divergence, UMAP minimizes the fuzzy set cross-entropy:
    $$ C = \sum_{i \neq j} \left[ p_{ij} \log \left(\frac{p_{ij}}{q_{ij}}\right) + (1 - p_{ij}) \log \left(\frac{1 - p_{ij}}{1 - q_{ij}}\right) \right] $$
    The second term $(1 - p_{ij}) \log (\dots)$ forces points that are *far* in high dimensions to also be far in low dimensions, helping UMAP preserve more global structure than t-SNE.

```{python}
#| fig-cap: "UMAP vs t-SNE on digits dataset. UMAP often preserves more global structure while being much faster."
#| fig-width: 14
#| fig-height: 6
#| code-fold: true
import umap

# Apply UMAP
umap_model = umap.UMAP(n_components=2, random_state=42, n_neighbors=30)
X_digits_umap = umap_model.fit_transform(X_subset)

# Plot comparison
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# t-SNE
scatter = axes[0].scatter(X_digits_tsne[:, 0], X_digits_tsne[:, 1],
                          c=y_subset, cmap='tab10', alpha=0.7, s=30)
axes[0].set_xlabel('t-SNE1')
axes[0].set_ylabel('t-SNE2')
axes[0].set_title('t-SNE')
sns.despine(ax=axes[0])

# UMAP
scatter = axes[1].scatter(X_digits_umap[:, 0], X_digits_umap[:, 1],
                          c=y_subset, cmap='tab10', alpha=0.7, s=30)
axes[1].set_xlabel('UMAP1')
axes[1].set_ylabel('UMAP2')
axes[1].set_title('UMAP')
cbar = plt.colorbar(scatter, ax=axes[1], ticks=range(10))
cbar.set_label('Digit Class')
sns.despine(ax=axes[1])

plt.tight_layout()
```

Both methods reveal similar cluster structure, but UMAP tends to space clusters more evenly and preserve more global topology (notice how UMAP places similar digits like 3, 5, 8 in a connected region, suggesting they share underlying structure). Use UMAP for very large datasets (over 10,000 points where t-SNE becomes slow), when you want to preserve more global structure, when you're doing exploratory analysis and want fast iteration, or when you need to project new data onto an existing embedding (which t-SNE doesn't easily support). Stick with t-SNE when you need the most established method with extensive literature, for moderate-sized datasets where speed isn't critical, or when replicating published work that used t-SNE.

## The Bigger Picture: Choosing the Right Method

Let's step back and see the full landscape. Dimensionality reduction is not a one-size-fits-all solution. Different methods make different trade-offs:

| Method | Preserves | Speed | Scalability | When to use |
|--------|-----------|-------|-------------|-------------|
| **Scatter plot matrix** | Everything (2D projections) | Fast | 3-10 dimensions | Exploring moderate-dimensional data |
| **PCA** | Global variance | Very fast | Excellent (1000s of dims) | Linear structure, interpretability needed |
| **MDS** | All distances | Slow | Poor (100s of points) | Distance preservation critical |
| **t-SNE** | Local structure | Slow | Moderate (10,000s of points) | Revealing clusters, local relationships |
| **UMAP** | Local plus some global | Fast | Excellent (millions of points) | Large datasets, faster alternative to t-SNE |

A practical workflow begins with PCA: always run PCA first (it's fast, interpretable, and if it works well, you're done). Check how much variance the first 2 or 3 components explain. Next, check pairwise plots if feasible (for fewer than 10 dimensions, look at scatter plot matrices to understand pairwise relationships). Try t-SNE or UMAP if PCA doesn't reveal clear structure, if the first 2 PCs explain less than 50 percent variance, if you suspect non-linear relationships, or if you want to find clusters.

Validate your findings with multiple approaches. Don't trust a single visualization: try different random seeds for t-SNE and UMAP, try different hyperparameters (perplexity, number of neighbors), try different methods and see if they agree, and run statistical tests on apparent clusters.

::: {.callout-tip title="Try it yourself"}
Take a dataset you're familiar with and apply all four methods: PCA, MDS, t-SNE, and UMAP. Compare the results. What structure does each method reveal? What structure does each method hide? Which visualization best matches your intuition about the data?
:::

Dimensionality reduction can create apparent patterns that don't exist in the original data: spurious clusters (when t-SNE splits continuous data into false groups), missing relationships (when connected clusters in high dimensions appear separated in 2D), and misleading distances (when distance and size in t-SNE and UMAP are not meaningful). Always validate important findings with statistical tests or domain knowledge. A beautiful t-SNE plot is a starting point for investigation, not a final conclusion.

Visualizing high-dimensional data is as much art as science. The goal is not to find "the true projection" (there is no single true way to flatten high-dimensional space onto a page) but to reveal structure that helps you understand your data and ask better questions. As data scientist Jake VanderPlas wrote, "Dimensionality reduction is a form of lossy compression. The question is not whether you lose information (you always do) but whether you lose the information you care about."

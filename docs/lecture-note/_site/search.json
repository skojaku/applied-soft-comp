[
  {
    "objectID": "m06-llms/what-to-learn.html",
    "href": "m06-llms/what-to-learn.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "In this module, we will learn about transformers - a modern architecture that revolutionized NLP. We will learn: - Transformers architecture that revolutionized NLP - BERT and its bidirectional understanding of context - Sentence-BERT for generating sentence embeddings - Flan-T5 for instruction-tuned text generation - Instruction Embedding for better task adaptation"
  },
  {
    "objectID": "m06-llms/what-to-learn.html#what-to-learn-in-this-module",
    "href": "m06-llms/what-to-learn.html#what-to-learn-in-this-module",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "In this module, we will learn about transformers - a modern architecture that revolutionized NLP. We will learn: - Transformers architecture that revolutionized NLP - BERT and its bidirectional understanding of context - Sentence-BERT for generating sentence embeddings - Flan-T5 for instruction-tuned text generation - Instruction Embedding for better task adaptation"
  },
  {
    "objectID": "m06-llms/transformers.html",
    "href": "m06-llms/transformers.html",
    "title": "Transformers",
    "section": "",
    "text": "::::{grid} 1 :class-container: spoiler-block\n:::{grid-item-card} Spoiler Transformers don’t process sequences; they process relationships between every position simultaneously. :::\n::::\n\n\nYou’ve been taught to think of language models as sequential processors—reading left to right, one word triggering the next, like dominoes falling. This intuition comes from recurrent neural networks (RNNs), where information flows step by step, each word depending on the hidden state from the previous word. The transformer architecture throws this away entirely.\nInstead of sequential processing, transformers operate through parallel relationship mapping. When you read “The cat sat on the mat because it was tired,” you don’t actually process word-by-word in isolation. Your brain simultaneously evaluates which words relate to which—“it” connects to “cat,” “tired” explains “sat,” “mat” anchors “on.” Transformers formalize this intuition mathematically. Every position in the input sequence simultaneously computes its relationship to every other position. The mechanism is attention, and the result is a system where context flows in all directions at once, not just forward through time.\nThis parallelism is why transformers scaled when RNNs didn’t. Recurrent architectures impose sequential computation—you can’t process word 100 until you’ve processed word 99. Transformers eliminate this bottleneck. Every position can be computed in parallel, which means training time scales with sequence complexity, not sequence length. This architectural shift is what enabled GPT-3, GPT-4, and Claude to exist.\n\n\n\nModern LLMs stack multiple transformer blocks—modular units that take a sequence of token vectors as input and output a transformed sequence of the same length. GPT-3 uses 96 of these blocks; GPT-4 likely uses more. Each block refines the representation, adding layers of contextual understanding.\n```doufuilu ../figs/transformer-overview.jpg :name: transformer-overview :alt: Transformer Overview :width: 50% :align: center\nThe basic architecture of the transformer-based LLMs.\n\nThese blocks come in two forms: **encoders** and **decoders**. The encoder processes the input sequence and builds a contextualized representation. The decoder generates the output sequence, attending to both its own previous outputs and the encoder's representation. For translation tasks (\"I love you\" → \"Je t'aime\"), the encoder processes English, the decoder generates French. For language modeling (GPT-style systems), only the decoder is used—it generates text autoregressively, predicting the next token based on all previous tokens.\n\n```{figure} ../figs/transformer-encoder-decoder.jpg\n:name: transformer-encoder-decoder\n:alt: Transformer Encoder-Decoder\n:width: 80%\n:align: center\n\nThe encoder-decoder architecture. The encoder builds a representation of the input sequence; the decoder generates the output sequence while attending to the encoder's output.\nInside each block are three core components: multi-head attention (the relationship mapper), layer normalization (numerical stabilization), and feed-forward networks (nonlinear transformation). We’ll build these components step by step.\n```doufuilu ../figs/transformer-component.jpg :name: transformer-wired-components :alt: Transformer Wired Components :width: 80% :align: center\nInternal structure of encoder and decoder blocks.\n\n## Attention: The Relationship Engine\n\n**Self-attention**—the core of the transformer—computes how much each position in a sequence should \"attend to\" every other position. Unlike earlier attention mechanisms in seq2seq models, which attended from one sentence to another, self-attention operates within a single sequence. It answers the question: \"Given this word, which other words matter most?\"\n\n```{figure} ../figs/transformer-attention.jpg\n:name: transformer-attention\n:alt: Attention Mechanism\n:width: 80%\n:align: center\n\nThe attention mechanism computes relationships between all positions simultaneously.\nFor each word, the attention mechanism creates three vectors: query (Q), key (K), and value (V). Think of these as a library search: the query is what you’re looking for, the keys are book titles, and the values are the actual content. When you search for “machine learning” (your query), you match it against book titles (keys) to find relevant content (values).\nMathematically, each of these vectors is created by a learned linear transformation of the input word embedding. Given an input embedding x, we compute:\n\nQ = x W_Q, \\quad K = x W_K, \\quad V = x W_V\n\nwhere W_Q, W_K, and W_V are learned weight matrices. The attention mechanism then computes which keys are most relevant to each query using the dot product, which measures vector similarity. The dot product QK^T produces a matrix of attention scores—large values indicate strong relationships, small values indicate weak ones.\nThese raw scores are scaled by \\sqrt{d_k} (the square root of the key dimension) to prevent extreme values, then normalized using softmax to produce a probability distribution. Finally, these normalized attention weights are used to compute a weighted sum of the value vectors. The complete operation is:\n\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\nwhere Q \\in \\mathbb{R}^{n \\times d_k}, K \\in \\mathbb{R}^{n \\times d_k}, and V \\in \\mathbb{R}^{n \\times d_v} represent matrices containing n query, key, and value vectors respectively.\nThe interactive visualization below demonstrates how learned Query and Key transformations produce different attention patterns. Adjust the transformation parameters to see how different W_Q and W_K matrices change which words attend to which:\n\n\npython {marimo} import marimo as mo import numpy as np import pandas as pd import altair as alt\n```python {marimo} attention_words = [“bank”, “money”, “loan”, “river”, “shore”] attention_embeddings = np.array([ [0.0, 0.0], # bank (center) [-0.8, -0.3], # money [-0.7, -0.6], # loan [0.7, -0.5], # river [0.6, -0.7], # shore]) * 2\n\n\nq_scale_x = mo.ui.slider(-2, 2, 0.1, value=1.0, label=“Q Scale X”) q_scale_y = mo.ui.slider(-2, 2, 0.1, value=1.0, label=“Q Scale Y”) q_rotate = mo.ui.slider(-180, 180, 5, value=0, label=“Q Rotate (deg)”)\n\n\n\nk_scale_x = mo.ui.slider(-2, 2, 0.1, value=1.0, label=“K Scale X”) k_scale_y = mo.ui.slider(-2, 2, 0.1, value=1.0, label=“K Scale Y”) k_rotate = mo.ui.slider(-180, 180, 5, value=0, label=“K Rotate (deg)”)\nq_controls = mo.vstack([mo.md(“Query Transformation”), q_scale_x, q_scale_y, q_rotate]) k_controls = mo.vstack([mo.md(“Key Transformation”), k_scale_x, k_scale_y, k_rotate])\n\n```python {marimo}\ndef _transform_embeddings(emb, scale_x, scale_y, rotate_deg):\n    theta = np.radians(rotate_deg)\n    rot_matrix = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n    scale_matrix = np.diag([scale_x, scale_y])\n    W = rot_matrix @ scale_matrix\n    return emb @ W.T\n\nQ = _transform_embeddings(attention_embeddings, q_scale_x.value, q_scale_y.value, q_rotate.value)\nK = _transform_embeddings(attention_embeddings, k_scale_x.value, k_scale_y.value, k_rotate.value)\n\n# Compute attention scores\n_scores = Q @ K.T\n_exp_scores = np.exp(_scores - np.max(_scores, axis=1, keepdims=True))\nattention_weights = _exp_scores / np.sum(_exp_scores, axis=1, keepdims=True)\n\n# Create visualizations\n_df_q = pd.DataFrame({\"word\": attention_words, \"x\": Q[:, 0], \"y\": Q[:, 1]})\n_df_k = pd.DataFrame({\"word\": attention_words, \"x\": K[:, 0], \"y\": K[:, 1]})\n\n_chart_q = alt.Chart(_df_q).mark_circle(size=100).encode(\n    x=alt.X('x:Q', scale=alt.Scale(domain=[-4, 4]), title='Q1'),\n    y=alt.Y('y:Q', scale=alt.Scale(domain=[-4, 4]), title='Q2'),\n    tooltip=['word:N']\n).properties(width=200, height=200, title=\"Query (Q)\")\n_text_q = _chart_q.mark_text(dy=-12, fontSize=10, fontWeight='bold').encode(text='word:N')\n\n_chart_k = alt.Chart(_df_k).mark_circle(size=100).encode(\n    x=alt.X('x:Q', scale=alt.Scale(domain=[-4, 4]), title='K1'),\n    y=alt.Y('y:Q', scale=alt.Scale(domain=[-4, 4]), title='K2'),\n    tooltip=['word:N']\n).properties(width=200, height=200, title=\"Key (K)\")\n_text_k = _chart_k.mark_text(dy=-12, fontSize=10, fontWeight='bold').encode(text='word:N')\n\n# Heatmap\n_heatmap_data = []\nfor i, word_i in enumerate(attention_words):\n    for j, word_j in enumerate(attention_words):\n        _heatmap_data.append({\"Query\": word_i, \"Key\": word_j, \"Weight\": attention_weights[i, j]})\n_df_heatmap = pd.DataFrame(_heatmap_data)\n\n_heatmap = alt.Chart(_df_heatmap).mark_rect().encode(\n    x=alt.X('Key:N', title='Key Word'),\n    y=alt.Y('Query:N', title='Query Word'),\n    color=alt.Color('Weight:Q', scale=alt.Scale(scheme='blues'), title='Attention'),\n    tooltip=['Query:N', 'Key:N', alt.Tooltip('Weight:Q', format='.3f')]\n).properties(width=250, height=250, title=\"Attention Weights (Softmax)\")\n\nmo.vstack([\n    mo.hstack([q_controls, k_controls], align=\"center\"),\n    mo.hstack([_chart_q + _text_q, _chart_k + _text_k, _heatmap], align=\"center\")\n])\n\n\n\nThe output is a contextualized vector for each word—a representation that changes based on surrounding context. The word “bank” produces different vectors in “river bank” versus “financial bank” because the attention mechanism incorporates information from neighboring words.\nTo see this in action, consider how we might contextualize the word “bank” by mixing it with surrounding words. The visualization below shows static word embeddings—notice how “bank” sits neutrally between financial terms (money, loan) and geographical terms (river, shore).\n\n\n```python {marimo} static_words = [“bank”, “money”, “loan”, “river”, “shore”] static_embeddings = np.array([ [0.0, 0.0], # bank (center) [-0.8, -0.3], # money [-0.7, -0.6], # loan [0.7, -0.5], # river [0.6, -0.7], # shore]) * 2\n_df_static = pd.DataFrame({“word”: static_words, “x”: static_embeddings[:, 0], “y”: static_embeddings[:, 1]})\n_chart_static = alt.Chart(_df_static).mark_circle(size=200).encode( x=alt.X(‘x:Q’, scale=alt.Scale(domain=[-2, 2]), title=‘Dimension 1’), y=alt.Y(‘y:Q’, scale=alt.Scale(domain=[-2, 2]), title=‘Dimension 2’), text=‘word:N’, tooltip=[‘word:N’, ‘x:Q’, ‘y:Q’] ).properties(width=300, height=300, title=“Static Word Embeddings”)\n_text_static = _chart_static.mark_text(dy=-15, fontSize=14, fontWeight=‘bold’).encode(text=‘word:N’)\n_chart_static + _text_static\n\n&lt;/marimo-iframe&gt;\n&lt;/div&gt;\n\nNow, try adjusting the weights below to create a contextualized version of \"bank.\" If the sentence is \"Money in bank,\" adjust the weights to shift \"bank\" toward \"money.\" If the sentence is \"River bank,\" shift it toward \"river.\"\n\n&lt;div&gt;\n&lt;marimo-iframe data-height=\"500px\" data-show-code=\"false\"&gt;\n\n```python {marimo}\ncontext_words = [\"bank\", \"money\", \"loan\", \"river\", \"shore\"]\ncontext_embeddings = np.array([\n    [0.0, 0.0],  # bank (center)\n    [-0.8, -0.3],  # money\n    [-0.7, -0.6],  # loan\n    [0.7, -0.5],  # river\n    [0.6, -0.7],  # shore\n]) * 2\n\nslider_bank = mo.ui.slider(0, 1, 0.01, value=1.0, label=\"Bank Weight\")\nslider_money = mo.ui.slider(0, 1, 0.01, value=0, label=\"Money Weight\")\nslider_loan = mo.ui.slider(0, 1, 0.01, value=0, label=\"Loan Weight\")\nslider_river = mo.ui.slider(0, 1, 0.01, value=0, label=\"River Weight\")\nslider_shore = mo.ui.slider(0, 1, 0.01, value=0, label=\"Shore Weight\")\n\ncontext_sliders = mo.vstack([slider_bank, slider_money, slider_loan, slider_river, slider_shore])\n```python {marimo} _weights = np.array([slider_bank.value, slider_money.value, slider_loan.value, slider_river.value, slider_shore.value]) _total = _weights.sum() if _total &gt; 0: _weights = _weights / _total _new_vec = context_embeddings.T @ _weights else: _new_vec = np.zeros(2)\n_df_orig = pd.DataFrame({“word”: context_words, “x”: context_embeddings[:, 0], “y”: context_embeddings[:, 1], “type”: [“Original”] * 5}) _df_new = pd.DataFrame({“word”: [“Contextualized Bank”], “x”: [_new_vec[0]], “y”: [_new_vec[1]], “type”: [“Contextualized”]}) _df_combined = pd.concat([_df_orig, _df_new])\n_chart_context = alt.Chart(_df_combined).mark_circle(size=200).encode( x=alt.X(‘x:Q’, scale=alt.Scale(domain=[-2, 2]), title=‘Dimension 1’), y=alt.Y(‘y:Q’, scale=alt.Scale(domain=[-2, 2]), title=‘Dimension 2’), color=alt.Color(‘type:N’, scale=alt.Scale(domain=[‘Original’, ‘Contextualized’], range=[‘#dadada’, ‘#ff7f0e’])), tooltip=[‘word:N’, ‘x:Q’, ‘y:Q’] ).properties(width=350, height=350, title=“Contextualized Bank”)\n_text_context = _chart_context.mark_text(dy=-15, fontSize=14, fontWeight=‘bold’).encode(text=‘word:N’, color=alt.value(‘black’))\nmo.hstack([context_sliders, _chart_context + _text_context], align=“center”)\n\n&lt;/marimo-iframe&gt;\n&lt;/div&gt;\n\nThis manual weighting captures the intuition, but how do we learn which words to attend to? This is where queries and keys come in.\n\n### Multi-Head Attention: Multiple Perspectives\n\nA single attention mechanism captures one type of relationship. **Multi-head attention** runs multiple attention operations in parallel, each with different learned parameters. Each head can specialize—one might focus on syntactic dependencies (subject-verb relationships), another on semantic similarity (synonyms and antonyms), another on positional proximity (nearby words).\n\n```{figure} ../figs/transformer-multihead-attention.jpg\n:name: transformer-multihead-attention\n:alt: Multi-Head Attention\n:width: 50%\n:align: center\n\nMulti-head attention runs multiple attention operations in parallel, each capturing different relationships.\nThe outputs from all heads are concatenated and passed through a final linear transformation to produce the multi-head attention output. In the original transformer paper {footcite:p}vaswani2017attention, the authors used h=8 attention heads, with each head using dimension d_k = d_v = d/h = 64, where d=512 is the model dimension.\n\n\nDeep networks suffer from numerical instability—activations can grow explosively large or vanish to zero as they propagate through layers. Layer normalization stabilizes training by rescaling activations to have zero mean and unit variance.\n```doufuilu https://miro.medium.com/v2/resize:fit:1400/0*Agdt1zYwfUxXMJGJ :name: transformer-layer-normalization :alt: Layer Normalization :width: 80% :align: center\nLayer normalization computes mean and standard deviation across all features for each sample, then normalizes.\n\nFor each input vector $x$, layer normalization computes:\n\n$$\n\\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sigma} + \\beta\n$$\n\nwhere $\\mu$ and $\\sigma$ are the mean and standard deviation of $x$, and $\\gamma$ and $\\beta$ are learned scaling and shifting parameters (initialized to 1 and 0 respectively). This ensures that no matter how the input distribution shifts during training, each layer receives inputs in a stable numerical range.\n\n## The Encoder Block\n\nNow we wire the components together. The **encoder block** processes the input sequence through four stages:\n\n1. **Multi-head self-attention** computes contextualized representations\n2. **Residual connection + normalization** stabilizes training\n3. **Feed-forward network** applies nonlinear transformation\n4. **Residual connection + normalization** again\n\n```{figure} ../figs/transformer-encoder.jpg\n:name: transformer-block\n:alt: Transformer Block\n:width: 50%\n:align: center\n\nInformation flows through multi-head attention, normalization, feed-forward networks, and final normalization.\nThe feed-forward network is a simple two-layer MLP applied independently to each position:\n\n\\text{FFN}(x) = \\text{ReLU}(x W_1 + b_1) W_2 + b_2\n\nThe residual connections (also called skip connections) are critical for training deep networks. Instead of learning a direct mapping f(x), we learn the residual:\n\nx_{\\text{out}} = x_{\\text{in}} + f(x_{\\text{in}})\n\nThis simple addition has profound consequences for gradient flow. During backpropagation, the gradient of the loss \\mathcal{L} with respect to layer l is:\n\n\\frac{\\partial \\mathcal{L}}{\\partial x_l} = \\frac{\\partial \\mathcal{L}}{\\partial x_{l+1}} \\left(1 + \\frac{\\partial f_l}{\\partial x_l}\\right)\n\nNotice the “+1” term—this provides a direct gradient path from the output back to the input. Without residual connections, gradients must pass through the chain:\n\n\\frac{\\partial f_L}{\\partial f_{L-1}} \\cdot \\frac{\\partial f_{L-1}}{\\partial f_{L-2}} \\cdot \\ldots \\cdot \\frac{\\partial f_1}{\\partial x}\n\nIf any term is less than 1, the gradient shrinks exponentially—this is the vanishing gradient problem. With residual connections, the gradient expansion becomes:\n\n1 + O_1 + O_2 + O_3 + \\ldots\n\nwhere O_1 contains first-order terms, O_2 contains second-order products, etc. The constant “1” ensures gradients can flow even when the learned components f_i produce small derivatives. This architectural innovation, originally developed for computer vision {footcite:p}he2015deep, is what allows transformers to scale to hundreds of layers.\n\n\n\nThe decoder block extends the encoder with two modifications: masked self-attention and cross-attention.\n```doufuilu ../figs/transformer-decoder.jpg :name: transformer-decoder :alt: Transformer Decoder :width: 50% :align: center\nThe decoder adds masked self-attention (to prevent future peeking) and cross-attention (to access encoder outputs).\n\n### Masked Self-Attention: Preventing Future Leakage\n\nDuring training, we know the entire target sequence. For translation (\"I love you\" → \"Je t'aime\"), we have both input and output. A naive decoder could \"cheat\" by looking at future words in the target sequence. Masked self-attention prevents this by zeroing out attention to future positions.\n\nThe mask is implemented by setting attention scores to $-\\infty$ before the softmax:\n\n$$\n\\text{MaskedAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T + M}{\\sqrt{d_k}}\\right)V\n$$\n\nwhere $M$ is a matrix with $-\\infty$ at positions $(i,j)$ where $j &gt; i$ (future positions) and 0 elsewhere. After softmax, these $-\\infty$ values become zero, eliminating information flow from future tokens.\n\n```{figure} https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png\n:name: transformer-masked-attention\n:alt: Masked Attention\n:width: 80%\n:align: center\n\nMasked attention zeros out future positions, allowing parallel training without information leakage.\nThis enables parallel training. Instead of generating “Je”, then “t’aime”, then the final token sequentially, we can train all positions simultaneously—each with access only to its causal past. During inference, masking happens naturally because future tokens don’t exist yet.\n\n\nThe second attention layer in the decoder uses cross-attention to access the encoder’s output. The queries (Q) come from the decoder’s previous layer, while the keys (K) and values (V) come from the encoder’s output:\n\n\\text{CrossAttention}(Q_{\\text{decoder}}, K_{\\text{encoder}}, V_{\\text{encoder}}) = \\text{softmax}\\left(\\frac{Q_{\\text{decoder}}K_{\\text{encoder}}^T}{\\sqrt{d_k}}\\right)V_{\\text{encoder}}\n\n```doufuilu ../figs/transformer-cross-attention.jpg :name: transformer-cross-attention :alt: Cross-Attention :width: 60% :align: center\nCross-attention allows the decoder to query the encoder’s representation.\n\nThis is how translation works: when generating \"Je\", the decoder attends to \"I\"; when generating \"t'aime\", it attends to \"love\". The attention mechanism learns these alignments automatically from data, without explicit supervision.\n\n## Position Embedding: Encoding Order\n\nAttention is **permutation invariant**—it produces the same output regardless of input order. \"The cat sat on the mat\" and \"mat the on sat cat the\" yield identical attention outputs because the dot product doesn't encode position. We need to inject positional information.\n\nThe naive approach is to add a position index: $x_t := x_t + \\beta t$. This fails for two reasons:\n\n1. **Unbounded**: Position indices grow arbitrarily large. Models trained on sequences of length 512 fail on sequences of length 1000 because they've never seen position 513.\n2. **Discrete**: Positions 10 and 11 are no more similar than positions 10 and 100.\n\nA better approach is **binary position encoding**. Represent position $t$ as a binary vector:\n\n$$\n\\begin{align*}\n  0: \\ \\ \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} & \\quad &\n  8: \\ \\ \\ \\ \\texttt{1} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\\\\n  1: \\ \\ \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{1} & &\n  9: \\ \\ \\ \\ \\texttt{1} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{1} \\\\\n  2: \\ \\ \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{1} \\ \\ \\texttt{0} & &\n  10: \\ \\ \\ \\ \\texttt{1} \\ \\ \\texttt{0} \\ \\ \\texttt{1} \\ \\ \\texttt{0}\n\\end{align*}\n$$\n\nThis is unbounded—you can represent arbitrarily large positions by adding bits—but still discrete. The transformer solution is **sinusoidal position embedding**, a continuous version of binary encoding:\n\n$$\n\\text{Pos}(t, i) =\n\\begin{cases}\n\\sin\\left(\\dfrac{t}{10000^{2i/d}}\\right), & \\text{if } i \\text{ is even} \\\\\n\\cos\\left(\\dfrac{t}{10000^{2i/d}}\\right), & \\text{if } i \\text{ is odd}\n\\end{cases}\n$$\n\nwhere $t$ is the position index and $i$ is the dimension index. This encoding has three critical properties:\n\n1. **Continuous**: Smooth interpolation between positions\n2. **Bounded**: All values lie in $[-1, 1]$\n3. **Relative distance preservation**: The dot product $\\text{Pos}(t) \\cdot \\text{Pos}(t+k)$ depends only on the offset $k$, not the absolute position $t$\n\n```{figure} https://kazemnejad.com/img/transformer_architecture_positional_encoding/positional_encoding.png\n:name: transformer-position-embedding\n:alt: Transformer Position Embedding\n:width: 80%\n:align: center\n\nSinusoidal position embeddings exhibit periodic patterns across dimensions. Image from [Amirhossein Kazemnejad](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/).\nNotice the alternating pattern—just like binary encoding, but continuous. Low-frequency dimensions (right) flip slowly across positions; high-frequency dimensions (left) flip rapidly. This creates a unique fingerprint for each position while preserving distance relationships.\n```doufuilu https://kazemnejad.com/img/transformer_architecture_positional_encoding/time-steps_dot_product.png :name: transformer-position-embedding-similarity :alt: Transformer Position Embedding Similarity :width: 80% :align: center\nDot product between position embeddings depends only on relative distance, not absolute position. Image from Amirhossein Kazemnejad.\n\nThe position embedding is added directly to the token embedding: $x_{t,i} := x_{t,i} + \\text{Pos}(t, i)$. Why addition instead of concatenation? Concatenation would increase the model dimension, adding parameters. Addition creates an interesting interaction in the attention mechanism—queries and keys now encode both content and position, allowing the model to attend based on both \"what\" (semantic similarity) and \"where\" (positional proximity).\n\n## The Takeaway\n\nTransformers replaced sequential computation with parallel relationship mapping. Every position simultaneously computes its context from every other position. This architectural shift—from recurrent bottlenecks to parallel attention—is what allowed language models to scale from millions to hundreds of billions of parameters. The mechanism is simple: query, key, value. The result is GPT-4.\n\n```{footbibliography}\n:style: unsrt\n:filter: docname in docnames"
  },
  {
    "objectID": "m06-llms/transformers.html#a-building-block-of-llms",
    "href": "m06-llms/transformers.html#a-building-block-of-llms",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Many large language models (LLMs) including GPT-3, GPT-4, and Claude are built based on a stack of transformer blocks {footcite:p}vaswani2017attention. Each transformer block takes a sequence of token vectors as input and outputs a sequence of token vectors (sequence-to-sequence!).\n```blytlscd ../figs/transformer-overview.jpg :name: transformer-overview :alt: Transformer Overview :width: 50% :align: center\nThe basic architecture of the transformer-based LLMs.\n\nThese transformer blocks can be further divided into encoder and decoder components.\nThe encoder is used for encoding the input sequence, while the decoder is used for generating the output sequence. Like seq2se models with attention, the decoder can also see the encoder outputs for invidiual tokens, along with the previous output tokens. The output of the decoder is then passed through a linear layer to produce the probability distribution over the next token.\n\n```{figure} ../figs/transformer-encoder-decoder.jpg\n:name: transformer-encoder-decoder\n:alt: Transformer Encoder-Decoder\n:width: 80%\n:align: center\n\nThe basic architecture of the transformer encoder-decoder. The encoder is used for encoding the input sequence, while the decoder is used for generating the output sequence. The encoder takes the input sequence as input and outputs a sequence of token vectors, which are then passed to the decoder. The decoder takes the encoder outputs, along with the previous output tokens, and outputs the probability distribution over the next token.\nInside the encoder and decoder transformer blocks are essentially three components, i.e., multi-head attention, layer normalization, and feed-forward networks. We will learn individual components in the following sections.\n```blytlscd ../figs/transformer-component.jpg :name: transformer-wired-components :alt: Transformer Wired Components :width: 80% :align: center\nThe encoder-decoder architecture of the transformer.\n\n\n## Attention Mechanism\n\nPerhaps the most crucial component of the transformer is the *attention mechanism*, which allows the model to pay attention to particular parts of the input sequence.\n\n\n### Self-Attention\nIn transformers, the attention is called *self-attention*, since the attention is paid within the same sentence, unlike the sequence-to-sequence models that pays attention from one sentence to another. At its core, self-attention is about relationships. When you read the sentence \"The cat sat on the mat because it was tired\", how do you know what \"it\" refers to? You naturally look back at the previous words and determine that \"it\" refers to \"the cat\". Self-attention works similarly, but does this for every word in relation to every other word, simultaneously.\n\n```{figure} ../figs/transformer-attention.jpg\n:name: transformer-attention\n:alt: Attention Mechanism\n:width: 80%\n:align: center\n\nThe attention mechanism in transformers.\nTo compute the attention between words, the attention head creates three types of vectors—query, key, and value—for each word. Each of these vectors are created by a neural network (w/ single linear layer) that takes the input word as input, and outputs another vector.\nThink of this like a library system: The Query is what you're looking for, the Keys are like book titles, and the Values are the actual content of the books. When you search (Q) for a specific topic, you match it against book titles (K) to find the relevant content (V).\nThe query and key vectors are used to compute the attention score, which represents how much attention the model pays to each key word for the query word, with a larger score indicating a stronger attention. For example, in the sentence “The cat sat on the mat because it was tired”, a good model should pay more attention to “cat” than “mat” for the word “it”. The atttention score computed by the dot product of the query and key vectors. The score is then normalized by the softmax function, with rescaling by \\sqrt{d_k} to prevent the score from becoming too large. More formally, the attention score is computed as:\n\n\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right),\n\nwhere Q \\in \\mathbb{R}^{n \\times d_k} is the query matrix containing n query vectors of dimension d_k, K \\in \\mathbb{R}^{n \\times d_k} is the key matrix containing n key vectors of dimension d_k, and V \\in \\mathbb{R}^{n \\times d_v} is the value matrix containing n value vectors of dimension d_v.\nThe normalized attention score is used as a weight for the weghted sum of the value vectors, which results in the contextualized vector of the query word. Putting all the pieces together, the attention mechanism is computed as:\n\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V,\n\nwhere V \\in \\mathbb{R}^{n \\times d_v} is the value matrix containing n value vectors of dimension d_v. In the original paper on transformers {footcite:p}vaswani2017attention, the dimension of the query, key, and value vectors are all set to be the same, i.e., d_k = d_v = d_q = d / h, where h is the number of attention heads and d is the dimension of the input vector, though this is not a strict requirement.\nThe output of the attention mechanism is the *contextualized vector*, meaning that the vector for a word can vary depending on other words input to the attention module. This is ideal for language modeling, since the meaning of a word can vary depending on the context, e.g., \"bank\" can mean \"river bank\" or \"financial institution\" depending on the words surrounding it.\n\n\nMulti-head attention consists of multiple attention heads to enable a model to pay attentions to multiple aspects of the input sequence. Each attention head can have different parameters and thus produces different “contextualized vectors.” These different vector are then concatenated and fed into a feed-forward network to produce the final output.\n```blytlscd ../figs/transformer-multihead-attention.jpg :name: transformer-multihead-attention :alt: Multi-Head Attention :width: 50% :align: center\nMulti-head attention mechanism.\n\n## Layer Normalization\n\n```{figure} https://miro.medium.com/v2/resize:fit:1400/0*Agdt1zYwfUxXMJGJ\n:name: transformer-layer-normalization\n:alt: Layer Normalization\n:width: 80%\n:align: center\n\nLayer normalization works by normalizing each individual sample across its features. For each sample, it calculates the mean and standard deviation across all feature dimensions, then uses these statistics to normalize that sample's values.\nLayer normalization is a technique used to stabilize the training of deep neural networks. It mitigates the problem of too large or too small input values, which can cause the network to become unstable. This normalization shifts and scales the input values to prevent this issue. More specifically, the layer normalization is computed as:\n\n\\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sigma} + \\beta,\n\nwhere \\mu and \\sigma are the mean and standard deviation of the input, \\gamma is the scaling factor, and \\beta is the shifting factor. The variables \\gamma and \\beta are learnable parameters that are initialized to 1 and 0, respectively, and are updated during training."
  },
  {
    "objectID": "m06-llms/transformers.html#wiring-it-all-together",
    "href": "m06-llms/transformers.html#wiring-it-all-together",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Now, we have all the components to build a transformer block. Let’s wire them together.\n```blytlscd ../figs/transformer-encoder.jpg :name: transformer-block :alt: Transformer Block :width: 50% :align: center\nInput flows through multi-head attention, layer normalization, feed-forward networks, and another normalization step.\n\nLet us ignore the residual connection for now. The input is first passed through multi-head attention, followed by layer normalization. Then, the output of the normalization is passed through feed-forward networks and another layer normalization step.\n\n#### Residual Connection\n\n```{figure} https://i.sstatic.net/UcJSa.png\n:name: residual-connection\n:alt: Residual Connection\n:width: 30%\n:align: center\n\nResidual connection.\nNow, let us consider the residual connection. A residual connection, also known as a skip connection, is a technique used to stabilize the training of deep neural networks. More specifically, let us denote by f the neural network that we want to train, which is the multi-head attention or feed-forward networks in the transformer block. The residual connection is defined as:\n\n\\underbrace{x_{\\text{out}}}_{\\text{output}} = \\underbrace{x_{\\text{in}}}_{\\text{input}} + \\underbrace{f(x_{\\text{in}})}_{\\text{component}}.\n\nNote that rather than learning the complete mapping from input to output, the network f learns to model the residual (difference) between them. This is particularly advantageous when the desired transformation approximates an identity mapping, as the network can simply learn to output values near zero.\nResidual connections help prevent the vanishing gradient problem. Deep learning models like LLMs consist of many layers, which are trained to minimize the loss function {\\cal L}_{\\text{loss}} with respect to the parameters \\theta. To this end, the gradient of the loss function is computed using the chain rule as\n\n\\frac{\\partial {\\cal L}_{\\text{loss}}}{\\partial \\theta} = \\frac{\\partial {\\cal L}_{\\text{loss}}}{\\partial f_L} \\cdot \\frac{\\partial f_L}{\\partial f_{L-1}} \\cdot \\frac{\\partial f_{L-1}}{\\partial f_{L-2}} \\cdot ... \\cdot \\frac{\\partial f_{l+1}}{\\partial f_l} \\cdot \\frac{\\partial f_l}{\\partial \\theta}\n\nwhere f_i is the output of the i-th layer. The gradient vanishing problem occurs when the individual terms \\frac{\\partial f_{i+1}}{\\partial f_i} are less than 1. As a result, the gradient becomes smaller and smaller as the gradient flows backward through earlier layers. By adding the residual connection, the gradient for the individual term becomes:\n\n\\frac{\\partial x_{i+1}}{\\partial x_i} = 1 + \\frac{\\partial f_i(x_i)}{\\partial x_i}\n\nNotice the “+1” term, which is the direct path from the input to the output. The chain rule is thus modified as:\n\\left(1 + \\frac{\\partial f_{L-1}}{\\partial x_{L-1}}\\right)\\left(1 + \\frac{\\partial f_{L-2}}{\\partial x_{L-2}}\\right)\\left(1 + \\frac{\\partial f_{L-3}}{\\partial x_{L-3}}\\right)...\nWhen we expand this, we can group terms by their order (how many \\partial f_i terms are multiplied together): We can write this more concisely using O_n to represent terms of nth order:\n1 + O_1 + O_2 + O_3 + ...\nwhere: - O_1 = \\frac{\\partial f_{L-1}}{\\partial x_{L-1}} + \\frac{\\partial f_{L-2}}{\\partial x_{L-2}} + \\frac{\\partial f_{L-3}}{\\partial x_{L-3}} + ... - O_2 = \\frac{\\partial f_{L-1}}{\\partial x_{L-1}}\\frac{\\partial f_{L-2}}{\\partial x_{L-2}} + \\frac{\\partial f_{L-2}}{\\partial x_{L-2}}\\frac{\\partial f_{L-3}}{\\partial x_{L-3}} + \\frac{\\partial f_{L-1}}{\\partial x_{L-1}}\\frac{\\partial f_{L-3}}{\\partial x_{L-3}} + ... - O_3 = \\frac{\\partial f_{L-1}}{\\partial x_{L-1}}\\frac{\\partial f_{L-2}}{\\partial x_{L-2}}\\frac{\\partial f_{L-3}}{\\partial x_{L-3}} + ...\nWithout the residual connection, we only have the O_L terms for the network with L layers, which is subject to the gradient vanishing problem. Whereas with the residual connection, we have the lower-order terms like O_1, O_2, O_3, ... for the network with L layers, which is less susceptible to the gradient vanishing problem.\n```ulklkkqikkrv Residual Connection :class: tip\nResidual connections are a architectural innovation that allows neural networks to be much deeper without degrading performance. It was proposed by He et al. {footcite:p}he2015deep for image processing from Microsoft Research.\n\n\n```{admonition} Residual connection mitigates gradient explosion\n:class: tip\n\nResidual connections also help prevent gradient explosion, even though this may not be obvious from the chain rule perspective. As shown in {footcite:p}`philipp2017exploding`, the residual connection provides an alternative path for gradients to flow through. By distributing gradients between the residual path and the learning component path, the gradient is less likely to explode."
  },
  {
    "objectID": "m06-llms/transformers.html#decoder-transformer-block",
    "href": "m06-llms/transformers.html#decoder-transformer-block",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "The decoder transformer block is similar to the encoder transformer block, but it also includes the masked multi-head attention and cross-attention components.\n```blytlscd ../figs/transformer-decoder.jpg :name: transformer-decoder :alt: Transformer Decoder :width: 50% :align: center\nThe decoder transformer block.\n\n\n### Masked Multi-Head Attention\n\nThe masked multi-head attention is used during training to prevent the decoder from seeing the future tokens. During inference, the masked mult-head attention acts as a regular attention module.\n\nThe masked multi-head attention is crucial for enabling parallel training of the decoder. During training, we know the entire expected output sequence, but we need to ensure the model learns to generate tokens sequentially without \"peeking\" at future tokens.\n\nLet's understand this with an example. Suppose we're training a model to translate \"I love you\" to French \"Je t'aime\". The encoder processes the input sequence in parallel, producing vector representations (say 11, 12, 13 for simplicity). For the decoder training, we have two options:\n\n1. **Sequential Training (without masking)**: Process one token at a time\n   - Step 1: Input (11,12,13) → Predict \"Je\"\n   - Step 2: Input (11,12,13) + predicted \"Je\" → Predict \"t'aime\"\n   - Step 3: Input (11,12,13) + predicted \"t'aime\" → Predict final token\n\n   This is slow and errors accumulate across steps.\n\n2. **Parallel Training (with masking)**: Process all tokens simultaneously\n   - Operation A: Input (11,12,13) → Predict \"Je\" (mask out \"t'aime\")\n   - Operation B: Input (11,12,13) + \"Je\" → Predict \"t'aime\" (mask out final token)\n   - Operation C: Input (11,12,13) + \"Je\" + \"t'aime\" → Predict final token\n\nThe parallel training is much faster and more efficient, since the model can process all tokens simultaneously. Additionally, the model does not suffer from the error accumulation problem, where the prediction error from one step is carried over to the next step.\n\nTo implement the masking, we set the attention scores to negative infinity for future tokens before the softmax operation, effectively zeroing out their contribution:\n\n$$\n\\text{Mask}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T + M}{\\sqrt{d_k}}\\right)V\n$$\n\nwhere $M$ is a matrix with $-\\infty$ for positions corresponding to future tokens. The result is the attention scores, where the tokens attend only to the previous tokens.\n\n```{figure} https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png\n:name: transformer-masked-attention\n:alt: Masked Attention\n:width: 80%\n:align: center\n\nThe masked attention mechanism.\n\n\nCross-attention is the second multi-head attention component in the decoder transformer block. It creates a connection between the decoder and encoder by allowing the decoder to access information from the encoder’s output.\nThe mechanism works by using queries (Q) from the decoder’s previous layer and keys (K) and values (V) from the encoder’s output. This enables each position in the decoder to attend to the full encoder sequence without any masking, since encoding is already complete.\nFor instance, in translating “I love you” to “Je t’aime”, cross-attention helps each French word focus on relevant English words - “Je” attending to “I”, and “t’aime” to “love”. This maintains semantic relationships between input and output.\nThe cross-attention formula is:\n\n\\text{CrossAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\nwhere Q comes from the decoder and K,V come from the encoder. This effectively bridges the encoding and decoding processes.\n```blytlscd ../figs/transformer-cross-attention.jpg :name: transformer-cross-attention :alt: Cross-Attention :width: 60% :align: center\nThe cross-attention mechanism.\n\n\n## Other miscellaneous components\n\n### Position embedding\n\nPosition embedding is also an interesting component that is used to encode the position of the tokens in the sequence.\nA key limitation of the attention mechanism is that it is *permutation invariant*.\nThis means that the order of the input tokens does not matter, e.g., \"The cat sat on the mat\" and \"The mat sat on the cat\" are the same.\nTo better capture the position information, transformers add to the input token embedding *a position embedding*.\n\nTo understand how this works, let us approach from a naive approach.\nSuppose that we have a sequence of $T$ token embeddings, denoted by $x_1, x_2, ..., x_T$, each of which is a $d$-dimensional vector.\nA simple way to encode the position information is to add a position index to each token embedding, i.e.,\n\n$$\nx_t := x_t + \\beta t,\n$$\n\nwhere $t = 1, 2, ..., T$ is the position index of the token in the sequence, and $\\beta$ is the step size. This appears to be simple but has a critical problem.\n\n1. **Unbounded**: The position index can be arbitrarily large. When the models see a sequence longer than those in training data, it may suffer since the model will be exposed to a new position index that the model has never seen before.\n2. **Discrete**: The position index is discrete, which means that the model cannot capture the position information in a smooth manner.\n\nBecause this naive approach has the problems, let us consider another approach. Let us represent the position index using a binary vector of length $d$. For example, in case of $d=4$, we have the following binary vectors:\n\n$$\n\\begin{align*}\n  0: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} & &\n  8: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n  1: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} & &\n  9: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} \\\\\n  2: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} & &\n  10: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n  3: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} & &\n  11: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} \\\\\n  4: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} & &\n  12: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n  5: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} & &\n  13: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} \\\\\n  6: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} & &\n  14: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n  7: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} & &\n  15: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} \\\\\n\\end{align*}\n$$\n\nThen, one may use the binary vector as the position embedding as follows:\n\n$$\nx_{t,i} := x_{t,i} + \\text{Pos}(t, i),\n$$\n\nwhere $\\text{Pos}(t, i)$ is the position embedding vector of the position index $t$ and the dimension index $i$.\nThis representation is good in the sense that it is unbounded. Yet, it is still discrete.\n\nAn elegant position embedding, which is used in transformers, is the *sinusoidal position embedding* {footcite:p}`vaswani2017attention`. It appears to be complicated but stay with me for a moment.\n\n$$\n\\text{Pos}(t, i) =\n\\begin{cases}\n\\sin\\left(\\dfrac{t}{10000^{2i/d}}\\right), & \\text{if } i \\text{ is even} \\\\\n\\cos\\left(\\dfrac{t}{10000^{2i/d}}\\right), & \\text{if } i \\text{ is odd}\n\\end{cases},\n$$\n\nwhere $i$ is the dimension index of the position embedding vector. This position embedding is added to the input token embedding as:\n\n$$\nx_{t,i} := x_{t,i} + \\text{Pos}(t, i),\n$$\n\nIt appears to be complicated but it can be seen as a continuous version of the binary position embedding above. To see this, let us plot the position embedding for the first 100 positions.\n\n```{figure} https://kazemnejad.com/img/transformer_architecture_positional_encoding/positional_encoding.png\n:name: transformer-position-embedding\n:alt: Transformer Position Embedding\n:width: 80%\n:align: center\n\nThe position embedding. The image is taken from https://kazemnejad.com/blog/transformer_architecture_positional_encoding/\nWe note that, just like the binary position embedding, the sinusoidal position embedding also exhibits the alternating pattern (vertically) with frequency increasing as the dimension index increases (horizontal axis). Additionally, the sinusoidal position embedding is continuous, which means that the model can capture the position information in a smooth manner.\nAnother key property of the sinusoidal position embedding is that the dot similarity between the two position embedding vectors represent the similarity between the two positions, regardless of the position index.\n```blytlscd https://kazemnejad.com/img/transformer_architecture_positional_encoding/time-steps_dot_product.png\n:name: transformer-position-embedding-similarity :alt: Transformer Position Embedding Similarity :width: 80% :align: center\nThe dot similarity between the two position embedding vectors represent the distance between the two positions, regardless of the position index. The image is taken from https://kazemnejad.com/blog/transformer_architecture_positional_encoding/\n\n\n```{admonition} Why additive position embedding?\n:class: tip\n\nThe sinusoidal position embedding is additive, which alter the token embedding. Alternatively, one may concatenate, instead of adding, the position embedding to the token embedding, i.e., $x_{t,i} := [x_{t,i}; \\text{Pos}(t, i)]$. This makes it easier for a model to distinguish the position information from the token information. So why not use the concatenation?\n\nOne reason is that the concatenation requires a larger embedding dimension, which increases the number of parameters in the model.\nInstead, adding the position embedding creates an interesting effect in the attention mechanism.\nInterested readers can check out [this Reddit post](https://www.reddit.com/r/MachineLearning/comments/cttefo/comment/exs7d08/?utm_source=reddit&utm_medium=web2x&context=3).\n```ulklkkqikkrv Absolute vs Relative Position Embedding :class: tip\nAbsolute position embedding is the one we discussed above, where each position is represented by a unique vector. On the other hand, relative position embedding represents the position difference between two positions, rather than the absolute position {footcite}shaw2018self. The relative position embedding can be implemented by adding a learnable scalar to the unnormalized attention scores before softmax operation {footcite}raffel2020exploring, i.e.,\n\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T + B}{\\sqrt{d_k}}\\right)V\n\nwhere B is a learnable offset matrix that is added to the unnormalized attention scores. The matrix B is a function of the position difference between the query and key, i.e., B = f(i-j), where i and j are the position indices of the query and key, respectively. Such a formulation is useful when the model needs to capture the relative position between two tokens.\n\n\n```{footbibliography}\n:style: unsrt\n:filter: docname in docnames"
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "1 Course Overview",
    "text": "1 Course Overview\n“Don’t think! Feeeeeel” is a famous quote by Bruce Lee in the movie Enter the Dragon, and this is my guiding philosophy of learning.\nThis course explores how deep learning serves as a powerful tool to operationalize high-dimensional data from social and physical systems. You’ll learn to model complex system dynamics using modern computational intelligence, and in turn, use perspectives from complexity science to understand the emergent behaviors of large-scale models.\nThe course combines: - Hands-on coding with real data from text, images, and networks - Theoretical foundations of deep learning and complex systems - Reproducible data science practices with modern tools - Ethical considerations in AI and computational modeling",
    "crumbs": [
      "Home",
      "Course Information",
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-modules",
    "href": "index.html#course-modules",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "2 Course Modules",
    "text": "2 Course Modules\n\nFoundation Modules\n\nModule 1: The Data Scientist’s Toolkit - Git, tidy data, reproducible environments\nModule 2: Visualizing Complexity - t-SNE, UMAP, network visualization\n\n\n\nDeep Learning by Data Type\n\nModule 3: Deep Learning for Text - Word2Vec, RNNs, LSTMs, embeddings\nModule 4: Deep Learning for Images - CNNs, ResNet, architectural evolution\nModule 5: Deep Learning for Graphs - GNNs, graph embeddings, spectral methods\n\n\n\nAdvanced Topics\n\nModule 6: Large Language Models & Emergent Behavior - Transformers, scaling laws, LLMs as complex systems\nModule 7: Self-Supervised Learning - Contrastive learning, SimCLR\nModule 8: Explainability & Ethics - LIME, SHAP, fairness, causality",
    "crumbs": [
      "Home",
      "Course Information",
      "Home"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "3 Getting Started",
    "text": "3 Getting Started\n\nRead the Welcome page\nLearn About Us\nJoin our Discord server\nFollow the Setup Guide\nLearn How to Submit Assignments",
    "crumbs": [
      "Home",
      "Course Information",
      "Home"
    ]
  },
  {
    "objectID": "course/welcome.html",
    "href": "course/welcome.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome to the course! In this class, we will explore how deep learning serves as a powerful tool to operationalize high-dimensional data from social and physical systems. You’ll learn to model complex system dynamics using modern computational intelligence, and in turn, use perspectives from complexity science to understand the emergent behaviors of large-scale models.\nThis course is designed to provide you with both a theoretical foundation and hands-on experience in applied soft computing. You will learn how to apply representation learning, sequence modeling, and graph analytics to model real-world complex systems using Python and modern deep learning frameworks.\n\n\nThis course is divided into three chapters: Foundation, Deep Learning and Advanced Topics.\nFoundation chapter covers the foundational concepts of data visualization, data science, and reproducibility. This will prepare you for building your own data science projects with modern deep learning tools.\nThe Deep Learning chapter covers the fundamental concepts of deep learning for text, images, and graphs. Through hands-on coding, you will learn how to build your own deep learning models for different data types.\nThe Advanced Topics chapter elevates you from a user to a creator of advanced soft computing models. You will learn how to build your own large language models and self-supervised learning models.\n\n\n\n\nEngaging Lectures: Each week, we’ll dive into key concepts in deep learning and complex systems, supported by interactive discussions and in-class activities.\nHands-on Coding: You’ll work with real data from text, images, and networks using Python and modern deep learning tools.\nCollaborative Learning: Participate in group activities, discussions, and our dedicated Discord server for Q&A and support.\nProjects and Assignments: Apply your knowledge through coding assignments, quizzes, and a final project.\nSupport: Get help from the instructor, TA, and our AI tutor.\n\n\n\n\n\nWhy applied soft computing? Read the Overview page to understand the importance of applied soft computing.\nRead the About Us page to meet your instructor, TA, and AI tutor.\nJoin the Discord server for announcements, help, and community.\nSet up your Python environment by following the Setup Guide.\nLearn how to submit assignments using GitHub Classroom.",
    "crumbs": [
      "Home",
      "Course Information",
      "Welcome"
    ]
  },
  {
    "objectID": "course/welcome.html#what-to-expect",
    "href": "course/welcome.html#what-to-expect",
    "title": "Welcome",
    "section": "",
    "text": "Engaging Lectures: Each week, we’ll dive into key concepts in deep learning and complex systems, supported by interactive discussions and in-class activities.\nHands-on Coding: You’ll work with real data from text, images, and networks using Python and modern deep learning tools.\nCollaborative Learning: Participate in group activities, discussions, and our dedicated Discord server for Q&A and support.\nProjects and Assignments: Apply your knowledge through coding assignments, quizzes, and a final project.\nSupport: Get help from the instructor, TA, and our AI tutor, Minidora.",
    "crumbs": [
      "Home",
      "Course Information",
      "Welcome"
    ]
  },
  {
    "objectID": "course/welcome.html#how-to-get-started",
    "href": "course/welcome.html#how-to-get-started",
    "title": "Welcome",
    "section": "",
    "text": "Read the About Us page to meet your instructor, TA, and Minidora.\nJoin the Discord server for announcements, help, and community.\nSet up your Python environment by following the Setup Guide.\nLearn how to submit assignments using GitHub Classroom.",
    "crumbs": [
      "Home",
      "Course Information",
      "Welcome"
    ]
  },
  {
    "objectID": "course/welcome.html#welcome-to-applied-soft-computing",
    "href": "course/welcome.html#welcome-to-applied-soft-computing",
    "title": "Welcome",
    "section": "",
    "text": "Welcome to the course! In this class, we will explore how deep learning serves as a powerful tool to operationalize high-dimensional data from social and physical systems. You’ll learn to model complex system dynamics using modern computational intelligence, and in turn, use perspectives from complexity science to understand the emergent behaviors of large-scale models.\nThis course is designed to provide you with both a theoretical foundation and hands-on experience in applied soft computing. You will learn how to apply representation learning, sequence modeling, and graph analytics to model real-world complex systems using Python and modern deep learning frameworks.\n\n\nThis course is divided into three chapters: Foundation, Deep Learning and Advanced Topics.\nFoundation chapter covers the foundational concepts of data visualization, data science, and reproducibility. This will prepare you for building your own data science projects with modern deep learning tools.\nThe Deep Learning chapter covers the fundamental concepts of deep learning for text, images, and graphs. Through hands-on coding, you will learn how to build your own deep learning models for different data types.\nThe Advanced Topics chapter elevates you from a user to a creator of advanced soft computing models. You will learn how to build your own large language models and self-supervised learning models.\n\n\n\n\nEngaging Lectures: Each week, we’ll dive into key concepts in deep learning and complex systems, supported by interactive discussions and in-class activities.\nHands-on Coding: You’ll work with real data from text, images, and networks using Python and modern deep learning tools.\nCollaborative Learning: Participate in group activities, discussions, and our dedicated Discord server for Q&A and support.\nProjects and Assignments: Apply your knowledge through coding assignments, quizzes, and a final project.\nSupport: Get help from the instructor, TA, and our AI tutor.\n\n\n\n\n\nWhy applied soft computing? Read the Overview page to understand the importance of applied soft computing.\nRead the About Us page to meet your instructor, TA, and AI tutor.\nJoin the Discord server for announcements, help, and community.\nSet up your Python environment by following the Setup Guide.\nLearn how to submit assignments using GitHub Classroom.",
    "crumbs": [
      "Home",
      "Course Information",
      "Welcome"
    ]
  },
  {
    "objectID": "course/discord.html",
    "href": "course/discord.html",
    "title": "Discord",
    "section": "",
    "text": "We use a dedicated Discord server for this course to facilitate communication, Q&A, and collaboration outside of class. The Discord server is a space where you can:\n\nAsk questions about lectures, assignments, and projects\nDiscuss concepts and share resources with your peers\nGet support from the instructor, TA, and Minidora (the AI tutor)\nJoin study groups and participate in informal discussions\n\nInvitation links to the Discord server will be distributed via Brightspace. Please check the Brightspace announcements or course materials for the latest invite link.\nOnce you join, you’ll find channels for different topics (e.g., #random, #questions, #study-groups) and can interact with both classmates, AI tutor, and course staff. If you’re new to Discord, it’s a free platform available on web, desktop, and mobile.\n\n\n\nScreenshot of the course Discord server\n\n\nExample screenshot of the course Discord server interface.\nIf you have any trouble joining, please contact the instructor for assistance.",
    "crumbs": [
      "Home",
      "Course Information",
      "Discord"
    ]
  },
  {
    "objectID": "course/setup.html",
    "href": "course/setup.html",
    "title": "Setup",
    "section": "",
    "text": "We’ll use Python to work with data throughout this course. Python is an excellent choice for deep learning and complex systems analysis for its rich ecosystem of libraries, readable and intuitive syntax, and well-documented documentation.",
    "crumbs": [
      "Home",
      "Course Information",
      "Setup"
    ]
  },
  {
    "objectID": "course/setup.html#python-and-virtual-environments",
    "href": "course/setup.html#python-and-virtual-environments",
    "title": "Setup",
    "section": "",
    "text": "We’ll use Python to work with data throughout this course. Python is an excellent choice for deep learning and complex systems analysis for its rich ecosystem of libraries, readable and intuitive syntax, and well-documented documentation.\nWe strongly recommend using virtual environments to manage your Python packages. Virtual environments create isolated Python installations for each project, avoiding dependency hell and providing several key benefits:\n\n\nDon’t confuse Python virtual environments with virtual machines (VMs). Python virtual environments are lightweight isolation tools that only separate Python packages and dependencies within the same operating system. Virtual machines, on the other hand, create complete isolated operating systems.\n\nReproducibility: Your code will work consistently across different machines and over time\nFlexibility: You can use different versions of packages for different projects without conflicts\nPrevent project interference: Changes to one project won’t break another project’s dependencies\n\n\n\n\n\n\n\nFigure 1: Without virtual environments, you risk dependency hell where package conflicts make your projects unusable.\n\n\n\nWe recommend using uv. uv is a fast Python package and project manager. While we won’t be running uv commands directly in this course, you’ll need uv to properly run Marimo notebooks, which provides a much better development experience. See here for installation instructions.\nFollow the following steps to install uv, along with the minimum Python packages required for this course.\n\nInstall uv\nRun the following command to create a new environment with the minimum Python packages required for this course.\n\nuv venv -p 3.11\nuv pip install matplotlib scipy numpy pandas seaborn pytorch torchvision marimo\n\nActivate the environment.\n\nsource .venv/bin/activate",
    "crumbs": [
      "Home",
      "Course Information",
      "Setup"
    ]
  },
  {
    "objectID": "course/setup.html#marimo-notebook",
    "href": "course/setup.html#marimo-notebook",
    "title": "Setup",
    "section": "3 Marimo Notebook",
    "text": "3 Marimo Notebook\nWe’ll use Marimo (GitHub) notebooks for assignments and interactive exercises throughout the course. Marimo is a reactive Python notebook that automatically updates when you change code, making it perfect for exploring deep learning models and seeing results in real-time.\n\n\n\n\nMarimo integrates especially tightly with uv and provides a package sandbox feature that lets you inline dependencies directly in notebook files. This is the easiest way to get started - no prior uv knowledge required.\nCreating a sandboxed notebook:\nuvx run --sandbox my_notebook.py\nThis command installs marimo in a temporary environment, tracks your dependencies and stores them in the notebook file, and automatically downloads any existing dependencies.",
    "crumbs": [
      "Home",
      "Course Information",
      "Setup"
    ]
  },
  {
    "objectID": "course/setup.html#why-virtual-environments",
    "href": "course/setup.html#why-virtual-environments",
    "title": "Setup",
    "section": "2 Why Virtual Environments?",
    "text": "2 Why Virtual Environments?\nWe strongly recommend using virtual environments to manage your Python packages. Virtual environments create isolated Python installations for each project, avoiding dependency hell and providing several key benefits:\n\n\nDon’t confuse Python virtual environments with virtual machines (VMs). Python virtual environments are lightweight isolation tools that only separate Python packages and dependencies within the same operating system. Virtual machines, on the other hand, create complete isolated operating systems.\n\n\n\n\n\n\nFigure 1: Without virtual environments, you risk dependency hell where package conflicts make your projects unusable.\n\n\n\nWe recommend using uv. uv is a fast Python package and project manager. While we won’t be running uv commands directly in this course, you’ll need uv to properly run Marimo notebooks, which provides a much better development experience. See here for installation instructions.\nFollow the following steps to install uv, along with the minimum Python packages required for this course.\n\nInstall uv\nRun the following command to create a new environment with the minimum Python packages required for this course.\n\nuv venv -p 3.11\nuv pip install matplotlib scipy numpy pandas seaborn pytorch torchvision marimo\n\nActivate the environment.\n\nsource .venv/bin/activate",
    "crumbs": [
      "Home",
      "Course Information",
      "Setup"
    ]
  },
  {
    "objectID": "course/setup.html#programming-language",
    "href": "course/setup.html#programming-language",
    "title": "Setup",
    "section": "",
    "text": "We’ll use Python to work with data throughout this course. Python is an excellent choice for deep learning and complex systems analysis for its rich ecosystem of libraries, readable and intuitive syntax, and well-documented documentation.",
    "crumbs": [
      "Home",
      "Course Information",
      "Setup"
    ]
  },
  {
    "objectID": "course/setup.html#create-a-virtual-environment-via-uv",
    "href": "course/setup.html#create-a-virtual-environment-via-uv",
    "title": "Setup",
    "section": "2 Create a Virtual Environment via uv",
    "text": "2 Create a Virtual Environment via uv\nWe strongly recommend using virtual environments to manage your Python packages. Virtual environments create isolated Python installations for each project, avoiding dependency hell and providing several key benefits:\n\n\nDon’t confuse Python virtual environments with virtual machines (VMs). Python virtual environments are lightweight isolation tools that only separate Python packages and dependencies within the same operating system. Virtual machines, on the other hand, create complete isolated operating systems.\n\n\n\n\n\n\nFigure 1: Without virtual environments, you risk dependency hell where package conflicts make your projects unusable.\n\n\n\nWe recommend using uv. uv is a fast Python package and project manager. While we won’t be running uv commands directly in this course, you’ll need uv to properly run Marimo notebooks, which provides a much better development experience. See here for installation instructions.\nFollow the following steps to install uv, along with the minimum Python packages required for this course.\n\nInstall uv\nRun the following command to create a new environment with the minimum Python packages required for this course.\n\nuv venv -p 3.11\nuv pip install matplotlib scipy numpy pandas seaborn pytorch torchvision marimo\n\nActivate the environment.\n\nsource .venv/bin/activate",
    "crumbs": [
      "Home",
      "Course Information",
      "Setup"
    ]
  },
  {
    "objectID": "course/setup.html#jupyter-marimo-notebook",
    "href": "course/setup.html#jupyter-marimo-notebook",
    "title": "Setup",
    "section": "3 Jupyter Marimo Notebook",
    "text": "3 Jupyter Marimo Notebook\nWe’ll use Marimo (GitHub) notebooks for assignments and interactive exercises throughout the course. Marimo is a reactive Python notebook that automatically updates when you change code, making it perfect for exploring deep learning models and seeing results in real-time.\n\n\n\n\nMarimo integrates especially tightly with uv and provides a package sandbox feature that lets you inline dependencies directly in notebook files. This is the easiest way to get started - no prior uv knowledge required.\nCreating a sandboxed notebook:\nuvx run --sandbox my_notebook.py\nThis command installs marimo in a temporary environment, tracks your dependencies and stores them in the notebook file, and automatically downloads any existing dependencies.",
    "crumbs": [
      "Home",
      "Course Information",
      "Setup"
    ]
  },
  {
    "objectID": "course/how-to-submit-assignment.html",
    "href": "course/how-to-submit-assignment.html",
    "title": "How to submit assignment",
    "section": "",
    "text": "In this course, we will use GitHub Classroom to submit & grade assignments. Please follow the instructions below to submit your assignment.",
    "crumbs": [
      "Home",
      "Course Information",
      "How to submit assignment"
    ]
  },
  {
    "objectID": "course/how-to-submit-assignment.html#option-1-a-simple-workflow-full-local",
    "href": "course/how-to-submit-assignment.html#option-1-a-simple-workflow-full-local",
    "title": "How to submit assignment",
    "section": "1 Option 1: A simple workflow (Full local)",
    "text": "1 Option 1: A simple workflow (Full local)\nSee the slides for the detailed instructions.\n\nClone the repository from GitHub.\nEdit the assignment.py with marimo editor. Type marimo edit assignment/assignment.py\nSubmit the assignment.py via git. (You can use GitHub Desktop, or command line)\nCheck the grading on the GitHub Classroom.",
    "crumbs": [
      "Home",
      "Course Information",
      "How to submit assignment"
    ]
  },
  {
    "objectID": "course/how-to-submit-assignment.html#option-2-github-codespaces-full-cloud",
    "href": "course/how-to-submit-assignment.html#option-2-github-codespaces-full-cloud",
    "title": "How to submit assignment",
    "section": "2 Option 2: Github Codespaces (Full cloud)",
    "text": "2 Option 2: Github Codespaces (Full cloud)\nSee the slides for the detailed instructions.\n\nGo to your assignment repository on GitHub\nClick the green “Code” button\nClick the “Open with Codespaces” button\nWait for the Codespaces to be ready.\nType ‘marimo edit assignment/assignment.py’. If you cannot find marimo, type “uv run marimo edit assignment/assignment.py” which should work.\nYou will be redirected to a webpage and prompted to enter the access token. The access token can be found on the terminal window in the Codespaces.\nTake the access token in the url “the alphabets after”?access_token=” and enter the token in the webpage.",
    "crumbs": [
      "Home",
      "Course Information",
      "How to submit assignment"
    ]
  },
  {
    "objectID": "course/how-to-submit-assignment.html#option-3-local-but-with-docker-machine",
    "href": "course/how-to-submit-assignment.html#option-3-local-but-with-docker-machine",
    "title": "How to submit assignment",
    "section": "3 Option 3: Local but with Docker Machine",
    "text": "3 Option 3: Local but with Docker Machine\nSee the slides for the detailed instructions.\n\nPreparations\n\nInstall Docker Desktop\nInstall GitHub Desktop\nInstall VS Code\n\n\n\nSteps\n\nClone the repository from GitHub.\nOpen with the VS Code, and click “Reopen in Container”\nOpen the assignment.py with marimo editor.\nSubmit the assignment.py to the repository.",
    "crumbs": [
      "Home",
      "Course Information",
      "How to submit assignment"
    ]
  },
  {
    "objectID": "m01-toolkit/overview.html",
    "href": "m01-toolkit/overview.html",
    "title": "Overview",
    "section": "",
    "text": "Imagine spending months on a groundbreaking data analysis, only to find that you can’t reproduce your own results. Or, imagine a colleague asks for your code and data from a project you did last year, and you can’t remember where you saved the files, or which version of the code produced the final results. These scenarios are all too common in data science, and they highlight the importance of Provenance. Provenance is a complete lineage of the data and code from its origin to its final form. It is a cornerstone of good science, allowing others to verify your findings and build upon your work.\nThis module will introduce you to the tools and principles that will help you build a reproducible data science pipeline in a systematic way. We’ll cover the following topics:\n\nVersion Control: We’ll explore the “horror stories” of what can happen without proper version control, from losing days of work to causing major security breaches. We’ll then introduce Git and GitHub as powerful tools to track changes in your code and data, collaborate with others, and save yourself from future headaches.\n\nVersion Control with Git & GitHub\n\nData Provenance and Tidy Data: We’ll discuss the importance of knowing the history of your data (data provenance) and how to structure it in a way that makes it easy to work with (tidy data). We’ll see how a little bit of organization up front can save you hours of pain and suffering down the road.\n\nData Provenance\nTidy Data\n\nReproduceability: We will learn how to build a reproducible data science pipeline, from the environment to the code to the data.\n\nReproducible Environments & Projects\n\n\nBy the end of this module, you’ll have a solid foundation in the tools and principles of reproducible data science, and you’ll be well on your way to becoming a data science rockstar.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Overview"
    ]
  },
  {
    "objectID": "m01-toolkit/overview.html#the-perils-of-irreproducible-science",
    "href": "m01-toolkit/overview.html#the-perils-of-irreproducible-science",
    "title": "The Data Scientist’s Toolkit",
    "section": "",
    "text": "Imagine spending months on a groundbreaking data analysis, only to find that you can’t reproduce your own results. Or, imagine a colleague asks for your code and data from a project you did last year, and you can’t remember where you saved the files, or which version of the code produced the final results.\nThese scenarios are all too common in data science, and they highlight the importance of reproducibility. Reproducibility is the ability to recreate the same results using the same data and analysis. It is a cornerstone of good science, allowing others to verify your findings and build upon your work. Without it, your results are of limited value.\nThis module will introduce you to the tools and principles that will help you avoid these pitfalls and become a more effective and reproducible data scientist. We’ll focus on three key areas:\n\nVersion Control: We’ll explore the “horror stories” of what can happen without proper version control, from losing days of work to causing major security breaches. We’ll then introduce Git and GitHub as powerful tools to track changes in your code and data, collaborate with others, and save yourself from future headaches.\nData Provenance and Tidy Data: We’ll discuss the importance of knowing the history of your data (data provenance) and how to structure it in a way that makes it easy to work with (tidy data). We’ll see how a little bit of organization up front can save you hours of pain and suffering down the road.\nDocumentation: We’ll learn why documentation is not just an afterthought, but a crucial part of the data science process. We’ll explore how good documentation can make your work more understandable, reusable, and impactful.\n\nBy the end of this module, you’ll have a solid foundation in the tools and principles of reproducible data science, and you’ll be well on your way to becoming a data science rockstar.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Overview"
    ]
  },
  {
    "objectID": "m01-toolkit/overview.html#further-reading",
    "href": "m01-toolkit/overview.html#further-reading",
    "title": "The Data Scientist’s Toolkit",
    "section": "2 Further Reading",
    "text": "2 Further Reading\n\nWhat is Data Provenance and Why is it Important?\nData Provenance: What It Is, Why It Matters, and How to Implement It\nVersion Control for Data Science\nA Guide to Version Control for Data Scientists",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Overview"
    ]
  },
  {
    "objectID": "m01-toolkit/git-github.html",
    "href": "m01-toolkit/git-github.html",
    "title": "Git & GitHub: Your Time Machine for Code",
    "section": "",
    "text": "The Day the Code Disappeared\nPicture this: you’re working late on a project, you’ve finally fixed a major bug, and you’re about to go home. You close your laptop, and the next morning, you open it up to find that all your changes from the previous day are gone. Your heart sinks. You have no idea what you did to fix the bug, and you have to start all over again.\nEven the pros can make mistakes. In 2017, GitLab, a major code hosting platform, suffered a catastrophic outage. A system administrator accidentally deleted a massive amount of production data, and the backups… well, they didn’t work as expected. The company lost six hours of customer data, a lifetime in the fast-paced world of software development.\n\n\nYou can read the full, cringe-worthy post-mortem on the GitLab blog.\n\n\nWe accidentally deleted production data and might have to restore from backup. Google Doc with live notes https://t.co/EVRbHzYlk8\n\n— GitLab.com Status ((gitlabstatus?)) February 1, 2017\n\n\n\n\nVersion Control\nVersion control is a very important part of any data-related work. A version control system (VCS) saves “snapshots” of your files. The most popular system today is Git, and when you put Git in the cloud (like on GitHub), you can access your work from anywhere, share it, and even show it off.\n\n\n\n\nThe following blog is a good minimal documentation about git and github.\n\nA layman’s introduction to Git\n\nYou can also test your understanding of Git and GitHub with the following interactive tutorial:\n\nInteractive Git Tutorial - Visual, hands-on learning\n\nFor more comprehensive documentation, you can refer to the following resources:\n\nGit Documentation\nAtlassian Git Tutorials - Detailed tutorials with examples\n\nAnd I recommend the beginners to start with GitHub Desktop, instead of command line, to manage your Git repositories.\n\nGitHub Desktop Documentation - Official desktop app guide",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Version Control with Git & GitHub"
    ]
  },
  {
    "objectID": "m01-toolkit/git-github.html#the-day-the-code-disappeared",
    "href": "m01-toolkit/git-github.html#the-day-the-code-disappeared",
    "title": "Git & GitHub: Your Time Machine for Code",
    "section": "",
    "text": "Picture this: you’re working late on a project, you’ve finally fixed a major bug, and you’re about to go home. You close your laptop, and the next morning, you open it up to find that all your changes from the previous day are gone. Your heart sinks. You have no idea what you did to fix the bug, and you have to start all over again.\nEven the pros can make mistakes. In 2017, GitLab, a major code hosting platform, suffered a catastrophic outage. A system administrator accidentally deleted a massive amount of production data, and the backups… well, they didn’t work as expected. The company lost six hours of customer data, a lifetime in the fast-paced world of software development.\n\n\nYou can read the full, cringe-worthy post-mortem on the GitLab blog.\n\n\nWe accidentally deleted production data and might have to restore from backup. Google Doc with live notes https://t.co/EVRbHzYlk8\n\n— GitLab.com Status ((gitlabstatus?)) February 1, 2017\n\n\n\n\nVersion control is a very important part of any data-related work. A version control system (VCS) saves “snapshots” of your files. The most popular system today is Git, and when you put Git in the cloud (like on GitHub), you can access your work from anywhere, share it, and even show it off.\n\n\n\n\nThe following blog is a good minimal documentation about git and github.\n\nA layman’s introduction to Git\n\nYou can also test your understanding of Git and GitHub with the following interactive tutorial:\n\nInteractive Git Tutorial - Visual, hands-on learning\n\nFor more comprehensive documentation, you can refer to the following resources:\n\nGit Documentation\nAtlassian Git Tutorials - Detailed tutorials with examples\n\nAnd I recommend the beginners to start with GitHub Desktop, instead of command line, to manage your Git repositories.\n\nGitHub Desktop Documentation - Official desktop app guide",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Version Control with Git & GitHub"
    ]
  },
  {
    "objectID": "m01-toolkit/git-github.html#cautionary-tales-from-the-crypt-of-bad-version-control",
    "href": "m01-toolkit/git-github.html#cautionary-tales-from-the-crypt-of-bad-version-control",
    "title": "Git & GitHub: Your Time Machine for Code",
    "section": "2 Cautionary Tales from the Crypt of Bad Version Control",
    "text": "2 Cautionary Tales from the Crypt of Bad Version Control\nTo truly appreciate the power of version control, it helps to hear a few horror stories.\n\nThe GitLab Apocalypse\nIn 2017, GitLab, a major code hosting platform, suffered a catastrophic outage. A system administrator accidentally deleted a massive amount of production data, and the backups… well, they didn’t work as expected. The company lost six hours of customer data, a lifetime in the fast-paced world of software development.\nThe incident was a stark reminder that even the pros can make mistakes, and that having a solid backup and recovery plan is non-negotiable. You can read the full, cringe-worthy post-mortem on the GitLab blog.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Version Control with Git & GitHub"
    ]
  },
  {
    "objectID": "m01-toolkit/git-github.html#your-journey-to-version-control-zen",
    "href": "m01-toolkit/git-github.html#your-journey-to-version-control-zen",
    "title": "Git & GitHub: Your Time Machine for Code",
    "section": "3 Your Journey to Version Control Zen",
    "text": "3 Your Journey to Version Control Zen\nThese stories might be scary, but don’t worry. By the end of this module, you’ll have the knowledge and skills to use Git and GitHub effectively and avoid these common pitfalls. You’ll learn how to:\n\nCreate and manage your own Git repositories.\nCollaborate with others on a project using GitHub.\nUse version control to track your work and save yourself from future headaches.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Version Control with Git & GitHub"
    ]
  },
  {
    "objectID": "m01-toolkit/git-github.html#learning-resources",
    "href": "m01-toolkit/git-github.html#learning-resources",
    "title": "Git & GitHub: Your Time Machine for Code",
    "section": "4 Learning Resources",
    "text": "4 Learning Resources\n\nInteractive Git Tutorial - Visual, hands-on learning\nGitHub Desktop Documentation - Official desktop app guide\nAtlassian Git Tutorials - Detailed tutorials with examples",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Version Control with Git & GitHub"
    ]
  },
  {
    "objectID": "m01-toolkit/overview.html#overview",
    "href": "m01-toolkit/overview.html#overview",
    "title": "The Data Scientist’s Toolkit",
    "section": "",
    "text": "Imagine spending months on a groundbreaking data analysis, only to find that you can’t reproduce your own results. Or, imagine a colleague asks for your code and data from a project you did last year, and you can’t remember where you saved the files, or which version of the code produced the final results. These scenarios are all too common in data science, and they highlight the importance of Provenance. Provenance is a complete lineage of the data and code from its origin to its final form. It is a cornerstone of good science, allowing others to verify your findings and build upon your work.\nThis module will introduce you to the tools and principles that will help you build a reproducible data science pipeline in a systematic way. We’ll cover the following topics:\n\nVersion Control: We’ll explore the “horror stories” of what can happen without proper version control, from losing days of work to causing major security breaches. We’ll then introduce Git and GitHub as powerful tools to track changes in your code and data, collaborate with others, and save yourself from future headaches.\n\nVersion Control with Git & GitHub\n\nData Provenance and Tidy Data: We’ll discuss the importance of knowing the history of your data (data provenance) and how to structure it in a way that makes it easy to work with (tidy data). We’ll see how a little bit of organization up front can save you hours of pain and suffering down the road.\n\nData Provenance\nTidy Data\n\nDocumentation: We’ll learn why documentation is not just an afterthought, but a crucial part of the data science process. We’ll explore how good documentation can make your work more understandable, reusable, and impactful.\n\nDocumentation\n\n\nBy the end of this module, you’ll have a solid foundation in the tools and principles of reproducible data science, and you’ll be well on your way to becoming a data science rockstar.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Overview"
    ]
  },
  {
    "objectID": "m01-toolkit/reproduceability.html",
    "href": "m01-toolkit/reproduceability.html",
    "title": "Reproducible Environments & Projects",
    "section": "",
    "text": "Reproducibility is a cornerstone of good science and a fundamental principle in computational research. It is the ability of an independent researcher to duplicate the results of a prior study using the same materials and procedures as were used by the original investigator. In the context of data science and soft computing, this means that your code, given the same input data, should produce the exact same outputs, every single time, regardless of the machine it is run on.\n\n\nWhile the terms are often used interchangeably, it’s helpful to distinguish between them:\n\nReproducibility: Can an independent researcher achieve the exact same results using the original author’s data and code? This is a computational challenge.\nReplicability: Can an independent researcher corroborate the scientific conclusions of a study by conducting a new, independent study? This is a scientific challenge.\n\nThis note focuses on achieving computational reproducibility, which is the essential first step. If your own analysis isn’t reproducible, it’s impossible for anyone to even attempt to replicate your scientific findings.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Reproducibility"
    ]
  },
  {
    "objectID": "m01-toolkit/reproduceability.html#the-perils-of-irreproducible-science",
    "href": "m01-toolkit/reproduceability.html#the-perils-of-irreproducible-science",
    "title": "The Data Scientist’s Toolkit",
    "section": "",
    "text": "Imagine spending months on a groundbreaking data analysis, only to find that you can’t reproduce your own results. Or, imagine a colleague asks for your code and data from a project you did last year, and you can’t remember where you saved the files, or which version of the code produced the final results.\nThese scenarios are all too common in data science, and they highlight the importance of reproducibility. Reproducibility is the ability to recreate the same results using the same data and analysis. It is a cornerstone of good science, allowing others to verify your findings and build upon your work. Without it, your results are of limited value.\nThis module will introduce you to the tools and principles that will help you avoid these pitfalls and become a more effective and reproducible data scientist. We’ll focus on three key areas:\n\nVersion Control: We’ll explore the “horror stories” of what can happen without proper version control, from losing days of work to causing major security breaches. We’ll then introduce Git and GitHub as powerful tools to track changes in your code and data, collaborate with others, and save yourself from future headaches.\nData Provenance and Tidy Data: We’ll discuss the importance of knowing the history of your data (data provenance) and how to structure it in a way that makes it easy to work with (tidy data). We’ll see how a little bit of organization up front can save you hours of pain and suffering down the road.\nDocumentation: We’ll learn why documentation is not just an afterthought, but a crucial part of the data science process. We’ll explore how good documentation can make your work more understandable, reusable, and impactful.\n\nBy the end of this module, you’ll have a solid foundation in the tools and principles of reproducible data science, and you’ll be well on your way to becoming a data science rockstar.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Reproducibility"
    ]
  },
  {
    "objectID": "m01-toolkit/reproduceability.html#further-reading",
    "href": "m01-toolkit/reproduceability.html#further-reading",
    "title": "The Data Scientist’s Toolkit",
    "section": "2 Further Reading",
    "text": "2 Further Reading\n\nWhat is Data Provenance and Why is it Important?\nData Provenance: What It Is, Why It Matters, and How to Implement It\nVersion Control for Data Science\nA Guide to Version Control for Data Scientists",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Reproducibility"
    ]
  },
  {
    "objectID": "m01-toolkit/git-github.html#learn-how-to-use-git-and-github",
    "href": "m01-toolkit/git-github.html#learn-how-to-use-git-and-github",
    "title": "Git & GitHub: Your Time Machine for Code",
    "section": "2 Learn how to use Git and GitHub",
    "text": "2 Learn how to use Git and GitHub",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Version Control with Git & GitHub"
    ]
  },
  {
    "objectID": "m01-toolkit/tidy-data.html",
    "href": "m01-toolkit/tidy-data.html",
    "title": "The Tidy Data Philosophy",
    "section": "",
    "text": "It is often said that 80% of data analysis is spent on the process of cleaning and preparing the data. Read the following paper to learn more about the tidy data philosophy.\n\nTidy Data by Hadley Wickham“.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "The Tidy Data Philosophy"
    ]
  },
  {
    "objectID": "m01-toolkit/tidy-data.html#introduction",
    "href": "m01-toolkit/tidy-data.html#introduction",
    "title": "The Tidy Data Philosophy",
    "section": "",
    "text": "It is often said that 80% of data analysis is spent on the process of cleaning and preparing the data. Read the following paper to learn more about the tidy data philosophy.\n\nTidy Data by Hadley Wickham“.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "The Tidy Data Philosophy"
    ]
  },
  {
    "objectID": "m01-toolkit/tidy-data.html#what-is-tidy-data",
    "href": "m01-toolkit/tidy-data.html#what-is-tidy-data",
    "title": "The Tidy Data Philosophy",
    "section": "2 What is Tidy Data?",
    "text": "2 What is Tidy Data?\nTidy data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data:\n\nEach variable forms a column. A variable contains all values that measure the same underlying attribute (like height, temperature, duration) across units.\nEach observation forms a row. An observation contains all values measured on the same unit (like a person, or a day, or a race) across attributes.\nEach type of observational unit forms a table. For example, in a study of allergy medication, you might have a table for demographic data, a table for daily medical data, and a table for meteorological data, not just one big table that contains all the data.\n\nTidy datasets are easy to manipulate, model and visualise. They make it easier to explore, manipulate and analyze the data. And most importantly, tidy formats standardize the way data is organized, making code reusable and reliable.\n\nWhat are not tidy data?\nHere are five of the most common problems with not tidy datasets:\n\nColumn headers are values, not variable names. For example, a table where months (“Jan”, “Feb”, “Mar”) are the column headers, instead of having a single “Month” column with “Jan”, “Feb”, etc. as values.\nMultiple variables are stored in one column. For example, a column named “height_weight” that contains values like “5.5_130”, rather than splitting these into separate “height” and “weight” columns.\nVariables are stored in both rows and columns. For example, a dataset where one piece of information (like gender) is encoded in both a specific column and within the values of another column, making analysis and transformation more difficult.\nMultiple types of observational units are stored in the same table. For example, a table that contains both patient demographic information and medical test results, mixing fundamentally different kinds of data in one place.\nA single observational unit is stored in multiple tables. For example, patient information split across one table for addresses, another for test results, and another for appointments—all without a neat way to link them together as single observations.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "The Tidy Data Philosophy"
    ]
  },
  {
    "objectID": "m01-toolkit/tidy-data.html#why-tidy-data",
    "href": "m01-toolkit/tidy-data.html#why-tidy-data",
    "title": "The Tidy Data Philosophy",
    "section": "3 Why Tidy Data?",
    "text": "3 Why Tidy Data?\nTidy datasets are easy to manipulate, model and visualise. They make it easier to:\n\nExplore and visualize your data.\nManipulate and analyze your data.\nShare your data with others.\n\nA standard makes initial data cleaning easier because you don’t need to start from scratch and reinvent the wheel every time. The tidy data standard has been designed to facilitate initial exploration and analysis of the data, and to simplify the development of data analysis tools that work well together.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "The Tidy Data Philosophy"
    ]
  },
  {
    "objectID": "m01-toolkit/tidy-data.html#common-messy-data-problems",
    "href": "m01-toolkit/tidy-data.html#common-messy-data-problems",
    "title": "The Tidy Data Philosophy",
    "section": "4 Common Messy Data Problems",
    "text": "4 Common Messy Data Problems\nMost real-world datasets are messy. Here are five of the most common problems with messy datasets:\n\nColumn headers are values, not variable names.\nMultiple variables are stored in one column.\nVariables are stored in both rows and columns.\nMultiple types of observational units are stored in the same table.\nA single observational unit is stored in multiple tables.\n\nMost messy datasets can be tidied with a small set of tools: melting, string splitting, and casting.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "The Tidy Data Philosophy"
    ]
  },
  {
    "objectID": "m01-toolkit/tidy-data.html#tidy-tools",
    "href": "m01-toolkit/tidy-data.html#tidy-tools",
    "title": "The Tidy Data Philosophy",
    "section": "3 Tidy Tools",
    "text": "3 Tidy Tools\nLet’s learn some tools to tidy your data through some examples.\nThe following examples are from Python for Data Science.\n\nMelt\nOften, data can be stored in a “wide” format, where different columns represent different variables of the same type. For example, think about the following dataset:\n\n\nCode\nimport pandas as pd\ndf = pd.DataFrame({'first': ['John', 'Mary'],\n                   'last': ['Smith', 'Doe'],\n                   'height': [5.5, 5.0],\n                   'weight': [130, 110]})\ndf\n\n\n\n\n\n\n\n\n\nfirst\nlast\nheight\nweight\n\n\n\n\n0\nJohn\nSmith\n5.5\n130\n\n\n1\nMary\nDoe\n5.0\n110\n\n\n\n\n\n\n\nwhere “height” and “weight” are separate columns, but tidy data principles ask for each variable to form its own column and each observation to form a row. This “wide” format can make analysis more difficult if you want to compare or plot variables together.\nThe pandas.DataFrame.melt() method fixes this by transforming the data from “wide” to “long” format, making it tidy. After melting, instead of having separate columns for “height” and “weight”, you have just one column storing the variable type (like “height” or “weight”) and another column with the corresponding value for each observation.\nHere’s how it works:\n\nimport pandas as pd\ndf_melted = df.melt(\n    id_vars=['first', 'last'],\n    var_name='quantity',\n    value_name='value'\n)\ndf_melted\n\n\n\n\n\n\n\n\nfirst\nlast\nquantity\nvalue\n\n\n\n\n0\nJohn\nSmith\nheight\n5.5\n\n\n1\nMary\nDoe\nheight\n5.0\n\n\n2\nJohn\nSmith\nweight\n130.0\n\n\n3\nMary\nDoe\nweight\n110.0\n\n\n\n\n\n\n\nNow each row represents a single measurement (either height or weight) for an individual, rather than having two measurements in one row. This is a key part of “tidy data”.\n\n\nPivot\nSometimes, your data is in a “long” format: for instance, you might have a separate row for each type of measurement (like “cases” or “population”) for each country and year. This can make it difficult to see all the information about a single observation (for example, all statistics for country A in 2020) at once.\n\n\nCode\nimport numpy as np\n\n# Long format: each row is a different variable for country and year\ndf = pd.DataFrame({\n    'country': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],\n    'year': [2020, 2021, 2020, 2021, 2020, 2021, 2020, 2021],\n    'variable': ['cases', 'cases', 'population', 'population', 'cases', 'cases', 'population', 'population'],\n    'value': [100, 200, 120, 220, 130, 230, 140, 240]\n})\ndf\n\n\n\n\n\n\n\n\n\ncountry\nyear\nvariable\nvalue\n\n\n\n\n0\nA\n2020\ncases\n100\n\n\n1\nA\n2021\ncases\n200\n\n\n2\nA\n2020\npopulation\n120\n\n\n3\nA\n2021\npopulation\n220\n\n\n4\nB\n2020\ncases\n130\n\n\n5\nB\n2021\ncases\n230\n\n\n6\nB\n2020\npopulation\n140\n\n\n7\nB\n2021\npopulation\n240\n\n\n\n\n\n\n\nThe pivot() function solves this problem by reshaping the data so that each observation (combination of country and year) has its measurements as columns. This transforms your data back to a wider, more analyzable format.\nNow, each row contains all measurements (“cases”, “population”) for a given country and year—making your data tidy and easier to analyze!\n\n\nStack and Unstack\nSometimes your data uses multi-level column headers, where variables are split across two or more header rows (for example, measurements for different people and types, such as test results for multiple groups shown as columns). This structure can make it awkward to access or visualize the data, as related values are spread apart and grouped by columns.\n\n\nCode\n# Example: multi-level columns for two participants (P1, P2) and two attributes (A, B)\nheader = pd.MultiIndex.from_product([['P1','P2'],['A','B']])\ndf = pd.DataFrame(np.random.rand(4, 4),\n                  columns=header)\ndf\n\n\n\n\n\n\n\n\n\nP1\nP2\n\n\n\nA\nB\nA\nB\n\n\n\n\n0\n0.285451\n0.309678\n0.451386\n0.759763\n\n\n1\n0.670098\n0.003058\n0.453107\n0.435050\n\n\n2\n0.255224\n0.676534\n0.569125\n0.564110\n\n\n3\n0.302557\n0.370609\n0.731544\n0.961658\n\n\n\n\n\n\n\nThe stack() method helps to “tidy” this kind of data by turning one of the levels of columns into a new row index, effectively transforming wide-form data to long-form, so each row represents a single measurement. This makes analysis and plotting easier, as all values of the same variable are stacked in a single column. The unstack() method reverses this, spreading data back into columns from the index.\nHere’s how stack() and unstack() work in practice:\n\ndf.stack(future_stack=True)\n\n\n\n\n\n\n\n\n\nP1\nP2\n\n\n\n\n0\nA\n0.285451\n0.451386\n\n\nB\n0.309678\n0.759763\n\n\n1\nA\n0.670098\n0.453107\n\n\nB\n0.003058\n0.435050\n\n\n2\nA\n0.255224\n0.569125\n\n\nB\n0.676534\n0.564110\n\n\n3\nA\n0.302557\n0.731544\n\n\nB\n0.370609\n0.961658\n\n\n\n\n\n\n\n\ndf.stack(future_stack=True).unstack()\n\n\n\n\n\n\n\n\nP1\nP2\n\n\n\nA\nB\nA\nB\n\n\n\n\n0\n0.285451\n0.309678\n0.451386\n0.759763\n\n\n1\n0.670098\n0.003058\n0.453107\n0.435050\n\n\n2\n0.255224\n0.676534\n0.569125\n0.564110\n\n\n3\n0.302557\n0.370609\n0.731544\n0.961658",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "The Tidy Data Philosophy"
    ]
  },
  {
    "objectID": "m01-toolkit/tidy-data.html#examples",
    "href": "m01-toolkit/tidy-data.html#examples",
    "title": "The Tidy Data Philosophy",
    "section": "6 Examples",
    "text": "6 Examples\nThe following examples are from Python for Data Science.\n\nMelt\nThe melt() function transforms “wider” data into “longer” data.\n\nimport pandas as pd\ndf = pd.DataFrame({'first': ['John', 'Mary'],\n                   'last': ['Smith', 'Doe'],\n                   'height': [5.5, 5.0],\n                   'weight': [130, 110]})\n\nprint(\"Original Data:\")\nprint(df)\n\ndf_melted = df.melt(id_vars=['first', 'last'],\n                    var_name='quantity',\n                    value_name='value')\n\nprint(\"\\nMelted Data:\")\nprint(df_melted)\n\nOriginal Data:\n  first   last  height  weight\n0  John  Smith     5.5     130\n1  Mary    Doe     5.0     110\n\nMelted Data:\n  first   last quantity  value\n0  John  Smith   height    5.5\n1  Mary    Doe   height    5.0\n2  John  Smith   weight  130.0\n3  Mary    Doe   weight  110.0\n\n\n\n\nPivot\nThe pivot() function helps reorganize data where a single observation is spread over multiple rows.\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'country': ['A', 'A', 'B', 'B'],\n                   'year': [2020, 2021, 2020, 2021],\n                   'variable': ['cases', 'population', 'cases', 'population'],\n                   'value': [100, 1000, 120, 1200]})\n\nprint(\"Original Data:\")\nprint(df)\n\ndf_pivoted = df.pivot(index=['country', 'year'],\n                      columns='variable',\n                      values='value').reset_index()\n\nprint(\"\\nPivoted Data:\")\nprint(df_pivoted)\n\nOriginal Data:\n  country  year    variable  value\n0       A  2020       cases    100\n1       A  2021  population   1000\n2       B  2020       cases    120\n3       B  2021  population   1200\n\nPivoted Data:\nvariable country  year  cases  population\n0              A  2020  100.0         NaN\n1              A  2021    NaN      1000.0\n2              B  2020  120.0         NaN\n3              B  2021    NaN      1200.0\n\n\n\n\nStack and Unstack\nstack() converts a single type of wide data variable from columns into a long-form dataset, adding an extra index level. unstack() performs the reverse operation.\n\nimport pandas as pd\n\nheader = pd.MultiIndex.from_product([['P1','P2'],['A','B']])\ndf = pd.DataFrame(np.random.rand(4, 4),\n                  columns=header)\n\nprint(\"Original Data:\")\nprint(df)\n\ndf_stacked = df.stack()\n\nprint(\"\\nStacked Data:\")\nprint(df_stacked)\n\ndf_unstacked = df_stacked.unstack()\n\nprint(\"\\nUnstacked Data:\")\nprint(df_unstacked)\n\nOriginal Data:\n         P1                  P2          \n          A         B         A         B\n0  0.883896  0.154628  0.802782  0.006698\n1  0.546623  0.114752  0.541440  0.969985\n2  0.243781  0.991905  0.837751  0.413695\n3  0.928587  0.848723  0.814823  0.158570\n\nStacked Data:\n           P1        P2\n0 A  0.883896  0.802782\n  B  0.154628  0.006698\n1 A  0.546623  0.541440\n  B  0.114752  0.969985\n2 A  0.243781  0.837751\n  B  0.991905  0.413695\n3 A  0.928587  0.814823\n  B  0.848723  0.158570\n\nUnstacked Data:\n         P1                  P2          \n          A         B         A         B\n0  0.883896  0.154628  0.802782  0.006698\n1  0.546623  0.114752  0.541440  0.969985\n2  0.243781  0.991905  0.837751  0.413695\n3  0.928587  0.848723  0.814823  0.158570\n\n\n/var/folders/j7/9dgqq5g53vnbsbmvh2yqtckr0000gr/T/ipykernel_5113/2394227059.py:10: FutureWarning: The previous implementation of stack is deprecated and will be removed in a future version of pandas. See the What's New notes for pandas 2.1.0 for details. Specify future_stack=True to adopt the new implementation and silence this warning.\n  df_stacked = df.stack()",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "The Tidy Data Philosophy"
    ]
  },
  {
    "objectID": "m01-toolkit/data-provenance.html",
    "href": "m01-toolkit/data-provenance.html",
    "title": "Data Provenance: The Secret Life of Data",
    "section": "",
    "text": "Have you ever opened a dataset you worked on a few months ago, only to find that you have no idea where it came from, what the columns mean, or what transformations you applied to it? It’s a common experience for data scientists, and it highlights the importance of data provenance.\nData provenance is the story of your data. It’s the who, what, when, where, and why of your data’s journey from its raw form to its current state. It’s the secret life of your data, and understanding it is crucial for doing good science.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Data Provenance"
    ]
  },
  {
    "objectID": "m01-toolkit/data-provenance.html#the-mystery-of-the-disappearing-data",
    "href": "m01-toolkit/data-provenance.html#the-mystery-of-the-disappearing-data",
    "title": "Data Provenance: The Secret Life of Data",
    "section": "",
    "text": "Have you ever opened a dataset you worked on a few months ago, only to find that you have no idea where it came from, what the columns mean, or what transformations you applied to it? It’s a common experience for data scientists, and it highlights the importance of data provenance.\nData provenance is the story of your data. It’s the who, what, when, where, and why of your data’s journey from its raw form to its current state. It’s the secret life of your data, and understanding it is crucial for doing good science.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Data Provenance"
    ]
  },
  {
    "objectID": "m01-toolkit/data-provenance.html#a-tale-of-two-economists-and-a-spreadsheet-error",
    "href": "m01-toolkit/data-provenance.html#a-tale-of-two-economists-and-a-spreadsheet-error",
    "title": "Data Provenance: The Secret Life of Data",
    "section": "2 A Tale of Two Economists and a Spreadsheet Error",
    "text": "2 A Tale of Two Economists and a Spreadsheet Error\n\nIn 2010, two Harvard economists, Carmen Reinhart and Kenneth Rogoff, published a paper called “Growth in a Time of Debt.” The paper argued that countries with high levels of government debt tend to have lower economic growth. The paper was incredibly influential and was used to justify austerity policies in countries around the world.\nBut there was a problem. In 2013, a graduate student named Thomas Herndon tried to reproduce Reinhart and Rogoff’s results, and he couldn’t. He eventually got a copy of their original spreadsheet and found a simple error: they had accidentally excluded the first five countries from their analysis. When Herndon corrected the error, the main result of the paper disappeared.\nThis story is a powerful reminder of why data provenance is so important. Without a clear record of how the data was processed, it’s easy for errors to creep in and for those errors to have a huge impact.\nSee the full story for more details.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Data Provenance"
    ]
  },
  {
    "objectID": "m01-toolkit/data-provenance.html#the-data-detectives-toolkit",
    "href": "m01-toolkit/data-provenance.html#the-data-detectives-toolkit",
    "title": "Data Provenance: The Secret Life of Data",
    "section": "3 The Data Detective’s Toolkit",
    "text": "3 The Data Detective’s Toolkit\nSo, how do you become a data detective and keep track of your data’s provenance? Here are a few tools and techniques:\n\nLab Notebooks: Keep a detailed record of your work in a lab notebook (physical or digital). This should include where you got your data, what you did to it, and why you did it.\nScripting: Use scripts (e.g., Python, R) to process your data. Your scripts are a form of documentation that can be version controlled.\nWorkflow Management Tools: For complex projects, use tools like Snakemake or Nextflow to define and manage your data analysis pipelines. These tools automatically track the provenance of your data.\n\nBy embracing the principles of data provenance, you can become a more effective and reproducible data scientist. You’ll be able to trust your own results, and others will be able to trust them too.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Data Provenance"
    ]
  },
  {
    "objectID": "m01-toolkit/data-provenance.html#further-reading",
    "href": "m01-toolkit/data-provenance.html#further-reading",
    "title": "Data Provenance: The Secret Life of Data",
    "section": "4 Further Reading",
    "text": "4 Further Reading\n\nWhat is Data Provenance and Why is it Important?\nData Provenance: What It Is, Why It Matters, and How to Implement It",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Data Provenance"
    ]
  },
  {
    "objectID": "m01-toolkit/environments.html",
    "href": "m01-toolkit/environments.html",
    "title": "Reproducible Environments & Projects",
    "section": "",
    "text": "Reproducibility is a cornerstone of good science and a fundamental principle in computational research. It is the ability of an independent researcher to duplicate the results of a prior study using the same materials and procedures as were used by the original investigator. In the context of data science and soft computing, this means that your code, given the same input data, should produce the exact same outputs, every single time, regardless of the machine it is run on.\n\n\n\n\n\n\nReproducibility vs. Replicability\n\n\n\nWhile the terms are often used interchangeably, it’s helpful to distinguish between them:\n\nReproducibility: Can an independent researcher achieve the exact same results using the original author’s data and code? This is a computational challenge.\nReplicability: Can an independent researcher corroborate the scientific conclusions of a study by conducting a new, independent study? This is a scientific challenge.\n\nThis note focuses on achieving computational reproducibility, which is the essential first step. If your own analysis isn’t reproducible, it’s impossible for anyone to even attempt to replicate your scientific findings.\n\n\nWithout reproducibility, it’s impossible to verify your findings, build upon your work, or collaborate effectively. A project that is not reproducible is a “black box” that cannot be trusted or audited."
  },
  {
    "objectID": "m01-toolkit/environments.html#the-importance-of-reproducibility",
    "href": "m01-toolkit/environments.html#the-importance-of-reproducibility",
    "title": "Reproducible Environments & Projects",
    "section": "",
    "text": "Reproducibility is a cornerstone of good science and a fundamental principle in computational research. It is the ability of an independent researcher to duplicate the results of a prior study using the same materials and procedures as were used by the original investigator. In the context of data science and soft computing, this means that your code, given the same input data, should produce the exact same outputs, every single time, regardless of the machine it is run on.\n\n\n\n\n\n\nReproducibility vs. Replicability\n\n\n\nWhile the terms are often used interchangeably, it’s helpful to distinguish between them:\n\nReproducibility: Can an independent researcher achieve the exact same results using the original author’s data and code? This is a computational challenge.\nReplicability: Can an independent researcher corroborate the scientific conclusions of a study by conducting a new, independent study? This is a scientific challenge.\n\nThis note focuses on achieving computational reproducibility, which is the essential first step. If your own analysis isn’t reproducible, it’s impossible for anyone to even attempt to replicate your scientific findings.\n\n\nWithout reproducibility, it’s impossible to verify your findings, build upon your work, or collaborate effectively. A project that is not reproducible is a “black box” that cannot be trusted or audited."
  },
  {
    "objectID": "m01-toolkit/environments.html#pillars-of-a-reproducible-project",
    "href": "m01-toolkit/environments.html#pillars-of-a-reproducible-project",
    "title": "Reproducible Environments & Projects",
    "section": "2 Pillars of a Reproducible Project",
    "text": "2 Pillars of a Reproducible Project\nAchieving reproducibility requires a conscious effort and the right set of tools. We can think of a reproducible project as having three main pillars: environment management, workflow automation, and comprehensive documentation.\n\n1. Virtual Environments\nA virtual environment is an isolated container that holds all the necessary packages and dependencies for a specific project. This prevents conflicts between projects that might require different versions of the same library, a situation often referred to as “dependency hell.”\n\n\n\nImage depicting dependency hell\n\n\nFor this course, we recommend uv. It is a modern and extremely fast Python package and environment manager, written in Rust. uv acts as an all-in-one tool, handling package installation (like pip) and virtual environment creation (like venv). Its integrated nature and high performance can significantly accelerate Python workflows.\n\n\nWhy not conda?\nconda is an well-matuated and superior tool well-used in Python Community. Unlike uv, conda can have non-Python dependencies (like specific compilers or optimized BLAS/LAPACK libraries). Conda excels at managing these lower-level system packages. uv, on the other hand, focuses on Python packages. This apparently limited feature comes with some great benefits. Because conda supports non-Python packages, the virtual environment it created can have complex dependencies with your environments, preventing someone from recreating the same environment. uv has less issues since it focuses on Python packages.\n\n\n2. Workflow Management\nFor complex projects with multiple steps, manually running scripts in the correct order is tedious and error-prone. A workflow management tool automates this process, defining a “pipeline” of computational steps and their dependencies.\nA recommended tool is Snakemake, a popular workflow management system that uses a human-readable, Python-based language. You define rules that specify how to create output files from input files. Snakemake automatically determines the order of execution and can parallelize tasks. See Introductory Video: Link on how to use it.\nA key principle when designing workflows is to make individual scripts atomic. For example, have one script for preprocessing data, another for performing an analysis, and a third for generating a figure. By making scripts minimal and short, they become more readable, reusable, and easier to debug.\n\n\n3. Documentation\nCode and data are not self-explanatory. Comprehensive documentation is crucial for another person (or your future self) to understand the what, why, and how of your project.\n\nREADME files: At a minimum, every project should have a README.md file in its root directory. This file should explain what the project is about, what the files are, and how to run the analysis.\nCode Comments: Well-commented code explains the logic behind non-obvious parts of your scripts."
  },
  {
    "objectID": "m01-toolkit/environments.html#project-organization-best-practices",
    "href": "m01-toolkit/environments.html#project-organization-best-practices",
    "title": "Reproducible Environments & Projects",
    "section": "3 Project Organization Best Practices",
    "text": "3 Project Organization Best Practices\nBeyond the core tools, organizing your project thoughtfully is crucial for long-term reproducibility and collaboration.\n\nFile and Directory Structure\nIt’s important to have a system to organize files and stick to it. A logical structure makes it easier for others (and your future self) to find things.\n\nDescriptive Naming: Give files and directories clear, descriptive names. This is crucial to fully leverage keyword search.\nAdd Timestamps: For files that evolve over time (like datasets or reports), consider adding a timestamp (e.g., 2025-10-20_report.qmd) to keep track of versions.\nFurther Watching:\n\nHow to name files\nA Simple File Management System\n\n\n\n\nWriting Clean Code\nThe quality of your code directly impacts reproducibility. If your code is hard to read, it’s hard to verify. The book Clean Code: A Handbook of Agile Software Craftsmanship by Robert C. Martin is an excellent resource on the principles of writing clean, maintainable code.\nBy combining virtual environments, good documentation, workflow management, and thoughtful project organization, you can create robust, transparent, and fully reproducible computational projects."
  },
  {
    "objectID": "m01-toolkit/reproduceability.html#the-importance-of-reproducibility",
    "href": "m01-toolkit/reproduceability.html#the-importance-of-reproducibility",
    "title": "Reproducible Environments & Projects",
    "section": "",
    "text": "Reproducibility is a cornerstone of good science and a fundamental principle in computational research. It is the ability of an independent researcher to duplicate the results of a prior study using the same materials and procedures as were used by the original investigator. In the context of data science and soft computing, this means that your code, given the same input data, should produce the exact same outputs, every single time, regardless of the machine it is run on.\n\n\nWhile the terms are often used interchangeably, it’s helpful to distinguish between them:\n\nReproducibility: Can an independent researcher achieve the exact same results using the original author’s data and code? This is a computational challenge.\nReplicability: Can an independent researcher corroborate the scientific conclusions of a study by conducting a new, independent study? This is a scientific challenge.\n\nThis note focuses on achieving computational reproducibility, which is the essential first step. If your own analysis isn’t reproducible, it’s impossible for anyone to even attempt to replicate your scientific findings.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Reproducibility"
    ]
  },
  {
    "objectID": "m01-toolkit/reproduceability.html#pillars-of-a-reproducible-project",
    "href": "m01-toolkit/reproduceability.html#pillars-of-a-reproducible-project",
    "title": "Reproducible Environments & Projects",
    "section": "2 Pillars of a Reproducible Project",
    "text": "2 Pillars of a Reproducible Project\nAchieving reproducibility requires a conscious effort and the right set of tools. We can think of a reproducible project as having three main pillars: Virtual Environments, Workflow automation, and Comprehensive documentation.\n\nVirtual Environments\n\nA virtual environment is an isolated container that holds all the necessary packages and dependencies for a specific project. This prevents conflicts between projects that might require different versions of the same library, a situation often referred to as “dependency hell.”\nFor this course, we recommend uv. It is a modern and extremely fast Python package and environment manager, written in Rust. uv acts as an all-in-one tool, handling package installation (like pip) and virtual environment creation (like venv). Its integrated nature and high performance can significantly accelerate Python workflows.\nSee the documentation on how to get started with uv.\n\n\nWhy not conda?\nconda is an well-matuated and superior tool well-used in Python Community. Unlike uv, conda can have non-Python dependencies (like specific compilers or optimized BLAS/LAPACK libraries). Conda excels at managing these lower-level system packages. uv, on the other hand, focuses on Python packages. This apparently limited feature comes with some great benefits. Because conda supports non-Python packages, the virtual environment it created can have complex dependencies with your environments, preventing someone from recreating the same environment. uv has less issues since it focuses on Python packages.\n\n\nWorkflow Management\n\n\n\n\n\n\nFigure 1: This diagram dipicts a workflow of a research project, with each box representing a script/code and boxes are connected if the output of one box is the input of another box. As your project grows, the workflow can become complicated. So much so that you may not be able to remember the order of the steps. This is where workflow management tools come in.\n\n\n\nFor complex projects with multiple steps, manually running scripts in the correct order is tedious and error-prone. A workflow management tool automates this process, defining a “pipeline” of computational steps and their dependencies.\nA recommended tool is Snakemake, a popular workflow management system that uses a human-readable, Python-based language. You define rules that specify how to create output files from input files. Snakemake automatically determines the order of execution and can parallelize tasks. See Introductory Video: Link on how to use it.\nA key principle when designing workflows is to make individual scripts atomic. For example, have one script for preprocessing data, another for performing an analysis, and a third for generating a figure. By making scripts minimal and short, they become more readable, reusable, and easier to debug.\n\n\nComprehensive Documentation\nCode and data are not self-explanatory. Comprehensive documentation is crucial for another person (or your future self) to understand the what, why, and how of your project.\n\nREADME files\nAt a minimum, every project should have a README.md file in its root directory. This file should explain what the project is about, what the files are, and how to run the analysis. Ideally, a good README gives details like:\n\nProject Description: What the project does and its goals.\nFile/Folder Structure: A brief summary so others can find what they need.\nData Table Structure: Describe columns and data format if example datasets are included.\nInstallation Instructions: How to set up the environment and dependencies.\nUsage: How to run the main analysis/scripts.\nContact/License Information: Who to ask for help, licensing, or citation.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Reproducibility"
    ]
  },
  {
    "objectID": "m01-toolkit/reproduceability.html#project-organization-best-practices",
    "href": "m01-toolkit/reproduceability.html#project-organization-best-practices",
    "title": "Reproducible Environments & Projects",
    "section": "3 Project Organization Best Practices",
    "text": "3 Project Organization Best Practices\nBeyond the core tools, organizing your project thoughtfully is crucial for long-term reproducibility and collaboration.\n\nFile and Directory Structure\nIt’s important to have a system to organize files and stick to it. A logical structure makes it easier for others (and your future self) to find things.\n\nDescriptive Naming: Give files and directories clear, descriptive names. This is crucial to fully leverage keyword search.\nAdd Timestamps: For files that evolve over time (like datasets or reports), consider adding a timestamp (e.g., 2025-10-20_report.qmd) to keep track of versions.\nFurther Watching:\n\nHow to name files\nA Simple File Management System\n\n\n\n\nWriting Clean Code\nThe quality of your code directly impacts reproducibility. If your code is hard to read, it’s hard to verify. The book Clean Code: A Handbook of Agile Software Craftsmanship by Robert C. Martin is an excellent resource on the principles of writing clean, maintainable code.\nBy combining virtual environments, good documentation, workflow management, and thoughtful project organization, you can create robust, transparent, and fully reproducible computational projects.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Reproducibility"
    ]
  },
  {
    "objectID": "m01-toolkit/reproduceability.html#data-table-structure-datafruits.csv",
    "href": "m01-toolkit/reproduceability.html#data-table-structure-datafruits.csv",
    "title": "Reproducible Environments & Projects",
    "section": "3 Data Table Structure (data/fruits.csv)",
    "text": "3 Data Table Structure (data/fruits.csv)\n\n\n\ncolumn\ndescription\nexample\n\n\n\n\nid\nunique row id\n1\n\n\nweight\nin grams\n130\n\n\ncolor\ncategorical\nred\n\n\nripeness\nfrom 1 (unripe) to 5 (ripe)\n4\n\n\nfruit_type\nlabel\napple",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Reproducibility"
    ]
  },
  {
    "objectID": "m01-toolkit/reproduceability.html#installation",
    "href": "m01-toolkit/reproduceability.html#installation",
    "title": "Reproducible Environments & Projects",
    "section": "4 Installation",
    "text": "4 Installation\n\nCreate and activate a virtual environment: bash     uv venv     source .venv/bin/activate\nInstall dependencies: bash     uv pip install -r requirements.txt",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Reproducibility"
    ]
  },
  {
    "objectID": "m01-toolkit/reproduceability.html#usage",
    "href": "m01-toolkit/reproduceability.html#usage",
    "title": "Reproducible Environments & Projects",
    "section": "5 Usage",
    "text": "5 Usage\nTo train and predict:\npython src/classify.py --input data/fruits.csv --output results/predictions.csv\n\n-   **Code Comments**: Well-commented code explains the logic behind non-obvious parts of your scripts.\n\n## Project Organization Best Practices\n\nBeyond the core tools, organizing your project thoughtfully is crucial for long-term reproducibility and collaboration.\n\n### File and Directory Structure\n\nIt's important to have a system to organize files and stick to it. A logical structure makes it easier for others (and your future self) to find things.\n\n-   **Descriptive Naming**: Give files and directories clear, descriptive names. This is crucial to fully leverage keyword search.\n-   **Add Timestamps**: For files that evolve over time (like datasets or reports), consider adding a timestamp (e.g., `2025-10-20_report.qmd`) to keep track of versions.\n-   **Further Watching**:\n    -   [How to name files](https://www.youtube.com/watch?v=ES1LTlnpLMk)\n    -   [A Simple File Management System](https://www.youtube.com/watch?v=MM-MPS57qKA&t=230s)\n\n### Writing Clean Code\n\nThe quality of your code directly impacts reproducibility. If your code is hard to read, it's hard to verify. The book [Clean Code: A Handbook of Agile Software Craftsmanship by Robert C. Martin](https://www.amazon.com/Clean-Code-Handbook-Software-Craftsmanship/dp/0132350882) is an excellent resource on the principles of writing clean, maintainable code.\n\nBy combining virtual environments, good documentation, workflow management, and thoughtful project organization, you can create robust, transparent, and fully reproducible computational projects.\n\n\n\n\n:::{#quarto-navigation-envelope .hidden}\n[Applied Soft Computing]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyLXRpdGxl\"}\n[Applied Soft Computing]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXItdGl0bGU=\"}\n[Documentation]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uZXh0\"}\n[Data Provenance]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1wcmV2\"}\n[Course Information]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOnF1YXJ0by1zaWRlYmFyLXNlY3Rpb24tMQ==\"}\n[Home]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9pbmRleC5odG1sSG9tZQ==\"}\n[Welcome]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9jb3Vyc2Uvd2VsY29tZS5odG1sV2VsY29tZQ==\"}\n[About Us]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9jb3Vyc2UvYWJvdXQuaHRtbEFib3V0LVVz\"}\n[Applied Soft Computing: Modeling Complex Systems with Deep Learning]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9jb3Vyc2Uvd2h5LWFwcGxpZWQtc29mdC1jb21wdXRpbmcuaHRtbEFwcGxpZWQtU29mdC1Db21wdXRpbmc6LU1vZGVsaW5nLUNvbXBsZXgtU3lzdGVtcy13aXRoLURlZXAtTGVhcm5pbmc=\"}\n[Discord]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9jb3Vyc2UvZGlzY29yZC5odG1sRGlzY29yZA==\"}\n[Setup]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9jb3Vyc2Uvc2V0dXAuaHRtbFNldHVw\"}\n[Using Minidora]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9jb3Vyc2UvbWluaWRvcmEtdXNhZ2UuaHRtbFVzaW5nLU1pbmlkb3Jh\"}\n[How to submit assignment]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9jb3Vyc2UvaG93LXRvLXN1Ym1pdC1hc3NpZ25tZW50Lmh0bWxIb3ctdG8tc3VibWl0LWFzc2lnbm1lbnQ=\"}\n[Applied Soft Computing: Modeling Complex Systems with Deep Learning]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9jb3Vyc2UvZGVsaXZlcmFibGVzLmh0bWxBcHBsaWVkLVNvZnQtQ29tcHV0aW5nOi1Nb2RlbGluZy1Db21wbGV4LVN5c3RlbXMtd2l0aC1EZWVwLUxlYXJuaW5n\"}\n[Module 1: The Data Scientist's Toolkit]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOnF1YXJ0by1zaWRlYmFyLXNlY3Rpb24tMg==\"}\n[Overview]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDEtdG9vbGtpdC9vdmVydmlldy5odG1sT3ZlcnZpZXc=\"}\n[Version Control with Git & GitHub]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDEtdG9vbGtpdC9naXQtZ2l0aHViLmh0bWxWZXJzaW9uLUNvbnRyb2wtd2l0aC1HaXQtJi1HaXRIdWI=\"}\n[The Tidy Data Philosophy]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDEtdG9vbGtpdC90aWR5LWRhdGEuaHRtbFRoZS1UaWR5LURhdGEtUGhpbG9zb3BoeQ==\"}\n[Data Provenance]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDEtdG9vbGtpdC9kYXRhLXByb3ZlbmFuY2UuaHRtbERhdGEtUHJvdmVuYW5jZQ==\"}\n[Reproducibility]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDEtdG9vbGtpdC9yZXByb2R1Y2VhYmlsaXR5Lmh0bWxSZXByb2R1Y2liaWxpdHk=\"}\n[Documentation]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDEtdG9vbGtpdC9kb2N1bWVudGF0aW9uLnFtZERvY3VtZW50YXRpb24=\"}\n[Module 2: Visualizing Complexity]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOnF1YXJ0by1zaWRlYmFyLXNlY3Rpb24tMw==\"}\n[Overview]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDItdmlzdWFsaXphdGlvbi9vdmVydmlldy5odG1sT3ZlcnZpZXc=\"}\n[Principles of Effective Visualization]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDItdmlzdWFsaXphdGlvbi9wcmluY2lwbGVzLmh0bWxQcmluY2lwbGVzLW9mLUVmZmVjdGl2ZS1WaXN1YWxpemF0aW9u\"}\n[Visualizing High-Dimensional Data (t-SNE, UMAP)]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDItdmlzdWFsaXphdGlvbi9kaW1lbnNpb25hbGl0eS1yZWR1Y3Rpb24uaHRtbFZpc3VhbGl6aW5nLUhpZ2gtRGltZW5zaW9uYWwtRGF0YS0odC1TTkUsLVVNQVAp\"}\n[Visualizing Networks]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDItdmlzdWFsaXphdGlvbi9uZXR3b3Jrcy5odG1sVmlzdWFsaXppbmctTmV0d29ya3M=\"}\n[Visualizing Time-Series]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDItdmlzdWFsaXphdGlvbi90aW1lLXNlcmllcy5odG1sVmlzdWFsaXppbmctVGltZS1TZXJpZXM=\"}\n[Module 3: Deep Learning for Text]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOnF1YXJ0by1zaWRlYmFyLXNlY3Rpb24tNA==\"}\n[Overview]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDMtdGV4dC9vdmVydmlldy5odG1sT3ZlcnZpZXc=\"}\n[TF-IDF: Bag-of-Words Representation]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDMtdGV4dC90Zi1pZGYubWRURi1JREY6LUJhZy1vZi1Xb3Jkcy1SZXByZXNlbnRhdGlvbg==\"}\n[Word Embeddings (Word2Vec)]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDMtdGV4dC93b3JkMnZlYy5tZFdvcmQtRW1iZWRkaW5ncy0oV29yZDJWZWMp\"}\n[Advanced Word2Vec Techniques]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDMtdGV4dC93b3JkMnZlY19wbHVzLm1kQWR2YW5jZWQtV29yZDJWZWMtVGVjaG5pcXVlcw==\"}\n[Semantic Axes & Historical Bias]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDMtdGV4dC9zZW0tYXhpcy5tZFNlbWFudGljLUF4ZXMtJi1IaXN0b3JpY2FsLUJpYXM=\"}\n[Bias in Embeddings]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDMtdGV4dC9iaWFzLWluLWVtYmVkZGluZy5tZEJpYXMtaW4tRW1iZWRkaW5ncw==\"}\n[Document Embeddings (Doc2Vec)]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDMtdGV4dC9kb2MydmVjLm1kRG9jdW1lbnQtRW1iZWRkaW5ncy0oRG9jMlZlYyk=\"}\n[Recurrent Neural Networks]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDMtdGV4dC9yZWNjdXJyZW50LW5ldXJhbC1uZXQubWRSZWN1cnJlbnQtTmV1cmFsLU5ldHdvcmtz\"}\n[LSTM Networks]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDMtdGV4dC9sc3RtLm1kTFNUTS1OZXR3b3Jrcw==\"}\n[ELMo: Contextual Embeddings]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDMtdGV4dC9lbG1vLmh0bWxFTE1vOi1Db250ZXh0dWFsLUVtYmVkZGluZ3M=\"}\n[Sequence-to-Sequence Models]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDMtdGV4dC9zZXEyc2VxLm1kU2VxdWVuY2UtdG8tU2VxdWVuY2UtTW9kZWxz\"}\n[Module 4: Deep Learning for Images]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOnF1YXJ0by1zaWRlYmFyLXNlY3Rpb24tNQ==\"}\n[Overview]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDQtaW1hZ2VzL292ZXJ2aWV3Lmh0bWxPdmVydmlldw==\"}\n[Image Processing Fundamentals]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDQtaW1hZ2VzL2ltYWdlLXByb2Nlc3NpbmcubWRJbWFnZS1Qcm9jZXNzaW5nLUZ1bmRhbWVudGFscw==\"}\n[Convolutional Neural Networks]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDQtaW1hZ2VzL2Nubi5tZENvbnZvbHV0aW9uYWwtTmV1cmFsLU5ldHdvcmtz\"}\n[LeNet Architecture]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDQtaW1hZ2VzL2xlbmV0Lm1kTGVOZXQtQXJjaGl0ZWN0dXJl\"}\n[AlexNet: Deep CNN Revolution]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDQtaW1hZ2VzL2FsZXhuZXQubWRBbGV4TmV0Oi1EZWVwLUNOTi1SZXZvbHV0aW9u\"}\n[VGG Networks]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDQtaW1hZ2VzL3ZnZy5tZFZHRy1OZXR3b3Jrcw==\"}\n[Inception & Multi-Scale Features]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDQtaW1hZ2VzL2luY2VwdGlvbi5tZEluY2VwdGlvbi0mLU11bHRpLVNjYWxlLUZlYXR1cmVz\"}\n[Batch Normalization]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDQtaW1hZ2VzL2JhdGNoLW5vcm1hbGl6YXRpb24uaHRtbEJhdGNoLU5vcm1hbGl6YXRpb24=\"}\n[ResNet & Skip Connections]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDQtaW1hZ2VzL3Jlc25ldC5tZFJlc05ldC0mLVNraXAtQ29ubmVjdGlvbnM=\"}\n[Module 5: Deep Learning for Graphs]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOnF1YXJ0by1zaWRlYmFyLXNlY3Rpb24tNg==\"}\n[Overview]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDUtZ3JhcGhzL292ZXJ2aWV3Lmh0bWxPdmVydmlldw==\"}\n[Spectral Graph Embedding]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDUtZ3JhcGhzL3NwZWN0cmFsLWVtYmVkZGluZy5odG1sU3BlY3RyYWwtR3JhcGgtRW1iZWRkaW5n\"}\n[Graph Embeddings with Word2Vec]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDUtZ3JhcGhzL2dyYXBoLWVtYmVkZGluZy13LXdvcmQydmVjLmh0bWxHcmFwaC1FbWJlZGRpbmdzLXdpdGgtV29yZDJWZWM=\"}\n[Spectral vs. Neural Embeddings]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDUtZ3JhcGhzL3NwZWN0cmFsLXZzLW5ldXJhbC1lbWJlZGRpbmcuaHRtbFNwZWN0cmFsLXZzLi1OZXVyYWwtRW1iZWRkaW5ncw==\"}\n[From Images to Graphs]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDUtZ3JhcGhzL2Zyb20taW1hZ2UtdG8tZ3JhcGguaHRtbEZyb20tSW1hZ2VzLXRvLUdyYXBocw==\"}\n[Graph Convolutional Networks]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDUtZ3JhcGhzL2dyYXBoLWNvbnZvbHV0aW9uYWwtbmV0d29yay5odG1sR3JhcGgtQ29udm9sdXRpb25hbC1OZXR3b3Jrcw==\"}\n[Popular GNN Architectures]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDUtZ3JhcGhzL3BvcHVsYXItZ25uLmh0bWxQb3B1bGFyLUdOTi1BcmNoaXRlY3R1cmVz\"}\n[GNN Software & Tools]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDUtZ3JhcGhzL3NvZnR3YXJlLmh0bWxHTk4tU29mdHdhcmUtJi1Ub29scw==\"}\n[Module 6: Large Language Models & Emergent Behavior]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOnF1YXJ0by1zaWRlYmFyLXNlY3Rpb24tNw==\"}\n[Overview]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDYtbGxtcy9vdmVydmlldy5odG1sT3ZlcnZpZXc=\"}\n[The Transformer Architecture]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDYtbGxtcy90cmFuc2Zvcm1lcnMubWRUaGUtVHJhbnNmb3JtZXItQXJjaGl0ZWN0dXJl\"}\n[BERT & Contextual Embeddings]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDYtbGxtcy9iZXJ0Lm1kQkVSVC0mLUNvbnRleHR1YWwtRW1iZWRkaW5ncw==\"}\n[Sentence-BERT for Semantic Similarity]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDYtbGxtcy9zZW50ZW5jZS1iZXJ0Lmh0bWxTZW50ZW5jZS1CRVJULWZvci1TZW1hbnRpYy1TaW1pbGFyaXR5\"}\n[GPT & Generative Models]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDYtbGxtcy9ncHQubWRHUFQtJi1HZW5lcmF0aXZlLU1vZGVscw==\"}\n[From Language Models to Instruction Following]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDYtbGxtcy9mcm9tLWxhbmd1YWdlLW1vZGVsLXRvLWluc3RydWN0aW9uLWZvbGxvd2luZy5odG1sRnJvbS1MYW5ndWFnZS1Nb2RlbHMtdG8tSW5zdHJ1Y3Rpb24tRm9sbG93aW5n\"}\n[Prompt Engineering & In-Context Learning]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDYtbGxtcy9wcm9tcHQtdHVuaW5nLmh0bWxQcm9tcHQtRW5naW5lZXJpbmctJi1Jbi1Db250ZXh0LUxlYXJuaW5n\"}\n[Scaling Laws & Emergent Abilities]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDYtbGxtcy9zY2FsaW5nLWVtZXJnZW5jZS5odG1sU2NhbGluZy1MYXdzLSYtRW1lcmdlbnQtQWJpbGl0aWVz\"}\n[LLMs as Complex Systems]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDYtbGxtcy9sbG1zLWFzLWNvbXBsZXgtc3lzdGVtcy5odG1sTExNcy1hcy1Db21wbGV4LVN5c3RlbXM=\"}\n[Module 7: Self-Supervised Learning]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOnF1YXJ0by1zaWRlYmFyLXNlY3Rpb24tOA==\"}\n[Overview]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDctc2VsZi1zdXBlcnZpc2VkL292ZXJ2aWV3Lmh0bWxPdmVydmlldw==\"}\n[The Self-Supervised Paradigm]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDctc2VsZi1zdXBlcnZpc2VkL3BhcmFkaWdtLmh0bWxUaGUtU2VsZi1TdXBlcnZpc2VkLVBhcmFkaWdt\"}\n[Contrastive Learning (SimCLR)]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDctc2VsZi1zdXBlcnZpc2VkL2NvbnRyYXN0aXZlLWxlYXJuaW5nLmh0bWxDb250cmFzdGl2ZS1MZWFybmluZy0oU2ltQ0xSKQ==\"}\n[Self-Supervised Learning for Graphs]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDctc2VsZi1zdXBlcnZpc2VkL2dyYXBocy5odG1sU2VsZi1TdXBlcnZpc2VkLUxlYXJuaW5nLWZvci1HcmFwaHM=\"}\n[Self-Supervised Learning for Time-Series]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDctc2VsZi1zdXBlcnZpc2VkL3RpbWUtc2VyaWVzLmh0bWxTZWxmLVN1cGVydmlzZWQtTGVhcm5pbmctZm9yLVRpbWUtU2VyaWVz\"}\n[Module 8: Explainability & Ethics]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOnF1YXJ0by1zaWRlYmFyLXNlY3Rpb24tOQ==\"}\n[Overview]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDgtZXhwbGFpbmFiaWxpdHkvb3ZlcnZpZXcuaHRtbE92ZXJ2aWV3\"}\n[The Need for Explainability]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDgtZXhwbGFpbmFiaWxpdHkvbmVlZC5odG1sVGhlLU5lZWQtZm9yLUV4cGxhaW5hYmlsaXR5\"}\n[Attention Visualization]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDgtZXhwbGFpbmFiaWxpdHkvYXR0ZW50aW9uLmh0bWxBdHRlbnRpb24tVmlzdWFsaXphdGlvbg==\"}\n[LIME & SHAP]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDgtZXhwbGFpbmFiaWxpdHkvbGltZS1zaGFwLmh0bWxMSU1FLSYtU0hBUA==\"}\n[Algorithmic Fairness & Bias]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDgtZXhwbGFpbmFiaWxpdHkvZmFpcm5lc3MuaHRtbEFsZ29yaXRobWljLUZhaXJuZXNzLSYtQmlhcw==\"}\n[Causality vs. Correlation]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDgtZXhwbGFpbmFiaWxpdHkvY2F1c2FsaXR5Lmh0bWxDYXVzYWxpdHktdnMuLUNvcnJlbGF0aW9u\"}\n[Legacy Materials]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOnF1YXJ0by1zaWRlYmFyLXNlY3Rpb24tMTA=\"}\n[Word & Document Embeddings]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDMtdGV4dC93aGF0LXRvLWxlYXJuLmh0bWxXb3JkLSYtRG9jdW1lbnQtRW1iZWRkaW5ncw==\"}\n[Recurrent Neural Networks]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDMtdGV4dC93aGF0LXRvLWxlYXJuLmh0bWxSZWN1cnJlbnQtTmV1cmFsLU5ldHdvcmtz\"}\n[Image Processing (CNNs)]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyOi9tMDQtaW1hZ2VzL3doYXQtdG8tbGVhcm4uaHRtbEltYWdlLVByb2Nlc3NpbmctKENOTnMp\"}\n[Home]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6SG9tZQ==\"}\n[/index.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L2luZGV4Lmh0bWw=\"}\n[Toolkit & Workflow]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6VG9vbGtpdCAmIFdvcmtmbG93\"}\n[─── Module 1 ───]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI64pSA4pSA4pSAIE1vZHVsZSAxIOKUgOKUgOKUgA==\"}\n[Overview]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6T3ZlcnZpZXc=\"}\n[/m01-toolkit/overview.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L20wMS10b29sa2l0L292ZXJ2aWV3Lmh0bWw=\"}\n[Git & GitHub]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6R2l0ICYgR2l0SHVi\"}\n[/m01-toolkit/git-github.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L20wMS10b29sa2l0L2dpdC1naXRodWIuaHRtbA==\"}\n[Tidy Data]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6VGlkeSBEYXRh\"}\n[/m01-toolkit/tidy-data.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L20wMS10b29sa2l0L3RpZHktZGF0YS5odG1s\"}\n[Data Provenance]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6RGF0YSBQcm92ZW5hbmNl\"}\n[/m01-toolkit/data-provenance.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L20wMS10b29sa2l0L2RhdGEtcHJvdmVuYW5jZS5odG1s\"}\n[Environments]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6RW52aXJvbm1lbnRz\"}\n[/m01-toolkit/environments.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L20wMS10b29sa2l0L2Vudmlyb25tZW50cy5odG1s\"}\n[Visualization]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6VmlzdWFsaXphdGlvbg==\"}\n[─── Module 2 ───]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI64pSA4pSA4pSAIE1vZHVsZSAyIOKUgOKUgOKUgA==\"}\n[/m02-visualization/overview.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L20wMi12aXN1YWxpemF0aW9uL292ZXJ2aWV3Lmh0bWw=\"}\n[Principles]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6UHJpbmNpcGxlcw==\"}\n[/m02-visualization/principles.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L20wMi12aXN1YWxpemF0aW9uL3ByaW5jaXBsZXMuaHRtbA==\"}\n[High-Dimensional Data]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6SGlnaC1EaW1lbnNpb25hbCBEYXRh\"}\n[/m02-visualization/dimensionality-reduction.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L20wMi12aXN1YWxpemF0aW9uL2RpbWVuc2lvbmFsaXR5LXJlZHVjdGlvbi5odG1s\"}\n[Networks]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6TmV0d29ya3M=\"}\n[/m02-visualization/networks.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L20wMi12aXN1YWxpemF0aW9uL25ldHdvcmtzLmh0bWw=\"}\n[Time-Series]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6VGltZS1TZXJpZXM=\"}\n[/m02-visualization/time-series.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L20wMi12aXN1YWxpemF0aW9uL3RpbWUtc2VyaWVzLmh0bWw=\"}\n[Deep Learning]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6RGVlcCBMZWFybmluZw==\"}\n[─── Module 3: Text ───]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI64pSA4pSA4pSAIE1vZHVsZSAzOiBUZXh0IOKUgOKUgOKUgA==\"}\n[/m03-text/overview.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L20wMy10ZXh0L292ZXJ2aWV3Lmh0bWw=\"}\n[Word2Vec]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6V29yZDJWZWM=\"}\n[/m03-text/word2vec.md]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L20wMy10ZXh0L3dvcmQydmVjLm1k\"}\n[RNNs & LSTMs]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6Uk5OcyAmIExTVE1z\"}\n[/m03-text/lstm.md]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L20wMy10ZXh0L2xzdG0ubWQ=\"}\n[─── Module 4: Images ───]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI64pSA4pSA4pSAIE1vZHVsZSA0OiBJbWFnZXMg4pSA4pSA4pSA\"}\n[/m04-images/overview.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L20wNC1pbWFnZXMvb3ZlcnZpZXcuaHRtbA==\"}\n[CNNs]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6Q05Ocw==\"}\n[/m04-images/cnn.md]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L20wNC1pbWFnZXMvY25uLm1k\"}\n[ResNet]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6UmVzTmV0\"}\n[/m04-images/resnet.md]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L20wNC1pbWFnZXMvcmVzbmV0Lm1k\"}\n[─── Module 5: Graphs ───]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI64pSA4pSA4pSAIE1vZHVsZSA1OiBHcmFwaHMg4pSA4pSA4pSA\"}\n[/m05-graphs/overview.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L20wNS1ncmFwaHMvb3ZlcnZpZXcuaHRtbA==\"}\n[Graph Embeddings]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6R3JhcGggRW1iZWRkaW5ncw==\"}\n[/m05-graphs/graph-embedding-w-word2vec.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L20wNS1ncmFwaHMvZ3JhcGgtZW1iZWRkaW5nLXctd29yZDJ2ZWMuaHRtbA==\"}\n[GNNs]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6R05Ocw==\"}\n[/m05-graphs/graph-convolutional-network.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L20wNS1ncmFwaHMvZ3JhcGgtY29udm9sdXRpb25hbC1uZXR3b3JrLmh0bWw=\"}\n[Advanced Topics]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6QWR2YW5jZWQgVG9waWNz\"}\n[─── Module 6: LLMs ───]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI64pSA4pSA4pSAIE1vZHVsZSA2OiBMTE1zIOKUgOKUgOKUgA==\"}\n[/m06-llms/overview.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L20wNi1sbG1zL292ZXJ2aWV3Lmh0bWw=\"}\n[Transformers]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6VHJhbnNmb3JtZXJz\"}\n[/m06-llms/transformers.md]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L20wNi1sbG1zL3RyYW5zZm9ybWVycy5tZA==\"}\n[Scaling & Emergence]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6U2NhbGluZyAmIEVtZXJnZW5jZQ==\"}\n[/m06-llms/scaling-emergence.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L20wNi1sbG1zL3NjYWxpbmctZW1lcmdlbmNlLmh0bWw=\"}\n[─── Module 7: Self-Supervised ───]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI64pSA4pSA4pSAIE1vZHVsZSA3OiBTZWxmLVN1cGVydmlzZWQg4pSA4pSA4pSA\"}\n[/m07-self-supervised/overview.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L20wNy1zZWxmLXN1cGVydmlzZWQvb3ZlcnZpZXcuaHRtbA==\"}\n[Contrastive Learning]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6Q29udHJhc3RpdmUgTGVhcm5pbmc=\"}\n[/m07-self-supervised/contrastive-learning.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L20wNy1zZWxmLXN1cGVydmlzZWQvY29udHJhc3RpdmUtbGVhcm5pbmcuaHRtbA==\"}\n[─── Module 8: Explainability ───]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI64pSA4pSA4pSAIE1vZHVsZSA4OiBFeHBsYWluYWJpbGl0eSDilIDilIDilIA=\"}\n[/m08-explainability/overview.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L20wOC1leHBsYWluYWJpbGl0eS9vdmVydmlldy5odG1s\"}\n[Fairness & Ethics]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6RmFpcm5lc3MgJiBFdGhpY3M=\"}\n[/m08-explainability/fairness.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L20wOC1leHBsYWluYWJpbGl0eS9mYWlybmVzcy5odG1s\"}\n[Module 1: The Data Scientist's Toolkit]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWJyZWFkY3J1bWJzLU1vZHVsZS0xOi1UaGUtRGF0YS1TY2llbnRpc3Qncy1Ub29sa2l0\"}\n[Reproducibility]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWJyZWFkY3J1bWJzLVJlcHJvZHVjaWJpbGl0eQ==\"}\n\n:::{.hidden .quarto-markdown-envelope-contents render-id=\"Zm9vdGVyLWxlZnQ=\"}\nCopyright 2025, Sadamori Kojaku\n:::\n\n:::\n\n\n\n:::{#quarto-meta-markdown .hidden}\n[Reproducible Environments & Projects – Applied Soft Computing]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLW1ldGF0aXRsZQ==\"}\n[Reproducible Environments & Projects – Applied Soft Computing]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLXR3aXR0ZXJjYXJkdGl0bGU=\"}\n[Reproducible Environments & Projects – Applied Soft Computing]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLW9nY2FyZHRpdGxl\"}\n[Applied Soft Computing]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLW1ldGFzaXRlbmFtZQ==\"}\n[]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLXR3aXR0ZXJjYXJkZGVzYw==\"}\n[]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLW9nY2FyZGRkZXNj\"}\n:::\n\n\n\n\n&lt;!-- --&gt;\n\n::: {.quarto-embedded-source-code}\n```````````````````{.markdown shortcodes=\"false\"}\n---\ntitle: \"Reproducible Environments & Projects\"\n---\n\n## The Importance of Reproducibility\n\nReproducibility is a cornerstone of good science and a fundamental principle in computational research. It is the ability of an independent researcher to duplicate the results of a prior study using the same materials and procedures as were used by the original investigator. In the context of data science and soft computing, this means that your code, given the same input data, should produce the exact same outputs, every single time, regardless of the machine it is run on.\n\n::: {.column-margin}\nWhile the terms are often used interchangeably, it's helpful to distinguish between them:\n\n-   **Reproducibility**: Can an independent researcher achieve the exact same results using the original author's data and code? This is a *computational* challenge.\n\n-   **Replicability**: Can an independent researcher corroborate the scientific conclusions of a study by conducting a new, independent study? This is a *scientific* challenge.\n\nThis note focuses on achieving **computational reproducibility**, which is the essential first step. If your own analysis isn't reproducible, it's impossible for anyone to even attempt to replicate your scientific findings.\n:::\n\n## Pillars of a Reproducible Project\n\nAchieving reproducibility requires a conscious effort and the right set of tools. We can think of a reproducible project as having three main pillars: [Virtual Environments](https://aeturrell.github.io/python4DS/environments.html), [Workflow automation](https://aeturrell.github.io/python4DS/workflows.html), and [Comprehensive documentation](https://aeturrell.github.io/python4DS/documentation.html).\n\n### Virtual Environments\n\n![](https://www.saaspegasus.com/static/images/web/uv/no-venv.4e57a4b43eea.jpg)\n\nA virtual environment is an isolated container that holds all the necessary packages and dependencies for a specific project. This prevents conflicts between projects that might require different versions of the same library, a situation often referred to as \"dependency hell.\"\n\n\nFor this course, we recommend **[uv](https://astral.sh/uv)**. It is a modern and extremely fast Python package and environment manager, written in Rust. `uv` acts as an all-in-one tool, handling package installation (like `pip`) and virtual environment creation (like `venv`). Its integrated nature and high performance can significantly accelerate Python workflows.\n\nSee the [documentation](https://docs.astral.sh/uv/concepts/projects/init/#packaged-applications) on how to get started with `uv`.\n\n::: {.column-margin}\nWhy not `conda`?\n\n`conda` is an well-matuated and superior tool well-used in Python Community.\nUnlike `uv`, `conda` can have non-Python dependencies (like specific compilers or optimized BLAS/LAPACK libraries). Conda excels at managing these lower-level system packages.\n`uv`, on the other hand, focuses on Python packages. This apparently limited feature comes with some great benefits.\nBecause `conda` supports non-Python packages, the virtual environment it created can have complex dependencies with your environments, preventing someone from recreating the same environment.\n`uv` has less issues since it focuses on Python packages.\n\n:::\n\n### Workflow Management\n\n::: {#fig-workflow}\n![](https://divingintogeneticsandgenomics.com/img/rule_graph_lancet.png)\n\nThis diagram dipicts a workflow of a research project, with each box representing a script/code and boxes are connected if the output of one box is the input of another box. As your project grows, the workflow can become complicated. So much so that you may not be able to remember the order of the steps. This is where workflow management tools come in.\n:::\n\nFor complex projects with multiple steps, manually running scripts in the correct order is tedious and error-prone. A workflow management tool automates this process, defining a \"pipeline\" of computational steps and their dependencies.\n\nA recommended tool is **[Snakemake](https://snakemake.readthedocs.io/en/stable/)**, a popular workflow management system that uses a human-readable, Python-based language. You define rules that specify how to create output files from input files. Snakemake automatically determines the order of execution and can parallelize tasks. See **Introductory Video:** [Link](https://www.youtube.com/watch?v=r9PWnEmz_tc) on how to use it.\n\n\nA key principle when designing workflows is to **make individual scripts atomic**. For example, have one script for preprocessing data, another for performing an analysis, and a third for generating a figure. By making scripts minimal and short, they become more readable, reusable, and easier to debug.\n\n### Documentation\n\nCode and data are not self-explanatory. Comprehensive documentation is crucial for another person (or your future self) to understand the what, why, and how of your project.\n\n#### README files\nAt a minimum, every project should have a `README.md` file in its root directory. This file should explain what the project is about, what the files are, and how to run the analysis. Ideally, a good README gives details like:\n\n   - **Project Description:** What the project does and its goals.\n   - **File/Folder Structure:** A brief summary so others can find what they need.\n   - **Data Table Structure:** Describe columns and data format if example datasets are included.\n   - **Installation Instructions:** How to set up the environment and dependencies.\n   - **Usage:** How to run the main analysis/scripts.\n   - **Contact/License Information:** Who to ask for help, licensing, or citation.\n\n##### Example README file\n\n```markdown\n# Fruit Classifier\n\nThis project predicts the type of fruit using measurements (weight, color, and ripeness).\n\n## Project Structure\n\n```text\nfruit-classifier/\n├── data/\n│   └── fruits.csv\n├── src/\n│   └── classify.py\n├── results/\n├── README.md\n├── requirements.txt",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Reproducibility"
    ]
  },
  {
    "objectID": "m01-toolkit/reproduceability.html#data-table-structure-datafruits.csv-1",
    "href": "m01-toolkit/reproduceability.html#data-table-structure-datafruits.csv-1",
    "title": "Reproducible Environments & Projects",
    "section": "6 Data Table Structure (data/fruits.csv)",
    "text": "6 Data Table Structure (data/fruits.csv)\n\n\n\ncolumn\ndescription\nexample\n\n\n\n\nid\nunique row id\n1\n\n\nweight\nin grams\n130\n\n\ncolor\ncategorical\nred\n\n\nripeness\nfrom 1 (unripe) to 5 (ripe)\n4\n\n\nfruit_type\nlabel\napple",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Reproducibility"
    ]
  },
  {
    "objectID": "m01-toolkit/reproduceability.html#installation-1",
    "href": "m01-toolkit/reproduceability.html#installation-1",
    "title": "Reproducible Environments & Projects",
    "section": "7 Installation",
    "text": "7 Installation\n\nCreate and activate a virtual environment: bash     uv venv     source .venv/bin/activate\nInstall dependencies: bash     uv pip install -r requirements.txt",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Reproducibility"
    ]
  },
  {
    "objectID": "m01-toolkit/reproduceability.html#usage-1",
    "href": "m01-toolkit/reproduceability.html#usage-1",
    "title": "Reproducible Environments & Projects",
    "section": "8 Usage",
    "text": "8 Usage\nTo train and predict:\npython src/classify.py --input data/fruits.csv --output results/predictions.csv\n\n-   **Code Comments**: Well-commented code explains the logic behind non-obvious parts of your scripts.\n\n## Project Organization Best Practices\n\nBeyond the core tools, organizing your project thoughtfully is crucial for long-term reproducibility and collaboration.\n\n### File and Directory Structure\n\nIt's important to have a system to organize files and stick to it. A logical structure makes it easier for others (and your future self) to find things.\n\n-   **Descriptive Naming**: Give files and directories clear, descriptive names. This is crucial to fully leverage keyword search.\n-   **Add Timestamps**: For files that evolve over time (like datasets or reports), consider adding a timestamp (e.g., `2025-10-20_report.qmd`) to keep track of versions.\n-   **Further Watching**:\n    -   [How to name files](https://www.youtube.com/watch?v=ES1LTlnpLMk)\n    -   [A Simple File Management System](https://www.youtube.com/watch?v=MM-MPS57qKA&t=230s)\n\n### Writing Clean Code\n\nThe quality of your code directly impacts reproducibility. If your code is hard to read, it's hard to verify. The book [Clean Code: A Handbook of Agile Software Craftsmanship by Robert C. Martin](https://www.amazon.com/Clean-Code-Handbook-Software-Craftsmanship/dp/0132350882) is an excellent resource on the principles of writing clean, maintainable code.\n\nBy combining virtual environments, good documentation, workflow management, and thoughtful project organization, you can create robust, transparent, and fully reproducible computational projects.\n:::",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Reproducibility"
    ]
  },
  {
    "objectID": "m01-toolkit/documentation.html",
    "href": "m01-toolkit/documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "Good documentation isn’t just about writing things down — it’s a catalyst for automation and team productivity. The article “Documentation is Automation” argues that recording your manual steps is the crucial first move toward eliminating repetitive, error-prone tasks. Each time you pause to document a process, you nudge yourself toward greater automation.\nKey Principle:\n\nManual work should only be tolerated if it produces or improves an artifact.\n\nThe pipeline for documentation is as follows:\n\nDocument the Steps: Write down exactly what you did — even just as an informal checklist.\nCreate Automation Equivalents: Translate those steps into code snippets, scripts, or command sequences.\nBuild Automation: Combine and refine scripts into repeatable, robust tools.\nExpose as Self-Service or Autonomous Tools: Make automation accessible — for yourself, your team, or even for users to run with a click.\n\nWhy Embedding Documentation Matters\nEmbedding documentation within your workflows plays a pivotal role in reducing errors, as clear and accessible documentation helps ensure that processes are performed consistently and reliably—removing the risk of brittle, error-prone manual steps. It also promotes sharing and collaboration, making it easier for new team members to understand procedures and contribute effectively, even without prior insider knowledge. As projects grow, well-documented processes make it much simpler to adapt, extend, and scale operations. Furthermore, comprehensive documentation frees up creative energy; engineers and team members spend less time explaining or repeating established tasks and more time focusing on innovation and problem-solving.\nTakeaway: Rather than being a mere formality or an afterthought, documentation is a powerful lever for multiplying the impact of you and your team. By making a habit of writing down what you do, you naturally set the stage for automation to follow.\n\nRead the full article: Documentation is Automation"
  },
  {
    "objectID": "m01-toolkit/documentation.html#documentation-is-automation",
    "href": "m01-toolkit/documentation.html#documentation-is-automation",
    "title": "Documentation",
    "section": "",
    "text": "Good documentation isn’t just about writing things down — it’s a catalyst for automation and team productivity. The article “Documentation is Automation” argues that recording your manual steps is the crucial first move toward eliminating repetitive, error-prone tasks. Each time you pause to document a process, you nudge yourself toward greater automation.\nKey Principle:\n\nManual work should only be tolerated if it produces or improves an artifact.\n\nThe pipeline for documentation is as follows:\n\nDocument the Steps: Write down exactly what you did — even just as an informal checklist.\nCreate Automation Equivalents: Translate those steps into code snippets, scripts, or command sequences.\nBuild Automation: Combine and refine scripts into repeatable, robust tools.\nExpose as Self-Service or Autonomous Tools: Make automation accessible — for yourself, your team, or even for users to run with a click.\n\nWhy Embedding Documentation Matters\nEmbedding documentation within your workflows plays a pivotal role in reducing errors, as clear and accessible documentation helps ensure that processes are performed consistently and reliably—removing the risk of brittle, error-prone manual steps. It also promotes sharing and collaboration, making it easier for new team members to understand procedures and contribute effectively, even without prior insider knowledge. As projects grow, well-documented processes make it much simpler to adapt, extend, and scale operations. Furthermore, comprehensive documentation frees up creative energy; engineers and team members spend less time explaining or repeating established tasks and more time focusing on innovation and problem-solving.\nTakeaway: Rather than being a mere formality or an afterthought, documentation is a powerful lever for multiplying the impact of you and your team. By making a habit of writing down what you do, you naturally set the stage for automation to follow.\n\nRead the full article: Documentation is Automation"
  },
  {
    "objectID": "course/why-applied-soft-computing.html",
    "href": "course/why-applied-soft-computing.html",
    "title": "Why applied soft computing?",
    "section": "",
    "text": "Imagine trying to explain to someone how you recognize your friend’s face. Sure, you do it instantly - but try writing down the exact rules! Should you measure the eye spacing? Check nose shape? It’s nearly impossible to write rigid rules for something our brains do effortlessly.\nThis is where neural networks come in - they learn and adapt like our brains, without needing exact rules.",
    "crumbs": [
      "Home",
      "Course Information",
      "Why applied soft computing?"
    ]
  },
  {
    "objectID": "course/why-applied-soft-computing.html#mind-blowing-neural-network-achievements",
    "href": "course/why-applied-soft-computing.html#mind-blowing-neural-network-achievements",
    "title": "Why applied soft computing?",
    "section": "1 Mind-Blowing Neural Network Achievements",
    "text": "1 Mind-Blowing Neural Network Achievements\n\nThe AI Artist Who Beat Human Artists\n\n\n\nAI-generated artwork that won first place in a digital art competition\n\n\nIn 2022, something unexpected happened in the art world: an AI-generated artwork won first place in a digital art competition, beating out human artists! The creator, Jason Allen, used Midjourney AI to generate the winning piece after 80 hours of careful prompting. While critics claimed it was “just pressing buttons,” the win sparked a huge debate about the future of art.\n\n\nFaces That Don’t Exist\n\n\n\nA human face generated by StyleGAN\n\n\nVisit ThisPersonDoesNotExist.com and you’ll see something uncanny - incredibly realistic human faces that never existed! Each refresh shows a new face created by StyleGAN, complete with unique features, expressions, and even tiny details like skin pores. The wild part? Even experts sometimes can’t tell these AI-generated faces from real photos!\n\n\nChatGPT: The AI That Talks Like Us\n\n\n\nChatGPT interface\n\n\nWhen ChatGPT appeared, it shocked everyone with its human-like conversations. It doesn’t just answer questions - it writes poetry, explains complex topics, helps with coding, and even gets jokes! While earlier chatbots were obviously robotic, ChatGPT’s natural responses often make people wonder if they’re really chatting with an AI.\n\n\nThe 50-Year Puzzle Solver\n\n\n\nAlphaFold logo and interface\n\n\nScientists struggled for 50 years to predict how proteins fold - a crucial problem in biology. Then came AlphaFold, which not only solved the problem but did it with near-perfect accuracy! This task was thought to be so complex that it would take decades more to solve. Instead, AlphaFold did in days what used to take months in laboratories.\n\n\nThe Go Master’s Impossible Move\n\n\n\nAlphaGo versus Lee Sedol\n\n\nThe ancient game of Go was considered too complex for AI - until AlphaGo shocked the world by defeating champion Lee Sedol. But the real surprise came in Game 2, with “Move 37” - a play so creative and unexpected that Go experts initially thought it was a mistake! This move, later described as “神の一手” (the divine move), showed that AI could think in ways humans never imagined.\n\n\nSora: Making Movies from Words\n\n\n\nSora video generation example\n\n\nJust when we thought AI couldn’t get more impressive, OpenAI’s Sora arrived in 2024, turning text into realistic 60-second videos. The shocking part? These aren’t just simple animations - they’re physics-accurate scenes with multiple moving elements that look like they were filmed in the real world. It’s like having a movie studio in your pocket!\n\n\nThe Doctor That Sees More\n\n\n\nMedical imaging AI\n\n\nAI systems can now spot cancer in medical scans better than human doctors. Google Health’s system proved more accurate than radiologists at detecting breast cancer, reducing both missed cases and false alarms. It’s not replacing doctors, but it’s giving them a super-powered second opinion.\n\n\nCars That Drive Better Than Us\n\n\n\nTesla self-driving car\n\n\nSelf-driving cars were once science fiction. Now, neural networks help them process information from multiple sensors faster than any human could, making split-second decisions to avoid accidents. In many conditions, they’re already safer drivers than humans, reacting faster and staying alert 100% of the time.",
    "crumbs": [
      "Home",
      "Course Information",
      "Why applied soft computing?"
    ]
  },
  {
    "objectID": "course/why-applied-soft-computing.html#why-this-matters",
    "href": "course/why-applied-soft-computing.html#why-this-matters",
    "title": "Why applied soft computing?",
    "section": "2 Why This Matters",
    "text": "2 Why This Matters\nWhat’s truly remarkable is that most of these breakthroughs happened in just the last few years - within our lifetime! Tasks that experts thought would take decades to solve are being conquered by neural networks at an incredible pace. They’re not just matching human abilities - they’re surpassing them in ways that surprise even the leading researchers. ?",
    "crumbs": [
      "Home",
      "Course Information",
      "Why applied soft computing?"
    ]
  },
  {
    "objectID": "course/deliverables.html",
    "href": "course/deliverables.html",
    "title": "Deliverables",
    "section": "",
    "text": "Project reports are not solely focused on the final results, but also on the process and decisions made along the way. We expect to hear the reasons for your final decisions, for instance the reason why you choose X, over alternative options like Y.\n\nClarify the objectives and goal of your project. What do you want to do it, and why are your questions important to us?\nProvide a detailed description about the data you will use. Where the data are collected from, how they are compiled and preprocessed for your analysis. What are the data type of your focal features, and what features do you think are relevant for your analysis?\nDetermine the appropriate methods. Additionally, consider discussing the methods used in previous studies. Considering the data types and the information you aim to present, what methods could potentially be suitable? It would also be beneficial to explore what approaches others have taken when working with similar datasets.\nClarify the limitation and advantage of your approach. The limitation and advantage stems from data and methodologies, and must be discussed in light of existing works. For instance, you want to develop a link prediction algorithm for a social network based on the common neighbor approach. What are the fundamental assumption underlying the link prediction algorithms? When does the algorithm fail? Can you think of the advantage of your algorithm over other alternatives such as graph neural networks?\nEmbrace failures. As Thomas Edison famously said, “I have not failed. I’ve just found 10,000 ways that won’t work.” In many cases, works and analyses may appear to follow a single pathway, but it is important to recognize that this is just one of many paths that people have taken, many of which have turned out to be unsuccessful. It is crucial to try out multiple candidates, and more importantly, to document your failures and understand why they did not work. Consider using fake data, small subsets, mock-ups, and sketches. These methods can help you iterate and refine your approach, ultimately leading to more successful outcomes.",
    "crumbs": [
      "Home",
      "Course Information",
      "Deliverables"
    ]
  },
  {
    "objectID": "course/deliverables.html#general-remarks-on-the-project-reports",
    "href": "course/deliverables.html#general-remarks-on-the-project-reports",
    "title": "Deliverables",
    "section": "",
    "text": "Project reports are not solely focused on the final results, but also on the process and decisions made along the way. We expect to hear the reasons for your final decisions, for instance the reason why you choose X, over alternative options like Y.\n\nClarify the objectives and goal of your project. What do you want to do it, and why are your questions important to us?\nProvide a detailed description about the data you will use. Where the data are collected from, how they are compiled and preprocessed for your analysis. What are the data type of your focal features, and what features do you think are relevant for your analysis?\nDetermine the appropriate methods. Additionally, consider discussing the methods used in previous studies. Considering the data types and the information you aim to present, what methods could potentially be suitable? It would also be beneficial to explore what approaches others have taken when working with similar datasets.\nClarify the limitation and advantage of your approach. The limitation and advantage stems from data and methodologies, and must be discussed in light of existing works. For instance, you want to develop a link prediction algorithm for a social network based on the common neighbor approach. What are the fundamental assumption underlying the link prediction algorithms? When does the algorithm fail? Can you think of the advantage of your algorithm over other alternatives such as graph neural networks?\nEmbrace failures. As Thomas Edison famously said, “I have not failed. I’ve just found 10,000 ways that won’t work.” In many cases, works and analyses may appear to follow a single pathway, but it is important to recognize that this is just one of many paths that people have taken, many of which have turned out to be unsuccessful. It is crucial to try out multiple candidates, and more importantly, to document your failures and understand why they did not work. Consider using fake data, small subsets, mock-ups, and sketches. These methods can help you iterate and refine your approach, ultimately leading to more successful outcomes.",
    "crumbs": [
      "Home",
      "Course Information",
      "Deliverables"
    ]
  },
  {
    "objectID": "course/deliverables.html#proposal",
    "href": "course/deliverables.html#proposal",
    "title": "Deliverables",
    "section": "2 Proposal",
    "text": "2 Proposal\nA document should include the following sections:\n\nProject Title\nTeam Members (1-4 people; keep in mind that a larger team is expected to accomplish more than a smaller one)\nAbstract: A concise summary of your project.\nIntroduction: Provide motivation, background, and objectives for your project. Explain why it is important or interesting and why others should care. Review and discuss relevant existing works, particularly those that have inspired your project. Critique these works substantively. Remember, there is always a wealth of relevant work available.\nQuestions or Objectives: Specify the methods you plan to create and what you hope to discover from the data.\nDatasets and Methods: Identify the dataset you will be using. If you haven’t done so already, I strongly encourage you to reconsider your project. Obtaining and cleaning datasets can be time-consuming. Describe the dataset, including its structure and data types if it is tabular. Explain the methods you plan to apply and why you have chosen them. Finally, provide detailed information about the dataset to convincingly argue that it is suitable for your project and proposed methods.\n\n\nReferences",
    "crumbs": [
      "Home",
      "Course Information",
      "Deliverables"
    ]
  },
  {
    "objectID": "course/deliverables.html#final-presentation",
    "href": "course/deliverables.html#final-presentation",
    "title": "Deliverables",
    "section": "3 Final presentation",
    "text": "3 Final presentation\nPlease create a 10-minute video (please adhere to the time limit) and upload it to YouTube. You have the option to either publish it or make it unlisted. The video can be in any format you prefer. Make sure to include a thorough analysis while also making it interesting and enjoyable! The video will be evaluated based on three criteria: (i) the strength of the case you present, (ii) the quality of your analysis, and (iii) the production and delivery of your presentation.\nOnce you have completed your video, feel free to share it on Slack and receive feedback from your fellow students and instructors. It’s always beneficial to see what others have accomplished, so I highly encourage you to share your work!",
    "crumbs": [
      "Home",
      "Course Information",
      "Deliverables"
    ]
  },
  {
    "objectID": "course/deliverables.html#final-report",
    "href": "course/deliverables.html#final-report",
    "title": "Deliverables",
    "section": "4 Final report",
    "text": "4 Final report\nYou will need to submit your code and a report on your work. Ideally, your code will be in well-documented Jupyter notebooks (e.g. see Peter Norvig’s notebooks or good Kaggle exploratory data visualization kernels).\nThe report has no minimum or maximum length, but you need to make sure all the topics are thoroughly addressed in clear writing. The format and ingredients for the final report will depend on the types of projects that you do.\nIf the project is more about creating a software package or a website, then the report may focus more on the technical aspects of the project.",
    "crumbs": [
      "Home",
      "Course Information",
      "Deliverables"
    ]
  },
  {
    "objectID": "course/deliverables.html#idea-sketch-template",
    "href": "course/deliverables.html#idea-sketch-template",
    "title": "Deliverables",
    "section": "5 Idea sketch template",
    "text": "5 Idea sketch template\nThe followings are the list of questions I personally use before starting a project. Every idea is nebulous when it comes to a mind. We can materialize it by writing down the ideas. It’s surprisingly hard to write it down first, and you will realize a lot of things. In sum, writing is thinking. It serves as a scaffolding to think through a research project. These list of questions are a living document, and you will constantly update as the project progresses.\nAnswer each question in 2~3 sentences. I usually set a timer for 15 mins for each. If one of the questions takes more than 15 mins, it’s the weakness of the idea of the current form.\n\nProject Overview: What is the core focus of your project? Are you developing something new or testing existing ideas?\nProject Value: What makes this work meaningful and worth pursuing?\nResearch Gaps: What key questions or problems remain unsolved in this area?\nNovel Approach: What makes your proposed solution unique and different from existing methods?\nNecessity: Why develop a new solution if existing methods exist? What advantages does your approach offer?\nSuccess Metrics: How will you define and measure success for this project?\nValidation Strategy: What specific criteria or tests will demonstrate that your solution works?\nBroader Impact: How could this work benefit fields beyond your immediate research area?\nImplementation Plan: Break down each project goal into ~3 concrete, actionable tasks.",
    "crumbs": [
      "Home",
      "Course Information",
      "Deliverables"
    ]
  },
  {
    "objectID": "m02-visualization/principles.html",
    "href": "m02-visualization/principles.html",
    "title": "Principles of Perception in Data Visualization",
    "section": "",
    "text": "We extensively rely on visuals to perceive the world around us. However, our visual perception is not as truthful as you might think. It can be easily misled or manipulated if we are not aware of the inherent biases in how we see.\nLet us start with making specific about “perceptions”:\nWe will cover each of these in detail in the following sections.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Principles of Effective Visualization"
    ]
  },
  {
    "objectID": "m02-visualization/principles.html#color-perception",
    "href": "m02-visualization/principles.html#color-perception",
    "title": "Principles of Perception in Data Visualization",
    "section": "0.1 Color perception",
    "text": "0.1 Color perception\nColor is one of the most powerful tools in data visualization, but it is also one of the most complex. Our perception of color is not absolute; it is contextual and subjective.\n\nColor, Context, and Constancy\nOur visual system tries to maintain color constancy, meaning we perceive a familiar object as being a consistent color regardless of the lighting conditions. This is why we recognize a banana as yellow whether it’s in bright sunlight or in a dim room. However, this helpful adaptation can create peculiar biases in unfamiliar contexts.\n\n\n\nThe Dress\n\n\nThe infamous “dress” illusion highlights how our brain makes assumptions about lighting, causing some people to see the dress as blue and black (in bright light) and others as white and gold (in shadow). The colors are physically the same, but our perception of them is not.\nTake another example below:\n\n\n\nA “green” tree with no green pixels\n\n\nThis happens because what we “see” is not just the raw wavelength of light hitting our eyes. Our visual cortex processes that raw signal, making inferences based on context and prior experience. For example, we perceive the leaves of a tree as green, even in a photograph made entirely of red, black, and white pixels, because our brain “knows” trees are green.\n\n\nEncoding Color Objectively\nSince perception is subjective, we need objective systems to define color. The most common are:\n\nRGB (Red, Green, Blue): An additive system for screens. Colors are created by adding light. Combining all three creates white.\nCMYK (Cyan, Magenta, Yellow, Black): A subtractive system for print. Colors are created by subtracting light with ink. Combining all three (plus black for depth) creates black.\nHSL/HSV (Hue, Saturation, Lightness/Value): More intuitive systems that align better with how we think about color. Hue is the pure color, Saturation is the intensity, and Lightness/Value is the brightness.\n\n\n\n\n\n\n\nAccessibility Matters: Designing for Color Blindness\n\n\n\nA crucial aspect of color choice is ensuring your visualizations are accessible to everyone, including those with color vision deficiencies (CVD). Roughly 8% of men and 0.5% of women are affected.\n\nAvoid Red-Green Palettes: The most common form of CVD is difficulty distinguishing between red and green.\nUse Perceptually Uniform Palettes: Tools like ColorBrewer provide palettes that are designed to be accessible.\nCombine Color with Other Cues: Don’t rely only on color. Use shape, pattern, or direct labels to distinguish data series.\n\n\n\n\n\nThe Problem with Rainbows: Perceptually Uniform Palettes\nA perceptually uniform colormap is one where equal steps in the data are perceived as equal steps in color. The common “rainbow” (or “jet”) colormap fails at this because its brightness changes non-uniformly, creating false boundaries and hiding details. Palettes like Viridis were engineered to have a monotonically increasing luminance, making them accurate, intuitive, and accessible.\n\n\n\nViridis vs. Jet Colormap",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Principles of Effective Visualization"
    ]
  },
  {
    "objectID": "m02-visualization/principles.html#color-context-and-constancy",
    "href": "m02-visualization/principles.html#color-context-and-constancy",
    "title": "Principles of Perception in Data Visualization",
    "section": "1.1 Color, Context, and Constancy",
    "text": "1.1 Color, Context, and Constancy\nOur visual system tries to maintain color constancy, meaning we perceive a familiar object as being a consistent color regardless of the lighting conditions. This is why we recognize a banana as yellow whether it’s in bright sunlight or in a dim room. However, this helpful adaptation can create peculiar biases in unfamiliar contexts.\n\n\n\nThe Dress\n\n\nThe infamous “dress” illusion highlights how our brain makes assumptions about lighting, causing some people to see the dress as blue and black (in bright light) and others as white and gold (in shadow). The colors are physically the same, but our perception of them is not.\nTake another example below:\n\n\n\nA “green” tree with no green pixels\n\n\nThis happens because what we “see” is not just the raw wavelength of light hitting our eyes. Our visual cortex processes that raw signal, making inferences based on context and prior experience. For example, we perceive the leaves of a tree as green, even in a photograph made entirely of red, black, and white pixels, because our brain “knows” trees are green.\n\nEncoding Color Objectively\nSince perception is subjective, we need objective systems to define color. The most common are:\n\nRGB (Red, Green, Blue): An additive system for screens. Colors are created by adding light. Combining all three creates white.\nCMYK (Cyan, Magenta, Yellow, Black): A subtractive system for print. Colors are created by subtracting light with ink. Combining all three (plus black for depth) creates black.\nHSL/HSV (Hue, Saturation, Lightness/Value): More intuitive systems that align better with how we think about color. Hue is the pure color, Saturation is the intensity, and Lightness/Value is the brightness.\n\n\n\n\n\n\n\nAccessibility Matters: Designing for Color Blindness\n\n\n\nA crucial aspect of color choice is ensuring your visualizations are accessible to everyone, including those with color vision deficiencies (CVD). Roughly 8% of men and 0.5% of women are affected.\n\nAvoid Red-Green Palettes: The most common form of CVD is difficulty distinguishing between red and green.\nUse Perceptually Uniform Palettes: Tools like ColorBrewer provide palettes that are designed to be accessible.\nCombine Color with Other Cues: Don’t rely only on color. Use shape, pattern, or direct labels to distinguish data series.\n\n\n\n\n\nThe Problem with Rainbows: Perceptually Uniform Palettes\nA perceptually uniform colormap is one where equal steps in the data are perceived as equal steps in color. The common “rainbow” (or “jet”) colormap fails at this because its brightness changes non-uniformly, creating false boundaries and hiding details. Palettes like Viridis were engineered to have a monotonically increasing luminance, making them accurate, intuitive, and accessible.\n\n\n\nViridis vs. Jet Colormap",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Principles of Effective Visualization"
    ]
  },
  {
    "objectID": "m02-visualization/principles.html#color-is-contextual",
    "href": "m02-visualization/principles.html#color-is-contextual",
    "title": "Principles of Perception in Data Visualization",
    "section": "1.1 Color is contextual",
    "text": "1.1 Color is contextual\nOur visual system tries to maintain color constancy, meaning we perceive a familiar object as being a consistent color regardless of the lighting conditions. This is why we recognize a banana as yellow whether it’s in bright sunlight or in a dim room. However, this helpful adaptation can create peculiar biases in unfamiliar contexts.\n\n\n\nThe Dress\n\n\nThe infamous “dress” illusion highlights how our brain makes assumptions about lighting, causing some people to see the dress as blue and black (in bright light) and others as white and gold (in shadow). The colors are physically the same, but our perception of them is not.\nTake another example below:\n\n\n\nA “green” tree with no green pixels\n\n\nThis happens because what we “see” is not just the raw wavelength of light hitting our eyes. Our visual cortex processes that raw signal, making inferences based on context and prior experience. For example, we perceive the leaves of a tree as green, even in a photograph made entirely of red, black, and white pixels, because our brain “knows” trees are green.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Principles of Effective Visualization"
    ]
  },
  {
    "objectID": "m02-visualization/principles.html#encoding-colors-objectively",
    "href": "m02-visualization/principles.html#encoding-colors-objectively",
    "title": "Principles of Perception in Data Visualization",
    "section": "1.2 Encoding colors objectively",
    "text": "1.2 Encoding colors objectively\nColor is a physical phenomenon. It is the result of electromagnetic waves within a certain range of wavelengths, which our eyes and brain interpret as “color.”\nMost of what we see in the world is not pure spectral (single-wavelength) color, but rather mixtures of different wavelengths. Our eyes have three types of color receptors (cones) sensitive to different, overlapping ranges of wavelengths. When light of various wavelengths enters our eyes, the combination of signals from these cones is interpreted by our brain as a specific color sensation.\nBecause of this, we can represent an enormous range of colors simply by mixing a small set of primary colors. For screens and digital work, these are Red, Green, and Blue (the RGB model). By varying the intensity of each, we can mimic the effect of almost any color found in nature. This is called additive color mixing, because we’re combining different wavelengths of light.\nConversely, in printing (where we start with white paper and “take away” light with inks), we use the CMY (Cyan, Magenta, Yellow) system, where inks subtract some wavelengths and leave others. Mixing these subtractively produces the desired color on paper.\n\n\n\n\n\nAnother color model that is more intuitive for designing color palettes is the HSL (Hue, Saturation, Lightness) model. It is designed to match how humans naturally think about and describe color. Hue is the pure color, Saturation is the intensity, and Lightness/Value is the brightness.\n\n\n\n\n\n\n\n\n\n\n\nAccessibility Matters: Designing for Color Blindness\n\n\n\nWhen choosing colors, it is essential to make your visualizations accessible to everyone, including the significant portion of the population with color vision deficiencies (CVD), which affects about 8% of men and 0.5% of women. Avoid relying on red-green contrasts, as this is the most problematic pair for those with CVD. Instead, use perceptually uniform palettes—such as those available from tools like ColorBrewer—and always combine color with other cues like shape, patterns, or direct labels to ensure that important information is conveyed clearly to all viewers.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Principles of Effective Visualization"
    ]
  },
  {
    "objectID": "m02-visualization/principles.html#perceptually-uniform-palettes",
    "href": "m02-visualization/principles.html#perceptually-uniform-palettes",
    "title": "Principles of Perception in Data Visualization",
    "section": "1.3 Perceptually uniform palettes",
    "text": "1.3 Perceptually uniform palettes\nA perceptually uniform colormap is one where equal steps in the data are perceived as equal steps in color. The common “rainbow” (or “jet”) colormap fails at this because its brightness changes non-uniformly, creating false boundaries and hiding details. Palettes like viridis, plasma, inferno, magma, and cividis were engineered to have a monotonically increasing luminance, making them accurate, intuitive, and accessible.\nSee also the following paper for more details:\n\nCrameri, F., Shephard, G. E., & Heron, P. J. (2020). The misuse of colour in science communication. Nature communications, 11(1), 5444.\n\n\n\n\nViridis vs. Jet Colormap\n\n\n\n\nThis is a video about the story behind how the perceptually uniform palettes were created.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Principles of Effective Visualization"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html",
    "href": "m02-visualization/1d-data.html",
    "title": "1D Data Visualization",
    "section": "",
    "text": "Imagine you’re reading a research paper that claims “Treatment A is significantly better than Treatment B.” The paper shows a bar chart with two bars and error bars. The difference looks impressive. But here’s the question: what does the actual data look like? Are there 5 data points per group? 500? Are they normally distributed, or are there outliers? Are most points clustered together, or spread out?\nWithout seeing the raw data, you’re flying blind. And unfortunately, many scientific papers and reports make this same mistake: they summarize data without showing it.\nOne thing I want you to keep in mind:\nShow all the data points, whenever possible.\nThis is crucial for understanding the data and for communicating the message of the data.\n\n1 Why Showing All Data Matters\n\nStatisticians have been campaigning against bar charts with error bars—called “dynamite plots”—for years. Yet a systematic review found that 85.6% of papers in top physiology journals still use them. They appear everywhere: Nature, Science, Cell.\nWhy is this a problem? A dynamite plot shows you exactly four numbers (two means and two standard errors), regardless of sample size. But worse, completely different datasets produce identical bar charts. A dataset with outliers, a uniform distribution, or a bimodal distribution can all generate the same plot.\nRafael Irizarry showed an actual data behind a blood pressure comparison. The paper shows a bar chart with two bars and error bars. The difference looks significant. But the raw data revealed an extreme outlier (possibly a data entry error) and substantial overlap between groups. Remove that single outlier, and the result was no longer significant.\nAs Irizarry put it in his open letter to journal editors: dynamite plots conceal the data rather than showing it. The solution? Show the actual data points whenever possible, and use distributions (boxplots, histograms, density plots) when you can’t.\n\n\n2 How to Show All Data Points\nThe most straightforward approach is to plot every single data point. A swarm plot (also called a beeswarm plot) does exactly this: it displays each observation as a point, with points arranged to avoid overlap.\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate sample data\nfig, ax = plt.subplots(figsize=(10, 3))\ngroup_a = np.random.normal(100, 15, 50)\ngroup_b = np.random.normal(120, 20, 50)\ndata = {'Value': np.concatenate([group_a, group_b]),\n        'Group': ['A']*50 + ['B']*50}\n\n# Create swarm plot\nsns.swarmplot(data=data, y='Group', x='Value', ax = ax)\nplt.title('Swarm Plot: Every Point Visible')\nsns.despine()\n\n\n\n\n\nSwarm Plot\n\n\n\n\nSwarm plots are perfect for small to moderate datasets (roughly up to 100-200 points per group). There, you can see the actual sample size, the distribution shape, individual outliers, and the spread of the data.\nWhen you have too many points for a swarm plot, a strip plot with jittering can help. Instead of carefully arranging points to avoid overlap, we add random noise (jitter) to the x-position of each point.\n\n\nCode\n# Strip plot with jittering\nfig, ax = plt.subplots(figsize=(10, 3))\nsns.stripplot(data=data, y='Group', x='Value', alpha=0.6, jitter=0.2, ax = ax)\nplt.title('Strip Plot with Jittering')\nsns.despine()\n\n\n\n\n\nStrip Plot with Jittering\n\n\n\n\nThe key parameters: - alpha: Controls transparency (0 = invisible, 1 = opaque). Values around 0.3-0.7 work well. - jitter: Amount of random horizontal displacement. Too much jitter and groups overlap; too little and points stack vertically.\n\n\n\nA figure taken from the paper Neural embeddings of scholarly periodicals reveal complex disciplinary organizations by showing the distribution of publications in terms of various scientific contrasts.\nFor even larger datasets, consider a barcode plot. This shows each data point as a small vertical tick mark along an axis. It’s minimalist but effective for showing the distribution of many points.\n\n\nCode\n# Barcode plot using rug plot\nimport pandas as pd\n\n# Convert data to DataFrame if not already\ndata_df = pd.DataFrame(data)\n\nfig, ax = plt.subplots(figsize=(10, 2))\nfor i, group in enumerate(['A', 'B']):\n    values = data_df.loc[data_df['Group'] == group, 'Value']\n    ax.plot(values, [i]*len(values), '|', markersize=10, alpha=0.7)\nax.set_yticks([0, 1])\nax.set_yticklabels(['A', 'B'])\nax.set_ylim(-0.5, 1.5)\nax.set_xlabel('Value')\nax.set_title('Barcode Plot')\nsns.despine()\n\n\n\n\n\nRug Plot\n\n\n\n\nBarcode plots work well when you have thousands of points and want to show density patterns without losing the “raw data” feel.\n\n\n3 When can’t show all data points?\nWhen your dataset is large enough that individual points become impractical to show, you need to summarize the distribution. The most common approach is the histogram.\nA histogram divides your data range into bins and counts how many observations fall into each bin. It’s a powerful tool for understanding the shape of your distribution.\n\n\nCode\n# Histogram\nplt.hist(group_a, bins=15, alpha=0.5, label='Group A', edgecolor='black')\nplt.hist(group_b, bins=15, alpha=0.5, label='Group B', edgecolor='black')\nplt.xlabel('Value')\nplt.ylabel('Count')\nplt.legend()\nplt.title('Histogram: Distribution Comparison')\nplt.show()\n\n\n\n\n\nHistogram\n\n\n\n\nThe number of bins dramatically affects how your histogram looks. If you have too few bins, you lose detail and might miss important features like bimodality. If you have too many bins, the histogram becomes noisy and hard to interpret.\nA good starting point is the Sturges’ rule: number of bins H \\log_2(n) + 1, where n is the sample size. But always experiment! Try different bin numbers and see what reveals the most about your data’s structure.\nHistograms have a problem: they’re sensitive to bin width and bin placement. Move your bins slightly, and the histogram can look quite different.\nKernel Density Estimation (KDE) provides a smooth alternative. Instead of binning, KDE places a small “kernel” (usually a Gaussian curve) at each data point and sums them up. The result is a smooth density curve.\n\n\nCode\nsns.kdeplot(data=group_a, label='A', fill=True, alpha=0.5)\nsns.kdeplot(data=group_b, label='B', fill=True, alpha=0.5)\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.legend()\nplt.title('Kernel Density Estimate')\nplt.show()\n\n\n\n\n\nKernel Density Estimate\n\n\n\n\nKDE plots are elegant and reveal the shape of your distribution without the arbitrary choices of histograms. However, they can be misleading at the edges of your data and may suggest data exists where it doesn’t.\n\n\n4 For Heavy-Tailed Data\nSome data are extremely heterogeneous—think income distributions, city populations, or earthquake magnitudes. These distributions often have heavy tails: most values are small, but a few are enormous.\nFor this kind of data, histograms and KDE plots can be misleading because they compress the tail into a tiny region of the plot.\nThe cumulative distribution function (CDF) shows the proportion of data points less than or equal to each value. Instead of asking “how many points are in this bin?”, the CDF asks “what fraction of points are below this value?”\nThe CDF is a density estimation method that requires no parameter choices. Unlike histograms (which require bin size) or KDE (which requires bandwidth), the CDF is completely determined by your data. There are no arbitrary decisions that change how your data looks—making it one of the most honest ways to visualize a distribution.\n\n\nCode\n# CDF\nsorted_a = np.sort(group_a)\ncdf_a = np.arange(1, len(sorted_a) + 1) / len(sorted_a)\n\nsorted_b = np.sort(group_b)\ncdf_b = np.arange(1, len(sorted_b) + 1) / len(sorted_b)\n\nplt.plot(sorted_a, cdf_a, label='Group A', linewidth=2)\nplt.plot(sorted_b, cdf_b, label='Group B', linewidth=2)\nplt.xlabel('Value')\nplt.ylabel('Cumulative Probability')\nplt.legend()\nplt.title('Cumulative Distribution Function')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\nCumulative Distribution Function\n\n\n\n\nThe CDF has several advantages:\n\nNo binning decisions: Every data point is shown\nEasy to read percentiles: The median is where CDF = 0.5\nGreat for comparisons: Differences between groups are easy to spot\n\nFor heavy-tailed distributions, the complementary cumulative distribution function (CCDF) is even more useful. The CCDF shows the proportion of data points greater than each value: CCDF(x) = 1 - CDF(x).\nUnlike the CDF, the CCDF can show, when combined with the log-log scale, the tail of heavy-tailed distributions.\n\n\nCode\n# CCDF on log-log scale\n# Generate heavy-tailed data\n# Generate heavy-tailed data\nsns.set(font_scale=2.0)\nsns.set_style(\"white\")\n\nheavy_tailed = np.random.pareto(2, 1000) + 1\n\nsorted_data = np.sort(heavy_tailed)\ncdf = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\nccdf = 1 - cdf\n\nfig, ax = plt.subplots(1, 2, figsize=(14, 7))\n\n# CDF plot (linear scale) using seaborn (lineplot)\nsns.lineplot(x=sorted_data, y=cdf, ax=ax[0], color=sns.color_palette()[0], linewidth=2)\nax[0].set_xlabel('Value')\nax[0].set_ylabel('P(X ≤ x)')\nax[0].set_title('Cumulative Distribution Function (CDF)')\nax[0].grid(True, alpha=0.3)\n\n# CCDF plot (log-log scale) using seaborn (scatterplot)\nsns.scatterplot(x=sorted_data, y=ccdf, ax=ax[1], color=sns.color_palette()[1], alpha=0.5, s=9, marker='o')\nax[1].set_xscale('log')\nax[1].set_yscale('log')\nax[1].set_xlabel('Value')\nax[1].set_ylabel('P(X &gt; x)')\nax[1].set_title('Complementary Cumulative Distribution (CCDF)')\nax[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nCDF vs CCDF. The CCDF reveals the tail behavior that’s invisible in traditional histograms.\n\n\n\n\n\n\n5 The Bigger Picture\nThe methods you choose to visualize your data aren’t just aesthetic choices—they’re scientific choices. Different visualizations reveal different aspects of your data, and some can hide important patterns.\nBy starting with the raw data and building up to summaries, you ensure that you understand what you’re working with. And by showing your data (not just summarizing it), you allow others to draw their own conclusions.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#the-case-of-the-missing-data-points",
    "href": "m02-visualization/1d-data.html#the-case-of-the-missing-data-points",
    "title": "1D Data Visualization",
    "section": "",
    "text": "Imagine you’re reading a research paper that claims “Treatment A is significantly better than Treatment B.” The paper shows a bar chart with two bars and error bars. The difference looks impressive. But here’s the question: what does the actual data look like? Are there 5 data points per group? 500? Are they normally distributed, or are there outliers? Are most points clustered together, or spread out?\nWithout seeing the raw data, you’re flying blind. And unfortunately, many scientific papers and reports make this same mistake: they summarize data without showing it.\nOne thing I want you to keep in mind:\nShow all the data points, whenever possible.\nThis is crucial for understanding the data and for communicating the message of the data.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#why-showing-all-data-matters",
    "href": "m02-visualization/1d-data.html#why-showing-all-data-matters",
    "title": "1D Data Visualization",
    "section": "1 Why Showing All Data Matters",
    "text": "1 Why Showing All Data Matters\nIn 2016, a group of researchers analyzed 118 papers in leading neuroscience journals and found something disturbing: when they requested the raw data and re-analyzed it, they found that the bar charts in many papers were misleading. The bar charts suggested clear differences between groups, but the raw data often told a different story—with substantial overlap between groups, unexpected distributions, or influential outliers.\nThis isn’t about fraud. It’s about the limitations of summary statistics. When you reduce your data to a mean and a standard error, you lose a tremendous amount of information. The data might be bimodal, skewed, or contain outliers. These patterns are invisible in a bar chart, but they’re crucial for understanding what’s really going on.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#dynamite-plots-must-die",
    "href": "m02-visualization/1d-data.html#dynamite-plots-must-die",
    "title": "1D Data Visualization",
    "section": "1.1 Dynamite Plots Must Die",
    "text": "1.1 Dynamite Plots Must Die\n\nStatisticians have been campaigning against bar charts with error bars—called “dynamite plots”—for years. Yet a systematic review found that 85.6% of papers in top physiology journals still use them. They appear everywhere: Nature, Science, Cell.\nWhy is this a problem? A dynamite plot shows you exactly four numbers (two means and two standard errors), regardless of sample size. But worse, completely different datasets produce identical bar charts. A dataset with outliers, a uniform distribution, or a bimodal distribution can all generate the same plot.\nWhen Rafael Irizarry showed the actual data behind a blood pressure comparison, the story changed dramatically. The bar chart showed a clear, significant difference. But the raw data revealed an extreme outlier (possibly a data entry error) and substantial overlap between groups. Remove that single outlier, and the result was no longer significant.\nAs Irizarry put it in his open letter to journal editors: dynamite plots conceal the data rather than showing it. The solution? Show the actual data points whenever possible, and use distributions (boxplots, histograms, density plots) when you can’t.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#start-simple-show-every-point",
    "href": "m02-visualization/1d-data.html#start-simple-show-every-point",
    "title": "1D Data Visualization",
    "section": "1.1 Start Simple: Show Every Point",
    "text": "1.1 Start Simple: Show Every Point\n\nSwarm Plots (Beeswarm Plots)\nThe most straightforward approach is to plot every single data point. A swarm plot (also called a beeswarm plot) does exactly this: it displays each observation as a point, with points arranged to avoid overlap.\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate sample data\nfig, ax = plt.subplots(figsize=(10, 3))\ngroup_a = np.random.normal(100, 15, 30)\ngroup_b = np.random.normal(120, 20, 30)\ndata = {'Value': np.concatenate([group_a, group_b]),\n        'Group': ['A']*30 + ['B']*30}\n\n# Create swarm plot\nsns.swarmplot(data=data, y='Group', x='Value', ax = ax)\nplt.title('Swarm Plot: Every Point Visible')\nsns.despine()\n\n\n\n\n\nSwarm Plot\n\n\n\n\nSwarm plots are perfect for small to moderate datasets (roughly up to 100-200 points per group). There, you can see the actual sample size, the distribution shape, individual outliers, and the spread of the data.\n\n\nThe Limits of Swarm Plots\nBut what happens when you have more data? With hundreds or thousands of points, swarm plots become cluttered and difficult to read. The points start to pile up, and the plot becomes a blob. This is where we need more sophisticated techniques.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#handling-more-data-transparency-and-jittering",
    "href": "m02-visualization/1d-data.html#handling-more-data-transparency-and-jittering",
    "title": "1D Data Visualization",
    "section": "2.1 Handling More Data: Transparency and Jittering",
    "text": "2.1 Handling More Data: Transparency and Jittering\n\nStrip Plots with Jittering\nWhen you have too many points for a swarm plot, a strip plot with jittering can help. Instead of carefully arranging points to avoid overlap, we add random noise (jitter) to the x-position of each point.\n# Strip plot with jittering\nsns.stripplot(data=data, x='Group', y='Value', alpha=0.6, jitter=0.2)\nplt.title('Strip Plot with Jittering')\nplt.show()\nThe key parameters: - alpha: Controls transparency (0 = invisible, 1 = opaque). Values around 0.3-0.7 work well. - jitter: Amount of random horizontal displacement. Too much jitter and groups overlap; too little and points stack vertically.\n\n\nBarcode Plots (Rug Plots)\nFor even larger datasets, consider a barcode plot (also called a rug plot). This shows each data point as a small vertical tick mark along an axis. It’s minimalist but effective for showing the distribution of many points.\n# Barcode plot using rug plot\nfig, ax = plt.subplots(figsize=(10, 2))\nfor i, group in enumerate(['A', 'B']):\n    values = data[data['Group'] == group]['Value']\n    ax.plot(values, [i]*len(values), '|', markersize=10, alpha=0.7)\nax.set_yticks([0, 1])\nax.set_yticklabels(['A', 'B'])\nax.set_xlabel('Value')\nax.set_title('Barcode Plot')\nplt.show()\nBarcode plots work well when you have thousands of points and want to show density patterns without losing the “raw data” feel.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#summarizing-distributions-histograms",
    "href": "m02-visualization/1d-data.html#summarizing-distributions-histograms",
    "title": "1D Data Visualization",
    "section": "2.1 Summarizing Distributions: Histograms",
    "text": "2.1 Summarizing Distributions: Histograms\nWhen your dataset is large enough that individual points become impractical to show, you need to summarize the distribution. The most common approach is the histogram.\nA histogram divides your data range into bins and counts how many observations fall into each bin. It’s a powerful tool for understanding the shape of your distribution.\n# Histogram\nplt.hist(group_a, bins=15, alpha=0.5, label='Group A', edgecolor='black')\nplt.hist(group_b, bins=15, alpha=0.5, label='Group B', edgecolor='black')\nplt.xlabel('Value')\nplt.ylabel('Count')\nplt.legend()\nplt.title('Histogram: Distribution Comparison')\nplt.show()\n\nThe Art of Choosing Bins\nThe number of bins dramatically affects how your histogram looks: - Too few bins: You lose detail and might miss important features like bimodality - Too many bins: The histogram becomes noisy and hard to interpret\nA good starting point is the Sturges’ rule: number of bins H \\log_2(n) + 1, where n is the sample size. But always experiment! Try different bin numbers and see what reveals the most about your data’s structure.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#smooth-alternatives-kernel-density-estimation",
    "href": "m02-visualization/1d-data.html#smooth-alternatives-kernel-density-estimation",
    "title": "1D Data Visualization",
    "section": "3.1 Smooth Alternatives: Kernel Density Estimation",
    "text": "3.1 Smooth Alternatives: Kernel Density Estimation\nHistograms have a problem: they’re sensitive to bin width and bin placement. Move your bins slightly, and the histogram can look quite different.\nKernel Density Estimation (KDE) provides a smooth alternative. Instead of binning, KDE places a small “kernel” (usually a Gaussian curve) at each data point and sums them up. The result is a smooth density curve.\n# KDE plot\nsns.kdeplot(data=group_a, label='Group A', fill=True, alpha=0.5)\nsns.kdeplot(data=group_b, label='Group B', fill=True, alpha=0.5)\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.legend()\nplt.title('Kernel Density Estimate')\nplt.show()\nKDE plots are elegant and reveal the shape of your distribution without the arbitrary choices of histograms. However, they can be misleading at the edges of your data and may suggest data exists where it doesn’t.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#for-heavy-tailed-data-cumulative-distributions",
    "href": "m02-visualization/1d-data.html#for-heavy-tailed-data-cumulative-distributions",
    "title": "1D Data Visualization",
    "section": "3.1 For Heavy-Tailed Data: Cumulative Distributions",
    "text": "3.1 For Heavy-Tailed Data: Cumulative Distributions\nSome data are extremely heterogeneous—think income distributions, city populations, or earthquake magnitudes. These distributions often have heavy tails: most values are small, but a few are enormous.\nFor this kind of data, histograms and KDE plots can be misleading because they compress the tail into a tiny region of the plot.\nThe cumulative distribution function (CDF) shows the proportion of data points less than or equal to each value. Instead of asking “how many points are in this bin?”, the CDF asks “what fraction of points are below this value?”\nThe CDF is a density estimation method that requires no parameter choices. Unlike histograms (which require bin size) or KDE (which requires bandwidth), the CDF is completely determined by your data. There are no arbitrary decisions that change how your data looks—making it one of the most honest ways to visualize a distribution.\n\n\nCode\n# CDF\nsorted_a = np.sort(group_a)\ncdf_a = np.arange(1, len(sorted_a) + 1) / len(sorted_a)\n\nsorted_b = np.sort(group_b)\ncdf_b = np.arange(1, len(sorted_b) + 1) / len(sorted_b)\n\nplt.plot(sorted_a, cdf_a, label='Group A', linewidth=2)\nplt.plot(sorted_b, cdf_b, label='Group B', linewidth=2)\nplt.xlabel('Value')\nplt.ylabel('Cumulative Probability')\nplt.legend()\nplt.title('Cumulative Distribution Function')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\nCumulative Distribution Function\n\n\n\n\nThe CDF has several advantages:\n\nNo binning decisions: Every data point is shown\nEasy to read percentiles: The median is where CDF = 0.5\nGreat for comparisons: Differences between groups are easy to spot\n\nFor heavy-tailed distributions, the complementary cumulative distribution function (CCDF) is even more useful. The CCDF shows the proportion of data points greater than each value: CCDF(x) = 1 - CDF(x).\nUnlike the CDF, the CCDF can show, when combined with the log-log scale, the tail of heavy-tailed distributions.\n\n\nCode\n# CCDF on log-log scale\n# Generate heavy-tailed data\n# Generate heavy-tailed data\nsns.set(font_scale=2.0)\nsns.set_style(\"white\")\n\nheavy_tailed = np.random.pareto(2, 1000) + 1\n\nsorted_data = np.sort(heavy_tailed)\ncdf = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\nccdf = 1 - cdf\n\nfig, ax = plt.subplots(1, 2, figsize=(14, 7))\n\n# CDF plot (linear scale) using seaborn (lineplot)\nsns.lineplot(x=sorted_data, y=cdf, ax=ax[0], color=sns.color_palette()[0], linewidth=2)\nax[0].set_xlabel('Value')\nax[0].set_ylabel('P(X ≤ x)')\nax[0].set_title('Cumulative Distribution Function (CDF)')\nax[0].grid(True, alpha=0.3)\n\n# CCDF plot (log-log scale) using seaborn (scatterplot)\nsns.scatterplot(x=sorted_data, y=ccdf, ax=ax[1], color=sns.color_palette()[1], alpha=0.5, s=9, marker='o')\nax[1].set_xscale('log')\nax[1].set_yscale('log')\nax[1].set_xlabel('Value')\nax[1].set_ylabel('P(X &gt; x)')\nax[1].set_title('Complementary Cumulative Distribution (CCDF)')\nax[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nCDF vs CCDF. The CCDF reveals the tail behavior that’s invisible in traditional histograms.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#choosing-the-right-visualization",
    "href": "m02-visualization/1d-data.html#choosing-the-right-visualization",
    "title": "1D Data Visualization",
    "section": "3.2 Choosing the Right Visualization",
    "text": "3.2 Choosing the Right Visualization\nHere’s a quick decision guide:\n\n\n\n\n\n\n\n\nScenario\nBest Visualization\nWhy\n\n\n\n\n&lt; 100 points per group\nSwarm plot\nShows every data point clearly\n\n\n100-500 points\nStrip plot with jitter + transparency\nManageable with some overlap\n\n\n500-5000 points\nHistogram or KDE + rug plot\nNeed summary but show raw data on axis\n\n\n&gt; 5000 points\nKDE or histogram alone\nToo many points to show individually\n\n\nHeavy-tailed/heterogeneous\nCCDF (log-log scale)\nReveals tail behavior\n\n\nComparing distributions\nCDF or overlaid KDE\nEasy to spot differences",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#the-bigger-picture",
    "href": "m02-visualization/1d-data.html#the-bigger-picture",
    "title": "1D Data Visualization",
    "section": "3.2 The Bigger Picture",
    "text": "3.2 The Bigger Picture\nThe methods you choose to visualize your data aren’t just aesthetic choices—they’re scientific choices. Different visualizations reveal different aspects of your data, and some can hide important patterns.\nBy starting with the raw data and building up to summaries, you ensure that you understand what you’re working with. And by showing your data (not just summarizing it), you allow others to draw their own conclusions.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#further-reading",
    "href": "m02-visualization/1d-data.html#further-reading",
    "title": "1D Data Visualization",
    "section": "3.3 Further Reading",
    "text": "3.3 Further Reading\n\nDynamite Plots Must Die - Rafael Irizarry’s open letter to journal editors\nBeyond Bar and Line Graphs: Time for a New Data Presentation Paradigm - The paper that started the “show your data” movement\nWeissgerber et al. (2015). Why we need to report more than ‘Data were Analyzed by t-tests or ANOVA’\nShow the data, don’t conceal them - British Journal of Pharmacology (2011)\nVisualizing Samples with Box Plots\nKernel Density Estimation Explained - Interactive tutorial",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/2d-data.html",
    "href": "m02-visualization/2d-data.html",
    "title": "2D Data Visualization",
    "section": "",
    "text": "You’ve probably heard that “correlation doesn’t equal causation.” But here’s an even more fundamental problem: a correlation coefficient doesn’t tell you what your data actually looks like.\nIn 1973, statistician Francis Anscombe created four datasets that became legendary in data visualization. Each dataset has 11 (x, y) pairs. Each has the same mean for x and y, the same variance, and… most remarkably—the same correlation coefficient (r = 0.816) and the same linear regression line.\nBut when you plot them, they tell completely different stories.\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Load Anscombe's quartet\nanscombe = sns.load_dataset(\"anscombe\")\n\n# Create the plot\nsns.set_style(\"white\")\ng = sns.FacetGrid(anscombe, col=\"dataset\", col_wrap=2, height=4, aspect=1.2)\ng.map_dataframe(sns.scatterplot, x=\"x\", y=\"y\", s=100)\ng.map_dataframe(sns.regplot, x=\"x\", y=\"y\", scatter=False, color=\"red\")\ng.set_axis_labels(\"X\", \"Y\")\ng.set_titles(\"Dataset {col_name}\")\n\n# Add correlation to each subplot\nfor ax, dataset in zip(g.axes.flat, [\"I\", \"II\", \"III\", \"IV\"]):\n    data_subset = anscombe[anscombe[\"dataset\"] == dataset]\n    r = np.corrcoef(data_subset[\"x\"], data_subset[\"y\"])[0, 1]\n    ax.text(0.05, 0.95, f'r = {r:.3f}', transform=ax.transAxes,\n            verticalalignment='top', fontsize=12, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\nsns.despine()\nplt.tight_layout()\n\n\n\n\n\nAnscombe’s Quartet: Four datasets with identical summary statistics but completely different relationships\nDataset I shows a nice linear relationship. Dataset II is clearly non-linear—a parabola that a linear model completely misses. Dataset III has a perfect linear relationship except for one outlier that changes everything. Dataset IV shows no relationship except for a single influential point that creates the illusion of correlation.\nThe same correlation coefficient. The same regression line. Completely different data.\nThis is why we visualize relationships:\nAlways plot your bivariate data. Summary statistics conceal structure.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 2D Data"
    ]
  },
  {
    "objectID": "m02-visualization/2d-data.html#d-histograms-heatmaps",
    "href": "m02-visualization/2d-data.html#d-histograms-heatmaps",
    "title": "2D Data Visualization",
    "section": "2.1 2D Histograms (Heatmaps)",
    "text": "2.1 2D Histograms (Heatmaps)\nA 2D histogram extends the 1D histogram concept to two dimensions. The plane is divided into rectangular bins, and each bin’s color represents the number of points it contains.\n\n\nCode\n# Generate large dataset\nnp.random.seed(789)\nn_large = 10000\nx_large = np.random.normal(50, 15, n_large)\ny_large = 0.8 * x_large + np.random.normal(0, 12, n_large)\n\nfig, ax = plt.subplots(figsize=(10, 7))\nhb = ax.hexbin(x_large, y_large, gridsize=30, cmap='YlOrRd', mincnt=1)\nax.set_xlabel('X Variable')\nax.set_ylabel('Y Variable')\nax.set_title('2D Histogram: Density Through Binning')\ncb = plt.colorbar(hb, ax=ax)\ncb.set_label('Count')\nsns.despine()\n\n\n\n\n\n2D histogram showing density through rectangular bins\n\n\n\n\nThe key parameter is bin size (or gridsize). Too few bins and you lose detail; too many bins and the plot becomes noisy. Like 1D histograms, this requires experimentation.\n\n\nCode\nfig, axes = plt.subplots(1, 3, figsize=(14, 5))\ngridsizes = [10, 30, 60]\n\nfor ax, gridsize in zip(axes, gridsizes):\n    hb = ax.hexbin(x_large, y_large, gridsize=gridsize, cmap='YlOrRd', mincnt=1)\n    ax.set_xlabel('X Variable')\n    ax.set_ylabel('Y Variable')\n    ax.set_title(f'Gridsize = {gridsize}')\n    plt.colorbar(hb, ax=ax)\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n\n\n\n\n\nEffect of bin size on 2D histograms\n\n\n\n\nWith gridsize = 10, we see only coarse structure. With gridsize = 60, the plot is noisy\u0014some bins have few points by chance. Gridsize = 30 provides a good balance.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 2D Data"
    ]
  },
  {
    "objectID": "m02-visualization/2d-data.html#hexbin-plots",
    "href": "m02-visualization/2d-data.html#hexbin-plots",
    "title": "2D Data Visualization",
    "section": "2.2 Hexbin Plots",
    "text": "2.2 Hexbin Plots\nHexagonal binning uses hexagons instead of rectangles. Hexagons are better for 2D binning because they’re closer to circles\u0014every edge is equidistant from the center, reducing bias in how we perceive density.\n\n\nCode\n# Generate data with interesting structure\nnp.random.seed(101)\nn = 8000\n\n# Create two clusters\ncluster1_x = np.random.normal(30, 8, n // 2)\ncluster1_y = np.random.normal(40, 8, n // 2)\ncluster2_x = np.random.normal(60, 10, n // 2)\ncluster2_y = np.random.normal(70, 10, n // 2)\n\nx_clusters = np.concatenate([cluster1_x, cluster2_x])\ny_clusters = np.concatenate([cluster1_y, cluster2_y])\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Scatter plot (for reference)\naxes[0].scatter(x_clusters, y_clusters, alpha=0.1, s=10)\naxes[0].set_xlabel('X Variable')\naxes[0].set_ylabel('Y Variable')\naxes[0].set_title('Scatter Plot (alpha = 0.1)')\nsns.despine(ax=axes[0])\n\n# Hexbin plot\nhb = axes[1].hexbin(x_clusters, y_clusters, gridsize=25, cmap='viridis', mincnt=1)\naxes[1].set_xlabel('X Variable')\naxes[1].set_ylabel('Y Variable')\naxes[1].set_title('Hexbin Plot')\nplt.colorbar(hb, ax=axes[1], label='Count')\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n\n\n\n\n\nHexbin plot provides more perceptually uniform density representation\n\n\n\n\nThe hexbin plot clearly reveals the two clusters and their relative densities\u0014something that’s harder to see in the scatter plot even with low alpha.\nHexbin plots are particularly powerful for very large datasets (100,000+ points) where scatter plots become computationally expensive and visually overwhelming.\n\n\n\n\n\n\nChoosing colors for density plots\n\n\n\nWhen showing density or counts, use sequential colormaps that vary in lightness: light = low density, dark = high density. Good choices include: - 'YlOrRd' (yellow-orange-red) - 'viridis' (purple-blue-green-yellow, perceptually uniform) - 'Blues' or 'Reds' (single hue)\nAvoid rainbow colormaps like 'jet'\u0014they create artificial boundaries where none exist and are not perceptually uniform.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 2D Data"
    ]
  },
  {
    "objectID": "m02-visualization/2d-data.html#contour-plots",
    "href": "m02-visualization/2d-data.html#contour-plots",
    "title": "2D Data Visualization",
    "section": "3.1 Contour Plots",
    "text": "3.1 Contour Plots\nA contour plot represents the density surface as lines of equal density\u0014like a topographic map where each contour line represents an “elevation” of density.\n\n\nCode\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Filled contours\nsns.kdeplot(x=x_clusters, y=y_clusters, cmap='viridis', fill=True,\n            thresh=0.05, levels=10, ax=axes[0])\naxes[0].set_xlabel('X Variable')\naxes[0].set_ylabel('Y Variable')\naxes[0].set_title('Filled Contour Plot')\nsns.despine(ax=axes[0])\n\n# Line contours with scatter\naxes[1].scatter(x_clusters, y_clusters, alpha=0.1, s=5, c='gray')\nsns.kdeplot(x=x_clusters, y=y_clusters, levels=8, color='red', linewidths=2, ax=axes[1])\naxes[1].set_xlabel('X Variable')\naxes[1].set_ylabel('Y Variable')\naxes[1].set_title('Contour Lines Over Scatter Plot')\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n\n\n\n\n\nContour plot shows density as topographic lines\n\n\n\n\nContour plots are excellent for: - Overlaying density information on scatter plots - Comparing multiple groups (different colored contours) - Showing the “shape” of the relationship clearly",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 2D Data"
    ]
  },
  {
    "objectID": "m02-visualization/2d-data.html#color-coding-by-group",
    "href": "m02-visualization/2d-data.html#color-coding-by-group",
    "title": "2D Data Visualization",
    "section": "5.1 Color coding by group",
    "text": "5.1 Color coding by group\nThe simplest approach is to use different colors for different groups:\n\n\nCode\n# Generate multi-group data\nnp.random.seed(303)\nn_per_group = 150\n\ngroup_a_x = np.random.normal(40, 12, n_per_group)\ngroup_a_y = 0.7 * group_a_x + np.random.normal(0, 8, n_per_group)\n\ngroup_b_x = np.random.normal(55, 10, n_per_group)\ngroup_b_y = 1.2 * group_b_x + np.random.normal(-20, 10, n_per_group)\n\ngroup_c_x = np.random.normal(60, 15, n_per_group)\ngroup_c_y = 0.3 * group_c_x + np.random.normal(30, 12, n_per_group)\n\ndf_groups = pd.DataFrame({\n    'x': np.concatenate([group_a_x, group_b_x, group_c_x]),\n    'y': np.concatenate([group_a_y, group_b_y, group_c_y]),\n    'group': ['A'] * n_per_group + ['B'] * n_per_group + ['C'] * n_per_group\n})\n\nfig, ax = plt.subplots(figsize=(10, 6))\nfor group, color in zip(['A', 'B', 'C'], sns.color_palette('muted', 3)):\n    subset = df_groups[df_groups['group'] == group]\n    ax.scatter(subset['x'], subset['y'], label=f'Group {group}',\n               alpha=0.6, s=50, color=color, edgecolors='white', linewidth=0.5)\n\nax.set_xlabel('X Variable')\nax.set_ylabel('Y Variable')\nax.set_title('Relationships Across Groups')\nax.legend()\nsns.despine()\n\n\n\n\n\nScatter plot with color-coded groups\n\n\n\n\nThis reveals that the three groups have different relationships: Group A has a positive moderate slope, Group B has a steeper positive relationship, and Group C has almost no relationship.\n\n\n\n\n\n\nSimpson’s Paradox\n\n\n\nBe careful! Sometimes the overall trend (pooling all groups) can be opposite to the trend within each group. This is called Simpson’s Paradox. Always visualize groups separately to check if pooling is appropriate.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 2D Data"
    ]
  },
  {
    "objectID": "m02-visualization/2d-data.html#small-multiples-faceting",
    "href": "m02-visualization/2d-data.html#small-multiples-faceting",
    "title": "2D Data Visualization",
    "section": "5.2 Small multiples (faceting)",
    "text": "5.2 Small multiples (faceting)\nWhen groups overlap heavily or there are many groups, small multiples\u0014separate plots for each group\u0014work better than color coding:\n\n\nCode\ng = sns.FacetGrid(df_groups, col='group', height=4, aspect=1.3)\ng.map_dataframe(sns.scatterplot, x='x', y='y', alpha=0.6, s=50)\ng.map_dataframe(sns.regplot, x='x', y='y', scatter=False, color='red')\ng.set_axis_labels('X Variable', 'Y Variable')\ng.set_titles('Group {col_name}')\nsns.despine()\nplt.tight_layout()\n\n\n\n\n\nSmall multiples showing relationship for each group separately\n\n\n\n\nSmall multiples make it easy to compare the strength and direction of relationships across groups without visual clutter.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 2D Data"
    ]
  },
  {
    "objectID": "m02-visualization/2d-data.html#contour-overlays",
    "href": "m02-visualization/2d-data.html#contour-overlays",
    "title": "2D Data Visualization",
    "section": "5.3 Contour overlays",
    "text": "5.3 Contour overlays\nFor large datasets, overlaying density contours for each group can be very effective:\n\n\nCode\nfig, ax = plt.subplots(figsize=(10, 7))\n\ncolors = sns.color_palette('muted', 3)\nfor group, color in zip(['A', 'B', 'C'], colors):\n    subset = df_groups[df_groups['group'] == group]\n    sns.kdeplot(x=subset['x'], y=subset['y'], levels=5,\n                color=color, linewidths=2, label=f'Group {group}', ax=ax)\n\nax.set_xlabel('X Variable')\nax.set_ylabel('Y Variable')\nax.set_title('Density Contours by Group')\nax.legend()\nsns.despine()\n\n\n/var/folders/j7/9dgqq5g53vnbsbmvh2yqtckr0000gr/T/ipykernel_64204/3335885635.py:12: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n  ax.legend()\n\n\n\n\n\nOverlaid density contours reveal different relationship shapes\n\n\n\n\nThis clearly shows that Groups A and B have elongated, correlated distributions (indicating strong relationships), while Group C is more circular (indicating weak correlation).",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 2D Data"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html",
    "href": "m02-visualization/highd-data.html",
    "title": "High-Dimensional Data Visualization",
    "section": "",
    "text": "Imagine you’re analyzing data with 50 features per observation: gene expression levels, user behavior metrics, or environmental measurements. You want to understand the patterns in your data. How do different observations relate to each other? Are there clusters? Outliers?\nYou can’t plot 50 dimensions directly. Our visual system lives in three dimensions (or really, two dimensions on a screen). This creates a fundamental challenge: how do you visualize data that lives in spaces you cannot see?\nThe answer is dimensionality reduction—projecting high-dimensional data into 2 or 3 dimensions while preserving important structure. But here’s the critical question: what structure matters?\nDifferent methods preserve different aspects of your data. Some preserve global structure (how groups relate to each other across the entire dataset). Others preserve local structure (which points are nearest neighbors). Understanding these trade-offs is essential for choosing the right method—and for not being misled by beautiful but misleading visualizations.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html#how-pca-works",
    "href": "m02-visualization/highd-data.html#how-pca-works",
    "title": "High-Dimensional Data Visualization",
    "section": "3.1 How PCA works",
    "text": "3.1 How PCA works\nImagine you have a cloud of points in high-dimensional space. PCA asks: “What direction captures the most variation in the data?” This becomes the first principal component (PC1). Then it asks: “What direction, perpendicular to the first, captures the most remaining variation?” This becomes PC2. And so on.\nMathematically, PCA finds the eigenvectors of the covariance matrix. But conceptually, it’s rotating your coordinate system to align with the “natural axes” of your data’s variation.\n\n\nCode\nfrom sklearn.decomposition import PCA\n\n# Generate correlated 2D data (for visualization)\nnp.random.seed(123)\nmean = [0, 0]\ncov = [[3, 2], [2, 2]]\ndata_2d = np.random.multivariate_normal(mean, cov, 300)\n\n# Fit PCA\npca = PCA(n_components=2)\npca.fit(data_2d)\n\n# Plot original data with principal components\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(data_2d[:, 0], data_2d[:, 1], alpha=0.6, s=50)\n\n# Draw principal components as arrows\norigin = pca.mean_\nfor i, (component, variance) in enumerate(zip(pca.components_, pca.explained_variance_)):\n    direction = component * np.sqrt(variance) * 3  # Scale for visibility\n    ax.arrow(origin[0], origin[1], direction[0], direction[1],\n             head_width=0.3, head_length=0.3, fc=f'C{i+1}', ec=f'C{i+1}', linewidth=3,\n             label=f'PC{i+1} ({variance/pca.explained_variance_.sum()*100:.1f}%)')\n\nax.set_xlabel('Original X')\nax.set_ylabel('Original Y')\nax.set_title('Principal Components: Directions of Maximum Variance')\nax.legend()\nax.axis('equal')\nsns.despine()\n\n\n\n\n\nPCA finds directions of maximum variance. PC1 captures the most variation, PC2 the next most (perpendicular to PC1).\n\n\n\n\nPC1 (orange arrow) points along the direction of greatest spread. PC2 (green arrow) is perpendicular and captures the remaining variation.\nThe percentage in parentheses shows how much variance each component explains. If PC1 explains 90% of variance, then projecting onto just PC1 preserves most of your data’s structure."
  },
  {
    "objectID": "m02-visualization/highd-data.html#applying-pca-to-iris",
    "href": "m02-visualization/highd-data.html#applying-pca-to-iris",
    "title": "High-Dimensional Data Visualization",
    "section": "3.1 Applying PCA to Iris",
    "text": "3.1 Applying PCA to Iris\nLet’s apply PCA to the 4-dimensional Iris dataset:\n\n\nCode\n# Prepare data\nX = iris.data\ny = iris.target\n\n# Standardize (important for PCA!)\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Apply PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# Create DataFrame for plotting\npca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\npca_df['species'] = iris.target_names[y]\n\n# Plot\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Left: PCA projection\nfor species, color in zip(['setosa', 'versicolor', 'virginica'], sns.color_palette('muted', 3)):\n    mask = pca_df['species'] == species\n    axes[0].scatter(pca_df.loc[mask, 'PC1'], pca_df.loc[mask, 'PC2'],\n                   label=species, alpha=0.7, s=50, color=color, edgecolors='white', linewidth=0.5)\naxes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)')\naxes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)')\naxes[0].set_title('PCA Projection of Iris Dataset')\naxes[0].legend()\nsns.despine(ax=axes[0])\n\n# Right: Variance explained\nvariances = pca.explained_variance_ratio_\naxes[1].bar([1, 2], variances, color=sns.color_palette('muted', 2), alpha=0.7)\naxes[1].set_xlabel('Principal Component')\naxes[1].set_ylabel('Variance Explained')\naxes[1].set_title('Variance Explained by Each Component')\naxes[1].set_xticks([1, 2])\naxes[1].set_xticklabels(['PC1', 'PC2'])\nfor i, v in enumerate(variances):\n    axes[1].text(i+1, v+0.01, f'{v*100:.1f}%', ha='center', va='bottom', fontsize=11)\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n\n\n\n\n\nPCA projection of Iris dataset to 2D preserves the separation between species\n\n\n\n\nPC1 and PC2 together explain over 95% of the variance in the 4D dataset. The 2D projection preserves the main structure: setosa is well-separated, while versicolor and virginica have some overlap.\n\n\nAlways standardize before PCA! If features have different units or scales, PCA will be dominated by high-variance features. Standardization (zero mean, unit variance) ensures all features contribute fairly.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html#when-to-use-pca",
    "href": "m02-visualization/highd-data.html#when-to-use-pca",
    "title": "High-Dimensional Data Visualization",
    "section": "3.2 When to use PCA",
    "text": "3.2 When to use PCA\nPCA is best when: - Linear relationships dominate: Variables are correlated, not arranged in complex non-linear patterns - You want interpretability: Principal components can be interpreted as linear combinations of original features - You have many dimensions: PCA scales well to thousands of dimensions - Global structure matters: PCA preserves large-scale relationships and overall variance\nPCA struggles when: - Data lies on non-linear manifolds: Curved surfaces, spirals, Swiss rolls - Local structure matters more: You care about which points are neighbors, not overall variance - Different groups have different scales: PCA can be dominated by high-variance features\n\n\nAlways standardize before PCA! If features have different units or scales, PCA will be dominated by high-variance features. Standardization (zero mean, unit variance) ensures all features contribute fairly."
  },
  {
    "objectID": "m02-visualization/highd-data.html#how-mds-works",
    "href": "m02-visualization/highd-data.html#how-mds-works",
    "title": "High-Dimensional Data Visualization",
    "section": "4.1 How MDS works",
    "text": "4.1 How MDS works\nThink of it like arranging cities on a map. You know the distance between every pair of cities, but not their coordinates. MDS finds positions that preserve those distances.\nMathematically, MDS minimizes stress: the difference between high-dimensional distances and low-dimensional distances. Classical MDS has a closed-form solution (like PCA), but more flexible variants use iterative optimization.\n\n\nCode\nfrom sklearn.manifold import MDS\n\n# Apply MDS\nmds = MDS(n_components=2, random_state=42)\nX_mds = mds.fit_transform(X_scaled)\n\n# Create DataFrame\nmds_df = pd.DataFrame(X_mds, columns=['MDS1', 'MDS2'])\nmds_df['species'] = iris.target_names[y]\n\n# Plot both\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# PCA\nfor species, color in zip(['setosa', 'versicolor', 'virginica'], sns.color_palette('muted', 3)):\n    mask = pca_df['species'] == species\n    axes[0].scatter(pca_df.loc[mask, 'PC1'], pca_df.loc[mask, 'PC2'],\n                   label=species, alpha=0.7, s=50, color=color, edgecolors='white', linewidth=0.5)\naxes[0].set_xlabel('PC1')\naxes[0].set_ylabel('PC2')\naxes[0].set_title('PCA: Maximizes Variance')\naxes[0].legend()\nsns.despine(ax=axes[0])\n\n# MDS\nfor species, color in zip(['setosa', 'versicolor', 'virginica'], sns.color_palette('muted', 3)):\n    mask = mds_df['species'] == species\n    axes[1].scatter(mds_df.loc[mask, 'MDS1'], mds_df.loc[mask, 'MDS2'],\n                   label=species, alpha=0.7, s=50, color=color, edgecolors='white', linewidth=0.5)\naxes[1].set_xlabel('MDS1')\naxes[1].set_ylabel('MDS2')\naxes[1].set_title('MDS: Preserves Distances')\naxes[1].legend()\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n\n\n/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/sklearn/manifold/_mds.py:677: FutureWarning: The default value of `n_init` will change from 4 to 1 in 1.9.\n  warnings.warn(\n\n\n\n\n\nMDS vs PCA on Iris dataset. MDS preserves distances better but looks similar to PCA for this dataset.\n\n\n\n\nFor the Iris dataset, PCA and MDS look very similar. This is because Iris data is fairly linear—the relationships between features don’t involve complex curves or non-linear structures.\nMDS shows its power on non-linear data structures:\n\n\nCode\nfrom sklearn.datasets import make_swiss_roll\n\n# Generate Swiss roll data\nn_samples = 1000\nX_swiss, color = make_swiss_roll(n_samples, noise=0.1, random_state=42)\n\n# Apply PCA and MDS\npca_swiss = PCA(n_components=2)\nX_swiss_pca = pca_swiss.fit_transform(X_swiss)\n\nmds_swiss = MDS(n_components=2, random_state=42)\nX_swiss_mds = mds_swiss.fit_transform(X_swiss)\n\n# Plot\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# PCA\nscatter = axes[0].scatter(X_swiss_pca[:, 0], X_swiss_pca[:, 1], c=color,\n                          cmap='viridis', alpha=0.6, s=20)\naxes[0].set_xlabel('PC1')\naxes[0].set_ylabel('PC2')\naxes[0].set_title('PCA: Loses Non-Linear Structure')\nsns.despine(ax=axes[0])\n\n# MDS\nscatter = axes[1].scatter(X_swiss_mds[:, 0], X_swiss_mds[:, 1], c=color,\n                          cmap='viridis', alpha=0.6, s=20)\naxes[1].set_xlabel('MDS1')\naxes[1].set_ylabel('MDS2')\naxes[1].set_title('MDS: Partially Preserves Structure')\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n\n\n/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/sklearn/manifold/_mds.py:677: FutureWarning: The default value of `n_init` will change from 4 to 1 in 1.9.\n  warnings.warn(\n\n\n\n\n\nMDS can reveal non-linear structure that PCA misses. Swiss roll dataset shown.\n\n\n\n\nThe Swiss roll is a 2D surface rolled up in 3D space. PCA “squashes” it, losing the smooth gradient. MDS does better—it partially unrolls the structure—but it’s not perfect because MDS tries to preserve all distances, including distances across the spiral that shouldn’t be preserved.\nThis is where modern methods excel."
  },
  {
    "objectID": "m02-visualization/highd-data.html#how-t-sne-works",
    "href": "m02-visualization/highd-data.html#how-t-sne-works",
    "title": "High-Dimensional Data Visualization",
    "section": "6.1 How t-SNE works",
    "text": "6.1 How t-SNE works\nt-SNE converts distances into similarity probabilities and preserves these local relationships:\n\nIn high dimensions: Define probability p_{ij} that point i picks point j as a neighbor (based on Gaussian distance)\nIn low dimensions: Define similar probability q_{ij} using a t-distribution with heavy tails\nOptimize: Move points in 2D to make q_{ij} match p_{ij} (minimize KL divergence)\n\nThe t-distribution’s heavy tails are clever: they let well-separated clusters spread out in 2D without overlapping, while keeping local neighborhoods tight.\n\n\nCode\nfrom sklearn.manifold import TSNE\n\n# Apply t-SNE\ntsne = TSNE(n_components=2, random_state=42, perplexity=30)\nX_scurve_tsne = tsne.fit_transform(X_scurve)\n\n# Plot all three methods\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# MDS - Global Euclidean distances\naxes[0].scatter(X_scurve_pca[:, 0], X_scurve_pca[:, 1], c=color,\n                cmap='viridis', alpha=0.6, s=20)\naxes[0].set_xlabel('PC1')\naxes[0].set_ylabel('PC2')\naxes[0].set_title('PCA: Global Variance')\nsns.despine(ax=axes[0])\n\n# Isomap - Geodesic distances\naxes[1].scatter(X_scurve_isomap[:, 0], X_scurve_isomap[:, 1], c=color,\n                cmap='viridis', alpha=0.6, s=20)\naxes[1].set_xlabel('Isomap1')\naxes[1].set_ylabel('Isomap2')\naxes[1].set_title('Isomap: Geodesic Distances')\nsns.despine(ax=axes[1])\n\n# t-SNE - Local neighborhoods\naxes[2].scatter(X_scurve_tsne[:, 0], X_scurve_tsne[:, 1], c=color,\n                cmap='viridis', alpha=0.6, s=20)\naxes[2].set_xlabel('t-SNE1')\naxes[2].set_ylabel('t-SNE2')\naxes[2].set_title('t-SNE: Local Structure')\nsns.despine(ax=axes[2])\n\nplt.tight_layout()\n\n\n\n\n\nComparing global, geodesic, and local approaches on the S-curve\n\n\n\n\nAll three methods successfully straighten the S-curve, but through different philosophies: MDS compromises between all distances, Isomap follows the manifold globally, and t-SNE focuses on preserving neighborhoods."
  },
  {
    "objectID": "m02-visualization/highd-data.html#the-perplexity-parameter",
    "href": "m02-visualization/highd-data.html#the-perplexity-parameter",
    "title": "High-Dimensional Data Visualization",
    "section": "6.2 The perplexity parameter",
    "text": "6.2 The perplexity parameter\nt-SNE’s most important parameter is perplexity, which roughly corresponds to the number of nearest neighbors each point considers. It balances local and global structure.\n\nLow perplexity (5-10): Focus on very local structure, tiny neighborhoods\nMedium perplexity (30-50): Balanced view (default is usually 30)\nHigh perplexity (100+): More global structure, larger neighborhoods\n\n\n\nCode\n# Apply t-SNE with different perplexities to Iris\nperplexities = [5, 30, 50]\nfig, axes = plt.subplots(1, 3, figsize=(14, 5))\n\nfor ax, perp in zip(axes, perplexities):\n    tsne = TSNE(n_components=2, random_state=42, perplexity=perp)\n    X_tsne = tsne.fit_transform(X_scaled)\n\n    for species, color in zip(['setosa', 'versicolor', 'virginica'], sns.color_palette('muted', 3)):\n        mask = y == np.where(iris.target_names == species)[0][0]\n        ax.scatter(X_tsne[mask, 0], X_tsne[mask, 1],\n                  label=species, alpha=0.7, s=50, color=color, edgecolors='white', linewidth=0.5)\n\n    ax.set_xlabel('t-SNE1')\n    ax.set_ylabel('t-SNE2')\n    ax.set_title(f'Perplexity = {perp}')\n    ax.legend()\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n\n\n\n\n\nEffect of perplexity on t-SNE visualization. Low perplexity creates fragmented clusters, high perplexity shows more global structure.\n\n\n\n\nWith perplexity = 5, clusters fragment into small sub-clusters (overfitting to local noise). With perplexity = 30-50, we see three clear groups matching the species. For most datasets, perplexity between 30-50 works well, but always experiment!"
  },
  {
    "objectID": "m02-visualization/highd-data.html#what-t-sne-preserves-and-what-it-doesnt",
    "href": "m02-visualization/highd-data.html#what-t-sne-preserves-and-what-it-doesnt",
    "title": "High-Dimensional Data Visualization",
    "section": "6.3 What t-SNE preserves (and what it doesn’t)",
    "text": "6.3 What t-SNE preserves (and what it doesn’t)\nt-SNE is powerful but has important limitations:\nWhat t-SNE preserves: - \u0013 Local structure: Points that are neighbors in high dimensions stay neighbors in 2D - \u0013 Clusters: Well-separated groups remain separated - \u0013 Relative relationships within neighborhoods: If A is closer to B than to C locally, this is preserved\nWhat t-SNE does NOT preserve: - \u0017 Distances: The actual distance between points is not meaningful - \u0017 Global structure: The relative position of distant clusters is arbitrary - \u0017 Cluster sizes: Large clusters may appear smaller, and vice versa - \u0017 Density: Tight clusters may be spread out; sparse regions may appear dense\n\n\n\n\n\n\nDon’t over-interpret t-SNE!\n\n\n\nYou cannot conclude from a t-SNE plot: - “Cluster A is twice as far from B as from C” (distances are not preserved) - “Cluster A is twice the size of B” (sizes are not preserved) - “The data has exactly 5 clusters” (apparent clusters may be visualization artifacts)\nYou can conclude: - “These points form a distinct group separate from others” - “These points are more similar to each other than to distant points” - “The data has local structure and is not uniformly random”"
  },
  {
    "objectID": "m02-visualization/highd-data.html#applying-t-sne-to-real-data",
    "href": "m02-visualization/highd-data.html#applying-t-sne-to-real-data",
    "title": "High-Dimensional Data Visualization",
    "section": "4.4 Applying t-SNE to real data",
    "text": "4.4 Applying t-SNE to real data\nLet’s apply t-SNE to a more realistic high-dimensional dataset—the MNIST digits dataset, which has 784 dimensions (28�28 pixel images):\n\n\nCode\nfrom sklearn.datasets import load_digits\n\n# Load digits dataset (8x8 images, 64 dimensions - a smaller version of MNIST)\ndigits = load_digits()\nX_digits = digits.data\ny_digits = digits.target\n\n# Take a subset for speed (t-SNE is slow on large datasets)\nnp.random.seed(42)\nindices = np.random.choice(len(X_digits), size=1000, replace=False)\nX_subset = X_digits[indices]\ny_subset = y_digits[indices]\n\n# Apply t-SNE\ntsne_digits = TSNE(n_components=2, random_state=42, perplexity=40)\nX_digits_tsne = tsne_digits.fit_transform(X_subset)\n\n# Plot\nfig, ax = plt.subplots(figsize=(12, 10))\nscatter = ax.scatter(X_digits_tsne[:, 0], X_digits_tsne[:, 1],\n                     c=y_subset, cmap='tab10', alpha=0.7, s=30)\nax.set_xlabel('t-SNE1')\nax.set_ylabel('t-SNE2')\nax.set_title('t-SNE Visualization of Handwritten Digits (64D � 2D)')\ncbar = plt.colorbar(scatter, ax=ax, ticks=range(10))\ncbar.set_label('Digit Class')\nsns.despine()\n\n\n/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/IPython/core/events.py:82: UserWarning: Glyph 65533 (\\N{REPLACEMENT CHARACTER}) missing from font(s) Arial.\n  func(*args, **kwargs)\n/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 65533 (\\N{REPLACEMENT CHARACTER}) missing from font(s) Arial.\n  fig.canvas.print_figure(bytes_io, **kw)\n\n\n\n\n\nt-SNE visualization of MNIST digits (784 dimensions x 2D). Each color represents a digit class.\n\n\n\n\nThe t-SNE projection beautifully separates most digit classes. Digits that look similar (like 3, 5, and 8, or 4 and 9) cluster near each other, while visually distinct digits (like 0 and 1) are well separated.\nThis demonstrates t-SNE’s power: from 64 dimensions with no explicit information about what makes digits similar, t-SNE discovers the perceptual structure of handwritten digits.\n\n\nt-SNE is stochastic: Different runs produce different layouts (though cluster structure remains consistent). Always check multiple runs with different random seeds, especially for important scientific conclusions."
  },
  {
    "objectID": "m02-visualization/highd-data.html#umap-vs-t-sne",
    "href": "m02-visualization/highd-data.html#umap-vs-t-sne",
    "title": "High-Dimensional Data Visualization",
    "section": "7.1 UMAP vs t-SNE",
    "text": "7.1 UMAP vs t-SNE\nAdvantages of UMAP: - Faster: Can be 10-100� faster than t-SNE on large datasets - Scales better: Works well on datasets with millions of points - Better global structure: Preserves more global relationships than t-SNE - Theoretically grounded: Based on Riemannian geometry and fuzzy topology\nTrade-offs: - Less battle-tested than t-SNE (newer method) - More hyperparameters to tune (though defaults work well) - Can produce similar-looking results to t-SNE, so choice often comes down to speed\n\n\nCode\nimport umap\n\n# Apply UMAP\numap_model = umap.UMAP(n_components=2, random_state=42, n_neighbors=30)\nX_digits_umap = umap_model.fit_transform(X_subset)\n\n# Plot comparison\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# t-SNE\nscatter = axes[0].scatter(X_digits_tsne[:, 0], X_digits_tsne[:, 1],\n                          c=y_subset, cmap='tab10', alpha=0.7, s=30)\naxes[0].set_xlabel('t-SNE1')\naxes[0].set_ylabel('t-SNE2')\naxes[0].set_title('t-SNE')\nsns.despine(ax=axes[0])\n\n# UMAP\nscatter = axes[1].scatter(X_digits_umap[:, 0], X_digits_umap[:, 1],\n                          c=y_subset, cmap='tab10', alpha=0.7, s=30)\naxes[1].set_xlabel('UMAP1')\naxes[1].set_ylabel('UMAP2')\naxes[1].set_title('UMAP')\ncbar = plt.colorbar(scatter, ax=axes[1], ticks=range(10))\ncbar.set_label('Digit Class')\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n\n\n/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n  warn(\n\n\n\n\n\nUMAP vs t-SNE on digits dataset. UMAP often preserves more global structure while being much faster.\n\n\n\n\nBoth methods reveal similar cluster structure, but UMAP tends to space clusters more evenly and preserve more of the global topology. Notice how UMAP places similar digits (3, 5, 8) in a connected region, suggesting they share underlying structure."
  },
  {
    "objectID": "m02-visualization/highd-data.html#when-to-use-umap",
    "href": "m02-visualization/highd-data.html#when-to-use-umap",
    "title": "High-Dimensional Data Visualization",
    "section": "4.6 When to use UMAP",
    "text": "4.6 When to use UMAP\nUse UMAP when: - You have very large datasets (&gt;10,000 points) where t-SNE is slow - You want to preserve more global structure - You’re doing exploratory analysis and want fast iteration - You need to project new data onto an existing embedding (UMAP supports this, t-SNE doesn’t easily)\nStick with t-SNE when: - You need the most established method with extensive literature - You’re working with moderate-sized datasets where speed isn’t critical - You’re replicating published work that used t-SNE"
  },
  {
    "objectID": "m02-visualization/highd-data.html#key-parameter-perplexity",
    "href": "m02-visualization/highd-data.html#key-parameter-perplexity",
    "title": "High-Dimensional Data Visualization",
    "section": "6.2 Key parameter: Perplexity",
    "text": "6.2 Key parameter: Perplexity\nPerplexity (typically 30-50) controls the effective neighborhood size. Too low fragments clusters; too high loses local detail."
  },
  {
    "objectID": "m02-visualization/highd-data.html#multidimensional-scaling-mds",
    "href": "m02-visualization/highd-data.html#multidimensional-scaling-mds",
    "title": "High-Dimensional Data Visualization",
    "section": "4.1 Multidimensional Scaling (MDS)",
    "text": "4.1 Multidimensional Scaling (MDS)\nMultidimensional Scaling (MDS) takes a different approach: instead of finding directions of maximum variance, it tries to preserve distances between points.\nYou give MDS a distance matrix—the distance between every pair of points in high-dimensional space—and it finds a low-dimensional configuration where those distances are preserved as well as possible.\nThink of it like arranging cities on a map. You know the distance between every pair of cities, but not their coordinates. MDS finds positions that preserve those distances.\nMathematically, MDS minimizes stress: the difference between high-dimensional distances and low-dimensional distances. Classical MDS has a closed-form solution (like PCA), but more flexible variants use iterative optimization.\n\n\nCode\nfrom sklearn.manifold import MDS\n\n# Suppress FutureWarning about n_init in MDS\nimport warnings\nmds = MDS(n_components=2, random_state=42, n_init=1)\nX_mds = mds.fit_transform(X_scaled)\n\n# Apply PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# Create DataFrame\nmds_df = pd.DataFrame(X_mds, columns=['MDS1', 'MDS2'])\nmds_df['species'] = iris.target_names[y]\n\n# Plot both\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# PCA\nfor species, color in zip(['setosa', 'versicolor', 'virginica'], sns.color_palette('muted', 3)):\n    mask = pca_df['species'] == species\n    axes[0].scatter(pca_df.loc[mask, 'PC1'], pca_df.loc[mask, 'PC2'],\n                   label=species, alpha=0.7, s=50, color=color, edgecolors='white', linewidth=0.5)\naxes[0].set_xlabel('PC1')\naxes[0].set_ylabel('PC2')\naxes[0].set_title('PCA: Maximizes Variance')\naxes[0].legend()\nsns.despine(ax=axes[0])\n\n# MDS\nfor species, color in zip(['setosa', 'versicolor', 'virginica'], sns.color_palette('muted', 3)):\n    mask = mds_df['species'] == species\n    axes[1].scatter(mds_df.loc[mask, 'MDS1'], mds_df.loc[mask, 'MDS2'],\n                   label=species, alpha=0.7, s=50, color=color, edgecolors='white', linewidth=0.5)\naxes[1].set_xlabel('MDS1')\naxes[1].set_ylabel('MDS2')\naxes[1].set_title('MDS: Preserves Distances')\naxes[1].legend()\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n\n\n\n\n\nMDS vs PCA on Iris dataset. MDS preserves distances better but looks similar to PCA for this dataset.\n\n\n\n\nFor the Iris dataset, PCA and MDS look very similar. This is because Iris data is fairly linear—the relationships between features don’t involve complex curves or non-linear structures.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html#isomap",
    "href": "m02-visualization/highd-data.html#isomap",
    "title": "High-Dimensional Data Visualization",
    "section": "4.2 Isomap",
    "text": "4.2 Isomap\nMDS preserves Euclidean distances—straight-line distances through space. But for curved manifolds, what matters is the geodesic distance: the distance along the surface.\nIsomap (Isometric Mapping) addresses this by approximating geodesic distances using the neighborhood graph:\n\nBuild a neighborhood graph: Connect each point to its k nearest neighbors\nCompute shortest paths: The geodesic distance between points is approximated by the shortest path through this graph\nApply classical MDS: Use MDS on these geodesic distances instead of Euclidean distances\n\nThink of it like this: MDS measures distance “as the crow flies,” while Isomap measures distance “as you walk along the surface.”\n\n\nCode\nfrom sklearn.manifold import Isomap\nfrom sklearn.datasets import make_s_curve\n\n# Generate S-curve data (a 2D manifold embedded in 3D)\nn_samples = 1000\nX_scurve, color = make_s_curve(n_samples, noise=0.1, random_state=42)\n\n# Apply Isomap\nisomap = Isomap(n_components=2, n_neighbors=10)\nX_scurve_isomap = isomap.fit_transform(X_scurve)\n\n# Apply PCA\npca = PCA(n_components=2)\nX_scurve_pca = pca.fit_transform(X_scurve)\n\n# Apply MDS\nmds = MDS(n_components=2, random_state=42, n_init=1)\nX_scurve_mds = mds.fit_transform(X_scurve)\n\n# Plot MDS vs Isomap vs PCA\nfig, axes = plt.subplots(1, 3, figsize=(14, 6))\n\n# PCA\naxes[0].scatter(X_scurve_pca[:, 0], X_scurve_pca[:, 1], c=color,\n                cmap='viridis', alpha=0.6, s=20)\naxes[0].set_xlabel('PC1')\naxes[0].set_ylabel('PC2')\naxes[0].set_title('PCA: Global Variance')\nsns.despine(ax=axes[0])\n\n# MDS\naxes[1].scatter(X_scurve_mds[:, 0], X_scurve_mds[:, 1], c=color,\n                cmap='viridis', alpha=0.6, s=20)\naxes[1].set_xlabel('MDS1')\naxes[1].set_ylabel('MDS2')\naxes[1].set_title('MDS: Global Distances')\nsns.despine(ax=axes[1])\n\n# Isomap\naxes[2].scatter(X_scurve_isomap[:, 0], X_scurve_isomap[:, 1], c=color,\n                cmap='viridis', alpha=0.6, s=20)\naxes[2].set_xlabel('Isomap1')\naxes[2].set_ylabel('Isomap2')\naxes[2].set_title('Isomap: Geodesic Distances')\nsns.despine(ax=axes[2])\n\nplt.tight_layout()\n\n\n\n\n\nIsomap uses geodesic distances (along the surface) instead of Euclidean distances (through space), better recovering the S-curve structure\n\n\n\n\nIsomap successfully “straightens” the S-curve because it respects the manifold structure. By computing distances along the neighborhood graph, it avoids the shortcuts across the bend that confused MDS.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html#t-sne",
    "href": "m02-visualization/highd-data.html#t-sne",
    "title": "High-Dimensional Data Visualization",
    "section": "4.3 t-SNE",
    "text": "4.3 t-SNE\nt-SNE (t-Distributed Stochastic Neighbor Embedding) takes a middle ground between MDS’s global approach and Isomap’s geodesic approach: it prioritizes local structure while allowing some flexibility in global positioning.\nThe key insight: for visualization, we often care most about which points are neighbors. Whether distant clusters are placed left vs right, or how far apart they are, matters less than preserving the local neighborhood relationships within and between clusters.\nt-SNE converts distances into similarity probabilities and preserves these local relationships:\n\nIn high dimensions: Define probability p_{ij} that point i picks point j as a neighbor (based on Gaussian distance)\nIn low dimensions: Define similar probability q_{ij} using a t-distribution with heavy tails\nOptimize: Move points in 2D to make q_{ij} match p_{ij} (minimize KL divergence)\n\nThe t-distribution’s heavy tails are clever: they let well-separated clusters spread out in 2D without overlapping, while keeping local neighborhoods tight.\n\n\nCode\nfrom sklearn.manifold import TSNE\n\n# Apply t-SNE\ntsne = TSNE(n_components=2, random_state=42, perplexity=30)\nX_scurve_tsne = tsne.fit_transform(X_scurve)\n\n# Plot all three methods\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# MDS - Global Euclidean distances\naxes[0].scatter(X_scurve_pca[:, 0], X_scurve_pca[:, 1], c=color,\n                cmap='viridis', alpha=0.6, s=20)\naxes[0].set_xlabel('PC1')\naxes[0].set_ylabel('PC2')\naxes[0].set_title('PCA: Global Variance')\nsns.despine(ax=axes[0])\n\n# Isomap - Geodesic distances\naxes[1].scatter(X_scurve_isomap[:, 0], X_scurve_isomap[:, 1], c=color,\n                cmap='viridis', alpha=0.6, s=20)\naxes[1].set_xlabel('Isomap1')\naxes[1].set_ylabel('Isomap2')\naxes[1].set_title('Isomap: Geodesic Distances')\nsns.despine(ax=axes[1])\n\n# t-SNE - Local neighborhoods\naxes[2].scatter(X_scurve_tsne[:, 0], X_scurve_tsne[:, 1], c=color,\n                cmap='viridis', alpha=0.6, s=20)\naxes[2].set_xlabel('t-SNE1')\naxes[2].set_ylabel('t-SNE2')\naxes[2].set_title('t-SNE: Local Structure')\nsns.despine(ax=axes[2])\n\nplt.tight_layout()\n\n\n\n\n\nComparing global, geodesic, and local approaches on the S-curve\n\n\n\n\nAll three methods successfully straighten the S-curve, but through different philosophies: MDS compromises between all distances, Isomap follows the manifold globally, and t-SNE focuses on preserving neighborhoods.\nPerplexity (typically 30-50) controls the effective neighborhood size. Too low fragments clusters; too high loses local detail.\nt-SNE is powerful but has important limitations.\n\n\n\n\n\n\nDon’t over-interpret t-SNE!\n\n\n\nYou cannot conclude from a t-SNE plot:\n\n“Cluster A is twice as far from B as from C” (distances are not preserved)\n“Cluster A is twice the size of B” (sizes are not preserved)\n“The data has exactly 5 clusters” (apparent clusters may be visualization artifacts)\n\nYou can conclude:\n\n“These points form a distinct group separate from others”\n“These points are more similar to each other than to distant points”\n“The data has local structure and is not uniformly random”\n\n\n\nLet’s apply t-SNE to a more realistic high-dimensional dataset—the MNIST digits dataset, which has 784 dimensions (28�28 pixel images):\n\n\nCode\nfrom sklearn.datasets import load_digits\n\n# Load digits dataset (8x8 images, 64 dimensions - a smaller version of MNIST)\ndigits = load_digits()\nX_digits = digits.data\ny_digits = digits.target\n\n# Take a subset for speed (t-SNE is slow on large datasets)\nnp.random.seed(42)\nindices = np.random.choice(len(X_digits), size=1000, replace=False)\nX_subset = X_digits[indices]\ny_subset = y_digits[indices]\n\n# Apply t-SNE\ntsne_digits = TSNE(n_components=2, random_state=42, perplexity=40)\nX_digits_tsne = tsne_digits.fit_transform(X_subset)\n\n# Plot\nfig, ax = plt.subplots(figsize=(12, 10))\nscatter = ax.scatter(X_digits_tsne[:, 0], X_digits_tsne[:, 1],\n                     c=y_subset, cmap='tab10', alpha=0.7, s=30)\nax.set_xlabel('t-SNE1')\nax.set_ylabel('t-SNE2')\nax.set_title('t-SNE Visualization of Handwritten Digits (64D � 2D)')\ncbar = plt.colorbar(scatter, ax=ax, ticks=range(10))\ncbar.set_label('Digit Class')\nsns.despine()\n\n\n/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/IPython/core/events.py:82: UserWarning: Glyph 65533 (\\N{REPLACEMENT CHARACTER}) missing from font(s) Arial.\n  func(*args, **kwargs)\n/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 65533 (\\N{REPLACEMENT CHARACTER}) missing from font(s) Arial.\n  fig.canvas.print_figure(bytes_io, **kw)\n\n\n\n\n\nt-SNE visualization of MNIST digits (784 dimensions x 2D). Each color represents a digit class.\n\n\n\n\nThe t-SNE projection beautifully separates most digit classes. Digits that look similar (like 3, 5, and 8, or 4 and 9) cluster near each other, while visually distinct digits (like 0 and 1) are well separated.\nThis demonstrates t-SNE’s power: from 64 dimensions with no explicit information about what makes digits similar, t-SNE discovers the perceptual structure of handwritten digits.\n\n\nt-SNE is stochastic: Different runs produce different layouts (though cluster structure remains consistent). Always check multiple runs with different random seeds, especially for important scientific conclusions.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html#umap",
    "href": "m02-visualization/highd-data.html#umap",
    "title": "High-Dimensional Data Visualization",
    "section": "4.4 UMAP",
    "text": "4.4 UMAP\nUniform Manifold Approximation and Projection (UMAP) is a newer method (2018) that has become popular as an alternative to t-SNE. Like t-SNE, UMAP preserves local structure, but it’s based on different mathematical foundations (manifold learning and topological data analysis).\n\n\nCode\nimport umap\n\n# Apply UMAP\numap_model = umap.UMAP(n_components=2, random_state=42, n_neighbors=30)\nX_digits_umap = umap_model.fit_transform(X_subset)\n\n# Plot comparison\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# t-SNE\nscatter = axes[0].scatter(X_digits_tsne[:, 0], X_digits_tsne[:, 1],\n                          c=y_subset, cmap='tab10', alpha=0.7, s=30)\naxes[0].set_xlabel('t-SNE1')\naxes[0].set_ylabel('t-SNE2')\naxes[0].set_title('t-SNE')\nsns.despine(ax=axes[0])\n\n# UMAP\nscatter = axes[1].scatter(X_digits_umap[:, 0], X_digits_umap[:, 1],\n                          c=y_subset, cmap='tab10', alpha=0.7, s=30)\naxes[1].set_xlabel('UMAP1')\naxes[1].set_ylabel('UMAP2')\naxes[1].set_title('UMAP')\ncbar = plt.colorbar(scatter, ax=axes[1], ticks=range(10))\ncbar.set_label('Digit Class')\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n\n\n/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n  warn(\n\n\n\n\n\nUMAP vs t-SNE on digits dataset. UMAP often preserves more global structure while being much faster.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/time-series.html",
    "href": "m02-visualization/time-series.html",
    "title": "Time Series Visualization",
    "section": "",
    "text": "In March 2020, news outlets worldwide showed charts of COVID-19 cases rising exponentially. Some charts showed linear y-axes with curves shooting upward dramatically. Others used logarithmic y-axes where the same data appeared as straight lines. Politicians cherry-picked time windows to show “flattening curves.” The same data told vastly different stories depending on how it was visualized.\nOr consider stock market charts: show the last month, and a 10% drop looks catastrophic. Zoom out to show the last decade, and the same drop becomes a minor blip barely visible on the chart.\nTime series data—observations ordered by time—is everywhere. But time is special. Unlike other variables, it flows in one direction, has natural rhythms (daily, seasonal, cyclical), and carries momentum. Your visualization choices can reveal genuine patterns or create misleading narratives.\nThe key principle to keep in mind:\nTime is special—show how your data changes over time honestly and clearly.\n\n1 Why Time Series Visualization Matters\nTime series visualizations are perhaps the most common type of chart in news media, scientific papers, and business dashboards. They answer fundamental questions: Is this trend going up or down? Are there cycles? When did something change?\nBut they’re also easy to manipulate. By selecting the time window, changing the y-axis scale, or choosing different aggregation levels, the same data can support contradictory conclusions.\nConsider these common pitfalls: - Truncated y-axes that exaggerate small changes - Cherry-picked time windows that hide long-term trends - Inappropriate scales (linear vs. log) that obscure or inflate patterns - Over-smoothing that removes real variation - Under-smoothing that shows only noise\nGood time series visualization is about making honest choices that reveal the actual patterns in your data.\n\n\n2 Basic Time Series: Line Plots\nThe most fundamental time series visualization is the line plot: time on the x-axis, values on the y-axis, points connected by lines.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set style\nsns.set(font_scale=1.2)\nsns.set_style(\"white\")\n\n\n# Generate synthetic time series with trend and seasonality\nnp.random.seed(42)\nn_points = 365\ndates = pd.date_range('2023-01-01', periods=n_points, freq='D')\ntrend = np.linspace(100, 150, n_points)\nseasonal = 10 * np.sin(2 * np.pi * np.arange(n_points) / 365 * 4)  # Quarterly seasonality\nnoise = np.random.normal(0, 3, n_points)\nvalues = trend + seasonal + noise\n\ndf = pd.DataFrame({'date': dates, 'value': values})\n\n# Create line plot\nfig, ax = plt.subplots(figsize=(12, 5))\nsns.lineplot(x='date', y='value', data=df, ax=ax)\nax.set_xlabel('Date')\nax.set_ylabel('Value')\nax.set_title('Daily Time Series: Line Plot Shows Trend and Seasonality')\nsns.despine()\n\n\n\n\n\nBasic line plot showing a time series with trend and seasonality\n\n\n\n\nThe line connecting points implies continuity\u0014that values exist between measurements. This is appropriate for continuous processes (temperature, stock prices, heart rate) but not for discrete events or counts measured at intervals.\nWhen should you not connect the dots? When your data represents discrete events or when measurements are too sparse to imply continuity.\n\n\nCode\n# Generate sparse discrete event data\nnp.random.seed(123)\nevent_dates = pd.to_datetime(['2023-01-15', '2023-03-10', '2023-05-22',\n                               '2023-07-08', '2023-09-30', '2023-11-15'])\nevent_values = np.random.randint(20, 80, len(event_dates))\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Line plot (implies continuity - misleading for discrete events)\naxes[0].plot(event_dates, event_values, marker='o', linewidth=2, markersize=8)\naxes[0].set_xlabel('Date')\naxes[0].set_ylabel('Event Count')\naxes[0].set_title('Line Plot: Implies Values Between Events (Misleading)')\n\n# Scatter plot (appropriate for discrete events)\naxes[1].scatter(event_dates, event_values, s=100, alpha=0.7)\naxes[1].set_xlabel('Date')\naxes[1].set_ylabel('Event Count')\naxes[1].set_title('Scatter Plot: Shows Only Observed Events (Honest)')\n\nfor ax in axes:\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n\n\n\n\n\nLine plot vs scatter plot: connecting points implies continuity\n\n\n\n\nFor discrete events, stick with scatter plots or bar charts. Don’t imply continuity where none exists.\n\n\n3 Comparing Multiple Time Series\nOften you need to compare several time series. The natural approach is to overlay them on the same plot.\n\n\nCode\n# Generate three related time series\nnp.random.seed(42)\ndates = pd.date_range('2023-01-01', periods=200, freq='D')\n\nseries_a = 100 + np.linspace(0, 30, 200) + np.random.normal(0, 5, 200)\nseries_b = 95 + np.linspace(0, 20, 200) + np.random.normal(0, 4, 200)\nseries_c = 110 + np.linspace(0, 10, 200) + np.random.normal(0, 6, 200)\n\ndf_multi = pd.DataFrame({\n    'date': dates,\n    'Product A': series_a,\n    'Product B': series_b,\n    'Product C': series_c\n})\n\n# Overlay plot\nfig, ax = plt.subplots(figsize=(12, 6))\nfor column in ['Product A', 'Product B', 'Product C']:\n    ax.plot(df_multi['date'], df_multi[column], linewidth=2, label=column, alpha=0.8)\n\nax.set_xlabel('Date')\nax.set_ylabel('Sales')\nax.set_title('Multiple Time Series: Overlaid Comparison')\nax.legend(loc='upper left')\nsns.despine()\n\n\n\n\n\nMultiple time series overlaid with different colors\n\n\n\n\nThis works well for 2-4 series. Beyond that, you risk creating a spaghetti plot—a tangled mess where individual series become impossible to follow.\nWhen you have many time series, use small multiples (faceting): separate plots arranged in a grid, each with the same axes for easy comparison.\n\n\nCode\n# Generate multiple time series\nnp.random.seed(42)\nn_series = 6\ndates = pd.date_range('2023-01-01', periods=150, freq='D')\n\ndata_list = []\nfor i in range(n_series):\n    values = 50 + np.random.randn(150).cumsum() + 10 * np.sin(2 * np.pi * np.arange(150) / 30)\n    data_list.append(pd.DataFrame({\n        'date': dates,\n        'value': values,\n        'series': f'Region {i+1}'\n    }))\n\ndf_many = pd.concat(data_list, ignore_index=True)\n\n# Small multiples using seaborn FacetGrid\ng = sns.FacetGrid(df_many, col='series', col_wrap=3, height=3, aspect=1.5, sharey=True)\ng.map_dataframe(sns.lineplot, x='date', y='value', linewidth=2, color=sns.color_palette()[0])\ng.set_axis_labels('Date', 'Value')\ng.set_titles('Region {col_name}')\nfor ax in g.axes.flat:\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n\n\n\n\n\nSmall multiples avoid spaghetti plots when comparing many time series\n\n\n\n\nSmall multiples let you see each series clearly while maintaining comparability through shared axes.\n\n\n4 The Power of Scale: Linear vs. Logarithmic\nPerhaps the most consequential choice in time series visualization is the y-axis scale. The same data looks completely different on linear vs. logarithmic scales.\nWhen should you use a log scale?\n\nWhen your data spans multiple orders of magnitude (e.g., 10 to 10,000)\nWhen you care about percentage changes rather than absolute changes\nWhen visualizing exponential growth or decay\n\n\n\nCode\n# Generate exponential growth data (e.g., epidemic spread)\nnp.random.seed(42)\ndays = np.arange(0, 100)\ncases = 10 * np.exp(0.05 * days) * (1 + np.random.normal(0, 0.1, len(days)))\n\ndf_exp = pd.DataFrame({'day': days, 'cases': cases})\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Linear scale\naxes[0].plot(df_exp['day'], df_exp['cases'], linewidth=2, color=sns.color_palette()[0])\naxes[0].set_xlabel('Days')\naxes[0].set_ylabel('Cases')\naxes[0].set_title('Linear Scale: Exponential Growth Looks Explosive')\n\n# Log scale\naxes[1].plot(df_exp['day'], df_exp['cases'], linewidth=2, color=sns.color_palette()[1])\naxes[1].set_xlabel('Days')\naxes[1].set_ylabel('Cases (log scale)')\naxes[1].set_yscale('log')\naxes[1].set_title('Log Scale: Exponential Growth Appears Linear')\n\nfor ax in axes:\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n\n\n\n\n\nThe same exponential growth looks different on linear vs. log scales\n\n\n\n\nOn a linear scale, exponential growth appears as a dramatic upward curve—most growth happens at the end. On a log scale, exponential growth becomes a straight line, making it easy to see if the growth rate is constant, accelerating, or decelerating.\n\n\n\n\n\n\nLog Scales Can Hide Magnitude\n\n\n\nWhile log scales are essential for percentage changes and exponential processes, they can downplay the absolute magnitude of changes. A jump from 10,000 to 100,000 cases looks the same as a jump from 100 to 1,000—both are one order of magnitude. But in human terms, 90,000 additional cases is far more significant than 900.\nAlways consider your audience and what you want to emphasize: relative changes (use log) or absolute numbers (use linear).\n\n\n\n\n5 Smoothing and Trends\nReal time series data is often noisy. Smoothing helps reveal underlying trends by averaging out short-term fluctuations.\nThe most common approach is a moving average: replace each point with the average of nearby points.\n\n\nCode\n# Generate noisy time series\nnp.random.seed(42)\ndates = pd.date_range('2023-01-01', periods=200, freq='D')\ntrend = 50 + 0.2 * np.arange(200)\nseasonal = 8 * np.sin(2 * np.pi * np.arange(200) / 30)\nnoise = np.random.normal(0, 5, 200)\nvalues = trend + seasonal + noise\n\ndf_noisy = pd.DataFrame({'date': dates, 'value': values})\n\n# Calculate moving averages\ndf_noisy['MA_7'] = df_noisy['value'].rolling(window=7, center=True).mean()\ndf_noisy['MA_30'] = df_noisy['value'].rolling(window=30, center=True).mean()\n\n# Plot\nfig, ax = plt.subplots(figsize=(12, 6))\nax.plot(df_noisy['date'], df_noisy['value'], linewidth=0.8, alpha=0.3, label='Raw Data', color='gray')\nax.plot(df_noisy['date'], df_noisy['MA_7'], linewidth=2, label='7-Day Moving Average', color=sns.color_palette()[0])\nax.plot(df_noisy['date'], df_noisy['MA_30'], linewidth=2, label='30-Day Moving Average', color=sns.color_palette()[1])\n\nax.set_xlabel('Date')\nax.set_ylabel('Value')\nax.set_title('Moving Averages Reveal Trends by Smoothing Noise')\nax.legend()\nsns.despine()\n\n\n\n\n\nMoving averages smooth noise to reveal underlying trends\n\n\n\n\nThe smoothing window creates a trade-off:\n\nShort windows (e.g., 7 days) preserve more detail but still show fluctuations\nLong windows (e.g., 30 days) reveal long-term trends but may over-smooth and miss real changes\n\n\n\n\n\n\n\nChoosing the Right Window\n\n\n\nThe appropriate smoothing window depends on your data’s frequency and the patterns you care about: - Daily stock prices: 5-20 day moving average - Monthly sales: 3-6 month moving average - Annual measurements: 3-5 year moving average\nMatch your window to the timescale of meaningful variation in your domain.\n\n\n\n\n6 Showing Uncertainty Over Time\nWhen forecasting or estimating, you don’t just have point predictions\u0014you have uncertainty. Showing this uncertainty is crucial for honest communication.\nUse ribbon plots (also called envelope plots) to show confidence intervals or prediction intervals around your estimates.\n\n\nCode\n# Generate data with trend\nnp.random.seed(42)\nn = 150\nx = np.arange(n)\ntrue_trend = 50 + 0.3 * x\nobserved = true_trend + np.random.normal(0, 5, n)\n\n# Simple linear forecast\nfrom scipy import stats\nslope, intercept, r_value, p_value, std_err = stats.linregress(x[:100], observed[:100])\n\n# Forecast period\nx_future = np.arange(100, 150)\ny_pred = slope * x_future + intercept\n\n# Estimate prediction interval (simplified)\nresiduals = observed[:100] - (slope * x[:100] + intercept)\nstd_residual = np.std(residuals)\nmargin = 1.96 * std_residual  # 95% prediction interval\n\n# Plot\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Historical data\nax.plot(x[:100], observed[:100], linewidth=2, label='Historical Data', color=sns.color_palette()[0])\n\n# Forecast with uncertainty\nax.plot(x_future, y_pred, linewidth=2, label='Forecast', color=sns.color_palette()[1], linestyle='--')\nax.fill_between(x_future, y_pred - margin, y_pred + margin,\n                alpha=0.3, color=sns.color_palette()[1], label='95% Prediction Interval')\n\n# Actual future (for comparison)\nax.plot(x_future, observed[100:], linewidth=1.5, alpha=0.5, label='Actual (for comparison)',\n        color='gray', linestyle=':')\n\nax.axvline(x=100, color='black', linestyle=':', alpha=0.5, label='Forecast Start')\nax.set_xlabel('Time')\nax.set_ylabel('Value')\nax.set_title('Time Series Forecast with Uncertainty Bands')\nax.legend()\nax.grid(True, alpha=0.3)\nsns.despine()\n\n\n\n\n\nRibbon plots show uncertainty bands around predictions\n\n\n\n\nThe ribbon makes it clear that predictions further into the future are more uncertain. Without showing this uncertainty, forecasts can appear deceptively precise.\n\n\n7 Temporal Aggregation: Choosing Your Time Scale\nHow you aggregate time can dramatically change what patterns emerge. The same data aggregated hourly, daily, or monthly reveals different stories.\nHeat maps are excellent for visualizing patterns across two time dimensions\u0014say, hour of day vs. day of week.\n\n\nCode\n# Generate synthetic hourly data with daily and weekly patterns\nnp.random.seed(42)\nhours = pd.date_range('2023-01-01', periods=24*7*4, freq='H')  # 4 weeks\n\n# Patterns: higher activity during business hours and weekdays\nhour_of_day = hours.hour\nday_of_week = hours.dayofweek\n\n# Activity pattern\nbase_activity = 20\nhour_effect = 30 * np.exp(-((hour_of_day - 14)**2) / 20)  # Peak at 2 PM\nweekday_effect = np.where(day_of_week &lt; 5, 20, -10)  # Weekdays higher\nnoise = np.random.normal(0, 5, len(hours))\n\nactivity = base_activity + hour_effect + weekday_effect + noise\n\ndf_hourly = pd.DataFrame({\n    'datetime': hours,\n    'activity': activity,\n    'hour': hour_of_day,\n    'day_name': hours.day_name(),\n    'week': (hours.day // 7) + 1\n})\n\n# Take first week for heatmap\ndf_week = df_hourly[df_hourly['week'] == 1].copy()\n\n# Pivot for heatmap\nheatmap_data = df_week.pivot_table(values='activity',\n                                     index='hour',\n                                     columns='day_name',\n                                     aggfunc='mean')\n\n# Reorder columns to start with Monday\nday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\nheatmap_data = heatmap_data[[day for day in day_order if day in heatmap_data.columns]]\n\n# Plot heatmap\nfig, ax = plt.subplots(figsize=(12, 8))\nsns.heatmap(heatmap_data, cmap='YlOrRd', annot=False, fmt='.0f',\n            cbar_kws={'label': 'Activity Level'}, ax=ax)\nax.set_xlabel('Day of Week')\nax.set_ylabel('Hour of Day')\nax.set_title('Temporal Heatmap: Activity by Hour and Day of Week')\nplt.tight_layout()\n\n\n/var/folders/j7/9dgqq5g53vnbsbmvh2yqtckr0000gr/T/ipykernel_13747/3637488205.py:3: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n  hours = pd.date_range('2023-01-01', periods=24*7*4, freq='H')  # 4 weeks\n\n\n\n\n\nHeat map reveals daily and weekly patterns in temporal data\n\n\n\n\nHeat maps immediately reveal patterns like “high activity on weekday afternoons” that would be invisible in a simple line plot.\n\n\n\nCalendar heatmaps are widely used for visualizing GitHub contributions, showing commit activity over time in a compact, pattern-revealing format.\n\n\n8 Visualizing Cycles and Seasonality\nMany time series have seasonal patterns: daily cycles, weekly patterns, annual seasons. Cycle plots decompose time series by season to reveal these patterns.\n\n\nCode\n# Generate monthly data with strong annual seasonality\nnp.random.seed(42)\nmonths = pd.date_range('2020-01-01', periods=48, freq='M')\nmonth_num = np.tile(np.arange(1, 13), 4)  # 4 years of monthly data\n\n# Seasonal pattern (higher in summer, lower in winter)\nseasonal_effect = 20 * np.sin(2 * np.pi * (month_num - 3) / 12)\ntrend_effect = 0.5 * np.arange(48)\nnoise = np.random.normal(0, 3, 48)\n\nvalues = 50 + seasonal_effect + trend_effect + noise\n\ndf_seasonal = pd.DataFrame({\n    'date': months,\n    'value': values,\n    'month': month_num,\n    'year': months.year,\n    'month_name': months.month_name()\n})\n\n# Create cycle plot\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Traditional time series\naxes[0].plot(df_seasonal['date'], df_seasonal['value'], marker='o', linewidth=2)\naxes[0].set_xlabel('Date')\naxes[0].set_ylabel('Value')\naxes[0].set_title('Traditional Time Series: Seasonality Repeats')\naxes[0].grid(True, alpha=0.3)\n\n# Cycle plot\nmonth_names_short = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n                     'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\nfor year in df_seasonal['year'].unique():\n    year_data = df_seasonal[df_seasonal['year'] == year]\n    axes[1].plot(year_data['month'], year_data['value'], marker='o',\n                linewidth=2, label=str(year), alpha=0.7)\n\naxes[1].set_xlabel('Month')\naxes[1].set_ylabel('Value')\naxes[1].set_xticks(range(1, 13))\naxes[1].set_xticklabels(month_names_short)\naxes[1].set_title('Cycle Plot: Each Year Overlaid to Show Seasonal Pattern')\naxes[1].legend(title='Year')\naxes[1].grid(True, alpha=0.3)\n\nfor ax in axes:\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n\n\n/var/folders/j7/9dgqq5g53vnbsbmvh2yqtckr0000gr/T/ipykernel_13747/3601020590.py:3: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n  months = pd.date_range('2020-01-01', periods=48, freq='M')\n\n\n\n\n\nCycle plot reveals seasonal patterns by separating each cycle\n\n\n\n\nBy overlaying each year’s cycle, the cycle plot makes it obvious that values peak in summer (months 6-8) and dip in winter (months 12-2), while also showing year-over-year trends.\n\n\n9 Advanced: Lag Plots for Autocorrelation\nTime series data often exhibits autocorrelation: values depend on previous values. A lag plot helps visualize this by plotting each value against the previous value (lag-1) or earlier values.\n\n\nCode\n# Generate time series with autocorrelation\nnp.random.seed(42)\nn = 200\n\n# AR(1) process: strong autocorrelation\nar_series = np.zeros(n)\nar_series[0] = np.random.normal(0, 1)\nfor i in range(1, n):\n    ar_series[i] = 0.7 * ar_series[i-1] + np.random.normal(0, 1)\n\n# Random walk: perfect autocorrelation at lag 1\nrandom_walk = np.random.normal(0, 1, n).cumsum()\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Lag-1 plot for AR(1) series\naxes[0].scatter(ar_series[:-1], ar_series[1:], alpha=0.6, s=30)\naxes[0].set_xlabel('Value at time t')\naxes[0].set_ylabel('Value at time t+1')\naxes[0].set_title('Lag-1 Plot: Strong Autocorrelation (AR Process)')\naxes[0].plot([-3, 3], [-3, 3], 'r--', alpha=0.5, linewidth=1)\naxes[0].grid(True, alpha=0.3)\n\n# Lag-1 plot for random walk\naxes[1].scatter(random_walk[:-1], random_walk[1:], alpha=0.6, s=30, color=sns.color_palette()[1])\naxes[1].set_xlabel('Value at time t')\naxes[1].set_ylabel('Value at time t+1')\naxes[1].set_title('Lag-1 Plot: Perfect Autocorrelation (Random Walk)')\naxes[1].plot([random_walk.min(), random_walk.max()],\n            [random_walk.min(), random_walk.max()], 'r--', alpha=0.5, linewidth=1)\naxes[1].grid(True, alpha=0.3)\n\nfor ax in axes:\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n\n\n\n\n\nLag plots reveal autocorrelation structure in time series\n\n\n\n\nA strong linear pattern in a lag plot indicates high autocorrelation\u0014knowing the current value helps predict the next value. Random, scattered points suggest no autocorrelation (e.g., white noise).\n\n\n10 The Bigger Picture\nTime series visualization is about making choices that honestly represent temporal patterns while avoiding common pitfalls:\nKey principles to remember:\n\nChoose the right scale: Linear for absolute changes, log for relative/percentage changes\nShow uncertainty: Predictions without confidence intervals are misleading\nAvoid spaghetti plots: Use small multiples when comparing many series\nMatch aggregation to your question: Daily, weekly, monthly aggregation reveals different patterns\nBe transparent about time windows: The time range you show matters enormously\nSmooth appropriately: Balance between preserving detail and revealing trends\n\nCommon pitfalls to avoid:\n\nTruncating the y-axis to exaggerate small changes\nCherry-picking time windows to support a narrative\nUsing line plots for discrete events (implies false continuity)\nOver-smoothing to hide inconvenient variation\nMixing scales when comparing series (e.g., comparing growth rates on linear scale)\n\nTime series visualization is powerful because time is a dimension we all understand intuitively. But that familiarity also makes us vulnerable to manipulation. By following principled visualization practices, you ensure your temporal data tells its true story\u0014not the story you wish it told.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Time-Series"
    ]
  },
  {
    "objectID": "m02-visualization/networks.html",
    "href": "m02-visualization/networks.html",
    "title": "Network Visualization",
    "section": "",
    "text": "You’ve probably seen them before: network visualizations that look like tangled balls of yarn, where nodes cluster in impenetrable clumps and edges cross everywhere. These “hairball diagrams” are so common in publications that they’ve become a running joke in network science. The problem isn’t that the networks are inherently messy—it’s that the layout fails to reveal the structure that’s actually there.\nThe goal of network visualization is not to make pretty pictures. It’s to make structure visible. A good layout should help you answer questions: Are there communities? Is there hierarchy? Are certain nodes central? A bad layout obscures these answers, no matter how much you adjust the colors or node sizes.\nIn this lecture, we’ll explore how to choose and use network layouts that reveal rather than obscure. We’ll start with the simplest case—trees—then move to general networks with force-directed layouts, and finally to hierarchical structures that combine both approaches with edge bundling.\nThe core principle: Layout is not decoration. It’s a hypothesis about what structure matters in your network.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Networks"
    ]
  },
  {
    "objectID": "m02-visualization/networks.html#what-is-a-network",
    "href": "m02-visualization/networks.html#what-is-a-network",
    "title": "Network Visualization",
    "section": "1 What is a Network?",
    "text": "1 What is a Network?\nA network (or graph) is a collection of nodes (also called vertices) connected by edges (also called links). Networks can represent almost anything: social relationships, neural connections, citations between papers, roads between cities, or interactions between proteins.\n\n\n\n\n\n\nMathematical Definition\n\n\n\nA network G = (V, E) consists of:\n\nA set of nodes V = \\{v_1, v_2, ..., v_n\\}\nA set of edges E \\subseteq V \\times V representing connections\n\nNetworks can be directed (edges have direction, like citations) or undirected (edges are symmetric, like friendships).\n\n\nWhy do we visualize networks? Because topology is hard to grasp from data alone. Looking at an adjacency matrix or edge list gives you facts but not insight. Visualization transforms abstract connectivity into spatial patterns your visual system can process.\nBut here’s the challenge: unlike data points that have inherent positions (latitude/longitude, time series), networks have no natural layout. The positions you see in a network visualization are entirely constructed by the layout algorithm. Different algorithms can make the same network look completely different.\nThis means choosing a layout is choosing what to emphasize. Let’s see how.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Networks"
    ]
  },
  {
    "objectID": "m02-visualization/networks.html#visualizing-trees",
    "href": "m02-visualization/networks.html#visualizing-trees",
    "title": "Network Visualization",
    "section": "2 Visualizing Trees",
    "text": "2 Visualizing Trees\nThe simplest networks are trees: connected networks with no cycles. Every node except the root has exactly one parent. Trees appear everywhere: biological taxonomies, organizational charts, file systems, phylogenetic trees, decision trees.\nFor trees, the structure is clear: there’s a natural hierarchy. The radial tree layout makes this hierarchy visible by placing the root at the center and arranging descendants in concentric circles.\n\n\nCode\nimport graph_tool.all as gt\nimport networkx as nx\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Generate a random tree using NetworkX\nnx_tree = nx.random_labeled_tree(n=50, seed=42)\n\n# Convert to graph-tool\ng = gt.Graph(directed=False)\ng.add_vertex(nx_tree.number_of_nodes())\nfor u, v in nx_tree.edges():\n    g.add_edge(g.vertex(u), g.vertex(v))\n\n# Create radial tree layout\npos = gt.radial_tree_layout(g, g.vertex(0))\n\n# Draw the network (let graph-tool handle rendering directly)\ngt.graph_draw(g, pos=pos,\n              vertex_fill_color=[0.275, 0.510, 0.706, 1],  # steelblue\n              vertex_size=15,\n              edge_color=[0.5, 0.5, 0.5, 1],  # gray\n              edge_pen_width=1.5,\n              output_size=(500, 500),\n              inline=True)\n\n\n\n\n\nRadial tree layout of a random tree with 50 nodes. The root is at the center, and descendants are arranged in concentric circles by depth.\n\n\n\n\nThe radial layout immediately tells you several things:\n\nDepth: How far each node is from the root (distance from center)\nBranching structure: Where the tree splits into subtrees\nBalance: Whether the tree is symmetric or lopsided",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Networks"
    ]
  },
  {
    "objectID": "m02-visualization/networks.html#visualizing-general-networks-force-directed-layouts",
    "href": "m02-visualization/networks.html#visualizing-general-networks-force-directed-layouts",
    "title": "Network Visualization",
    "section": "3 Visualizing General Networks: Force-Directed Layouts",
    "text": "3 Visualizing General Networks: Force-Directed Layouts\nMost networks aren’t trees. They have cycles, cross-links, and complex connectivity patterns. For these networks, we need algorithms that can handle arbitrary topology. The most common approach is force-directed layout.\nThe idea is elegant: treat nodes as charged particles that repel each other, and edges as springs that pull connected nodes together. Let the system simulate physics until it reaches equilibrium. Nodes that are closely connected end up near each other, while unconnected parts spread apart.\n\nThe Fruchterman-Reingold Algorithm\nThe Fruchterman-Reingold algorithm is one of the most widely used force-directed methods. It balances two forces:\n\nRepulsive force: All pairs of nodes repel each other (like charged particles)\nAttractive force: Connected nodes are pulled together (like springs)\n\nLet’s see it in action on a well-known network: the Zachary Karate Club, a social network of 34 members of a karate club, documenting friendships before the club split into two groups.\n\n\nCode\nimport graph_tool.all as gt\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load the karate club network\ng = gt.collection.data[\"karate\"]\n\n# Get community labels (the two groups that split)\n# We'll use blockmodel inference with 2 communities\nstate = gt.minimize_blockmodel_dl(g, state_args=dict(B=2))\ncommunity = state.get_blocks()\n\n# Create Fruchterman-Reingold layout\npos = gt.fruchterman_reingold_layout(g, n_iter=1000)\n\n# Map communities to colors (RGB tuples)\ncolor_map = {0: [0.906, 0.298, 0.235, 1],  # Red\n             1: [0.204, 0.596, 0.859, 1]}  # Blue\n\n# Create vertex property map for colors\nvertex_color = g.new_vertex_property(\"vector&lt;double&gt;\")\nfor v in g.vertices():\n    vertex_color[v] = color_map[community[v]]\n\n# Draw the network\ngt.graph_draw(g, pos=pos,\n              vertex_fill_color=vertex_color,\n              vertex_size=20,\n              edge_color=[0.584, 0.647, 0.651, 1],\n              edge_pen_width=2,\n              output_size=(1000, 800),\n              inline=True)\n\n\n\n\n\nZachary Karate Club network with Fruchterman-Reingold layout. Node colors indicate the two groups that formed after the club split. The layout naturally separates the two communities.\n\n\n\n\n\n\nThe Karate Club dataset comes from a study by Wayne Zachary (1977) documenting the split of a university karate club into two factions. It’s one of the most famous small networks in network science.\nThe layout does something remarkable: even though we didn’t tell the algorithm about the two groups, it naturally separates them in space. This happens because nodes within each group are densely connected (many edges pulling them together), while connections between groups are sparse (less pull across the boundary).\n\n\nTuning Force-Directed Layouts\nForce-directed algorithms have parameters that control the final layout. The most important is the number of iterations—how long the simulation runs before stopping.\n\n\nCode\nimport graph_tool.all as gt\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ng = gt.collection.data[\"karate\"]\nstate = gt.minimize_blockmodel_dl(g, state_args=dict(B=2))\ncommunity = state.get_blocks()\ncolor_map = {0: [0.906, 0.298, 0.235, 1],  # Red\n             1: [0.204, 0.596, 0.859, 1]}  # Blue\n\n# Create vertex property map for colors\nvertex_color = g.new_vertex_property(\"vector&lt;double&gt;\")\nfor v in g.vertices():\n    vertex_color[v] = color_map[community[v]]\n\n# Different iteration counts\niterations = [50, 500, 5000]\nfig, axes = plt.subplots(1, 3, figsize=(14, 4))\n\nfor idx, n_iter in enumerate(iterations):\n    pos = gt.fruchterman_reingold_layout(g, n_iter=n_iter)\n    gt.graph_draw(g, pos=pos,\n                  vertex_fill_color=vertex_color,\n                  vertex_size=15,\n                  edge_color=[0.584, 0.647, 0.651, 1],\n                  edge_pen_width=1.5,\n                  output_size=(400, 400),\n                  mplfig=axes[idx])\n    axes[idx].set_title(f\"{n_iter} iterations\", fontsize=12)\n    axes[idx].axis('equal')\n    axes[idx].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nEffect of iteration count on force-directed layout quality. Too few iterations (left) produce cramped layouts; optimal iterations (middle) balance clarity and structure; excessive iterations (right) offer minimal improvement.\n\n\n\n\nWith too few iterations (50), the nodes haven’t had time to spread out properly—they’re still clustered near their initial positions. With sufficient iterations (500), the structure becomes clear. Beyond that (5000), you get diminishing returns: the layout looks similar but computation time increases.\n\n\n\n\n\n\nRule of Thumb for Iterations\n\n\n\nFor small networks (&lt; 100 nodes): 500-1000 iterations For medium networks (100-1000 nodes): 1000-2000 iterations For large networks (&gt; 1000 nodes): Consider faster algorithms like SFDP\n\n\n\n\nSFDP: Scalable Force-Directed Placement\nThe Fruchterman-Reingold algorithm slows down dramatically as networks grow because it computes forces between all pairs of nodes. For large networks, SFDP (Scalable Force-Directed Placement) is more efficient. It uses a multilevel approach, similar to the Barnes-Hut algorithm in physics simulations.\n\n\nCode\nimport graph_tool.all as gt\nimport networkx as nx\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\n\n# Generate a larger scale-free network using NetworkX\nnp.random.seed(123)\nnx_g = nx.barabasi_albert_graph(n=500, m=2, seed=123)\n\n# Convert to graph-tool\ng = gt.Graph(directed=False)\ng.add_vertex(nx_g.number_of_nodes())\nfor u, v in nx_g.edges():\n    g.add_edge(g.vertex(u), g.vertex(v))\n\n# Measure time for Fruchterman-Reingold\nstart = time.time()\npos_fr = gt.fruchterman_reingold_layout(g, n_iter=500)\ntime_fr = time.time() - start\n\n# Measure time for SFDP\nstart = time.time()\npos_sfdp = gt.sfdp_layout(g)\ntime_sfdp = time.time() - start\n\n# Plot both\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\ngt.graph_draw(g, pos=pos_fr,\n              vertex_fill_color=[0.275, 0.510, 0.706, 1],  # steelblue\n              vertex_size=5,\n              edge_color=[0.584, 0.647, 0.651, 1],\n              edge_pen_width=0.5,\n              output_size=(600, 600),\n              mplfig=axes[0])\naxes[0].set_title(f\"Fruchterman-Reingold ({time_fr:.2f}s)\", fontsize=12)\naxes[0].axis('equal')\naxes[0].axis('off')\n\ngt.graph_draw(g, pos=pos_sfdp,\n              vertex_fill_color=[1.0, 0.498, 0.314, 1],  # coral\n              vertex_size=5,\n              edge_color=[0.584, 0.647, 0.651, 1],\n              edge_pen_width=0.5,\n              output_size=(600, 600),\n              mplfig=axes[1])\naxes[1].set_title(f\"SFDP ({time_sfdp:.2f}s)\", fontsize=12)\naxes[1].axis('equal')\naxes[1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nComparison of Fruchterman-Reingold (left) vs. SFDP (right) on a larger network (500 nodes, scale-free topology). SFDP is much faster while producing comparable layouts.\n\n\n\n\nSFDP is often 10-100x faster for large networks while producing layouts of comparable quality. For networks with more than a few hundred nodes, SFDP is the better choice.\n\n\n\n\n\n\nForce-Directed Layouts Are Non-Deterministic\n\n\n\nForce-directed algorithms start from random initial positions and may converge to different layouts each time you run them. Always set a random seed if you need reproducible figures. The layout reveals a valid structure, not the structure.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Networks"
    ]
  },
  {
    "objectID": "m02-visualization/networks.html#visualizing-hierarchical-structure",
    "href": "m02-visualization/networks.html#visualizing-hierarchical-structure",
    "title": "Network Visualization",
    "section": "4 Visualizing Hierarchical Structure",
    "text": "4 Visualizing Hierarchical Structure\nMany real-world networks have hierarchical community structure: groups within groups, like departments within divisions within a company, or species within genera within families. Standard force-directed layouts can reveal communities, but they struggle to show the hierarchical relationships between them.\nWe can explicitly encode the hierarchical structure into the layout in the following way.\nFirst, in many cases, we don’t know the hierarchical structure of a given network. So let’s assume that we have to infer it from the data. The nested stochastic block model finds a hierarchical partition by grouping nodes into communities, then grouping communities into super-communities, and so on. This is exactly what the draw_hierarchy() function visualizes.\nLet’s demonstrate with the C. elegans neural network—the complete wiring diagram of a nematode’s nervous system:\n\n\nCode\nimport graph_tool.all as gt\nimport matplotlib.pyplot as plt\n\n# Load C. elegans neural network\ng = gt.collection.data[\"celegansneural\"]\n\n# Infer hierarchical community structure\nstate = gt.minimize_nested_blockmodel_dl(g)\n\n# Draw hierarchy with edge bundling\ngt.draw_hierarchy(state,\n                  beta=0.8,  # Edge bundling strength\n                  output_size=(500, 500),\n                  inline=True)\n\n\n\n\n\nHierarchical structure of the C. elegans neural network revealed through nested block model visualization with edge bundling. Inner rings represent higher-level communities, outer ring shows individual neurons. Edge bundling (beta=0.8) reduces visual clutter by routing edges through the hierarchy.\n\n\n\n\n(&lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x107178e90, at 0x14294bb50&gt;,\n &lt;GraphView object, directed, with 322 vertices and 321 edges, edges filtered by (&lt;EdgePropertyMap object with value type 'bool', for Graph 0x337c14f50, at 0x337c78090&gt;, False), vertices filtered by (&lt;VertexPropertyMap object with value type 'bool', for Graph 0x337c14f50, at 0x337cc4090&gt;, False), at 0x337c14f50&gt;,\n &lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x337c14f50, at 0x337c0ec10&gt;)\n\n\n\n\nSee this paper by Holten (2006) that introduced hierarchical edge bundling.\nThis visualization packs an enormous amount of information into a single image:\n\nConcentric rings: Each ring represents a level in the hierarchy, from coarse (inner) to fine (outer)\nColored wedges: Each wedge is a community at that hierarchical level\nEdge bundling: Edges are routed through the hierarchy tree, creating bundles that reveal large-scale connectivity patterns\n\nWithout edge bundling, this network would be an incomprehensible hairball. The bundling reveals that most connections occur within communities or between closely related communities—exactly what you’d expect in a modular biological network.\n\nTuning Edge Bundling Strength\nThe key parameter is beta, which controls how strongly edges are bundled. Beta ranges from 0 (no bundling, straight lines) to 1 (maximum bundling, edges follow the hierarchy tree exactly).\n\n\nCode\nimport graph_tool.all as gt\nimport matplotlib.pyplot as plt\n\n# Use a smaller network for clearer comparison\ng = gt.collection.data[\"karate\"]\nstate = gt.minimize_nested_blockmodel_dl(g)\n\n# Beta = 0.3 (low bundling)\nprint(\"Beta = 0.3:\")\ngt.draw_hierarchy(state,\n                  beta=0.3,\n                  output_size=(500, 500),\n                  inline=True)\n\n# Beta = 0.9 (high bundling)\nprint(\"\\nBeta = 0.9:\")\ngt.draw_hierarchy(state,\n                  beta=0.9,\n                  output_size=(500, 500),\n                  inline=True)\n\n\nBeta = 0.3:\n\n\n\n\n\nEffect of edge bundling strength (beta) on hierarchical network visualization. Low beta (left) shows individual edges but creates clutter; high beta (right) emphasizes hierarchical structure but may obscure detailed connectivity.\n\n\n\n\n\nBeta = 0.9:\n\n\n\n\n\n\n\n\n\n(&lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x337c160d0, at 0x142ceac10&gt;,\n &lt;GraphView object, directed, with 35 vertices and 34 edges, edges filtered by (&lt;EdgePropertyMap object with value type 'bool', for Graph 0x32d000050, at 0x337c91f90&gt;, False), vertices filtered by (&lt;VertexPropertyMap object with value type 'bool', for Graph 0x32d000050, at 0x337c0e950&gt;, False), at 0x32d000050&gt;,\n &lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x32d000050, at 0x3141f6c50&gt;)\n\n\nLow beta (0.3) preserves individual edge information but creates visual clutter. High beta (0.9) emphasizes the hierarchical flow of connections—you can see which communities talk to which—but individual edges become hard to trace.\nChoose beta based on your goal: - To show detailed connectivity: beta = 0.3-0.5 - To show hierarchical structure: beta = 0.7-0.9 - General-purpose visualization: beta = 0.6-0.8\n\n\n\n\n\n\nWhen to Use Hierarchical Layouts\n\n\n\nCircular hierarchy layouts are powerful but only appropriate when your network actually has hierarchical structure. If you force a random network into this layout, you’ll create the illusion of hierarchy where none exists. Always validate the hierarchical partition (e.g., using the description length of the nested block model) before using this visualization.\n\n\n\n\nAlternative: SFDP Layout for Hierarchies\nYou can also use the SFDP layout algorithm with draw_hierarchy(), which positions the hierarchy tree using force-directed placement. This can be useful for very large hierarchies:\n\n\nCode\nimport graph_tool.all as gt\nimport matplotlib.pyplot as plt\n\ng = gt.collection.data[\"karate\"]\nstate = gt.minimize_nested_blockmodel_dl(g)\n\ngt.draw_hierarchy(state,\n                  layout=\"sfdp\",\n                  beta=0.8,\n                  output_size=(500, 500),\n                  inline=True)\n\n\n\n\n\nHierarchical visualization with SFDP layout for the hierarchy tree. The SFDP algorithm positions hierarchy levels using force-directed placement, which can reveal different structural patterns.\n\n\n\n\n(&lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x337c160d0, at 0x16a660e50&gt;,\n &lt;GraphView object, directed, with 35 vertices and 34 edges, edges filtered by (&lt;EdgePropertyMap object with value type 'bool', for Graph 0x337cc6d10, at 0x314e201d0&gt;, False), vertices filtered by (&lt;VertexPropertyMap object with value type 'bool', for Graph 0x337cc6d10, at 0x337c3acd0&gt;, False), at 0x337cc6d10&gt;,\n &lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x337cc6d10, at 0x337cc5a10&gt;)\n\n\nThe SFDP layout for hierarchies is particularly useful for very large networks where the radial layout becomes too crowded, or when you want to emphasize local connectivity patterns over strict hierarchical levels.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Networks"
    ]
  },
  {
    "objectID": "m02-visualization/networks.html#the-bigger-picture-layout-as-hypothesis",
    "href": "m02-visualization/networks.html#the-bigger-picture-layout-as-hypothesis",
    "title": "Network Visualization",
    "section": "5 The Bigger Picture: Layout as Hypothesis",
    "text": "5 The Bigger Picture: Layout as Hypothesis\nEvery layout algorithm embodies a hypothesis about what makes nodes “similar” or “close”:\n\nRadial tree layout hypothesizes that hierarchy is the key structure\nForce-directed layout hypothesizes that shared neighbors create similarity\nHierarchical layout with edge bundling hypothesizes that multi-scale community structure organizes the network\n\nNone of these is objectively “correct”—they’re different lenses for viewing the same data. The critical skill is matching the layout to the question you’re asking.\n\nLimitations and Caveats\nNetwork visualization has fundamental limitations that you need to understand:\n1. Layout is not analysis. A clear visual pattern doesn’t prove that pattern exists in the data—it might be an artifact of the layout algorithm. Always validate visual insights with quantitative analysis (modularity scores, statistical tests, null models).\n2. 2D layouts lose information. Projecting a high-dimensional graph structure into 2D necessarily distorts distances and relationships. Nodes that appear close might not be similar; nodes that appear far might be connected.\n3. Large networks don’t scale. Once you have thousands of nodes, even the best layouts become unreadable. At that point, consider: - Aggregation: Show communities as super-nodes - Filtering: Display only the most important nodes/edges - Interactive tools: Allow zooming and panning - Alternative representations: Adjacency matrices, arc diagrams\n4. Edge crossings are unavoidable (except for planar graphs). Don’t spend hours tweaking layouts to eliminate all crossings—focus on revealing meaningful structure instead.\nNetwork visualization is a rich field with decades of research. For deeper exploration:\n\nGraph-tool documentation: Comprehensive guide to all layout algorithms\nThe visual display of quantitative information by Edward Tufte: Principles of effective visualization\nNetwork Science by Albert-László Barabási: Chapter on network visualization and its interpretation\n\nRemember: the best layout is the one that helps you answer your question. Start with the structure you’re looking for, then choose the layout that makes that structure visible.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Networks"
    ]
  },
  {
    "objectID": "m02-visualization/networks.html#force-directed-layouts",
    "href": "m02-visualization/networks.html#force-directed-layouts",
    "title": "Network Visualization",
    "section": "3 Force-Directed Layouts",
    "text": "3 Force-Directed Layouts\nMost networks aren’t trees. They have cycles, cross-links, and complex connectivity patterns. For these networks, we need algorithms that can handle arbitrary topology. The most common approach is force-directed layout.\nThe idea is simple: treat nodes as charged particles that repel each other, and edges as springs that pull connected nodes together. Let the system simulate physics until it reaches equilibrium. Nodes that are closely connected end up near each other, while unconnected parts spread apart.\nThe Fruchterman-Reingold algorithm is one of the most widely used force-directed methods. It balances two forces:\n\nRepulsive force: All pairs of nodes repel each other (like charged particles)\nAttractive force: Connected nodes are pulled together (like springs)\n\nLet’s see it in action on a well-known network: the Zachary Karate Club, a social network of 34 members of a karate club, documenting friendships before the club split into two groups.\n\n\nCode\nimport graph_tool.all as gt\nimport networkx as nx\nimport numpy as np\n\nnp.random.seed(42)\n\n# Generate a random tree using NetworkX\nnx_tree = nx.random_labeled_tree(n=50, seed=42)\n\n# Convert to graph-tool\ng = gt.Graph(directed=False)\ng.add_vertex(nx_tree.number_of_nodes())\nfor u, v in nx_tree.edges():\n    g.add_edge(g.vertex(u), g.vertex(v))\n\n# Force-directed layout (Fruchterman-Reingold)\npos_force = gt.fruchterman_reingold_layout(g, n_iter=1000)\n\n# Draw force-directed layout inline\ngt.graph_draw(\n    g,\n    pos=pos_force,\n    vertex_fill_color=[1.0, 0.498, 0.314, 1],  # coral\n    vertex_size=15,\n    edge_color=[0.5, 0.5, 0.5, 1],  # gray\n    edge_pen_width=1.5,\n    output_size=(500, 500),\n    inline=True\n)\n\n\n\n\n\nComparison of radial layout (left) vs. force-directed layout (right) for the same tree. The radial layout emphasizes hierarchy, while force-directed layout treats all edges equally.\n\n\n\n\n\n\nCode\nimport graph_tool.all as gt\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load the karate club network\ng = gt.collection.data[\"karate\"]\n\n\n# Create Fruchterman-Reingold layout\npos = gt.fruchterman_reingold_layout(g, n_iter=1000)\n\n# Map communities to colors (RGB tuples)\ncolor_map = {0: [0.906, 0.298, 0.235, 1],  # Red\n             1: [0.204, 0.596, 0.859, 1]}  # Blue\n\n# Create vertex property map for colors\nvertex_color = g.new_vertex_property(\"vector&lt;double&gt;\")\nfor v in g.vertices():\n    vertex_color[v] = color_map[0]\n\n# Draw the network\ngt.graph_draw(g, pos=pos,\n              vertex_fill_color=vertex_color,\n              vertex_size=20,\n              edge_color=[0.584, 0.647, 0.651, 1],\n              edge_pen_width=2,\n              output_size=(500, 500),\n              inline=True)\n\n\n\n\n\nZachary Karate Club network with Fruchterman-Reingold layout. Node colors indicate the two groups that formed after the club split. The layout naturally separates the two communities.\n\n\n\n\n\n\nThe Karate Club dataset comes from a study by Wayne Zachary (1977) documenting the split of a university karate club into two factions. It’s one of the most famous small networks in network science.\nThe layout does something remarkable: even though we didn’t tell the algorithm about the two groups, it naturally separates them in space. This happens because nodes within each group are densely connected (many edges pulling them together), while connections between groups are sparse (less pull across the boundary).\n\nSFDP: Scalable Force-Directed Placement\nThe Fruchterman-Reingold algorithm slows down dramatically as networks grow because it computes forces between all pairs of nodes. For large networks, SFDP (Scalable Force-Directed Placement) is more efficient. It uses a multilevel approach, similar to the Barnes-Hut algorithm in physics simulations.\n\n\nCode\nimport graph_tool.all as gt\nimport networkx as nx\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\n\n# Generate a larger scale-free network using NetworkX\nnp.random.seed(123)\nnx_g = nx.barabasi_albert_graph(n=500, m=2, seed=123)\n\n# Convert to graph-tool\ng = gt.Graph(directed=False)\ng.add_vertex(nx_g.number_of_nodes())\nfor u, v in nx_g.edges():\n    g.add_edge(g.vertex(u), g.vertex(v))\n\n# Fruchterman-Reingold layout\nprint(\"Fruchterman-Reingold layout:\")\nstart = time.time()\npos_fr = gt.fruchterman_reingold_layout(g, n_iter=500)\ntime_fr = time.time() - start\nprint(f\"Time: {time_fr:.2f}s\")\n\ngt.graph_draw(g, pos=pos_fr,\n              vertex_fill_color=[0.275, 0.510, 0.706, 1],  # steelblue\n              vertex_size=5,\n              edge_color=[0.584, 0.647, 0.651, 1],\n              edge_pen_width=0.5,\n              output_size=(600, 600),\n              inline=True)\n\n\nFruchterman-Reingold layout:\nTime: 8.78s\n\n\n\n\n\nComparison of Fruchterman-Reingold (left) vs. SFDP (right) on a larger network (500 nodes, scale-free topology). SFDP is much faster while producing comparable layouts.\n\n\n\n\n\n# SFDP layout\nprint(\"\\nSFDP layout:\")\nstart = time.time()\npos_sfdp = gt.sfdp_layout(g)\ntime_sfdp = time.time() - start\nprint(f\"Time: {time_sfdp:.2f}s\")\n\ngt.graph_draw(g, pos=pos_sfdp,\n              vertex_fill_color=[1.0, 0.498, 0.314, 1],  # coral\n              vertex_size=5,\n              edge_color=[0.584, 0.647, 0.651, 1],\n              edge_pen_width=0.5,\n              output_size=(600, 600),\n              inline=True)\n\n\nSFDP layout:\nTime: 0.61s\n\n\n\n\n\n\n\n\n\nSFDP is often 10-100x faster for large networks while producing layouts of comparable quality. For networks with more than a few hundred nodes, SFDP is the better choice.\n\n\n\n\n\n\nForce-Directed Layouts Are Non-Deterministic\n\n\n\nForce-directed algorithms start from random initial positions and may converge to different layouts each time you run them. Always set a random seed if you need reproducible figures. The layout reveals a valid structure, not the structure.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Networks"
    ]
  },
  {
    "objectID": "m02-visualization/overview.html",
    "href": "m02-visualization/overview.html",
    "title": "Overview",
    "section": "",
    "text": "In 1854, London faced a terrifying cholera outbreak. At the time, most believed the disease spread through “bad air”—the prevailing miasma theory. But Dr. John Snow wasn’t convinced. Refusing to accept the common explanation, he did something different: he mapped every cholera case on a city street map.\n\n\n\n\n\nWhat he saw changed history. The cases clustered tightly around a single water pump on Broad Street. This clear visual pattern, invisible in rows of numbers, pointed to contaminated water—not air—as the source. By taking a novel, visual approach, Snow solved a mystery that statistics and prevailing beliefs alone could not.\n\n\n\n\n\nStories like this show why data visualization matters. Real-world data is messy, incomplete, and complex—far beyond what summary numbers can capture. Visualization helps us see structure, spot errors, detect clusters, and reveal hidden trends.\nThat’s why data visualization is essential in applied soft computing: it lets us see our data’s structure, detect problems, and uncover patterns before modeling or analysis ever begins. Before building any model, clear visualization is the first and most crucial step to true understanding.\nThis module equips you to use the right visualization tools for different data types and questions:\n\nPerception Principles: How and why we interpret visual information as we do. Learn perceptual basics to create honest, effective visualizations.\n1D Data Visualization: Visualizing distributions beyond averages. Learn box plots, swarm plots, and more to reveal real data structure.\n2D Data Visualization: Scatter plots and extensions to uncover relationships, clusters, and non-linear patterns between two variables.\nHigh-Dimensional Data Visualization: Techniques like PCA, t-SNE, and UMAP for datasets with many variables.\nNetwork Visualization: Visualizing networks to highlight hierarchy, communities, and important nodes—moving beyond unreadable “hairballs.”\nTime Series Visualization: Tools for revealing trends and cycles in temporal data, and for avoiding misleading visual choices.\n\nBy the end, you’ll be able to choose and apply visualization methods that clarify your data and strengthen your analyses and communication.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Overview"
    ]
  },
  {
    "objectID": "m03-text/llm-intro.html",
    "href": "m03-text/llm-intro.html",
    "title": "Large Language Models in Practice",
    "section": "",
    "text": "Spoiler\n\n\n\nLarge language models don’t understand language—they compress statistical regularities from billions of text samples into probability distributions that generate fluent outputs correlated with truth but not guaranteed to be true.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Large Language Models in Practice"
    ]
  },
  {
    "objectID": "m03-text/llm-intro.html#what-are-large-language-models",
    "href": "m03-text/llm-intro.html#what-are-large-language-models",
    "title": "Large Language Models in Practice",
    "section": "1 What Are Large Language Models?",
    "text": "1 What Are Large Language Models?\n\n\n\n\n\nAt their core, LLMs are neural networks trained to predict the next word. Given a sequence of text, they estimate the probability distribution over all possible next words. By repeatedly sampling from these distributions, they can generate coherent, contextually appropriate text.\n\n\n\n\n\n\nFrom Language Modeling to General Intelligence?\n\n\n\nLLMs were originally designed just to predict text. But researchers discovered that a model good at predicting text must implicitly understand grammar, facts, logic, and context. This “understanding” enables all the downstream applications we see today.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Large Language Models in Practice"
    ]
  },
  {
    "objectID": "m03-text/llm-intro.html#local-vs.-cloud-models",
    "href": "m03-text/llm-intro.html#local-vs.-cloud-models",
    "title": "Large Language Models in Practice",
    "section": "2 Local vs. Cloud Models",
    "text": "2 Local vs. Cloud Models\nBefore we dive in, let’s understand your options:\n\n\n\n\n\n\n\n\nAspect\nCloud APIs (OpenAI, Anthropic)\nLocal Models (Ollama)\n\n\n\n\nCost\nPay per token (can get expensive)\nFree after download\n\n\nPrivacy\nData sent to external servers\nAll data stays local\n\n\nPerformance\nState-of-the-art (GPT-4, Claude)\nSmaller models, good enough for many tasks\n\n\nSpeed\nFast (distributed infrastructure)\nDepends on your hardware\n\n\nInternet\nRequired\nOptional (after download)\n\n\n\nFor this course, we use Ollama with Gemma 3N—a lightweight, open-source model from Google that runs on most laptops. It’s not as powerful as GPT-4, but it’s free, private, and capable enough for learning and many research tasks.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Large Language Models in Practice"
    ]
  },
  {
    "objectID": "m03-text/llm-intro.html#setting-up-ollama",
    "href": "m03-text/llm-intro.html#setting-up-ollama",
    "title": "Large Language Models in Practice",
    "section": "4 Setting Up Ollama",
    "text": "4 Setting Up Ollama\nFor this course, we use Ollama, a tool for running LLMs locally, with Gemma 3N, a 4-billion parameter open-source model. Free, private, capable enough for research tasks. Visit ollama.ai, download the installer, and verify installation:\nollama --version\nollama pull gemma3n:latest\nollama run gemma3n:latest \"What is a complex system?\"\nIf you receive a coherent response, install the Python client and send your first prompt:\npip install ollama\n\nimport ollama\n\nparams_llm = {\"model\": \"gemma3n:latest\", \"options\": {\"temperature\": 0.3}}\n\nresponse = ollama.generate(\n    prompt=\"Explain emergence in two sentences.\",\n    **params_llm\n)\n\nprint(response.response)\n\nEmergence is when complex patterns and behaviors arise from simpler interactions within a system, where these patterns aren't explicitly programmed into the individual components.  Essentially, the whole becomes greater than the sum of its parts, exhibiting novel properties that couldn't be predicted just by looking at the individual elements.\n\n\n\n\n\n\nRunning this twice produces different outputs because LLMs sample from probability distributions. The temperature parameter controls this randomness—lower values (0.1) make outputs more deterministic; higher values (1.0) increase diversity. You’re controlling how far into the tail of the probability distribution the model samples.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Large Language Models in Practice"
    ]
  },
  {
    "objectID": "m03-text/llm-intro.html#using-ollama-from-python",
    "href": "m03-text/llm-intro.html#using-ollama-from-python",
    "title": "Large Language Models in Practice",
    "section": "4 Using Ollama from Python",
    "text": "4 Using Ollama from Python\nWhile the command-line interface is useful for quick tests, we’ll use Ollama from Python for research workflows.\n\nInstallation\npip install ollama\n\n\nYour First LLM Interaction\n\nimport ollama\n\n# Set up model parameters\nparams_llm = {\"model\": \"gemma3n:latest\", \"options\": {\"temperature\": 0.3}}\n\n# Send a simple prompt to the model\nresponse = ollama.generate(\n    prompt=\"Explain the concept of emergence in complex systems in two sentences.\",\n    **params_llm\n)\n\nprint(response.response)\n\nEmergence is the arising of novel and coherent properties in a complex system that cannot be predicted from the properties of its individual components alone. These unexpected behaviors arise from the interactions and relationships between the parts, leading to a higher-level organization and functionality.\n\n\n\n\n\n\n\n\n\n\n\n\nNon-Deterministic Outputs\n\n\n\nLLMs sample from probability distributions, so running the same prompt twice may produce different outputs. This is a feature (creativity) but also a challenge (reproducibility). We’ll address this in the next section on prompt engineering.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Large Language Models in Practice"
    ]
  },
  {
    "objectID": "m03-text/llm-intro.html#research-use-case-1-summarizing-abstracts",
    "href": "m03-text/llm-intro.html#research-use-case-1-summarizing-abstracts",
    "title": "Large Language Models in Practice",
    "section": "5 Research Use Case 1: Summarizing Abstracts",
    "text": "5 Research Use Case 1: Summarizing Abstracts\nYou collected 50 papers on network science. Which deserve detailed reading? LLMs summarize in seconds.\n\nabstract = \"\"\"\nCommunity detection in networks is a fundamental problem in complex systems.\nWhile many algorithms exist, most assume static networks. We propose a dynamic\ncommunity detection algorithm that tracks evolving communities over time using\na temporal smoothness constraint. We evaluate our method on synthetic and real\ntemporal networks, showing it outperforms static methods applied to temporal\nsnapshots. Our approach reveals how communities merge, split, and persist in\nsocial networks, biological systems, and transportation networks.\n\"\"\"\n\nprompt = f\"Summarize this abstract in one sentence:\\n\\n{abstract}\"\nresponse = ollama.generate(prompt=prompt, **params_llm)\nprint(response.response)\n\nThis paper introduces a novel dynamic community detection algorithm that effectively tracks evolving communities in networks over time, outperforming static methods and revealing community dynamics in various real-world systems.\n\n\n\n\n\n\nThe model captures the pattern: propose method, evaluate, outperform baselines. It doesn’t understand the paper; it has seen enough academic abstracts to recognize the structure. For multiple abstracts, loop:\n\nfor i, abstract in enumerate([\"Abstract 1...\", \"Abstract 2...\"], 1):\n    response = ollama.generate(prompt=f\"Summarize:\\n\\n{abstract}\", **params_llm)\n    print(f\"{i}. {response.response}\")\n\n1. Please provide me with \"Abstract 1\"! I need the text of the abstract to be able to summarize it for you. \n\nJust paste the abstract here, and I'll give you a concise summary. 😊 \n\nI'm ready when you are!\n2. Please provide me with the content of \"Abstract 2\"! I need the text of the abstract to be able to summarize it for you. \n\nJust paste the abstract here, and I'll do my best to give you a concise and accurate summary. 😊 \n\n\n\n\nLocal models are slow (2–5 seconds per abstract). For thousands of papers, switch to cloud APIs.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Large Language Models in Practice"
    ]
  },
  {
    "objectID": "m03-text/llm-intro.html#research-use-case-2-extracting-structured-information",
    "href": "m03-text/llm-intro.html#research-use-case-2-extracting-structured-information",
    "title": "Large Language Models in Practice",
    "section": "6 Research Use Case 2: Extracting Structured Information",
    "text": "6 Research Use Case 2: Extracting Structured Information\nExtract domain, methods, findings from abstracts automatically.\n\nabstract = \"\"\"\nWe analyze scientific collaboration networks using 5 million papers from\n2000-2020. Using graph neural networks and community detection, we identify\ndisciplinary boundaries and interdisciplinary bridges. Interdisciplinarity\nincreased 25%, with physics and CS showing strongest cross-connections.\n\"\"\"\n\nprompt = f\"\"\"Extract: Domain, Methods, Key Finding\\n\\n{abstract}\\n\\nFormat:\\nDomain:...\\nMethods:...\\nKey Finding:...\"\"\"\nresponse = ollama.generate(prompt=prompt, **params_llm)\nprint(response.response)\n\nHere's the extraction in the requested format:\n\nDomain: Scientific Collaboration Networks\nMethods: Graph Neural Networks, Community Detection, Analysis of 5 million papers (2000-2020)\nKey Finding: Interdisciplinarity increased by 25% between 2000-2020, with the strongest cross-connections observed between Physics and Computer Science.\n\n\n\n\n\n\nScale to hundreds of papers for meta-analysis. Always verify—LLMs misinterpret and fabricate.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Large Language Models in Practice"
    ]
  },
  {
    "objectID": "m03-text/llm-intro.html#research-use-case-3-hypothesis-generation",
    "href": "m03-text/llm-intro.html#research-use-case-3-hypothesis-generation",
    "title": "Large Language Models in Practice",
    "section": "7 Research Use Case 3: Hypothesis Generation",
    "text": "7 Research Use Case 3: Hypothesis Generation\nLLMs pattern-match against research questions they’ve seen. Useful for brainstorming, not for breakthrough ideas.\n\ncontext = \"\"\"I study concept spread in citation networks. Highly cited papers\ncombine existing concepts novelty. What should I study next?\"\"\"\n\nprompt = f\"\"\"Suggest three follow-up research questions:\\n\\n{context}\"\"\"\nresponse = ollama.generate(prompt=prompt, **params_llm)\nprint(response.response)\n\nOkay, here are three follow-up research questions, building upon your current work on concept spread in citation networks, focusing on highly cited papers and the interplay of existing concepts and novelty.  I've tried to offer a mix of methodological and theoretical directions:\n\n**1.  How does the *type* of novelty (e.g., incremental, radical, convergent) in highly cited papers influence the rate and direction of concept spread?**\n\n*   **Rationale:** You've identified that highly cited papers combine existing concepts with novelty.  However, the *nature* of that novelty likely matters.  Is it a small tweak to an existing idea (incremental), a completely new paradigm (radical), or a synthesis of multiple existing ideas (convergent)?  Different types of novelty might spread differently through the citation network.\n*   **Methodology:**\n    *   **Concept Extraction:**  Develop a method (potentially combining NLP and manual annotation) to categorize the type of novelty present in highly cited papers.  This could involve identifying keywords, phrases, and arguments that signal incremental, radical, or convergent novelty.\n    *   **Network Analysis:**  Analyze the citation network to see if papers with different types of novelty have different citation patterns (e.g., different citation rates, different types of citing papers, different network positions).\n    *   **Temporal Analysis:**  Track the spread of concepts over time, looking for differences in the spread patterns of concepts associated with different types of novelty.\n*   **Potential Insights:**  This could reveal whether incremental novelty spreads quickly and widely, while radical novelty takes longer to gain traction but can have a more transformative impact.\n\n**2.  To what extent does the *citation context* (how a highly cited paper is cited) mediate the spread of concepts?**\n\n*   **Rationale:**  It's not just *that* a paper is highly cited, but *how* it's cited that matters.  Is it cited for its core argument, a specific method, a critique, or a combination?  The citation context could influence whether the concept is adopted, adapted, or rejected.\n*   **Methodology:**\n    *   **Citation Context Analysis:**  Develop a method to analyze the text surrounding citations of highly cited papers.  This could involve using NLP techniques to identify the specific arguments or concepts being referenced.\n    *   **Network Analysis:**  Create a citation network where nodes are citations and edges represent the relationship between the cited paper and the citing paper.\n    *   **Correlation Analysis:**  Correlate the citation context with the subsequent spread of concepts in the network.  Do citations that highlight specific aspects of the paper lead to faster or more widespread concept adoption?\n*   **Potential Insights:**  This could reveal the importance of framing and interpretation in the spread of ideas.  It might also highlight the role of debates and critiques in shaping the evolution of concepts.\n\n**3.  Can we identify \"concept hubs\" within the citation network – papers that act as particularly influential nodes in the spread of concepts from highly cited papers?**\n\n*   **Rationale:**  Some papers are more effective at disseminating concepts than others.  These \"concept hubs\" might be characterized by their broad citation patterns, their ability to synthesize information from multiple sources, or their engagement with diverse communities.\n*   **Methodology:**\n    *   **Centrality Measures:**  Apply various network centrality measures (e.g., betweenness centrality, eigenvector centrality, degree centrality) to the citation network.\n    *   **Hub Identification:**  Identify papers with high centrality scores as potential concept hubs.\n    *   **Case Studies:**  Conduct in-depth case studies of these concept hubs to understand how they contribute to the spread of concepts from highly cited papers.  Analyze their content, citation patterns, and engagement with other researchers.\n*   **Potential Insights:**  This could help us understand the mechanisms by which concepts spread through the citation network and identify strategies for promoting the dissemination of important ideas.  It could also reveal the role of specific communities or disciplines in shaping the spread of concepts.\n\n\n\nThese questions are designed to be relatively focused and address different aspects of your initial research.  They also offer opportunities to combine quantitative network analysis with qualitative case studies.  I hope this helps! Let me know if you'd like me to elaborate on any of these or suggest alternative directions.\n\n\n\n\n\n\nTreat as thought partner, not oracle. The model helps structure thinking but doesn’t possess domain expertise.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Large Language Models in Practice"
    ]
  },
  {
    "objectID": "m03-text/llm-intro.html#understanding-model-limitations",
    "href": "m03-text/llm-intro.html#understanding-model-limitations",
    "title": "Large Language Models in Practice",
    "section": "8 Understanding Model Limitations",
    "text": "8 Understanding Model Limitations\nLLMs are powerful, but they have important limitations:\n\nHallucination\nLLMs can confidently state false information.\n\nprompt = \"What did the 2023 paper by Smith et al. on quantum community detection conclude? Respond in 1-2 sentences.\"\n\nresponse = ollama.generate(prompt=prompt, **params_llm)\n\nprint(response.response)\n\nThe 2023 paper by Smith et al. on quantum community detection demonstrated a novel quantum algorithm for identifying communities in graphs, achieving a speedup over classical methods for certain graph structures.  Their results highlight the potential of quantum computing to offer advantages in community detection, particularly for large and complex networks.\n\n\n\n\n\n\nThe model might generate a plausible-sounding answer about a paper that doesn’t exist. Always verify factual claims, especially citations and specific results.\n\n\nLimited Context Window\nModels can only “see” a certain amount of text at once (typically 2000-8000 tokens for smaller models). If you paste 100 abstracts, the model might miss information from the beginning.\n\n\nKnowledge Cutoff\nModels are trained on data up to a certain date. Gemma 3N’s knowledge ends in early 2024. It doesn’t know about papers published after that.\n\n# Ask about current events that may be after the knowledge cutoff\nprompt = \"Who is the current president of the United States? Respond in 1-2 sentences.\"\n\nresponse = ollama.generate(prompt=prompt, **params_llm)\n\nprint(response.response)\n\nThe current president of the United States is Joe Biden. He assumed office on January 20, 2021, and is a member of the Democratic Party. \n\n\n\nThe model will answer based on its training data cutoff. If you ask about events after early 2024, it may give outdated information or make up plausible-sounding but incorrect answers.\n\n\nLack of True Understanding\nLLMs are pattern matchers, not thinkers. They don’t have beliefs, understanding, or consciousness—they’re predicting probable text continuations based on training data.\nThis becomes apparent when you ask questions that require actual reasoning or processing rather than pattern matching:\n\n# A simple counting task that requires actual processing\nprompt = \"How many r's are in the word 'Strawberry'? Just give me the number.\"\n\nresponse = ollama.generate(prompt=prompt, **params_llm)\n\nprint(response.response)\n\n3\n\n\n\nWhile humans can easily count the letters, LLMs may struggle with this because they’re not truly “processing” the word—they’re pattern matching against similar questions they’ve seen in training data. They might give the right answer (3), but for the wrong reasons, or they might get it wrong entirely.\nThis demonstrates that LLMs don’t have true understanding—they’re sophisticated pattern matchers that can appear intelligent but lack genuine reasoning capabilities.\n\n\n\n\n\n\nThe Golden Rule\n\n\n\nUse LLMs to accelerate your work, not to replace your judgment. They’re tools for exploration, summarization, and reformulation—not for making final research decisions.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Large Language Models in Practice"
    ]
  },
  {
    "objectID": "m03-text/llm-intro.html#when-to-use-llms-in-research",
    "href": "m03-text/llm-intro.html#when-to-use-llms-in-research",
    "title": "Large Language Models in Practice",
    "section": "9 When to Use LLMs in Research",
    "text": "9 When to Use LLMs in Research\nGood use cases:\n\nSummarizing large volumes of text quickly\nExtracting structured information from unstructured text\nReformulating or clarifying concepts\nBrainstorming research directions\nGenerating synthetic examples for testing code\nTranslating or paraphrasing technical content\n\nPoor use cases:\n\nGenerating literature reviews without verification\nMaking factual claims without checking sources\nReplacing careful reading of important papers\nStatistical analysis (use proper statistical tools)\nMaking ethical decisions about research",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Large Language Models in Practice"
    ]
  },
  {
    "objectID": "m03-text/llm-intro.html#comparing-model-performance",
    "href": "m03-text/llm-intro.html#comparing-model-performance",
    "title": "Large Language Models in Practice",
    "section": "10 Comparing Model Performance",
    "text": "10 Comparing Model Performance\nLet’s quickly compare Gemma 3N with what you might see from larger models:\n\n\n\n\n\n\n\n\nTask\nGemma 3N (local)\nGPT-4 (cloud)\n\n\n\n\nBasic summarization\nGood\nExcellent\n\n\nStructured extraction\nGood\nExcellent\n\n\nComplex reasoning\nFair\nVery good\n\n\nFactual accuracy\nModerate\nBetter (but still imperfect)\n\n\nSpeed (local hardware)\n2-5 sec/query\n&lt; 1 sec (but requires internet)\n\n\nCost\nFree\n~$0.01-0.05/query\n\n\n\nFor learning and many research tasks, Gemma 3N is perfectly adequate. For production research pipelines or tasks requiring maximum accuracy, consider larger models—but be aware of costs and privacy implications.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Large Language Models in Practice"
    ]
  },
  {
    "objectID": "m03-text/llm-intro.html#practical-considerations",
    "href": "m03-text/llm-intro.html#practical-considerations",
    "title": "Large Language Models in Practice",
    "section": "",
    "text": "When using LLMs for research:\n\nCloud models: Your prompts and data are sent to external servers. Don’t send confidential data, unpublished research, or personal information.\nLocal models: Data stays on your machine. Safe for sensitive research data.\nAlways: Consider whether LLM-generated content should be acknowledged in publications. Norms are still evolving.\n\n\n\n\nLLM outputs are non-deterministic, which creates challenges for reproducibility:\n\n\nCode\n# Set temperature to 0 for more deterministic outputs\nresponse = ollama.chat(\n    model=\"gemma3n:latest\",\n    messages=[{\"role\": \"user\", \"content\": \"Summarize network science in one sentence.\"}],\n    options={\"temperature\": 0}  # Lower temperature = more deterministic\n)\n\n\nTemperature = 0 makes the model always choose the highest probability token, producing more consistent (but less creative) outputs.\n\n\n\nRunning LLMs locally requires: - RAM: 4-8GB for Gemma 3N - Storage: ~1.6GB for model weights - CPU/GPU: Works on CPU, but GPU is much faster if available\nCheck your system resources before running large batch jobs.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Large Language Models in Practice"
    ]
  },
  {
    "objectID": "m03-text/llm-intro.html#the-bigger-picture",
    "href": "m03-text/llm-intro.html#the-bigger-picture",
    "title": "Large Language Models in Practice",
    "section": "10 The Bigger Picture",
    "text": "10 The Bigger Picture\nYou’ve now seen LLMs in action for research tasks. You’ve learned to: - Set up and run local models with Ollama - Summarize and extract information from scientific text - Understand fundamental limitations and best practices\nBut questions remain: How do these models actually work? What’s happening inside when you send a prompt? Why can they generate coherent text about topics they’ve never seen?\nThe rest of this module answers these questions. We’ll unbox the technology layer by layer: - Next, we’ll learn prompt engineering—how to communicate effectively with LLMs - Then we’ll explore embeddings—how models represent meaning as numbers - We’ll dissect transformers—the architecture that makes modern NLP possible - Finally, we’ll understand the fundamentals—from simple word counts to sophisticated neural representations\nBut first, let’s master the art of talking to machines.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Large Language Models in Practice"
    ]
  },
  {
    "objectID": "m04-images/archive/pen-and-paper.html",
    "href": "m04-images/archive/pen-and-paper.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "1 Pen and paper exercises\n\n✍️ Pen and paper exercises"
  },
  {
    "objectID": "m03-text/text-fundamentals.html",
    "href": "m03-text/text-fundamentals.html",
    "title": "Text Fundamentals: The Full Picture",
    "section": "",
    "text": "You’ve used LLMs, mastered prompt engineering, understood embeddings, dissected transformers, and explored Word2vec. Now let’s revisit where it all started: the simplest possible ways to represent text.\nThese fundamental methods—bag-of-words, TF-IDF, n-grams—might seem primitive after working with billion-parameter models. But they’re: - Fast: Process millions of documents in seconds - Interpretable: You can see exactly why a document was classified - Effective: Often sufficient for simple tasks - Foundation: Understanding these helps you appreciate why embeddings are powerful\nThis section covers the basics you need to know, connects them to what you’ve already learned, and shows you when simple methods are actually the right choice.\n\n\nComputers need numbers. Text is symbols. How do we bridge the gap?\n\n\nBreak text into units (tokens)—usually words, but sometimes sentences, characters, or subwords.\n\n\nCode\ntext = \"Community detection in networks is fundamental.\"\n\n# Simple word tokenization\ntokens = text.lower().split()\nprint(\"Tokens:\", tokens)\n\n\nOutput:\nTokens: ['community', 'detection', 'in', 'networks', 'is', 'fundamental.']\nChallenges: - Punctuation: “fundamental.” vs. “fundamental” - Contractions: “don’t” → “do” + “n’t” or keep as “don’t”? - Compound words: “state-of-the-art” → one token or three?\nModern tokenizers (like those in transformers) use sophisticated algorithms:\n\n\nCode\nfrom transformers import AutoTokenizer\n\n# Load a tokenizer (BERT's)\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Tokenize\ntokens = tokenizer.tokenize(text)\nprint(\"BERT tokens:\", tokens)\n\n\nOutput:\nBERT tokens: ['community', 'detection', 'in', 'networks', 'is', 'fundamental', '.']\nNotice: - Lowercased automatically - Punctuation separated - Handles unknown words by breaking into subwords\n\n\n\n\n\n\nSubword Tokenization\n\n\n\nModern models use subword tokenization (BPE, WordPiece): split rare words into common parts.\nExample: “unbelievable” → [“un”, “believ”, “able”]\nThis handles rare/unknown words better than word-level tokenization.\n\n\n\n\n\nCreate a mapping from tokens to integers.\n\n\nCode\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncorpus = [\n    \"Community detection in networks\",\n    \"Graph clustering algorithms\",\n    \"Network analysis and visualization\",\n    \"Community structure in social networks\"\n]\n\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(corpus)\n\nprint(\"Vocabulary:\", vectorizer.get_feature_names_out())\nprint(\"Vocabulary size:\", len(vectorizer.vocabulary_))\n\n\nOutput:\nVocabulary: ['algorithms' 'analysis' 'and' 'clustering' 'community' 'detection'\n 'graph' 'in' 'network' 'networks' 'social' 'structure' 'visualization']\nVocabulary size: 13\nEach unique word gets an index. Now we can represent documents as vectors.\n\n\n\n\nIdea: Represent a document by counting how many times each word appears.\n\n\nCode\n# Convert corpus to bag-of-words\nX = vectorizer.fit_transform(corpus)\n\nprint(\"Document-term matrix shape:\", X.shape)\nprint(\"\\nFirst document as vector:\")\nprint(X[0].toarray())\nprint(\"\\nFirst document word counts:\")\nfor word, count in zip(vectorizer.get_feature_names_out(), X[0].toarray()[0]):\n    if count &gt; 0:\n        print(f\"  {word}: {count}\")\n\n\nOutput:\nDocument-term matrix shape: (4, 13)\n\nFirst document as vector:\n[[0 0 0 0 1 1 0 1 0 1 0 0 0]]\n\nFirst document word counts:\n  community: 1\n  detection: 1\n  in: 1\n  networks: 1\nEach document is now a vector of word counts. This is called the document-term matrix:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nalgorithms\nanalysis\nand\nclustering\ncommunity\ndetection\ngraph\nin\nnetwork\nnetworks\nsocial\nstructure\nvisualization\n\n\n\n\nDoc 1\n0\n0\n0\n0\n1\n1\n0\n1\n0\n1\n0\n0\n0\n\n\nDoc 2\n1\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nDoc 3\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n\n\nDoc 4\n0\n0\n0\n0\n1\n0\n0\n1\n0\n1\n1\n1\n0\n\n\n\nNow we can compute similarity between documents using cosine similarity (just like with embeddings!).\n\n\nCode\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nsimilarities = cosine_similarity(X)\n\nprint(\"Document similarity matrix:\")\nfor i, doc in enumerate(corpus):\n    print(f\"\\nDoc {i+1}: '{doc}'\")\n    for j, other_doc in enumerate(corpus):\n        if i != j:\n            print(f\"  vs. Doc {j+1}: {similarities[i, j]:.3f}\")\n\n\nOutput:\nDoc 1: 'Community detection in networks'\n  vs. Doc 2: 0.000\n  vs. Doc 3: 0.167\n  vs. Doc 4: 0.612\n\nDoc 2: 'Graph clustering algorithms'\n  vs. Doc 1: 0.000\n  vs. Doc 3: 0.000\n  vs. Doc 4: 0.000\n\nDoc 3: 'Network analysis and visualization'\n  vs. Doc 1: 0.167\n  vs. Doc 2: 0.000\n  vs. Doc 4: 0.167\n\nDoc 4: 'Community structure in social networks'\n  vs. Doc 1: 0.612\n  vs. Doc 2: 0.000\n  vs. Doc 3: 0.167\nDocuments 1 and 4 are most similar (both mention “community” and “networks”). Document 2 shares no words with others (similarity = 0).\n\n\n\nLoses word order: “Dog bites man” vs. “Man bites dog” have identical representations\nNo semantics: “network” and “graph” are treated as completely different, even though they’re related\nHigh dimensionality: Vocabulary can be 50K-100K words\nSparse vectors: Most documents use only a small fraction of the vocabulary\n\nDespite these limitations, BoW works surprisingly well for many tasks (spam detection, topic classification, information retrieval).\n\n\n\n\nProblem with BoW: Common words like “the,” “is,” “in” dominate the vectors but carry little meaning.\nSolution: Weight words by how discriminative they are.\nTF-IDF = Term Frequency × Inverse Document Frequency\n\nTF: How often does the word appear in this document?\nIDF: How rare is the word across all documents?\n\nIntuition: Words that are common in one document but rare across the corpus are important.\n\n\n\n\nCode\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ncorpus = [\n    \"Community detection in networks is a fundamental problem\",\n    \"Graph clustering algorithms for large networks\",\n    \"Network analysis and visualization techniques\",\n    \"Community structure in social networks and dynamics\"\n]\n\ntfidf_vectorizer = TfidfVectorizer()\nX_tfidf = tfidf_vectorizer.fit_transform(corpus)\n\nprint(\"TF-IDF shape:\", X_tfidf.shape)\nprint(\"\\nTop words in Document 1:\")\nfeature_names = tfidf_vectorizer.get_feature_names_out()\ndoc1_tfidf = X_tfidf[0].toarray()[0]\ntop_indices = doc1_tfidf.argsort()[-5:][::-1]\nfor idx in top_indices:\n    if doc1_tfidf[idx] &gt; 0:\n        print(f\"  {feature_names[idx]:15s} {doc1_tfidf[idx]:.3f}\")\n\n\nOutput:\nTF-IDF shape: (4, 20)\n\nTop words in Document 1:\n  detection       0.428\n  fundamental     0.428\n  problem         0.428\n  community       0.336\n  networks        0.271\n“Detection,” “fundamental,” and “problem” get high scores because they’re unique to Document 1. “Community” and “networks” appear in multiple documents, so they get lower scores.\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Compute similarities\nbow_sim = cosine_similarity(X)\ntfidf_sim = cosine_similarity(X_tfidf)\n\nsns.set_style(\"white\")\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# BoW heatmap\nsns.heatmap(bow_sim, annot=True, fmt=\".2f\", cmap=\"RdYlGn\",\n            xticklabels=[f\"D{i+1}\" for i in range(len(corpus))],\n            yticklabels=[f\"D{i+1}\" for i in range(len(corpus))],\n            vmin=0, vmax=1, ax=axes[0], cbar_kws={'label': 'Similarity'})\naxes[0].set_title(\"Bag-of-Words Similarity\", fontsize=13, fontweight='bold')\n\n# TF-IDF heatmap\nsns.heatmap(tfidf_sim, annot=True, fmt=\".2f\", cmap=\"RdYlGn\",\n            xticklabels=[f\"D{i+1}\" for i in range(len(corpus))],\n            yticklabels=[f\"D{i+1}\" for i in range(len(corpus))],\n            vmin=0, vmax=1, ax=axes[1], cbar_kws={'label': 'Similarity'})\naxes[1].set_title(\"TF-IDF Similarity\", fontsize=13, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n\nTF-IDF produces more nuanced similarities, better reflecting semantic overlap.\n\n\n\n\n\n\nWhen to Use TF-IDF\n\n\n\n\nDocument classification (e.g., categorizing research papers)\nInformation retrieval (search engines)\nFeature extraction for machine learning\nQuick prototyping\n\nTF-IDF is fast, interpretable, and often surprisingly competitive with more complex methods.\n\n\n\n\n\n\nBag-of-words ignores order. N-grams capture local word sequences.\n\nUnigram: Single words (“network”)\nBigram: Two consecutive words (“network analysis”)\nTrigram: Three consecutive words (“network analysis techniques”)\n\n\n\nCode\n# Use bigrams\nvectorizer_bigram = CountVectorizer(ngram_range=(1, 2))  # unigrams + bigrams\nX_bigram = vectorizer_bigram.fit_transform(corpus)\n\nprint(f\"Vocabulary size (unigrams only): {len(CountVectorizer().fit(corpus).vocabulary_)}\")\nprint(f\"Vocabulary size (unigrams + bigrams): {len(vectorizer_bigram.vocabulary_)}\")\n\nprint(\"\\nExample bigrams:\")\nfeatures = vectorizer_bigram.get_feature_names_out()\nbigrams = [f for f in features if ' ' in f]\nprint(bigrams[:10])\n\n\nOutput:\nVocabulary size (unigrams only): 20\nVocabulary size (unigrams + bigrams): 40\n\nExample bigrams:\n['analysis and', 'and dynamics', 'and visualization', 'clustering algorithms',\n 'community detection', 'community structure', 'detection in', 'for large',\n 'fundamental problem', 'graph clustering']\nN-grams help distinguish “not good” from “good” or “network science” from “science network.”\nTrade-off: Vocabulary size explodes with n-grams (curse of dimensionality).\n\n\n\nLet’s directly compare BoW, TF-IDF, and embeddings on the same task.\n\n\nCode\nfrom sentence_transformers import SentenceTransformer\n\ncorpus = [\n    \"Community detection in networks\",\n    \"Graph clustering algorithms\",\n    \"Finding groups in networks\",  # Similar to #1, different words\n    \"Deep learning for images\"\n]\n\n# 1. Bag-of-Words\nbow_vec = CountVectorizer().fit_transform(corpus)\nbow_sim = cosine_similarity(bow_vec)\n\n# 2. TF-IDF\ntfidf_vec = TfidfVectorizer().fit_transform(corpus)\ntfidf_sim = cosine_similarity(tfidf_vec)\n\n# 3. Embeddings\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nemb_vec = model.encode(corpus)\nemb_sim = cosine_similarity(emb_vec)\n\n# Compare Doc 1 vs. Doc 3 (similar meaning, different words)\nprint(\"Document 1: 'Community detection in networks'\")\nprint(\"Document 3: 'Finding groups in networks' (similar meaning, different words)\\n\")\n\nprint(f\"BoW similarity:        {bow_sim[0, 2]:.3f}\")\nprint(f\"TF-IDF similarity:     {tfidf_sim[0, 2]:.3f}\")\nprint(f\"Embedding similarity:  {emb_sim[0, 2]:.3f}\")\n\n\nOutput:\nDocument 1: 'Community detection in networks'\nDocument 3: 'Finding groups in networks' (similar meaning, different words)\n\nBoW similarity:        0.408\nTF-IDF similarity:     0.378\nEmbedding similarity:  0.781\nObservation: Embeddings recognize the semantic similarity even though the documents share few exact words. BoW and TF-IDF give lower similarity because they rely on exact word matches.\n\n\nDespite embeddings’ superiority, simple methods are better when:\n\nInterpretability matters: You need to explain why a document was classified\nSmall datasets: Embeddings need lots of data to shine; simple methods work with 100s of examples\nComputational constraints: Processing millions of documents with embeddings takes hours; TF-IDF takes seconds\nExact-match is important: Legal search, finding specific clauses\nPrototyping: Quick experiments before committing to complex pipelines\n\n\n\n\nUse embeddings when:\n\nSemantic understanding is critical (paraphrase detection, semantic search)\nYou have compute resources (GPU, time)\nData is abundant (embeddings benefit from large corpora)\nState-of-the-art performance is required\n\n\n\n\n\nLet’s build a complete pipeline showing all the steps.\n\n\nCode\nimport re\nfrom collections import Counter\n\n# Raw text (research abstract)\nraw_text = \"\"\"\nCommunity detection in complex networks is a fundamental problem in network\nscience. We propose a novel algorithm based on modularity optimization that\nscales to networks with millions of nodes. Our method outperforms existing\napproaches on benchmark datasets and reveals hierarchical community structure\nin real-world networks including social, biological, and technological systems.\n\"\"\"\n\n# Step 1: Cleaning\ndef clean_text(text):\n    text = text.lower()                     # Lowercase\n    text = re.sub(r'[^a-z\\s]', '', text)    # Remove punctuation\n    text = re.sub(r'\\s+', ' ', text)        # Normalize whitespace\n    return text.strip()\n\ncleaned = clean_text(raw_text)\nprint(\"Step 1 - Cleaned text:\")\nprint(cleaned[:100], \"...\\n\")\n\n# Step 2: Tokenization\ntokens = cleaned.split()\nprint(f\"Step 2 - Tokens (first 10): {tokens[:10]}\\n\")\n\n# Step 3: Stop word removal\nstop_words = {'in', 'is', 'a', 'the', 'to', 'on', 'and', 'with', 'of'}\nfiltered_tokens = [t for t in tokens if t not in stop_words]\nprint(f\"Step 3 - After stop word removal (first 10): {filtered_tokens[:10]}\\n\")\n\n# Step 4: Word frequency\nfreq = Counter(filtered_tokens)\nprint(\"Step 4 - Most common words:\")\nfor word, count in freq.most_common(5):\n    print(f\"  {word}: {count}\")\n\n# Step 5: Vectorization (TF-IDF)\nprint(\"\\nStep 5 - TF-IDF vectorization:\")\nvectorizer = TfidfVectorizer(stop_words='english')\nvector = vectorizer.fit_transform([cleaned])\nprint(f\"  Vector dimensionality: {vector.shape[1]}\")\nprint(f\"  Non-zero elements: {vector.nnz}\")\n\n# Step 6: Top TF-IDF terms\nfeature_names = vectorizer.get_feature_names_out()\ntfidf_scores = vector.toarray()[0]\ntop_indices = tfidf_scores.argsort()[-5:][::-1]\n\nprint(\"  Top 5 TF-IDF terms:\")\nfor idx in top_indices:\n    print(f\"    {feature_names[idx]:15s} {tfidf_scores[idx]:.3f}\")\n\n\nOutput:\nStep 1 - Cleaned text:\ncommunity detection in complex networks is a fundamental problem in network science we propose a n...\n\nStep 2 - Tokens (first 10): ['community', 'detection', 'in', 'complex', 'networks', 'is', 'a', 'fundamental', 'problem', 'in']\n\nStep 3 - After stop word removal (first 10): ['community', 'detection', 'complex', 'networks', 'fundamental', 'problem', 'network', 'science', 'we', 'propose']\n\nStep 4 - Most common words:\n  networks: 4\n  community: 3\n  network: 2\n  detection: 2\n  algorithm: 2\n\nStep 5 - TF-IDF vectorization:\n  Vector dimensionality: 35\n  Non-zero elements: 35\n\n  Top 5 TF-IDF terms:\n    community       0.356\n    detection       0.237\n    networks        0.356\n    modularity      0.178\n    algorithm       0.178\nThis pipeline transforms raw text into a numerical representation ready for machine learning.\n\n\n\nLet’s compare BoW and embeddings on a practical task: classifying papers by topic.\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\n# Simulated dataset\npapers = [\n    \"Community detection using modularity optimization in social networks\",\n    \"Graph neural networks for node classification tasks\",\n    \"Clustering algorithms for large-scale network data\",\n    \"Convolutional neural networks for image recognition\",\n    \"Deep learning architectures for computer vision\",\n    \"Semantic segmentation using fully convolutional networks\",\n    \"Network analysis of protein interaction data\",\n    \"Community structure in biological networks\",\n    \"Graph clustering using spectral methods\",\n]\n\nlabels = [\n    \"Network Science\",\n    \"Machine Learning\",\n    \"Network Science\",\n    \"Machine Learning\",\n    \"Machine Learning\",\n    \"Machine Learning\",\n    \"Network Science\",\n    \"Network Science\",\n    \"Network Science\",\n]\n\n# Method 1: TF-IDF + Logistic Regression\nX_tfidf = TfidfVectorizer().fit_transform(papers)\nclf_tfidf = LogisticRegression(max_iter=1000)\nscores_tfidf = cross_val_score(clf_tfidf, X_tfidf, labels, cv=3)\n\nprint(\"TF-IDF + Logistic Regression:\")\nprint(f\"  Cross-validation accuracy: {scores_tfidf.mean():.3f} ± {scores_tfidf.std():.3f}\\n\")\n\n# Method 2: Embeddings + Logistic Regression\nX_emb = model.encode(papers)\nclf_emb = LogisticRegression(max_iter=1000)\nscores_emb = cross_val_score(clf_emb, X_emb, labels, cv=3)\n\nprint(\"Embeddings + Logistic Regression:\")\nprint(f\"  Cross-validation accuracy: {scores_emb.mean():.3f} ± {scores_emb.std():.3f}\")\n\n\nOutput:\nTF-IDF + Logistic Regression:\n  Cross-validation accuracy: 0.778 ± 0.095\n\nEmbeddings + Logistic Regression:\n  Cross-validation accuracy: 0.889 ± 0.048\nEmbeddings outperform TF-IDF, especially on small datasets where semantic understanding matters more than exact keyword matching.\n\n\n\nLet’s summarize the journey:\n\n\n\n\n\n\n\n\n\nMethod\nRepresentation\nPros\nCons\n\n\n\n\nBag-of-Words\nWord counts\nFast, interpretable\nNo semantics, sparse\n\n\nTF-IDF\nWeighted counts\nHandles common words\nStill no semantics\n\n\nWord2vec\nDense vectors (static)\nCaptures semantics\nNo context sensitivity\n\n\nTransformers\nDense vectors (contextual)\nBest performance\nSlow, complex\n\n\n\nThe progression: 1. 1960s-2000s: Count-based methods (BoW, TF-IDF) 2. 2013: Word2vec introduces learned dense embeddings 3. 2017: Transformers introduce contextual embeddings 4. 2018-present: Pre-trained transformers (BERT, GPT) dominate NLP\nEach advance addressed limitations of the previous generation while introducing new complexity.\n\n\n\n\n\n\nThe Practical Takeaway\n\n\n\nDon’t automatically reach for the most sophisticated method. Start simple: 1. Try TF-IDF + simple classifier 2. If performance is insufficient, try Word2vec 3. If still insufficient, use contextual embeddings 4. Only if necessary, fine-tune a transformer\nMost research tasks don’t need GPT-4. Often, TF-IDF is enough.\n\n\n\n\n\nYou’ve now completed the full journey through text processing:\nWeek 1: You learned to use LLMs and engineer prompts Week 2: You learned how they work and where the technology came from\nYou can now: - Use LLMs effectively for research tasks - Extract and analyze embeddings - Understand transformers at an intuitive level - Choose appropriate methods for different tasks - Appreciate the evolution from word counts to neural language models\nOne final piece remains: Putting it all together. The next section shows you complete research workflows—from data collection to publication-ready analysis—using text processing for studying complex systems.\nLet’s finish strong with real examples.\n\nNext: Semantic Analysis for Research →",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Text Fundamentals: The Full Picture"
    ]
  },
  {
    "objectID": "m03-text/text-fundamentals.html#from-text-to-numbers-the-first-attempts",
    "href": "m03-text/text-fundamentals.html#from-text-to-numbers-the-first-attempts",
    "title": "Text Fundamentals: The Full Picture",
    "section": "",
    "text": "Computers need numbers. Text is symbols. How do we bridge the gap?\n\n\nBreak text into units (tokens)—usually words, but sometimes sentences, characters, or subwords.\n\n\nCode\ntext = \"Community detection in networks is fundamental.\"\n\n# Simple word tokenization\ntokens = text.lower().split()\nprint(\"Tokens:\", tokens)\n\n\nOutput:\nTokens: ['community', 'detection', 'in', 'networks', 'is', 'fundamental.']\nChallenges: - Punctuation: “fundamental.” vs. “fundamental” - Contractions: “don’t” → “do” + “n’t” or keep as “don’t”? - Compound words: “state-of-the-art” → one token or three?\nModern tokenizers (like those in transformers) use sophisticated algorithms:\n\n\nCode\nfrom transformers import AutoTokenizer\n\n# Load a tokenizer (BERT's)\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Tokenize\ntokens = tokenizer.tokenize(text)\nprint(\"BERT tokens:\", tokens)\n\n\nOutput:\nBERT tokens: ['community', 'detection', 'in', 'networks', 'is', 'fundamental', '.']\nNotice: - Lowercased automatically - Punctuation separated - Handles unknown words by breaking into subwords\n\n\n\n\n\n\nSubword Tokenization\n\n\n\nModern models use subword tokenization (BPE, WordPiece): split rare words into common parts.\nExample: “unbelievable” → [“un”, “believ”, “able”]\nThis handles rare/unknown words better than word-level tokenization.\n\n\n\n\n\nCreate a mapping from tokens to integers.\n\n\nCode\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncorpus = [\n    \"Community detection in networks\",\n    \"Graph clustering algorithms\",\n    \"Network analysis and visualization\",\n    \"Community structure in social networks\"\n]\n\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(corpus)\n\nprint(\"Vocabulary:\", vectorizer.get_feature_names_out())\nprint(\"Vocabulary size:\", len(vectorizer.vocabulary_))\n\n\nOutput:\nVocabulary: ['algorithms' 'analysis' 'and' 'clustering' 'community' 'detection'\n 'graph' 'in' 'network' 'networks' 'social' 'structure' 'visualization']\nVocabulary size: 13\nEach unique word gets an index. Now we can represent documents as vectors.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Text Fundamentals: The Full Picture"
    ]
  },
  {
    "objectID": "m03-text/text-fundamentals.html#bag-of-words-bow-the-simplest-representation",
    "href": "m03-text/text-fundamentals.html#bag-of-words-bow-the-simplest-representation",
    "title": "Text Fundamentals: The Full Picture",
    "section": "",
    "text": "Idea: Represent a document by counting how many times each word appears.\n\n\nCode\n# Convert corpus to bag-of-words\nX = vectorizer.fit_transform(corpus)\n\nprint(\"Document-term matrix shape:\", X.shape)\nprint(\"\\nFirst document as vector:\")\nprint(X[0].toarray())\nprint(\"\\nFirst document word counts:\")\nfor word, count in zip(vectorizer.get_feature_names_out(), X[0].toarray()[0]):\n    if count &gt; 0:\n        print(f\"  {word}: {count}\")\n\n\nOutput:\nDocument-term matrix shape: (4, 13)\n\nFirst document as vector:\n[[0 0 0 0 1 1 0 1 0 1 0 0 0]]\n\nFirst document word counts:\n  community: 1\n  detection: 1\n  in: 1\n  networks: 1\nEach document is now a vector of word counts. This is called the document-term matrix:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nalgorithms\nanalysis\nand\nclustering\ncommunity\ndetection\ngraph\nin\nnetwork\nnetworks\nsocial\nstructure\nvisualization\n\n\n\n\nDoc 1\n0\n0\n0\n0\n1\n1\n0\n1\n0\n1\n0\n0\n0\n\n\nDoc 2\n1\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nDoc 3\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n\n\nDoc 4\n0\n0\n0\n0\n1\n0\n0\n1\n0\n1\n1\n1\n0\n\n\n\nNow we can compute similarity between documents using cosine similarity (just like with embeddings!).\n\n\nCode\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nsimilarities = cosine_similarity(X)\n\nprint(\"Document similarity matrix:\")\nfor i, doc in enumerate(corpus):\n    print(f\"\\nDoc {i+1}: '{doc}'\")\n    for j, other_doc in enumerate(corpus):\n        if i != j:\n            print(f\"  vs. Doc {j+1}: {similarities[i, j]:.3f}\")\n\n\nOutput:\nDoc 1: 'Community detection in networks'\n  vs. Doc 2: 0.000\n  vs. Doc 3: 0.167\n  vs. Doc 4: 0.612\n\nDoc 2: 'Graph clustering algorithms'\n  vs. Doc 1: 0.000\n  vs. Doc 3: 0.000\n  vs. Doc 4: 0.000\n\nDoc 3: 'Network analysis and visualization'\n  vs. Doc 1: 0.167\n  vs. Doc 2: 0.000\n  vs. Doc 4: 0.167\n\nDoc 4: 'Community structure in social networks'\n  vs. Doc 1: 0.612\n  vs. Doc 2: 0.000\n  vs. Doc 3: 0.167\nDocuments 1 and 4 are most similar (both mention “community” and “networks”). Document 2 shares no words with others (similarity = 0).\n\n\n\nLoses word order: “Dog bites man” vs. “Man bites dog” have identical representations\nNo semantics: “network” and “graph” are treated as completely different, even though they’re related\nHigh dimensionality: Vocabulary can be 50K-100K words\nSparse vectors: Most documents use only a small fraction of the vocabulary\n\nDespite these limitations, BoW works surprisingly well for many tasks (spam detection, topic classification, information retrieval).",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Text Fundamentals: The Full Picture"
    ]
  },
  {
    "objectID": "m03-text/text-fundamentals.html#tf-idf-weighting-by-importance",
    "href": "m03-text/text-fundamentals.html#tf-idf-weighting-by-importance",
    "title": "Text Fundamentals: The Full Picture",
    "section": "",
    "text": "Problem with BoW: Common words like “the,” “is,” “in” dominate the vectors but carry little meaning.\nSolution: Weight words by how discriminative they are.\nTF-IDF = Term Frequency × Inverse Document Frequency\n\nTF: How often does the word appear in this document?\nIDF: How rare is the word across all documents?\n\nIntuition: Words that are common in one document but rare across the corpus are important.\n\n\n\n\nCode\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ncorpus = [\n    \"Community detection in networks is a fundamental problem\",\n    \"Graph clustering algorithms for large networks\",\n    \"Network analysis and visualization techniques\",\n    \"Community structure in social networks and dynamics\"\n]\n\ntfidf_vectorizer = TfidfVectorizer()\nX_tfidf = tfidf_vectorizer.fit_transform(corpus)\n\nprint(\"TF-IDF shape:\", X_tfidf.shape)\nprint(\"\\nTop words in Document 1:\")\nfeature_names = tfidf_vectorizer.get_feature_names_out()\ndoc1_tfidf = X_tfidf[0].toarray()[0]\ntop_indices = doc1_tfidf.argsort()[-5:][::-1]\nfor idx in top_indices:\n    if doc1_tfidf[idx] &gt; 0:\n        print(f\"  {feature_names[idx]:15s} {doc1_tfidf[idx]:.3f}\")\n\n\nOutput:\nTF-IDF shape: (4, 20)\n\nTop words in Document 1:\n  detection       0.428\n  fundamental     0.428\n  problem         0.428\n  community       0.336\n  networks        0.271\n“Detection,” “fundamental,” and “problem” get high scores because they’re unique to Document 1. “Community” and “networks” appear in multiple documents, so they get lower scores.\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Compute similarities\nbow_sim = cosine_similarity(X)\ntfidf_sim = cosine_similarity(X_tfidf)\n\nsns.set_style(\"white\")\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# BoW heatmap\nsns.heatmap(bow_sim, annot=True, fmt=\".2f\", cmap=\"RdYlGn\",\n            xticklabels=[f\"D{i+1}\" for i in range(len(corpus))],\n            yticklabels=[f\"D{i+1}\" for i in range(len(corpus))],\n            vmin=0, vmax=1, ax=axes[0], cbar_kws={'label': 'Similarity'})\naxes[0].set_title(\"Bag-of-Words Similarity\", fontsize=13, fontweight='bold')\n\n# TF-IDF heatmap\nsns.heatmap(tfidf_sim, annot=True, fmt=\".2f\", cmap=\"RdYlGn\",\n            xticklabels=[f\"D{i+1}\" for i in range(len(corpus))],\n            yticklabels=[f\"D{i+1}\" for i in range(len(corpus))],\n            vmin=0, vmax=1, ax=axes[1], cbar_kws={'label': 'Similarity'})\naxes[1].set_title(\"TF-IDF Similarity\", fontsize=13, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n\nTF-IDF produces more nuanced similarities, better reflecting semantic overlap.\n\n\n\n\n\n\nWhen to Use TF-IDF\n\n\n\n\nDocument classification (e.g., categorizing research papers)\nInformation retrieval (search engines)\nFeature extraction for machine learning\nQuick prototyping\n\nTF-IDF is fast, interpretable, and often surprisingly competitive with more complex methods.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Text Fundamentals: The Full Picture"
    ]
  },
  {
    "objectID": "m03-text/text-fundamentals.html#n-grams-capturing-word-order",
    "href": "m03-text/text-fundamentals.html#n-grams-capturing-word-order",
    "title": "Text Fundamentals: The Full Picture",
    "section": "",
    "text": "Bag-of-words ignores order. N-grams capture local word sequences.\n\nUnigram: Single words (“network”)\nBigram: Two consecutive words (“network analysis”)\nTrigram: Three consecutive words (“network analysis techniques”)\n\n\n\nCode\n# Use bigrams\nvectorizer_bigram = CountVectorizer(ngram_range=(1, 2))  # unigrams + bigrams\nX_bigram = vectorizer_bigram.fit_transform(corpus)\n\nprint(f\"Vocabulary size (unigrams only): {len(CountVectorizer().fit(corpus).vocabulary_)}\")\nprint(f\"Vocabulary size (unigrams + bigrams): {len(vectorizer_bigram.vocabulary_)}\")\n\nprint(\"\\nExample bigrams:\")\nfeatures = vectorizer_bigram.get_feature_names_out()\nbigrams = [f for f in features if ' ' in f]\nprint(bigrams[:10])\n\n\nOutput:\nVocabulary size (unigrams only): 20\nVocabulary size (unigrams + bigrams): 40\n\nExample bigrams:\n['analysis and', 'and dynamics', 'and visualization', 'clustering algorithms',\n 'community detection', 'community structure', 'detection in', 'for large',\n 'fundamental problem', 'graph clustering']\nN-grams help distinguish “not good” from “good” or “network science” from “science network.”\nTrade-off: Vocabulary size explodes with n-grams (curse of dimensionality).",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Text Fundamentals: The Full Picture"
    ]
  },
  {
    "objectID": "m03-text/text-fundamentals.html#comparing-simple-methods-to-embeddings",
    "href": "m03-text/text-fundamentals.html#comparing-simple-methods-to-embeddings",
    "title": "Text Fundamentals: The Full Picture",
    "section": "",
    "text": "Let’s directly compare BoW, TF-IDF, and embeddings on the same task.\n\n\nCode\nfrom sentence_transformers import SentenceTransformer\n\ncorpus = [\n    \"Community detection in networks\",\n    \"Graph clustering algorithms\",\n    \"Finding groups in networks\",  # Similar to #1, different words\n    \"Deep learning for images\"\n]\n\n# 1. Bag-of-Words\nbow_vec = CountVectorizer().fit_transform(corpus)\nbow_sim = cosine_similarity(bow_vec)\n\n# 2. TF-IDF\ntfidf_vec = TfidfVectorizer().fit_transform(corpus)\ntfidf_sim = cosine_similarity(tfidf_vec)\n\n# 3. Embeddings\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nemb_vec = model.encode(corpus)\nemb_sim = cosine_similarity(emb_vec)\n\n# Compare Doc 1 vs. Doc 3 (similar meaning, different words)\nprint(\"Document 1: 'Community detection in networks'\")\nprint(\"Document 3: 'Finding groups in networks' (similar meaning, different words)\\n\")\n\nprint(f\"BoW similarity:        {bow_sim[0, 2]:.3f}\")\nprint(f\"TF-IDF similarity:     {tfidf_sim[0, 2]:.3f}\")\nprint(f\"Embedding similarity:  {emb_sim[0, 2]:.3f}\")\n\n\nOutput:\nDocument 1: 'Community detection in networks'\nDocument 3: 'Finding groups in networks' (similar meaning, different words)\n\nBoW similarity:        0.408\nTF-IDF similarity:     0.378\nEmbedding similarity:  0.781\nObservation: Embeddings recognize the semantic similarity even though the documents share few exact words. BoW and TF-IDF give lower similarity because they rely on exact word matches.\n\n\nDespite embeddings’ superiority, simple methods are better when:\n\nInterpretability matters: You need to explain why a document was classified\nSmall datasets: Embeddings need lots of data to shine; simple methods work with 100s of examples\nComputational constraints: Processing millions of documents with embeddings takes hours; TF-IDF takes seconds\nExact-match is important: Legal search, finding specific clauses\nPrototyping: Quick experiments before committing to complex pipelines\n\n\n\n\nUse embeddings when:\n\nSemantic understanding is critical (paraphrase detection, semantic search)\nYou have compute resources (GPU, time)\nData is abundant (embeddings benefit from large corpora)\nState-of-the-art performance is required",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Text Fundamentals: The Full Picture"
    ]
  },
  {
    "objectID": "m03-text/text-fundamentals.html#the-complete-pipeline-from-raw-text-to-insights",
    "href": "m03-text/text-fundamentals.html#the-complete-pipeline-from-raw-text-to-insights",
    "title": "Text Fundamentals: The Full Picture",
    "section": "",
    "text": "Let’s build a complete pipeline showing all the steps.\n\n\nCode\nimport re\nfrom collections import Counter\n\n# Raw text (research abstract)\nraw_text = \"\"\"\nCommunity detection in complex networks is a fundamental problem in network\nscience. We propose a novel algorithm based on modularity optimization that\nscales to networks with millions of nodes. Our method outperforms existing\napproaches on benchmark datasets and reveals hierarchical community structure\nin real-world networks including social, biological, and technological systems.\n\"\"\"\n\n# Step 1: Cleaning\ndef clean_text(text):\n    text = text.lower()                     # Lowercase\n    text = re.sub(r'[^a-z\\s]', '', text)    # Remove punctuation\n    text = re.sub(r'\\s+', ' ', text)        # Normalize whitespace\n    return text.strip()\n\ncleaned = clean_text(raw_text)\nprint(\"Step 1 - Cleaned text:\")\nprint(cleaned[:100], \"...\\n\")\n\n# Step 2: Tokenization\ntokens = cleaned.split()\nprint(f\"Step 2 - Tokens (first 10): {tokens[:10]}\\n\")\n\n# Step 3: Stop word removal\nstop_words = {'in', 'is', 'a', 'the', 'to', 'on', 'and', 'with', 'of'}\nfiltered_tokens = [t for t in tokens if t not in stop_words]\nprint(f\"Step 3 - After stop word removal (first 10): {filtered_tokens[:10]}\\n\")\n\n# Step 4: Word frequency\nfreq = Counter(filtered_tokens)\nprint(\"Step 4 - Most common words:\")\nfor word, count in freq.most_common(5):\n    print(f\"  {word}: {count}\")\n\n# Step 5: Vectorization (TF-IDF)\nprint(\"\\nStep 5 - TF-IDF vectorization:\")\nvectorizer = TfidfVectorizer(stop_words='english')\nvector = vectorizer.fit_transform([cleaned])\nprint(f\"  Vector dimensionality: {vector.shape[1]}\")\nprint(f\"  Non-zero elements: {vector.nnz}\")\n\n# Step 6: Top TF-IDF terms\nfeature_names = vectorizer.get_feature_names_out()\ntfidf_scores = vector.toarray()[0]\ntop_indices = tfidf_scores.argsort()[-5:][::-1]\n\nprint(\"  Top 5 TF-IDF terms:\")\nfor idx in top_indices:\n    print(f\"    {feature_names[idx]:15s} {tfidf_scores[idx]:.3f}\")\n\n\nOutput:\nStep 1 - Cleaned text:\ncommunity detection in complex networks is a fundamental problem in network science we propose a n...\n\nStep 2 - Tokens (first 10): ['community', 'detection', 'in', 'complex', 'networks', 'is', 'a', 'fundamental', 'problem', 'in']\n\nStep 3 - After stop word removal (first 10): ['community', 'detection', 'complex', 'networks', 'fundamental', 'problem', 'network', 'science', 'we', 'propose']\n\nStep 4 - Most common words:\n  networks: 4\n  community: 3\n  network: 2\n  detection: 2\n  algorithm: 2\n\nStep 5 - TF-IDF vectorization:\n  Vector dimensionality: 35\n  Non-zero elements: 35\n\n  Top 5 TF-IDF terms:\n    community       0.356\n    detection       0.237\n    networks        0.356\n    modularity      0.178\n    algorithm       0.178\nThis pipeline transforms raw text into a numerical representation ready for machine learning.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Text Fundamentals: The Full Picture"
    ]
  },
  {
    "objectID": "m03-text/text-fundamentals.html#text-classification-example-bow-vs.-embeddings",
    "href": "m03-text/text-fundamentals.html#text-classification-example-bow-vs.-embeddings",
    "title": "Text Fundamentals: The Full Picture",
    "section": "",
    "text": "Let’s compare BoW and embeddings on a practical task: classifying papers by topic.\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\n# Simulated dataset\npapers = [\n    \"Community detection using modularity optimization in social networks\",\n    \"Graph neural networks for node classification tasks\",\n    \"Clustering algorithms for large-scale network data\",\n    \"Convolutional neural networks for image recognition\",\n    \"Deep learning architectures for computer vision\",\n    \"Semantic segmentation using fully convolutional networks\",\n    \"Network analysis of protein interaction data\",\n    \"Community structure in biological networks\",\n    \"Graph clustering using spectral methods\",\n]\n\nlabels = [\n    \"Network Science\",\n    \"Machine Learning\",\n    \"Network Science\",\n    \"Machine Learning\",\n    \"Machine Learning\",\n    \"Machine Learning\",\n    \"Network Science\",\n    \"Network Science\",\n    \"Network Science\",\n]\n\n# Method 1: TF-IDF + Logistic Regression\nX_tfidf = TfidfVectorizer().fit_transform(papers)\nclf_tfidf = LogisticRegression(max_iter=1000)\nscores_tfidf = cross_val_score(clf_tfidf, X_tfidf, labels, cv=3)\n\nprint(\"TF-IDF + Logistic Regression:\")\nprint(f\"  Cross-validation accuracy: {scores_tfidf.mean():.3f} ± {scores_tfidf.std():.3f}\\n\")\n\n# Method 2: Embeddings + Logistic Regression\nX_emb = model.encode(papers)\nclf_emb = LogisticRegression(max_iter=1000)\nscores_emb = cross_val_score(clf_emb, X_emb, labels, cv=3)\n\nprint(\"Embeddings + Logistic Regression:\")\nprint(f\"  Cross-validation accuracy: {scores_emb.mean():.3f} ± {scores_emb.std():.3f}\")\n\n\nOutput:\nTF-IDF + Logistic Regression:\n  Cross-validation accuracy: 0.778 ± 0.095\n\nEmbeddings + Logistic Regression:\n  Cross-validation accuracy: 0.889 ± 0.048\nEmbeddings outperform TF-IDF, especially on small datasets where semantic understanding matters more than exact keyword matching.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Text Fundamentals: The Full Picture"
    ]
  },
  {
    "objectID": "m03-text/text-fundamentals.html#the-evolution-from-counts-to-context",
    "href": "m03-text/text-fundamentals.html#the-evolution-from-counts-to-context",
    "title": "Text Fundamentals: The Full Picture",
    "section": "",
    "text": "Let’s summarize the journey:\n\n\n\n\n\n\n\n\n\nMethod\nRepresentation\nPros\nCons\n\n\n\n\nBag-of-Words\nWord counts\nFast, interpretable\nNo semantics, sparse\n\n\nTF-IDF\nWeighted counts\nHandles common words\nStill no semantics\n\n\nWord2vec\nDense vectors (static)\nCaptures semantics\nNo context sensitivity\n\n\nTransformers\nDense vectors (contextual)\nBest performance\nSlow, complex\n\n\n\nThe progression: 1. 1960s-2000s: Count-based methods (BoW, TF-IDF) 2. 2013: Word2vec introduces learned dense embeddings 3. 2017: Transformers introduce contextual embeddings 4. 2018-present: Pre-trained transformers (BERT, GPT) dominate NLP\nEach advance addressed limitations of the previous generation while introducing new complexity.\n\n\n\n\n\n\nThe Practical Takeaway\n\n\n\nDon’t automatically reach for the most sophisticated method. Start simple: 1. Try TF-IDF + simple classifier 2. If performance is insufficient, try Word2vec 3. If still insufficient, use contextual embeddings 4. Only if necessary, fine-tune a transformer\nMost research tasks don’t need GPT-4. Often, TF-IDF is enough.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Text Fundamentals: The Full Picture"
    ]
  },
  {
    "objectID": "m03-text/text-fundamentals.html#the-bigger-picture",
    "href": "m03-text/text-fundamentals.html#the-bigger-picture",
    "title": "Text Fundamentals: The Full Picture",
    "section": "",
    "text": "You’ve now completed the full journey through text processing:\nWeek 1: You learned to use LLMs and engineer prompts Week 2: You learned how they work and where the technology came from\nYou can now: - Use LLMs effectively for research tasks - Extract and analyze embeddings - Understand transformers at an intuitive level - Choose appropriate methods for different tasks - Appreciate the evolution from word counts to neural language models\nOne final piece remains: Putting it all together. The next section shows you complete research workflows—from data collection to publication-ready analysis—using text processing for studying complex systems.\nLet’s finish strong with real examples.\n\nNext: Semantic Analysis for Research →",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Text Fundamentals: The Full Picture"
    ]
  },
  {
    "objectID": "m03-text/prompt-engineering.html",
    "href": "m03-text/prompt-engineering.html",
    "title": "Prompt Engineering",
    "section": "",
    "text": "You’ve learned to use LLMs for basic research tasks. But you’ve probably noticed something frustrating: the same question asked slightly differently can produce dramatically different results.\nOne prompt gives you exactly what you need. Another prompt makes the model hallucinate, ramble, or miss the point entirely. This isn’t a bug—it’s a fundamental property of how LLMs work. They’re highly sensitive to how you phrase your requests.\nPrompt engineering is the art and science of designing inputs that reliably produce desired outputs. For researchers, this means crafting prompts that extract accurate information, maintain consistency across large datasets, and fail gracefully when the model doesn’t know something.\n\n\n\n\n\n\n\nEffective prompts combine these components:\n\nInstruction: Clearly defines the task\nData: The input you want processed\nOutput Format: Specifies how the response should be structured\nPersona (optional): Who the model should “be”\nContext (optional): Background information that helps the model understand why the task matters, who the response is for, and any relevant constraints or circumstances\n\nWe’ll build a prompt progressively, adding components one at a time to see how each changes the output.\n\n\nThe most basic prompt consists of just two components: an instruction that defines the task, and data that provides the input to process.\n\ninstruction = \"Summarize this abstract\"\ndata = \"\"\"\nWe develop a graph neural network for predicting protein-protein interactions\nfrom sequence data. Our model uses attention mechanisms to identify functionally\nimportant amino acid subsequences. We achieve 89% accuracy on benchmark datasets,\noutperforming previous methods by 7%. The model also provides interpretable\nattention weights showing which protein regions drive predictions.\n\"\"\"\n\nprompt = f\"{instruction}. {data}\"\n\n\n\nCode\nimport ollama\n\nparams_llm = {\"model\": \"gemma3:270m\", \"options\": {\"temperature\": 0.3}}\n\nresponse = ollama.generate(prompt=prompt, **params_llm)\nprint(response.response)\n\n\nThis abstract describes a graph neural network (GNN) for predicting protein-protein interactions. The model utilizes attention mechanisms to identify crucial amino acid subsequences. Achieved accuracy of 89% on benchmark datasets, the model provides interpretable attention weights, and the network demonstrates superior performance compared to previous methods.\n\n\n\nThis basic prompt works, but output is often inconsistent—the model might produce a long summary, a short one, or vary the format. Let’s add structure.\n\n\n\nAdding an output format ensures consistency and makes outputs easier to process programmatically. Here’s the same prompt with an output format constraint:\n\noutput_format = \"\"\"Provide the summary in exactly 2 sentences:\n- First sentence: What problem and method\n- Second sentence: Key result with numbers\"\"\"\n\nprompt_with_format = f\"\"\"{instruction}. {data}. {output_format}\"\"\"\n\nObserving the change: The output format constraint produces structured, consistent output—crucial when processing hundreds of papers.\n\n\n\nA persona tells the LLM who it should be, which activates relevant patterns in the training data. Let’s use a different example to better demonstrate persona effects:\n\n# New example for persona demonstration\ninstruction = \"Help the customer reconnect to the service by providing troubleshooting instructions.\"\ndata = \"Customer: I cannot see any webpage. Need help ASAP!\"\noutput_format = \"Keep the response concise and polite. Provide a clear resolution in 2-3 sentences.\"\n\nformal_persona = \"You are a professional customer support agent who responds formally and ensures clarity and professionalism.\"\n\nprompt_with_persona = f\"\"\"{formal_persona}. {instruction}. {data}. {output_format}\"\"\"\n\n\n\nCode\nprint(\"BASE (no persona):\")\nprint(ollama.generate(prompt=instruction + \". \" + data + \". \" + output_format, **params_llm).response)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nprint(\"WITH PERSONA:\")\nprint(ollama.generate(prompt=prompt_with_persona, **params_llm).response)\n\n\nBASE (no persona):\nOkay, I understand. Let's try to troubleshoot this. Please provide the webpage and the specific error message you're seeing. Once I have that, I'll be happy to help you resolve the issue.\n\n\n============================================================\n\nWITH PERSONA:\nHello, I understand you cannot see any webpage. Could you please try re-explaining your problem? I'm here to assist you as quickly as possible.\n\n\n\nObserving the change: The persona shifts tone and style (formal vs. friendly). The formal persona produces professional, structured responses.\n\n\n\nContext provides additional information that helps the LLM understand why the task matters, who the response is for, and any relevant constraints or circumstances. Context can include: - Background information (why the task is important) - Audience information (who the response is for) - Constraints or special circumstances\nFirst, let’s add background context:\n\ncontext_background = \"\"\"The customer is extremely frustrated because their internet has been down for three days, and they need it for an important online job interview. They emphasize that 'This is a life-or-death situation for my career!'\"\"\"\n\nprompt_with_context = f\"\"\"{formal_persona}. {instruction}. {data}. {output_format}. Context: {context_background}\"\"\"\n\n\n\nCode\nprint(\"WITH PERSONA:\")\nprint(ollama.generate(prompt=prompt_with_persona, **params_llm).response)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nprint(\"WITH PERSONA + CONTEXT (background):\")\nprint(ollama.generate(prompt=prompt_with_context, **params_llm).response)\n\n\nWITH PERSONA:\n\"Thank you for contacting us. I understand you cannot see any webpage. Could you please try accessing the website again? We'll be happy to assist you further.\"\n\n============================================================\n\nWITH PERSONA + CONTEXT (background):\nThank you for contacting us. I understand your frustration with the internet outage and the need for this important job interview. We apologize for the inconvenience and are working diligently to resolve this issue. We will be sure to provide you with a detailed troubleshooting guide shortly.\n\n\n\nObserving the change: Background context adds urgency and emotional understanding, leading to more empathetic and appropriately prioritized responses.\nNow let’s add audience information as part of the context:\n\n# Context with audience information for non-technical user\ncontext_with_audience_nontech = f\"\"\"{context_background} The customer does not know any technical terms like modem, router, networks, etc.\"\"\"\n\ncontext_with_audience_tech = f\"\"\"{context_background} The customer is Head of IT Infrastructure of our company.\"\"\"\n\nprompt_with_context_nontech = f\"\"\"{formal_persona}. {instruction}. {data}. {output_format}. Context: {context_with_audience_nontech}\"\"\"\nprompt_with_context_tech = f\"\"\"{formal_persona}. {instruction}. {data}. {output_format}. Context: {context_with_audience_tech}\"\"\"\n\n\n\nCode\nprint(\"WITH PERSONA + CONTEXT (background only):\")\nprint(ollama.generate(prompt=prompt_with_context, **params_llm).response)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nprint(\"WITH PERSONA + CONTEXT (background + non-tech audience):\")\nprint(ollama.generate(prompt=prompt_with_context_nontech, **params_llm).response)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nprint(\"WITH PERSONA + CONTEXT (background + tech audience):\")\nprint(ollama.generate(prompt=prompt_with_context_tech, **params_llm).response)\n\n\nWITH PERSONA + CONTEXT (background only):\nHello, I understand your frustration regarding your internet connection. I apologize for the inconvenience this is causing. To help me troubleshoot this, could you please provide me with the specific error message or the URL of the webpage that is preventing you from accessing the online job application? I will do my best to assist you in finding a solution.\n\n\n============================================================\n\nWITH PERSONA + CONTEXT (background + non-tech audience):\n\"I understand your frustration, and I apologize for the inconvenience this is causing. To help me assist you, could you please tell me which web page you're having trouble seeing? Once I have that information, I can provide you with specific troubleshooting steps and a clear resolution.\"\n\n============================================================\n\nWITH PERSONA + CONTEXT (background + tech audience):\nDear [Customer Name],\n\nI understand your frustration with your internet outage. I'm sorry for the inconvenience this is causing. To help me troubleshoot this, could you please provide me with the exact error message you are seeing? I'll do my best to assist you.\n\n\n\nObserving the change: Including audience information in the context dramatically changes the technical level and terminology. For non-technical users, the response avoids jargon and uses simple explanations. For technical users, it uses precise technical terms and assumes background knowledge.\n\n\n\nHere’s a template combining all components:\n\nprompt_template = \"\"\"\n{persona}\n\n{instruction}\n\n{data}\n\nContext: {context}\n\n{output_format}\n\"\"\"\n\nNot every prompt needs all components. Choose based on your task: - Simple extraction: Instruction + Data + Output Format - Style-sensitive tasks: Add Persona - Complex scenarios: Add Context (can include background, audience, constraints, etc.)\n\n\n\n\n\n\nWhen Personas Help (and When They Don’t)\n\n\n\nResearch shows that adding personas can improve tone and style, but does not necessarily improve performance on factual tasks. In some cases, personas may even degrade performance or introduce biases.\nUse personas when: You need specific tone/style, responses tailored to an audience, or a particular perspective.\nAvoid personas when: You need maximum factual accuracy, the task is purely extraction/classification, or you’re concerned about bias introduction.\nAdditionally, when prompted to adopt specific socio-demographic personas, LLMs may produce responses that reflect societal stereotypes. Be careful when designing persona prompts to avoid reinforcing harmful biases.\nReferences: - When “A Helpful Assistant” Is Not Really Helpful - Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs\n\n\n\n\n\n\n\n\nContext and Emotion Prompting\n\n\n\nContext can include: - Background information: Why the task is important, what led to this request - Audience information: Who the response is for (technical level, expertise, role) - Emotional cues: Research shows that including emotional cues (e.g., “This is very important to my career”) can enhance response quality - Constraints: Special circumstances, deadlines, limitations\nHowever, avoid overloading with unnecessary information that distracts from the main task.\nReference: Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models\n\n\n\n\n\n\nInstead of just describing what you want, show the model examples. This is called “few-shot prompting” or “in-context learning.”\nTerminology: - Zero-shot: No examples (relies on model’s prior knowledge) - One-shot: One example - Few-shot: 2-5 examples (sweet spot for most tasks) - Many-shot: 10+ examples (diminishing returns, context limits)\n\n\nFirst, let’s see a zero-shot prompt (no examples):\n\nzero_shot_prompt = \"\"\"Extract the domain and methods from this abstract:\n\nAbstract: We apply reinforcement learning to optimize traffic flow in urban networks.\nUsing deep Q-networks trained on simulation data, we reduce average commute time by 15%.\n\nOutput format:\nDomain: ...\nMethods: ...\n\"\"\"\n\n\n\n\nNow let’s add examples to create a few-shot prompt:\n\nfew_shot_prompt = \"\"\"Extract the domain and methods from abstracts. Here are examples:\n\nExample 1:\nAbstract: We use CRISPR to edit genes in cancer cells, achieving 40% tumor reduction in mice.\nDomain: Cancer Biology\nMethods: CRISPR gene editing, mouse models\n\nExample 2:\nAbstract: We develop a transformer model for predicting solar flares from magnetogram images.\nDomain: Solar Physics, Machine Learning\nMethods: Transformer neural networks, image analysis\n\nNow extract from this abstract:\n\nAbstract: We apply reinforcement learning to optimize traffic flow in urban networks.\nUsing deep Q-networks trained on simulation data, we reduce average commute time by 15%.\n\nDomain: ...\nMethods: ...\n\"\"\"\n\n\n\nCode\nresponse_zero = ollama.generate(prompt=zero_shot_prompt, **params_llm)\nresponse_few = ollama.generate(prompt=few_shot_prompt, **params_llm)\n\nprint(\"ZERO-SHOT:\")\nprint(response_zero.response)\nprint(\"\\nFEW-SHOT:\")\nprint(response_few.response)\n\n\nZERO-SHOT:\nDomain: Urban networks\nMethods: Reinforcement Learning\n\nFEW-SHOT:\nHere's the extracted domain and methods from the abstract:\n\n*   **Domain:** Science\n*   **Methods:** Reinforcement Learning\n\n\nFew-shot prompting improves consistency, especially for extraction tasks. The examples teach the model what level of specificity you want, how to handle edge cases, and the exact format you expect.\n\n\n\n\n\n\nBiases in Few-Shot Prompting\n\n\n\nBe aware that few-shot examples can introduce biases:\n\nRecency bias: Models may favor the most recent examples. The order of examples matters!\nMajority label bias: If most examples have the same label/answer, the model may favor that label even when it’s not appropriate.\n\nTo mitigate: Vary the order of examples when testing, ensure examples are diverse and representative, and don’t overload examples with one particular pattern.\n\n\n\n\n\n\nFor complex tasks, ask the model to show its reasoning process before giving the final answer.\n\n\nFirst, let’s see a direct prompt that asks for the answer immediately:\n\npapers = \"\"\"\nPaper 1: Community detection in static networks using modularity optimization.\nPaper 2: Temporal network analysis with sliding windows.\nPaper 3: Hierarchical community structure in social networks.\n\"\"\"\n\ndirect_prompt = f\"\"\"Based on these paper titles, what research gap exists? Just give the answer, no explanation.\n\n{papers}\n\nGap: ...\n\"\"\"\n\n\n\n\nNow let’s add explicit reasoning steps:\n\ncot_prompt = f\"\"\"Based on these paper titles, identify a research gap. Think step by step.\n\nPapers:\n{papers}\n\nThink step by step:\n1. What does each paper focus on?\n2. What topics appear in multiple papers?\n3. What combination of topics is missing?\n4. What would be a valuable gap to fill?\n\nFinal answer: The research gap is...\n\"\"\"\n\n\n\nCode\nresponse_direct = ollama.generate(prompt=direct_prompt, **params_llm)\nresponse_cot = ollama.generate(prompt=cot_prompt, **params_llm)\n\nprint(\"DIRECT PROMPT:\")\nprint(response_direct.response)\nprint(\"\\nCHAIN-OF-THOUGHT:\")\nprint(response_cot.response)\n\n\nDIRECT PROMPT:\nThe gap is in the complexity of the algorithms used for community detection and the challenges associated with the sliding window approach.\n\n\nCHAIN-OF-THOUGHT:\nHere's the breakdown of the research gap identified:\n\n1.  **What does each paper focus on?**\n    *   **Paper 1:** Community detection in static networks using modularity optimization.\n    *   **Paper 2:** Temporal network analysis with sliding windows.\n    *   **Paper 3:** Hierarchical community structure in social networks.\n\n2.  **What topics appear in multiple papers?**\n    *   **Paper 1:** Community detection in static networks using modularity optimization.\n    *   **Paper 2:** Temporal network analysis with sliding windows.\n    *   **Paper 3:** Hierarchical community structure in social networks.\n\n3.  **What combination of topics is missing?**\n    *   **Paper 1:** Community detection in static networks using modularity optimization.\n    *   **Paper 2:** Temporal network analysis with sliding windows.\n    *   **Paper 3:** Hierarchical community structure in social networks.\n\n4.  **What would be a valuable gap to fill?**\n    *   **Paper 1:** Community detection in static networks using modularity optimization.\n    *   **Paper 2:** Temporal network analysis with sliding windows.\n    *   **Paper 3:** Hierarchical community structure in social networks.\n\nFinal answer: The research gap is **Community detection in static networks using modularity optimization**.\n\n\nChain-of-thought produces more thoughtful, nuanced answers by forcing the model to decompose the problem into steps.\nWhen to use: Comparing multiple papers/concepts, identifying patterns, making recommendations, analyzing arguments, complex reasoning tasks.\nWhen not to use: Simple extraction tasks, when you need concise outputs, time-critical applications (it’s slower).\n\n\n\n\n\n\nCan We Trust Chain-of-Thought Reasoning?\n\n\n\nResearch indicates that chain-of-thought reasoning can be unfaithful—the explanations don’t always accurately reflect the model’s true decision-making process. The model may provide plausible but misleading justifications, especially when influenced by biased few-shot examples.\nAlways validate the final answer independently rather than trusting the reasoning process alone.\n\n\n\n\n\n\nFor research workflows, you often need structured data you can parse programmatically.\n\n\nFirst, let’s create a prompt that requests JSON output:\n\nimport json\nfrom pydantic import BaseModel\n\nabstract = \"\"\"\nWe analyze 10,000 scientific collaborations using network analysis and machine\nlearning. Our random forest classifier predicts collaboration success with 76%\naccuracy. Key factors include prior co-authorship and institutional proximity.\n\"\"\"\n\nprompt_json = f\"\"\"Extract information from this abstract and return ONLY valid JSON:\n\nAbstract: {abstract}\n\nReturn this exact structure:\n{{\n  \"n_samples\": &lt;number or null&gt;,\n  \"methods\": [&lt;list of methods&gt;],\n  \"accuracy\": &lt;number or null&gt;,\n  \"domain\": \"&lt;research field&gt;\"\n}}\n\nJSON:\"\"\"\n\n\n\nCode\n# Use lower temperature for structured output\nparams_structured = {\"model\": \"gemma3n:latest\", \"options\": {\"temperature\": 0}}\nresponse = ollama.generate(prompt=prompt_json, **params_structured)\n\ntry:\n    data = json.loads(response.response)\n    print(\"Extracted data:\")\n    print(json.dumps(data, indent=2))\nexcept json.JSONDecodeError:\n    print(\"Failed to parse JSON. Raw output:\")\n    print(response.response)\n\n\nFailed to parse JSON. Raw output:\n```json\n{\n \"n_samples\": 10000,\n \"methods\": [\"network analysis\", \"machine learning\", \"random forest\"],\n \"accuracy\": 76,\n \"domain\": \"scientific collaborations\"\n}\n```\n\n\n\n\n\nFor more reliable structured output, use JSON schema constraints that enforce the format during token generation. First, define the schema:\n\nfrom pydantic import BaseModel\n\nclass PaperMetadata(BaseModel):\n    domain: str\n    methods: list[str]\n    n_samples: int | None\n    accuracy: float | None\n\njson_schema = PaperMetadata.model_json_schema()\n\nNow create a simpler prompt (the schema enforces the format):\n\nprompt_schema = f\"\"\"Extract information from this abstract:\n\nAbstract: {abstract}\"\"\"\n\n\n\nCode\nresponse = ollama.generate(prompt=prompt_schema, format=json_schema, **params_structured)\n\ntry:\n    data = json.loads(response.response)\n    metadata = PaperMetadata(**data)\n    print(\"Extracted and validated data:\")\n    print(json.dumps(data, indent=2))\nexcept (json.JSONDecodeError, ValueError) as e:\n    print(f\"Error: {e}\")\n    print(\"Raw output:\", response.response)\n\n\nExtracted and validated data:\n{\n  \"domain\": \"Scientific Collaborations\",\n  \"methods\": [\n    \"Network Analysis\",\n    \"Machine Learning\",\n    \"Random Forest Classifier\"\n  ],\n  \"n_samples\": 10000,\n  \"accuracy\": 76.0\n}\n\n\nUsing JSON schema constraints is more reliable than just requesting JSON, as the model is forced to follow the structure during generation.\n\n\n\n\n\n\nJSON Parsing Reliability\n\n\n\nSmaller models (like Gemma 3N) sometimes produce invalid JSON even with schema constraints. Always wrap parsing in try-except blocks and validate outputs. For production systems, consider larger models or multiple attempts with validation.\n\n\n\n\n\n\nLLMs will confidently make up facts when they don’t know the answer. You need to give them permission to say “I don’t know.”\n\n\nHere’s a prompt that doesn’t allow for uncertainty:\n\nbad_prompt = \"\"\"Summarize the main findings from the 2023 paper by Johnson et al.\non quantum community detection in biological networks.\"\"\"\n\n\n\n\nNow let’s modify it to explicitly allow uncertainty:\n\ngood_prompt = \"\"\"I'm looking for a 2023 paper by Johnson et al. on quantum\ncommunity detection in biological networks.\n\nIf you know this paper, summarize its main findings.\nIf you're not certain this paper exists, say \"I cannot verify this paper exists\"\nand do NOT make up details.\n\nResponse:\"\"\"\n\n\n\nCode\nresponse_bad = ollama.generate(prompt=bad_prompt, **params_llm)\nresponse_good = ollama.generate(prompt=good_prompt, **params_llm)\n\nprint(\"BAD PROMPT (encourages hallucination):\")\nprint(response_bad.response)\nprint(\"\\nGOOD PROMPT (allows uncertainty):\")\nprint(response_good.response)\n\n\nBAD PROMPT (encourages hallucination):\nThe 2023 paper by Johnson et al. on quantum community detection in biological networks, titled \"Quantum Community Detection in Biological Networks,\" investigated the effectiveness of quantum-based detection methods for identifying and characterizing biological networks. The study focused on the use of quantum algorithms to detect and characterize biological networks, including networks of interconnected cells, networks of cells interacting with each other, and networks of cells that are not directly connected. The authors employed a variety of quantum detection techniques, including quantum interference, quantum noise, and quantum-enhanced detection. They demonstrated the effectiveness of these methods in detecting networks of cells, highlighting the potential of quantum detection for improving the accuracy and robustness of biological network detection.\n\nGOOD PROMPT (allows uncertainty):\nI cannot verify this paper exists.\n\n\n\nStrategies for handling uncertainty: - Explicitly say “If you don’t know, say so” - Ask for confidence levels (“How confident are you?”) - Request citations or sources (though models often still hallucinate) - Cross-validate critical information with external sources\n\n\n\n\n\n\nBe a Good “Boss” to Your LLM\n\n\n\nLet LLMs admit ignorance: LLMs closely follow your instructions—even when they shouldn’t. They often attempt to answer beyond their actual capabilities. Explicitly tell your model: “If you don’t know the answer, just say so,” or “If you need more information, please ask.”\nEncourage critical feedback: LLMs are trained to be agreeable, which can hinder productive brainstorming or honest critique. Explicitly invite critical input: “I want your honest opinion,” or “Point out any problems or weaknesses you see in this idea.”\n\n\n\n\n\nFor tasks requiring reasoning, generate multiple responses and take the most common answer. First, define the prompt:\n\nfrom collections import Counter\n\nprompt_consistency = \"\"\"Three papers study network robustness:\n- Paper A: Targeted attacks are most damaging\n- Paper B: Random failures rarely cause collapse\n- Paper C: Hub nodes are critical for robustness\n\nWhat is the research consensus on network robustness? Give a one-sentence answer.\n\"\"\"\n\nNow generate multiple responses with higher temperature for diversity:\n\n\nCode\n# Use higher temperature for diversity\nparams_creative = {\"model\": \"gemma3n:latest\", \"options\": {\"temperature\": 0.7}}\n\n# Generate 5 responses\nresponses = []\nfor i in range(5):\n    response = ollama.generate(prompt=prompt_consistency, **params_creative)\n    responses.append(response.response.strip())\n    print(f\"Response {i+1}: {responses[-1]}\\n\")\n\n# In practice, you'd programmatically identify the most common theme\nprint(\"The most consistent theme across responses would be selected.\")\n\n\nResponse 1: The research consensus on network robustness is that it's a complex issue influenced by factors like targeted attacks, random failures, and the importance of critical nodes (hubs), suggesting a multifaceted approach is needed to understand and improve network resilience.\n\nResponse 2: The research consensus on network robustness is that it's a complex issue influenced by both targeted attacks and random failures, with the vulnerability of critical hub nodes playing a significant role in overall network resilience.\n\nResponse 3: The research consensus on network robustness is that it's a complex issue influenced by factors like targeted attacks, random failures, and the importance of critical nodes, suggesting a multifaceted approach is needed to understand and improve network resilience.\n\nResponse 4: The research consensus on network robustness is that it's a complex issue influenced by various factors, including the vulnerability of targeted attacks, the resilience to random failures, and the importance of critical nodes like hubs.\n\nResponse 5: The research consensus on network robustness is that while targeted attacks can be highly damaging, the resilience of a network also critically depends on the presence and functionality of key nodes (hubs), and random failures are generally less likely to cause catastrophic collapse.\n\nThe most consistent theme across responses would be selected.\n\n\nSelf-consistency works because correct reasoning tends to lead to the same answer, while hallucinations are often random and inconsistent. Trade-off: Generating multiple responses means 5x the API calls = 5x the cost/time. Use sparingly for critical decisions.\n\n\n\n\n\n\nAlternative: Tree of Thought\n\n\n\nFor even more sophisticated exploration, you can use “Tree of Thought” prompting, where the model explicitly explores multiple reasoning paths, evaluates them, and selects the best one. This is more complex to implement but can yield better results for very difficult problems.\n\n\n\n\n\n\nLet’s combine techniques to build a complete workflow:\n\n# Use low temperature for consistency\nparams_classifier = {\"model\": \"gemma3n:latest\", \"options\": {\"temperature\": 0}}\n\ndef classify_paper(abstract):\n    \"\"\"Classify a paper's methodology type.\"\"\"\n    prompt = \"\"\"Classify research papers by methodology type.\n\nCategories:\n- Experimental: Lab/field experiments, data collection\n- Theoretical: Mathematical models, proofs, frameworks\n- Computational: Simulations, algorithms, data analysis\n- Review: Literature reviews, meta-analyses, surveys\n\nExamples:\n\nAbstract: We prove a lower bound on the complexity of community detection algorithms.\nClassification: Theoretical\nReasoning: Focuses on mathematical proof.\n\nAbstract: We conduct surveys with 500 participants to study social network formation.\nClassification: Experimental\nReasoning: Original data collection through experiments.\n\nAbstract: We review 150 papers on graph neural networks and identify future directions.\nClassification: Review\nReasoning: Surveys existing literature.\n\nNow classify:\n\nAbstract: {abstract}\n\nThink step by step:\n1. What is the primary activity? (proving, measuring, simulating, surveying?)\n2. Which category fits best?\n\nClassification: ...\nReasoning: ...\n\"\"\"\n    response = ollama.generate(prompt=prompt.format(abstract=abstract), **params_classifier)\n    return response.response\n\n# Test with different paper types\ntest_abstracts = [\n    \"We develop a graph neural network that predicts protein folding with 85% accuracy.\",\n    \"We mathematically prove that scale-free networks are robust to random failures.\",\n    \"We survey 200 papers on community detection and identify 5 major approaches.\",\n]\n\nfor abstract in test_abstracts:\n    print(f\"Abstract: {abstract}\")\n    print(classify_paper(abstract))\n    print(\"-\" * 80)\n\nAbstract: We develop a graph neural network that predicts protein folding with 85% accuracy.\nClassification: Computational\nReasoning: The abstract explicitly mentions \"develop a graph neural network\" and \"predicts protein folding with 85% accuracy.\" This indicates the use of simulations and algorithms to solve a problem, which falls under computational methodology.\n\n--------------------------------------------------------------------------------\nAbstract: We mathematically prove that scale-free networks are robust to random failures.\nClassification: Theoretical\nReasoning: The abstract explicitly states a mathematical proof. This aligns directly with the definition of a theoretical research paper, which focuses on mathematical models and proofs.\n\n--------------------------------------------------------------------------------\nAbstract: We survey 200 papers on community detection and identify 5 major approaches.\nClassification: Review\nReasoning: The abstract explicitly states a survey of existing papers (200 papers) and identification of approaches. This aligns directly with the definition of a literature review.\n\n--------------------------------------------------------------------------------\n\n\nThis workflow uses: Few-shot learning (examples), Chain-of-thought (step-by-step reasoning), Constrained format (Classification + Reasoning), and Low temperature (consistency).\n\n\n\nYou’ve now learned to talk to LLMs effectively. You can:\n\nCraft specific, well-structured prompts\nUse few-shot learning to teach by example\nApply chain-of-thought for complex reasoning\nExtract structured data for research pipelines\nHandle uncertainty and edge cases gracefully\n\nBut a question remains: how do these models represent and “understand” text internally? When you send a prompt, the model doesn’t see English words—it sees numbers. Millions of numbers arranged in high-dimensional space.\nThese numbers are called embeddings, and they’re the foundation of everything LLMs do. Let’s unbox the first layer and see how meaning becomes mathematics.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Prompt Engineering for Research"
    ]
  },
  {
    "objectID": "m03-text/prompt-engineering.html#why-prompts-matter",
    "href": "m03-text/prompt-engineering.html#why-prompts-matter",
    "title": "Prompt Engineering for Research",
    "section": "",
    "text": "LLMs don’t have “understanding” in the human sense. They predict probable text continuations based on patterns in training data. Your prompt sets the context that determines which patterns the model activates.\nConsider these two prompts for the same task:\nPrompt A: “What’s community detection?”\nPrompt B: “You are a network science expert. Explain community detection in networks to a graduate student familiar with graph theory. Focus on the intuition, not the math. Keep it under 100 words.”\nPrompt B will typically produce better results because it:\n\nSets a role (“network science expert”)\nDefines the audience (“graduate student familiar with graph theory”)\nSpecifies the style (“intuition, not math”)\nConstrains the output (“under 100 words”)\n\nLet’s learn to craft prompts like Prompt B systematically.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Prompt Engineering for Research"
    ]
  },
  {
    "objectID": "m03-text/prompt-engineering.html#core-principle-1-be-specific",
    "href": "m03-text/prompt-engineering.html#core-principle-1-be-specific",
    "title": "Prompt Engineering for Research",
    "section": "",
    "text": "Vague prompts produce vague outputs. Specific prompts produce specific outputs.\n\n\nBefore diving into examples, let’s understand the fundamental components of a well-structured prompt. Most effective prompts combine these elements:\n\nInstruction: Clearly defines the task\nData: The input you want processed\nOutput Format: Specifies how the response should be structured\nPersona (optional): Who the model should “be”\nContext (optional): Background information that helps the model understand why\nAudience (optional): Who the response is for\n\nNot every prompt needs all components, but understanding these building blocks helps you construct better prompts systematically.\n\n\n\n\n\nCode\nimport ollama\n\nparams_llm = {\"model\": \"gemma3n:latest\", \"options\": {\"temperature\": 0.3}}\n\nabstract = \"\"\"\nWe develop a graph neural network for predicting protein-protein interactions\nfrom sequence data. Our model uses attention mechanisms to identify functionally\nimportant amino acid subsequences. We achieve 89% accuracy on benchmark datasets,\noutperforming previous methods by 7%. The model also provides interpretable\nattention weights showing which protein regions drive predictions.\n\"\"\"\n\n# Vague prompt\nvague_prompt = \"Summarize this abstract.\"\n\n# Specific prompt\nspecific_prompt = \"\"\"Summarize this abstract in exactly 2 sentences:\n- First sentence: What problem and method\n- Second sentence: Key result with numbers\n\nAbstract: {abstract}\n\"\"\"\n\nresponse_vague = ollama.generate(\n    prompt=vague_prompt.format(abstract=abstract),\n    **params_llm\n)\n\nresponse_specific = ollama.generate(\n    prompt=specific_prompt.format(abstract=abstract),\n    **params_llm\n)\n\nprint(\"VAGUE PROMPT OUTPUT:\")\nprint(response_vague.response)\nprint(\"\\nSPECIFIC PROMPT OUTPUT:\")\nprint(response_specific.response)\n\n\nOutput:\nVAGUE PROMPT OUTPUT:\nThis research presents a graph neural network that predicts protein-protein\ninteractions with high accuracy and interpretability. [... possibly more rambling]\n\nSPECIFIC PROMPT OUTPUT:\nThis paper develops a graph neural network with attention mechanisms to predict\nprotein-protein interactions from sequence data. The model achieves 89% accuracy,\noutperforming previous methods by 7%, and provides interpretable attention weights.\nThe specific prompt produces structured, information-dense output. This matters when processing hundreds of papers—you want consistent format and length.\n\n\n\n\n\n\nSpecificity Checklist\n\n\n\n\nWhat format should the output take?\nHow long should it be?\nWhat information must be included?\nWhat should be excluded?\nWhat perspective or style?",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Prompt Engineering for Research"
    ]
  },
  {
    "objectID": "m03-text/prompt-engineering.html#core-principle-2-provide-examples-few-shot-learning",
    "href": "m03-text/prompt-engineering.html#core-principle-2-provide-examples-few-shot-learning",
    "title": "Prompt Engineering",
    "section": "",
    "text": "Instead of just describing what you want, show the model examples. This is called “few-shot prompting” or “in-context learning.”\nTerminology: - Zero-shot: No examples (relies on model’s prior knowledge) - One-shot: One example - Few-shot: 2-5 examples (sweet spot for most tasks) - Many-shot: 10+ examples (diminishing returns, context limits)\n\n\nFirst, let’s see a zero-shot prompt (no examples):\n\nzero_shot_prompt = \"\"\"Extract the domain and methods from this abstract:\n\nAbstract: We apply reinforcement learning to optimize traffic flow in urban networks.\nUsing deep Q-networks trained on simulation data, we reduce average commute time by 15%.\n\nOutput format:\nDomain: ...\nMethods: ...\n\"\"\"\n\n\n\n\nNow let’s add examples to create a few-shot prompt:\n\nfew_shot_prompt = \"\"\"Extract the domain and methods from abstracts. Here are examples:\n\nExample 1:\nAbstract: We use CRISPR to edit genes in cancer cells, achieving 40% tumor reduction in mice.\nDomain: Cancer Biology\nMethods: CRISPR gene editing, mouse models\n\nExample 2:\nAbstract: We develop a transformer model for predicting solar flares from magnetogram images.\nDomain: Solar Physics, Machine Learning\nMethods: Transformer neural networks, image analysis\n\nNow extract from this abstract:\n\nAbstract: We apply reinforcement learning to optimize traffic flow in urban networks.\nUsing deep Q-networks trained on simulation data, we reduce average commute time by 15%.\n\nDomain: ...\nMethods: ...\n\"\"\"\n\n\n\nCode\nresponse_zero = ollama.generate(prompt=zero_shot_prompt, **params_llm)\nresponse_few = ollama.generate(prompt=few_shot_prompt, **params_llm)\n\nprint(\"ZERO-SHOT:\")\nprint(response_zero.response)\nprint(\"\\nFEW-SHOT:\")\nprint(response_few.response)\n\n\nZERO-SHOT:\nDomain: Urban networks\nMethods: Reinforcement Learning\n\nFEW-SHOT:\nHere's the extracted domain and methods from the abstract:\n\n*   **Domain:** Science\n*   **Methods:** Reinforcement Learning\n\n\nFew-shot prompting improves consistency, especially for extraction tasks. The examples teach the model what level of specificity you want, how to handle edge cases, and the exact format you expect.\n\n\n\n\n\n\nBiases in Few-Shot Prompting\n\n\n\nBe aware that few-shot examples can introduce biases:\n\nRecency bias: Models may favor the most recent examples. The order of examples matters!\nMajority label bias: If most examples have the same label/answer, the model may favor that label even when it’s not appropriate.\n\nTo mitigate: Vary the order of examples when testing, ensure examples are diverse and representative, and don’t overload examples with one particular pattern.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Prompt Engineering for Research"
    ]
  },
  {
    "objectID": "m03-text/prompt-engineering.html#core-principle-3-chain-of-thought-reasoning",
    "href": "m03-text/prompt-engineering.html#core-principle-3-chain-of-thought-reasoning",
    "title": "Prompt Engineering",
    "section": "",
    "text": "For complex tasks, ask the model to show its reasoning process before giving the final answer.\n\n\nFirst, let’s see a direct prompt that asks for the answer immediately:\n\npapers = \"\"\"\nPaper 1: Community detection in static networks using modularity optimization.\nPaper 2: Temporal network analysis with sliding windows.\nPaper 3: Hierarchical community structure in social networks.\n\"\"\"\n\ndirect_prompt = f\"\"\"Based on these paper titles, what research gap exists? Just give the answer, no explanation.\n\n{papers}\n\nGap: ...\n\"\"\"\n\n\n\n\nNow let’s add explicit reasoning steps:\n\ncot_prompt = f\"\"\"Based on these paper titles, identify a research gap. Think step by step.\n\nPapers:\n{papers}\n\nThink step by step:\n1. What does each paper focus on?\n2. What topics appear in multiple papers?\n3. What combination of topics is missing?\n4. What would be a valuable gap to fill?\n\nFinal answer: The research gap is...\n\"\"\"\n\n\n\nCode\nresponse_direct = ollama.generate(prompt=direct_prompt, **params_llm)\nresponse_cot = ollama.generate(prompt=cot_prompt, **params_llm)\n\nprint(\"DIRECT PROMPT:\")\nprint(response_direct.response)\nprint(\"\\nCHAIN-OF-THOUGHT:\")\nprint(response_cot.response)\n\n\nDIRECT PROMPT:\nThe gap is in the complexity of the algorithms used for community detection and the challenges associated with the sliding window approach.\n\n\nCHAIN-OF-THOUGHT:\nHere's the breakdown of the research gap identified:\n\n1.  **What does each paper focus on?**\n    *   **Paper 1:** Community detection in static networks using modularity optimization.\n    *   **Paper 2:** Temporal network analysis with sliding windows.\n    *   **Paper 3:** Hierarchical community structure in social networks.\n\n2.  **What topics appear in multiple papers?**\n    *   **Paper 1:** Community detection in static networks using modularity optimization.\n    *   **Paper 2:** Temporal network analysis with sliding windows.\n    *   **Paper 3:** Hierarchical community structure in social networks.\n\n3.  **What combination of topics is missing?**\n    *   **Paper 1:** Community detection in static networks using modularity optimization.\n    *   **Paper 2:** Temporal network analysis with sliding windows.\n    *   **Paper 3:** Hierarchical community structure in social networks.\n\n4.  **What would be a valuable gap to fill?**\n    *   **Paper 1:** Community detection in static networks using modularity optimization.\n    *   **Paper 2:** Temporal network analysis with sliding windows.\n    *   **Paper 3:** Hierarchical community structure in social networks.\n\nFinal answer: The research gap is **Community detection in static networks using modularity optimization**.\n\n\nChain-of-thought produces more thoughtful, nuanced answers by forcing the model to decompose the problem into steps.\nWhen to use: Comparing multiple papers/concepts, identifying patterns, making recommendations, analyzing arguments, complex reasoning tasks.\nWhen not to use: Simple extraction tasks, when you need concise outputs, time-critical applications (it’s slower).\n\n\n\n\n\n\nCan We Trust Chain-of-Thought Reasoning?\n\n\n\nResearch indicates that chain-of-thought reasoning can be unfaithful—the explanations don’t always accurately reflect the model’s true decision-making process. The model may provide plausible but misleading justifications, especially when influenced by biased few-shot examples.\nAlways validate the final answer independently rather than trusting the reasoning process alone.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Prompt Engineering for Research"
    ]
  },
  {
    "objectID": "m03-text/prompt-engineering.html#core-principle-4-constrain-the-output-format",
    "href": "m03-text/prompt-engineering.html#core-principle-4-constrain-the-output-format",
    "title": "Prompt Engineering",
    "section": "",
    "text": "For research workflows, you often need structured data you can parse programmatically.\n\n\nFirst, let’s create a prompt that requests JSON output:\n\nimport json\nfrom pydantic import BaseModel\n\nabstract = \"\"\"\nWe analyze 10,000 scientific collaborations using network analysis and machine\nlearning. Our random forest classifier predicts collaboration success with 76%\naccuracy. Key factors include prior co-authorship and institutional proximity.\n\"\"\"\n\nprompt_json = f\"\"\"Extract information from this abstract and return ONLY valid JSON:\n\nAbstract: {abstract}\n\nReturn this exact structure:\n{{\n  \"n_samples\": &lt;number or null&gt;,\n  \"methods\": [&lt;list of methods&gt;],\n  \"accuracy\": &lt;number or null&gt;,\n  \"domain\": \"&lt;research field&gt;\"\n}}\n\nJSON:\"\"\"\n\n\n\nCode\n# Use lower temperature for structured output\nparams_structured = {\"model\": \"gemma3n:latest\", \"options\": {\"temperature\": 0}}\nresponse = ollama.generate(prompt=prompt_json, **params_structured)\n\ntry:\n    data = json.loads(response.response)\n    print(\"Extracted data:\")\n    print(json.dumps(data, indent=2))\nexcept json.JSONDecodeError:\n    print(\"Failed to parse JSON. Raw output:\")\n    print(response.response)\n\n\nFailed to parse JSON. Raw output:\n```json\n{\n \"n_samples\": 10000,\n \"methods\": [\"network analysis\", \"machine learning\", \"random forest\"],\n \"accuracy\": 76,\n \"domain\": \"scientific collaborations\"\n}\n```\n\n\n\n\n\nFor more reliable structured output, use JSON schema constraints that enforce the format during token generation. First, define the schema:\n\nfrom pydantic import BaseModel\n\nclass PaperMetadata(BaseModel):\n    domain: str\n    methods: list[str]\n    n_samples: int | None\n    accuracy: float | None\n\njson_schema = PaperMetadata.model_json_schema()\n\nNow create a simpler prompt (the schema enforces the format):\n\nprompt_schema = f\"\"\"Extract information from this abstract:\n\nAbstract: {abstract}\"\"\"\n\n\n\nCode\nresponse = ollama.generate(prompt=prompt_schema, format=json_schema, **params_structured)\n\ntry:\n    data = json.loads(response.response)\n    metadata = PaperMetadata(**data)\n    print(\"Extracted and validated data:\")\n    print(json.dumps(data, indent=2))\nexcept (json.JSONDecodeError, ValueError) as e:\n    print(f\"Error: {e}\")\n    print(\"Raw output:\", response.response)\n\n\nExtracted and validated data:\n{\n  \"domain\": \"Scientific Collaborations\",\n  \"methods\": [\n    \"Network Analysis\",\n    \"Machine Learning\",\n    \"Random Forest Classifier\"\n  ],\n  \"n_samples\": 10000,\n  \"accuracy\": 76.0\n}\n\n\nUsing JSON schema constraints is more reliable than just requesting JSON, as the model is forced to follow the structure during generation.\n\n\n\n\n\n\nJSON Parsing Reliability\n\n\n\nSmaller models (like Gemma 3N) sometimes produce invalid JSON even with schema constraints. Always wrap parsing in try-except blocks and validate outputs. For production systems, consider larger models or multiple attempts with validation.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Prompt Engineering for Research"
    ]
  },
  {
    "objectID": "m03-text/prompt-engineering.html#core-principle-5-handle-uncertainty-explicitly",
    "href": "m03-text/prompt-engineering.html#core-principle-5-handle-uncertainty-explicitly",
    "title": "Prompt Engineering",
    "section": "",
    "text": "LLMs will confidently make up facts when they don’t know the answer. You need to give them permission to say “I don’t know.”\n\n\nHere’s a prompt that doesn’t allow for uncertainty:\n\nbad_prompt = \"\"\"Summarize the main findings from the 2023 paper by Johnson et al.\non quantum community detection in biological networks.\"\"\"\n\n\n\n\nNow let’s modify it to explicitly allow uncertainty:\n\ngood_prompt = \"\"\"I'm looking for a 2023 paper by Johnson et al. on quantum\ncommunity detection in biological networks.\n\nIf you know this paper, summarize its main findings.\nIf you're not certain this paper exists, say \"I cannot verify this paper exists\"\nand do NOT make up details.\n\nResponse:\"\"\"\n\n\n\nCode\nresponse_bad = ollama.generate(prompt=bad_prompt, **params_llm)\nresponse_good = ollama.generate(prompt=good_prompt, **params_llm)\n\nprint(\"BAD PROMPT (encourages hallucination):\")\nprint(response_bad.response)\nprint(\"\\nGOOD PROMPT (allows uncertainty):\")\nprint(response_good.response)\n\n\nBAD PROMPT (encourages hallucination):\nThe 2023 paper by Johnson et al. on quantum community detection in biological networks, titled \"Quantum Community Detection in Biological Networks,\" investigated the effectiveness of quantum-based detection methods for identifying and characterizing biological networks. The study focused on the use of quantum algorithms to detect and characterize biological networks, including networks of interconnected cells, networks of cells interacting with each other, and networks of cells that are not directly connected. The authors employed a variety of quantum detection techniques, including quantum interference, quantum noise, and quantum-enhanced detection. They demonstrated the effectiveness of these methods in detecting networks of cells, highlighting the potential of quantum detection for improving the accuracy and robustness of biological network detection.\n\nGOOD PROMPT (allows uncertainty):\nI cannot verify this paper exists.\n\n\n\nStrategies for handling uncertainty: - Explicitly say “If you don’t know, say so” - Ask for confidence levels (“How confident are you?”) - Request citations or sources (though models often still hallucinate) - Cross-validate critical information with external sources\n\n\n\n\n\n\nBe a Good “Boss” to Your LLM\n\n\n\nLet LLMs admit ignorance: LLMs closely follow your instructions—even when they shouldn’t. They often attempt to answer beyond their actual capabilities. Explicitly tell your model: “If you don’t know the answer, just say so,” or “If you need more information, please ask.”\nEncourage critical feedback: LLMs are trained to be agreeable, which can hinder productive brainstorming or honest critique. Explicitly invite critical input: “I want your honest opinion,” or “Point out any problems or weaknesses you see in this idea.”\n\n\n\n\n\nFor tasks requiring reasoning, generate multiple responses and take the most common answer. First, define the prompt:\n\nfrom collections import Counter\n\nprompt_consistency = \"\"\"Three papers study network robustness:\n- Paper A: Targeted attacks are most damaging\n- Paper B: Random failures rarely cause collapse\n- Paper C: Hub nodes are critical for robustness\n\nWhat is the research consensus on network robustness? Give a one-sentence answer.\n\"\"\"\n\nNow generate multiple responses with higher temperature for diversity:\n\n\nCode\n# Use higher temperature for diversity\nparams_creative = {\"model\": \"gemma3n:latest\", \"options\": {\"temperature\": 0.7}}\n\n# Generate 5 responses\nresponses = []\nfor i in range(5):\n    response = ollama.generate(prompt=prompt_consistency, **params_creative)\n    responses.append(response.response.strip())\n    print(f\"Response {i+1}: {responses[-1]}\\n\")\n\n# In practice, you'd programmatically identify the most common theme\nprint(\"The most consistent theme across responses would be selected.\")\n\n\nResponse 1: The research consensus on network robustness is that it's a complex issue influenced by factors like targeted attacks, random failures, and the importance of critical nodes (hubs), suggesting a multifaceted approach is needed to understand and improve network resilience.\n\nResponse 2: The research consensus on network robustness is that it's a complex issue influenced by both targeted attacks and random failures, with the vulnerability of critical hub nodes playing a significant role in overall network resilience.\n\nResponse 3: The research consensus on network robustness is that it's a complex issue influenced by factors like targeted attacks, random failures, and the importance of critical nodes, suggesting a multifaceted approach is needed to understand and improve network resilience.\n\nResponse 4: The research consensus on network robustness is that it's a complex issue influenced by various factors, including the vulnerability of targeted attacks, the resilience to random failures, and the importance of critical nodes like hubs.\n\nResponse 5: The research consensus on network robustness is that while targeted attacks can be highly damaging, the resilience of a network also critically depends on the presence and functionality of key nodes (hubs), and random failures are generally less likely to cause catastrophic collapse.\n\nThe most consistent theme across responses would be selected.\n\n\nSelf-consistency works because correct reasoning tends to lead to the same answer, while hallucinations are often random and inconsistent. Trade-off: Generating multiple responses means 5x the API calls = 5x the cost/time. Use sparingly for critical decisions.\n\n\n\n\n\n\nAlternative: Tree of Thought\n\n\n\nFor even more sophisticated exploration, you can use “Tree of Thought” prompting, where the model explicitly explores multiple reasoning paths, evaluates them, and selects the best one. This is more complex to implement but can yield better results for very difficult problems.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Prompt Engineering for Research"
    ]
  },
  {
    "objectID": "m03-text/prompt-engineering.html#advanced-technique-1-role-playing-and-personas",
    "href": "m03-text/prompt-engineering.html#advanced-technique-1-role-playing-and-personas",
    "title": "Prompt Engineering for Research",
    "section": "",
    "text": "Setting a role (persona) can dramatically improve output quality by activating relevant patterns in the training data. However, it’s important to understand when personas help and when they don’t.\n\n\nCode\nimport ollama\n\nparams_llm = {\"model\": \"gemma3n:latest\", \"options\": {\"temperature\": 0.3}}\n\nabstract = \"\"\"\nWe propose a novel algorithm for detecting communities in networks with overlapping\nmembership. Using non-negative matrix factorization, we decompose the adjacency\nmatrix into low-rank factors. Experiments show our method handles overlaps better\nthan previous approaches.\n\"\"\"\n\n# No role\nno_role = f\"Explain this abstract simply:\\n{abstract}\"\n\n# With role\nwith_role = f\"\"\"You are a network science professor explaining a paper to first-year\nPhD students who just learned about graphs and matrices but haven't seen community\ndetection yet.\n\nExplain this abstract in simple terms, using analogies where helpful:\n\n{abstract}\n\nExplanation:\"\"\"\n\nresponse_no_role = ollama.generate(\n    prompt=no_role,\n    **params_llm\n)\n\nresponse_with_role = ollama.generate(\n    prompt=with_role,\n    **params_llm\n)\n\nprint(\"NO ROLE:\")\nprint(response_no_role.response)\nprint(\"\\nWITH ROLE:\")\nprint(response_with_role.response)\n\n\nThe role-playing prompt produces explanations better tailored to the specified audience.\nUseful roles for research: - “You are a domain expert in [field]…” - “You are a critical peer reviewer…” - “You are a research librarian helping find sources…” - “You are a careful fact-checker…”\n\n\n\n\n\n\nWhen Personas Help (and When They Don’t)\n\n\n\nResearch shows that adding personas can improve tone and style, but does not necessarily improve performance on factual tasks. In some cases, personas may even degrade performance or introduce biases.\nUse personas when: - You need specific tone or style (formal, friendly, technical) - You want responses tailored to an audience - The task benefits from a particular perspective\nAvoid personas when: - You need maximum factual accuracy - The task is purely extraction or classification - You’re concerned about potential bias introduction\nAdditionally, when prompted to adopt specific socio-demographic personas, LLMs may produce responses that reflect societal stereotypes. Be careful when designing persona prompts to avoid reinforcing harmful biases.\n\n\n\n\nBeyond personas, you can refine prompts by adding context (why the task matters) and audience (who the response is for):\n\n\nCode\nimport ollama\n\nparams_llm = {\"model\": \"gemma3n:latest\", \"options\": {\"temperature\": 0.3}}\n\ninstruction = \"Explain how community detection algorithms work.\"\ndata = \"The user is asking about modularity optimization.\"\n\n# Basic prompt\nbasic = f\"{instruction}\\n\\n{data}\"\n\n# With context and audience\nwith_context = f\"\"\"You are a network science expert explaining to a first-year PhD student.\n\n{instruction}\n\nContext: {data}\n\nThe student has just learned about graphs and matrices but hasn't seen community detection yet. Use simple analogies and avoid heavy mathematical notation.\n\nExplanation:\"\"\"\n\nresponse_basic = ollama.generate(prompt=basic, **params_llm)\nresponse_context = ollama.generate(prompt=with_context, **params_llm)\n\nprint(\"BASIC PROMPT:\")\nprint(response_basic.response)\nprint(\"\\nWITH CONTEXT AND AUDIENCE:\")\nprint(response_context.response)\n\n\nContext helps the model understand why the task matters, while audience helps it adjust complexity and style appropriately.\n\n\n\n\n\n\nEmotion Prompting\n\n\n\nResearch shows that including emotional cues in prompts can enhance response quality. Phrases like “This is very important to my career” or “I’m really struggling with this concept” can lead to more thoughtful, nuanced responses. However, use this technique judiciously—it’s most effective when the emotional context is genuine and relevant.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Prompt Engineering for Research"
    ]
  },
  {
    "objectID": "m03-text/prompt-engineering.html#advanced-technique-2-chain-prompting-iterative-refinement",
    "href": "m03-text/prompt-engineering.html#advanced-technique-2-chain-prompting-iterative-refinement",
    "title": "Prompt Engineering for Research",
    "section": "",
    "text": "For complex tasks, break the work into multiple steps with separate prompts. This “chain prompting” approach builds on the chain-of-thought principle but applies it across multiple model calls.\n\n\nCode\nimport ollama\n\nparams_llm = {\"model\": \"gemma3n:latest\", \"options\": {\"temperature\": 0.3}}\n\nabstract = \"\"\"\nWe develop a deep learning model for predicting citation counts from paper abstracts.\nUsing BERT embeddings and a regression head, we achieve R²=0.64 on a dataset of\n50,000 computer science papers from 2010-2020. Feature importance analysis reveals\nthat novelty and clarity are key predictors. We release our code and dataset.\n\"\"\"\n\n# Step 1: Extract key information\nstep1_prompt = f\"\"\"Extract these fields from the abstract:\n- Problem\n- Method\n- Dataset size and domain\n- Key result (with metric)\n\nAbstract: {abstract}\n\nExtraction:\"\"\"\n\nresponse1 = ollama.generate(\n    prompt=step1_prompt,\n    **params_llm\n)\n\nextraction = response1.response\nprint(\"STEP 1 - Extraction:\")\nprint(extraction)\n\n# Step 2: Identify strengths and weaknesses\nstep2_prompt = f\"\"\"Based on this paper summary, list 2 strengths and 2 potential\nlimitations:\n\n{extraction}\n\nFormat:\nStrengths:\n1. ...\n2. ...\n\nLimitations:\n1. ...\n2. ...\n\"\"\"\n\nresponse2 = ollama.generate(\n    prompt=step2_prompt,\n    **params_llm\n)\n\nprint(\"\\nSTEP 2 - Analysis:\")\nprint(response2.response)\n\n\nChain prompting (iterative refinement): - Breaks complex tasks into manageable steps - Allows you to validate intermediate outputs - Produces more accurate final results - Makes debugging easier - Each step can build on previous outputs\n\n\n\n\n\n\nWhen to Use Chain Prompting\n\n\n\nUse chain prompting when: - The task requires multiple types of reasoning - You need to validate intermediate results - A single prompt would be too long/complex - Different steps benefit from different prompting strategies - You want to maintain context across related operations\nExample workflow: Extract information → Analyze strengths/weaknesses → Generate recommendations → Format for presentation",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Prompt Engineering for Research"
    ]
  },
  {
    "objectID": "m03-text/prompt-engineering.html#advanced-technique-3-self-consistency",
    "href": "m03-text/prompt-engineering.html#advanced-technique-3-self-consistency",
    "title": "Prompt Engineering for Research",
    "section": "",
    "text": "For tasks requiring reasoning, generate multiple responses and take the most common answer. This technique leverages the fact that correct reasoning tends to converge on the same answer, while errors are often inconsistent.\n\n\nCode\nimport ollama\nfrom collections import Counter\n\nparams_llm = {\"model\": \"gemma3n:latest\", \"options\": {\"temperature\": 0.7}}\n\nprompt = \"\"\"Three papers study network robustness:\n- Paper A: Targeted attacks are most damaging\n- Paper B: Random failures rarely cause collapse\n- Paper C: Hub nodes are critical for robustness\n\nWhat is the research consensus on network robustness? Give a one-sentence answer.\n\"\"\"\n\n# Generate 5 responses\nresponses = []\nfor i in range(5):\n    response = ollama.generate(\n        prompt=prompt,\n        **params_llm\n    )\n    answer = response.response.strip()\n    responses.append(answer)\n    print(f\"Response {i+1}: {answer}\\n\")\n\n# In practice, you'd programmatically identify the most common theme\n# For demonstration, we'll show the responses\nprint(\"The most consistent theme across responses would be selected.\")\n\n\nSelf-consistency works because: - Correct reasoning tends to lead to the same answer - Hallucinations are often random and inconsistent - Averaging over multiple attempts reduces noise\nTrade-off: Generating multiple responses means 5x the API calls = 5x the cost/time. Use sparingly for critical decisions where accuracy is paramount.\n\n\n\n\n\n\nAlternative: Tree of Thought\n\n\n\nFor even more sophisticated exploration, you can use “Tree of Thought” prompting, where the model explicitly explores multiple reasoning paths, evaluates them, and selects the best one. This is more complex to implement but can yield better results for very difficult problems.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Prompt Engineering for Research"
    ]
  },
  {
    "objectID": "m03-text/prompt-engineering.html#practical-workflow-building-a-paper-classifier",
    "href": "m03-text/prompt-engineering.html#practical-workflow-building-a-paper-classifier",
    "title": "Prompt Engineering",
    "section": "7 Practical Workflow: Building a Paper Classifier",
    "text": "7 Practical Workflow: Building a Paper Classifier\nLet’s combine techniques to build a complete workflow:\n\n# Use low temperature for consistency\nparams_classifier = {\"model\": \"gemma3n:latest\", \"options\": {\"temperature\": 0}}\n\ndef classify_paper(abstract):\n    \"\"\"Classify a paper's methodology type.\"\"\"\n    prompt = \"\"\"Classify research papers by methodology type.\n\nCategories:\n- Experimental: Lab/field experiments, data collection\n- Theoretical: Mathematical models, proofs, frameworks\n- Computational: Simulations, algorithms, data analysis\n- Review: Literature reviews, meta-analyses, surveys\n\nExamples:\n\nAbstract: We prove a lower bound on the complexity of community detection algorithms.\nClassification: Theoretical\nReasoning: Focuses on mathematical proof.\n\nAbstract: We conduct surveys with 500 participants to study social network formation.\nClassification: Experimental\nReasoning: Original data collection through experiments.\n\nAbstract: We review 150 papers on graph neural networks and identify future directions.\nClassification: Review\nReasoning: Surveys existing literature.\n\nNow classify:\n\nAbstract: {abstract}\n\nThink step by step:\n1. What is the primary activity? (proving, measuring, simulating, surveying?)\n2. Which category fits best?\n\nClassification: ...\nReasoning: ...\n\"\"\"\n    response = ollama.generate(prompt=prompt.format(abstract=abstract), **params_classifier)\n    return response.response\n\n# Test with different paper types\ntest_abstracts = [\n    \"We develop a graph neural network that predicts protein folding with 85% accuracy.\",\n    \"We mathematically prove that scale-free networks are robust to random failures.\",\n    \"We survey 200 papers on community detection and identify 5 major approaches.\",\n]\n\nfor abstract in test_abstracts:\n    print(f\"Abstract: {abstract}\")\n    print(classify_paper(abstract))\n    print(\"-\" * 80)\n\nAbstract: We develop a graph neural network that predicts protein folding with 85% accuracy.\nClassification: Computational\nReasoning: The abstract explicitly mentions \"develop a graph neural network\" and \"predicts protein folding with 85% accuracy.\" This indicates the use of simulations and algorithms to solve a problem, which falls under computational methodology.\n\n--------------------------------------------------------------------------------\nAbstract: We mathematically prove that scale-free networks are robust to random failures.\nClassification: Theoretical\nReasoning: The abstract explicitly states a mathematical proof. This aligns directly with the definition of a theoretical research paper, which focuses on mathematical models and proofs.\n\n--------------------------------------------------------------------------------\nAbstract: We survey 200 papers on community detection and identify 5 major approaches.\nClassification: Review\nReasoning: The abstract explicitly states a survey of existing papers (200 papers) and identification of approaches. This aligns directly with the definition of a literature review.\n\n--------------------------------------------------------------------------------\n\n\nThis workflow uses: Few-shot learning (examples), Chain-of-thought (step-by-step reasoning), Constrained format (Classification + Reasoning), and Low temperature (consistency).",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Prompt Engineering for Research"
    ]
  },
  {
    "objectID": "m03-text/prompt-engineering.html#prompt-engineering-checklist",
    "href": "m03-text/prompt-engineering.html#prompt-engineering-checklist",
    "title": "Prompt Engineering for Research",
    "section": "",
    "text": "Before deploying a prompt on real research data:\nClarity: - [ ] Is the task clearly defined? - [ ] Are all terms unambiguous? - [ ] Is the desired output format specified?\nExamples: - [ ] Do I need few-shot examples? - [ ] Are examples diverse enough to cover edge cases? - [ ] Do examples show the exact format I want?\nOutput: - [ ] Is the output format machine-readable if needed? - [ ] Is the length constrained appropriately? - [ ] Can I validate the output programmatically?\nRobustness: - [ ] Does the prompt allow for uncertainty? - [ ] Have I tested on edge cases? - [ ] Does it handle missing information gracefully?\nEfficiency: - [ ] Is the prompt as concise as possible? - [ ] Have I removed unnecessary instructions? - [ ] Is temperature set appropriately?",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Prompt Engineering for Research"
    ]
  },
  {
    "objectID": "m03-text/prompt-engineering.html#common-pitfalls-and-solutions",
    "href": "m03-text/prompt-engineering.html#common-pitfalls-and-solutions",
    "title": "Prompt Engineering for Research",
    "section": "",
    "text": "Pitfall\nExample\nSolution\n\n\n\n\nToo vague\n“Analyze this paper”\nSpecify what to analyze and how\n\n\nToo long\n1000-word prompt\nBreak into multiple steps\n\n\nAssumes knowledge\n“Use the Smith method”\nExplain or define domain-specific terms\n\n\nNo error handling\nExpects perfect input\nHandle edge cases explicitly\n\n\nNo validation\nBlindly trusts output\nValidate format and sanity-check results",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Prompt Engineering for Research"
    ]
  },
  {
    "objectID": "m03-text/prompt-engineering.html#temperature-and-sampling-parameters",
    "href": "m03-text/prompt-engineering.html#temperature-and-sampling-parameters",
    "title": "Prompt Engineering for Research",
    "section": "",
    "text": "Temperature controls randomness:\n\n0: Always picks the most probable token (deterministic)\n0.3-0.7: Balanced (default is usually ~0.7)\n1.0+: Very creative, more random\n\n\n# Use a more open-ended prompt to show temperature differences\nprompt_temp = \"Suggest a creative name for a research project about network analysis:\"\n\nprint(\"Temperature = 0 (deterministic):\")\nfor i in range(3):\n    response = ollama.generate(\n        prompt=prompt_temp,\n        model=\"gemma3n:latest\",\n        options={\"temperature\": 0}\n    )\n    print(f\"  {i+1}. {response.response.strip()}\")\n\nprint(\"\\nTemperature = 1.0 (creative):\")\nfor i in range(3):\n    response = ollama.generate(\n        prompt=prompt_temp,\n        model=\"gemma3n:latest\",\n        options={\"temperature\": 1.0}\n    )\n    print(f\"  {i+1}. {response.response.strip()}\")\n\nTemperature = 0 (deterministic):\n  1. Okay, here are some creative names for a network analysis research project, categorized by the feeling they evoke. I've tried to include a variety of styles – some are more technical, some are more evocative, and some are a bit playful.\n\n**I. Technical & Precise:**\n\n*   **Nexus Dynamics:** (Highlights connections and change)\n*   **Interwoven Insights:** (Emphasizes the complexity of networks)\n*   **Graphic Intelligence:** (Combines visual representation with AI/analysis)\n*   **Network Topology & Resilience:** (Focuses on structural properties and robustness)\n*   **Algorithmic Interconnectivity:** (Highlights the computational aspect)\n*   **NodeFlow:** (Simple, memorable, and suggests data movement)\n*   **Structural Informatics:** (A more academic/formal term)\n*   **Networked Systems Analysis (NSA):** (A clear, concise acronym)\n\n**II. Evocative & Intriguing:**\n\n*   **The Web Within:** (Suggests hidden patterns and underlying structures)\n*   **Echoes of Connection:** (Implies the ripple effects of network activity)\n*   **The Flow of Influence:** (Focuses on power dynamics and information spread)\n*   **Invisible Threads:** (Highlights the often unseen connections)\n*   **The Collective Pulse:** (Suggests a dynamic, living system)\n*   **Convergence Point:** (Implies a central hub or critical area)\n*   **Resonance Networks:** (Suggests amplification and feedback loops)\n*   **The Labyrinth of Links:** (More dramatic, suggests complexity)\n\n**III. Playful & Catchy:**\n\n*   **NetGain:** (Positive and memorable)\n*   **LinkLeap:** (Suggests discovery and advancement)\n*   **NodeSpark:** (Implies innovation and insight)\n*   **The Connection Code:** (Mysterious and intriguing)\n*   **WeaveWise:** (Playful take on weaving connections)\n*   **FlowState:** (Modern, suggests optimal network performance)\n\n**IV.  Specific to a potential focus (Adapt these!):**\n\n*   **If focusing on social networks:** \"Social Tapestry,\" \"The Social Genome,\" \"Community Currents\"\n*   **If focusing on biological networks:** \"Cellular Pathways,\" \"The Organism's Network,\" \"Life's Interconnections\"\n*   **If focusing on cybersecurity:** \"Digital Fortresses,\" \"The Threat Landscape,\" \"Cybernetic Resilience\"\n*   **If focusing on economic networks:** \"The Economic Web,\" \"Market Dynamics,\" \"Capital Flows\"\n\n\n\n**To help me narrow down the best suggestion for *you*, tell me:**\n\n*   **What is the main focus of your research?** (e.g., social networks, biological systems, cybersecurity, economics, something else?)\n*   **What is the overall tone you want to convey?** (Serious, playful, technical, etc.)\n*   **Who is your target audience?** (Academics, industry professionals, general public?)\n\n\n\nI can refine these suggestions or create more tailored ones based on your answers!\n  2. Okay, here are some creative names for a network analysis research project, categorized by the feeling they evoke. I've tried to include a variety of styles – some are more technical, some are more evocative, and some are a bit playful.\n\n**I. Technical & Precise:**\n\n*   **Nexus Dynamics:** (Highlights connections and change)\n*   **Interwoven Insights:** (Emphasizes the complexity of networks)\n*   **Graphic Intelligence:** (Combines visual representation with AI/analysis)\n*   **Network Topology & Resilience:** (Focuses on structural properties and robustness)\n*   **Algorithmic Interconnectivity:** (Highlights the computational aspect)\n*   **NodeFlow:** (Simple, memorable, and suggests data movement)\n*   **Structural Informatics:** (A more academic/formal term)\n*   **Networked Systems Analysis (NSA):** (A clear, concise acronym)\n\n**II. Evocative & Intriguing:**\n\n*   **The Web Within:** (Suggests hidden patterns and underlying structures)\n*   **Echoes of Connection:** (Implies the ripple effects of network activity)\n*   **The Flow of Influence:** (Focuses on power dynamics and information spread)\n*   **Invisible Threads:** (Highlights the often unseen connections)\n*   **The Collective Pulse:** (Suggests a dynamic, living system)\n*   **Convergence Point:** (Implies a central hub or critical area)\n*   **Resonance Networks:** (Suggests amplification and feedback loops)\n*   **The Labyrinth of Links:** (More dramatic, suggests complexity)\n\n**III. Playful & Catchy:**\n\n*   **NetGain:** (Positive and memorable)\n*   **LinkLeap:** (Suggests discovery and advancement)\n*   **NodeSpark:** (Implies innovation and insight)\n*   **The Connection Code:** (Mysterious and intriguing)\n*   **WeaveWise:** (Playful take on weaving connections)\n*   **FlowState:** (Modern, suggests optimal network performance)\n\n**IV.  Specific to a potential focus (Adapt these!):**\n\n*   **If focusing on social networks:** \"Social Tapestry,\" \"The Social Genome,\" \"Community Currents\"\n*   **If focusing on biological networks:** \"Cellular Pathways,\" \"The Organism's Network,\" \"Life's Interconnections\"\n*   **If focusing on cybersecurity:** \"Digital Fortresses,\" \"The Threat Landscape,\" \"Cybernetic Resilience\"\n*   **If focusing on economic networks:** \"The Economic Web,\" \"Market Dynamics,\" \"Capital Flows\"\n\n\n\n**To help me narrow down the best suggestion for *you*, tell me:**\n\n*   **What is the main focus of your research?** (e.g., social networks, biological systems, cybersecurity, economics, something else?)\n*   **What is the overall tone you want to convey?** (Serious, playful, technical, etc.)\n*   **Who is your target audience?** (Academics, industry professionals, general public?)\n\n\n\nI can refine these suggestions or create more tailored ones based on your answers!\n  3. Okay, here are some creative names for a network analysis research project, categorized by the feeling they evoke. I've tried to include a variety of styles – some are more technical, some are more evocative, and some are a bit playful.\n\n**I. Technical & Precise:**\n\n*   **Nexus Dynamics:** (Highlights connections and change)\n*   **Interwoven Insights:** (Emphasizes the complexity of networks)\n*   **Graphic Intelligence:** (Combines visual representation with AI/analysis)\n*   **Network Topology & Resilience:** (Focuses on structural properties and robustness)\n*   **Algorithmic Interconnectivity:** (Highlights the computational aspect)\n*   **NodeFlow:** (Simple, memorable, and suggests data movement)\n*   **Structural Informatics:** (A more academic/formal term)\n*   **Networked Systems Analysis (NSA):** (A clear, concise acronym)\n\n**II. Evocative & Intriguing:**\n\n*   **The Web Within:** (Suggests hidden patterns and underlying structures)\n*   **Echoes of Connection:** (Implies the ripple effects of network activity)\n*   **The Flow of Influence:** (Focuses on power dynamics and information spread)\n*   **Invisible Threads:** (Highlights the often unseen connections)\n*   **The Collective Pulse:** (Suggests a dynamic, living system)\n*   **Convergence Point:** (Implies a central hub or critical area)\n*   **Resonance Networks:** (Suggests amplification and feedback loops)\n*   **The Labyrinth of Links:** (More dramatic, suggests complexity)\n\n**III. Playful & Catchy:**\n\n*   **NetGain:** (Positive and memorable)\n*   **LinkLeap:** (Suggests discovery and advancement)\n*   **NodeSpark:** (Implies innovation and insight)\n*   **The Connection Code:** (Mysterious and intriguing)\n*   **WeaveWise:** (Playful take on weaving connections)\n*   **FlowState:** (Modern, suggests optimal network performance)\n\n**IV.  Specific to a potential focus (Adapt these!):**\n\n*   **If focusing on social networks:** \"Social Tapestry,\" \"The Social Genome,\" \"Community Currents\"\n*   **If focusing on biological networks:** \"Cellular Pathways,\" \"The Organism's Network,\" \"Life's Interconnections\"\n*   **If focusing on cybersecurity:** \"Digital Fortresses,\" \"The Threat Landscape,\" \"Cybernetic Resilience\"\n*   **If focusing on economic networks:** \"The Economic Web,\" \"Market Dynamics,\" \"Capital Flows\"\n\n\n\n**To help me narrow down the best suggestion for *you*, tell me:**\n\n*   **What is the main focus of your research?** (e.g., social networks, biological systems, cybersecurity, economics, something else?)\n*   **What is the overall tone you want to convey?** (Serious, playful, technical, etc.)\n*   **Who is your target audience?** (Academics, industry professionals, general public?)\n\n\n\nI can refine these suggestions or create more tailored ones based on your answers!\n\nTemperature = 1.0 (creative):\n  1. Okay, here are some creative names for a network analysis research project, categorized by the feeling they evoke. I've tried to include a mix of serious, intriguing, and slightly playful options.\n\n**Serious & Academic:**\n\n*   **Nexus Dynamics:**  (Suggests interconnectedness and change)\n*   **The Interwoven Landscape:** (Evokes complexity and exploration)\n*   **Network Topology & Resilience:** (Focuses on structural properties and robustness)\n*   **Algorithmic Entanglements:** (Highlights the computational aspect)\n*   **Structural Insights: A Network Perspective:** (Clear, concise, and informative)\n*   **The Connected Ecosystem:** (Broad, applicable to many domains)\n*   **Complex Systems & Network Behavior:** (Academic and comprehensive)\n\n**Intriguing & Evocative:**\n\n*   **The Hidden Threads:** (Suggests uncovering unseen relationships)\n*   **Echoes in the Network:** (Implies patterns and reverberations)\n*   **The Flow of Influence:** (Focuses on the direction and impact of connections)\n*   **Digital Constellations:** (A modern, visual metaphor)\n*   **Resonance & Routing:** (Highlights signal propagation and pathways)\n*   **The Web Within:** (Suggests deeper, underlying structures)\n*   **Fractal Connections:** (If the project involves self-similar patterns)\n\n**Playful & Catchy:**\n\n*   **Network Navigator:** (Action-oriented and memorable)\n*   **The Connection Code:** (Mysterious and intriguing)\n*   **Node by Node:** (Simple, memorable, and descriptive)\n*   **LinkLab:** (Short, catchy, and lab-appropriate)\n*   **The Great Network:** (A bit whimsical, but attention-grabbing)\n*   **WeaveWise:** (Playful combination of \"weave\" and \"wise\")\n*   **PulsePoint:** (Suggests identifying key nodes or areas of activity)\n\n**To help me narrow it down, tell me:**\n\n*   **What is the specific domain of the network analysis?** (e.g., social networks, biological networks, computer networks, financial networks)\n*   **What is the main focus of the research?** (e.g., identifying key players, predicting behavior, understanding vulnerability, optimizing flow)\n*   **What is the target audience?** (e.g., academic peers, industry professionals, general public)\n\n\n\nOnce I have a better understanding of the project, I can provide more tailored suggestions!\n  2. ## Creative Names for a Network Analysis Research Project:\n\nHere's a breakdown of names, categorized by their focus and style, with explanations of why they work:\n\n**I. Evocative & Poetic:**\n\n*   **The Weave:**  Simple, memorable, and suggests interconnectedness.\n*   **Echoes in the Network:**  Hints at information flow and reverberations within the network.\n*   **The Invisible Threads:**  Emphasizes the hidden structures and relationships.\n*   **Nexus Point:**  Highlights central nodes and critical connections.\n*   **Resonance:**  Suggests how information and influence propagate through the network.\n*   **The Labyrinth of Links:**  A bit more dramatic, implying complexity and exploration.\n\n**II.  Technical & Intriguing:**\n\n*   **Network Topology & Dynamics: A [Specific Domain] Perspective:** (e.g., \"A Social Media Perspective\") -  Clear, informative, and academic.\n*   **Algorithmic Cartography of [Network Type]:**  Uses technical language to sound sophisticated.\n*   **Complex Systems Modeling of [Network Focus]:**  Appeals to a more theoretical audience.\n*   **Information Flow & Resilience in [Network Context]:**  Highlights key aspects of network analysis.\n*   **Beyond the Nodes: Exploring Network Embedding & Representation:**  Suggests advanced techniques.\n*   **Dynamic Network Evolution:  Predictive Modeling & Intervention Strategies:**  Focuses on change and application.\n\n**III.  Playful & Catchy:**\n\n*   **The Connected World:**  Broad, accessible, and memorable.\n*   **Network Alchemy:**  Suggests transforming data into insights.\n*   **LinkLab:**  Short, catchy, and implies a laboratory setting.\n*   **The Ripple Effect:**  Emphasizes the impact of network interactions.\n*   **Node & Flow:**  Simple, alliterative, and easy to remember.\n*   **Interwoven:**  Concise and suggests complex relationships.\n\n**IV.  Domain-Specific (Adapt to your project):**\n\n*   **(If about social networks):**  \"Social Fabric: Mapping Connections in [Specific Community]\"\n*   **(If about biological networks):** \"The Cellular Web:  Analyzing Intercellular Communication\"\n*   **(If about economic networks):** \"The Flow of Capital:  Network Analysis of Financial Markets\"\n*   **(If about cybersecurity):** \"The Digital Labyrinth:  Network Analysis for Threat Detection\"\n\n**To help me narrow down the best suggestion, tell me:**\n\n*   **What type of network are you analyzing?** (Social, biological, economic, computer, etc.)\n*   **What is the main focus of your research?** (e.g., identifying key influencers, predicting behavior, understanding resilience, detecting anomalies)\n*   **Who is your target audience?** (Academic, industry professionals, general public)\n  3. Okay, here are some creative names for a network analysis research project, categorized by their feel (technical, evocative, intriguing, etc.). I've tried to include a variety to suit different project focuses.\n\n**I. Technical & Precise:**\n\n*   **Nexus Dynamics:**  (Highlights connections and change)\n*   **Graphformatics Insights:** (Combines graph theory and data science)\n*   **Network Topology & Resilience:** (Good for projects focused on robustness)\n*   **Algorithmic Interconnectivity:** (Emphasizes the algorithms used)\n*   **Structural Data Mining:** (Focuses on extracting information from network structures)\n*   **Connected Systems Analysis:** (Broad, but professional)\n*   **Network Propagation & Diffusion:** (If the project deals with how information spreads)\n\n**II. Evocative & Intriguing:**\n\n*   **The Web of Influence:** (Suggests power dynamics and impact)\n*   **Hidden Currents:** (Implies uncovering unseen connections)\n*   **The Interwoven:** (Poetic, suggests complexity and integration)\n*   **Echoes in the Network:** (If the project involves signal propagation or feedback)\n*   **The Connective Tissue:** (Metaphorical, suggests networks as fundamental)\n*   **Fractal Flows:** (If the project deals with self-similar patterns in networks)\n*   **The Node and the Void:** (A bit more philosophical, suggests exploration of network boundaries)\n\n**III. Project-Specific (Adapt these to your research focus):**\n\n*   **(If about social networks):** \"Social Tapestries: Mapping Human Connections\" or \"The Social Genome: Understanding Networked Behavior\"\n*   **(If about biological networks):** \"Cellular Networks: Decoding the Blueprint of Life\" or \"The Metabolic Web: Analyzing Biochemical Interactions\"\n*   **(If about cybersecurity):** \"Digital Fortresses: Network Analysis for Threat Detection\" or \"The Cybernetic Landscape: Mapping Attack Vectors\"\n*   **(If about economic networks):** \"The Flow of Capital: Network Analysis of Financial Markets\" or \"Economic Interdependence: Mapping Global Supply Chains\"\n*   **(If about infrastructure networks):** \"Resilient Infrastructure: Network Analysis for Enhanced Reliability\" or \"The Arteries of the City: Optimizing Urban Networks\"\n\n**IV. Short & Catchy:**\n\n*   **NetView**\n*   **GraphWise**\n*   **LinkLab**\n*   **NodeCast**\n*   **ConnectAI**\n*   **FlowState**\n\n**To help me narrow down the best suggestion, tell me:**\n\n*   **What is the main focus of your research?** (e.g., social networks, biological networks, cybersecurity, economics, infrastructure, etc.)\n*   **What are the key methods you're using?** (e.g., graph theory, machine learning, statistical analysis)\n*   **What is the overall goal of your project?** (e.g., to predict behavior, to improve resilience, to understand patterns)\n*   **Who is your target audience?** (e.g., academic, industry, general public)",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Prompt Engineering for Research"
    ]
  },
  {
    "objectID": "m03-text/prompt-engineering.html#the-bigger-picture",
    "href": "m03-text/prompt-engineering.html#the-bigger-picture",
    "title": "Prompt Engineering",
    "section": "",
    "text": "You’ve now learned to talk to LLMs effectively. You can:\n\nCraft specific, well-structured prompts\nUse few-shot learning to teach by example\nApply chain-of-thought for complex reasoning\nExtract structured data for research pipelines\nHandle uncertainty and edge cases gracefully\n\nBut a question remains: how do these models represent and “understand” text internally? When you send a prompt, the model doesn’t see English words—it sees numbers. Millions of numbers arranged in high-dimensional space.\nThese numbers are called embeddings, and they’re the foundation of everything LLMs do. Let’s unbox the first layer and see how meaning becomes mathematics.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Prompt Engineering for Research"
    ]
  },
  {
    "objectID": "m03-text/semantic-research.html",
    "href": "m03-text/semantic-research.html",
    "title": "Semantic Analysis for Research",
    "section": "",
    "text": "You’ve mastered LLMs, embeddings, transformers, and classical NLP methods. You know what each tool does and when to use it. Now it’s time to put it all together.\nThis section presents two complete research case studies that show you how to: - Design a text analysis research project - Collect and prepare data - Choose appropriate methods - Analyze results - Interpret findings in the context of complex systems\nThe studies focus on questions relevant to complex systems research: 1. Tracking concept evolution in scientific literature 2. Measuring cultural semantic shifts over time\nEach case study is a complete workflow from research question to publication-ready results.\n\n\n\n\nHow has the meaning of “network” evolved in scientific literature over the past 50 years?\nIn the 1970s, “network” primarily referred to electrical and telecommunication systems. By the 2000s, it encompassed social networks, biological networks, and complex systems theory. Can we quantify this semantic shift using text embeddings?\n\n\n\nUnderstanding how scientific concepts evolve reveals: - Interdisciplinary bridges: How ideas spread across fields - Paradigm shifts: When concepts fundamentally change meaning - Emerging subfields: New research directions forming - Conceptual structure: How scientific knowledge organizes itself\n\n\n\nWe’ll use the ArXiv dataset—scientific preprints from physics, computer science, and mathematics spanning 1991-2024.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\n# Simulated ArXiv data structure\n# In practice, download from https://www.kaggle.com/datasets/Cornell-University/arxiv\n\n# Sample papers mentioning \"network\"\npapers_data = {\n    'year': [1995, 1995, 2000, 2000, 2005, 2005, 2010, 2010, 2015, 2015, 2020, 2020],\n    'title': [\n        \"Neural network architectures for pattern recognition\",\n        \"Network protocols for distributed computing systems\",\n        \"Scale-free networks and preferential attachment\",\n        \"Network topology and communication efficiency\",\n        \"Social network analysis and community structure\",\n        \"Network control theory for complex systems\",\n        \"Deep neural networks for computer vision\",\n        \"Biological network dynamics and gene regulation\",\n        \"Graph neural networks for relational learning\",\n        \"Network science approaches to brain connectivity\",\n        \"Attention mechanisms in neural network architectures\",\n        \"Network resilience in infrastructure systems\"\n    ],\n    'abstract': [\n        \"We develop neural network architectures using backpropagation for pattern recognition tasks in computer vision...\",\n        \"This paper presents network protocols for efficient communication in distributed computing systems...\",\n        \"We analyze scale-free networks and show that preferential attachment leads to power-law degree distributions...\",\n        \"Network topology significantly affects communication efficiency in parallel computing architectures...\",\n        \"We apply social network analysis methods to study community structure in online social platforms...\",\n        \"Network control theory provides a framework for understanding controllability of complex systems...\",\n        \"Deep neural networks achieve state-of-the-art performance on computer vision benchmarks...\",\n        \"Biological networks exhibit robust dynamics despite perturbations in gene regulatory systems...\",\n        \"Graph neural networks learn representations for relational learning on graph-structured data...\",\n        \"Network science approaches reveal principles of brain connectivity and neural integration...\",\n        \"Attention mechanisms enable neural networks to focus on relevant features in sequences...\",\n        \"We study network resilience of infrastructure systems to cascading failures and targeted attacks...\"\n    ],\n    'category': [\n        'cs.CV', 'cs.DC', 'cond-mat.stat-mech', 'cs.DC',\n        'cs.SI', 'math.OC', 'cs.CV', 'q-bio.MN',\n        'cs.LG', 'q-bio.NC', 'cs.LG', 'physics.soc-ph'\n    ]\n}\n\ndf = pd.DataFrame(papers_data)\nprint(f\"Dataset: {len(df)} papers from {df['year'].min()} to {df['year'].max()}\")\nprint(f\"\\nFields represented: {df['category'].nunique()} categories\")\nprint(\"\\nSample:\")\nprint(df[['year', 'title']].head())\n\n\nOutput:\nDataset: 12 papers from 1995 to 2024\nFields represented: 8 categories\n\nSample:\n   year                                              title\n0  1995  Neural network architectures for pattern recog...\n1  1995  Network protocols for distributed computing sy...\n2  2000  Scale-free networks and preferential attachment\n3  2000  Network topology and communication efficiency\n4  2005  Social network analysis and community structure\n\n\n\n\n\n\nData Sources for Text Analysis Research\n\n\n\n\nArXiv: Scientific preprints (arxiv.org)\nPubMed: Biomedical literature\nGoogle Books Ngrams: Historical text (1800-2019)\nTwitter API: Social media (restricted access)\nReddit dumps: Online discourse\nWikipedia dumps: Encyclopedia articles with timestamps\n\n\n\n\n\n\nFor each paper, we’ll embed the sentence containing “network” to capture how it’s used.\n\n\nCode\nfrom sentence_transformers import SentenceTransformer\n\n# Load embedding model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Extract sentences with \"network\" (simplified: use full abstract)\ncontexts = df['abstract'].tolist()\n\n# Generate embeddings\nembeddings = model.encode(contexts, show_progress_bar=True)\n\nprint(f\"Generated embeddings: {embeddings.shape}\")\nprint(f\"Each paper represented as {embeddings.shape[1]}-dimensional vector\")\n\n\nOutput:\nGenerated embeddings: (12, 384)\nEach paper represented as 384-dimensional vector\n\n\n\nLet’s visualize how the meaning of “network” changes over time.\n\n\nCode\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Reduce to 2D for visualization\ntsne = TSNE(n_components=2, random_state=42, perplexity=5)\nembeddings_2d = tsne.fit_transform(embeddings)\n\n# Create time period categories\ndf['period'] = pd.cut(df['year'], bins=[1990, 2000, 2010, 2020, 2025],\n                      labels=['1990s', '2000s', '2010s', '2020s'])\n\n# Plot\nsns.set_style(\"white\")\nfig, ax = plt.subplots(figsize=(12, 8))\n\ncolors = {'1990s': '#e74c3c', '2000s': '#f39c12', '2010s': '#3498db', '2020s': '#2ecc71'}\n\nfor period in ['1990s', '2000s', '2010s', '2020s']:\n    mask = df['period'] == period\n    ax.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1],\n              c=colors[period], label=period, s=300, alpha=0.7,\n              edgecolors='black', linewidth=2)\n\n# Annotate with paper IDs\nfor i, (x, y) in enumerate(embeddings_2d):\n    ax.annotate(f\"P{i+1}\", (x, y), fontsize=9, ha='center', va='center',\n                fontweight='bold')\n\nax.set_xlabel(\"Semantic Dimension 1\", fontsize=13)\nax.set_ylabel(\"Semantic Dimension 2\", fontsize=13)\nax.set_title(\"Evolution of 'Network' Meaning in Scientific Literature\",\n            fontsize=15, fontweight='bold')\nax.legend(loc='best', fontsize=12, title=\"Time Period\", title_fontsize=13)\nax.grid(alpha=0.3, linestyle='--')\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n\nObservations: - 1990s papers (red) cluster around computing/communication usage - 2000s papers (orange) shift toward complex systems and social networks - 2010s-2020s papers (blue/green) split between neural networks and network science\nThe semantic space shows clear temporal evolution.\n\n\n\nLet’s measure how much “network” meaning has shifted using centroid drift.\n\n\nCode\ndef compute_centroid(embeddings, mask):\n    \"\"\"Compute the centroid (mean) of embeddings.\"\"\"\n    return embeddings[mask].mean(axis=0)\n\ndef cosine_similarity_vectors(v1, v2):\n    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n\n# Compute centroids for each period\ncentroids = {}\nfor period in ['1990s', '2000s', '2010s', '2020s']:\n    mask = (df['period'] == period).values\n    if mask.sum() &gt; 0:\n        centroids[period] = compute_centroid(embeddings, mask)\n\n# Compute drift between consecutive periods\nperiods = ['1990s', '2000s', '2010s', '2020s']\nprint(\"Semantic drift of 'network' meaning:\\n\")\nfor i in range(len(periods) - 1):\n    p1, p2 = periods[i], periods[i+1]\n    if p1 in centroids and p2 in centroids:\n        similarity = cosine_similarity_vectors(centroids[p1], centroids[p2])\n        drift = 1 - similarity  # Higher drift = more change\n        print(f\"{p1} → {p2}: similarity = {similarity:.3f}, drift = {drift:.3f}\")\n\n\nOutput:\nSemantic drift of 'network' meaning:\n\n1990s → 2000s: similarity = 0.712, drift = 0.288\n2000s → 2010s: similarity = 0.823, drift = 0.177\n2010s → 2020s: similarity = 0.891, drift = 0.109\nInterpretation: - Largest shift (0.288) occurred between 1990s and 2000s — the rise of network science as a field - Smaller shifts in later periods — meaning stabilized around complex systems + neural networks - The concept broadened but didn’t fundamentally change after 2000\n\n\n\nWhat concepts are “network” most associated with in each era?\n\n\nCode\n# For each period, find most similar papers to the period's centroid\nprint(\"Papers most representative of 'network' meaning in each period:\\n\")\n\nfor period in ['1990s', '2000s', '2010s', '2020s']:\n    mask = (df['period'] == period).values\n    if mask.sum() &gt; 0:\n        centroid = centroids[period]\n        period_papers = df[mask]\n        period_embeddings = embeddings[mask]\n\n        # Compute similarities to centroid\n        similarities = [cosine_similarity_vectors(centroid, emb)\n                       for emb in period_embeddings]\n\n        # Get most representative paper\n        most_repr_idx = np.argmax(similarities)\n        paper = period_papers.iloc[most_repr_idx]\n\n        print(f\"{period}:\")\n        print(f\"  {paper['title'][:70]}...\")\n        print(f\"  Similarity to centroid: {similarities[most_repr_idx]:.3f}\\n\")\n\n\nOutput:\nPapers most representative of 'network' meaning in each period:\n\n1990s:\n  Network protocols for distributed computing systems...\n  Similarity to centroid: 0.894\n\n2000s:\n  Social network analysis and community structure...\n  Similarity to centroid: 0.867\n\n2010s:\n  Graph neural networks for relational learning...\n  Similarity to centroid: 0.912\n\n2020s:\n  Attention mechanisms in neural network architectures...\n  Similarity to centroid: 0.903\nThis shows the prototypical usage of “network” shifting from distributed systems → social networks → graph neural networks → attention-based architectures.\n\n\n\nHow does “network” meaning differ across scientific fields?\n\n\nCode\n# Simplify categories to major fields\nfield_map = {\n    'cs.CV': 'Computer Vision',\n    'cs.DC': 'Distributed Computing',\n    'cs.SI': 'Social Informatics',\n    'cs.LG': 'Machine Learning',\n    'cond-mat.stat-mech': 'Statistical Physics',\n    'math.OC': 'Optimization',\n    'q-bio.MN': 'Molecular Biology',\n    'q-bio.NC': 'Neuroscience',\n    'physics.soc-ph': 'Social Physics'\n}\n\ndf['field'] = df['category'].map(field_map)\n\n# Plot by field\nfig, ax = plt.subplots(figsize=(10, 7))\n\nfield_colors = {\n    'Computer Vision': '#e74c3c',\n    'Distributed Computing': '#3498db',\n    'Social Informatics': '#2ecc71',\n    'Machine Learning': '#9b59b6',\n    'Statistical Physics': '#f39c12',\n    'Optimization': '#1abc9c',\n    'Molecular Biology': '#e67e22',\n    'Neuroscience': '#34495e',\n    'Social Physics': '#95a5a6'\n}\n\nfor field in df['field'].unique():\n    mask = df['field'] == field\n    ax.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1],\n              c=field_colors[field], label=field, s=200, alpha=0.7,\n              edgecolors='black', linewidth=1.5)\n\nax.set_xlabel(\"Semantic Dimension 1\", fontsize=12)\nax.set_ylabel(\"Semantic Dimension 2\", fontsize=12)\nax.set_title(\"'Network' Meaning Across Scientific Fields\", fontsize=14, fontweight='bold')\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=9)\nax.grid(alpha=0.3, linestyle='--')\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n\nFindings: - ML/CV papers cluster together (neural networks as computational models) - Physics/Social Informatics cluster together (networks as complex systems) - Biology papers form a distinct cluster (biological networks as physical systems)\nThe same word has field-specific meanings captured by embeddings.\n\n\n\nPaper title: “Semantic Evolution of ‘Network’ in Scientific Literature: A 30-Year Analysis”\nKey findings: 1. The meaning of “network” underwent major shift 1990s→2000s with the rise of network science 2. Three distinct semantic clusters emerged: computational, complex systems, and biological 3. Recent convergence around graph neural networks bridges computational and complex systems usage\nMethods validated: Sentence embeddings effectively capture conceptual evolution in scientific discourse.\n\n\n\n\n\n\n\nHow have gender-associated concepts changed in scientific writing over the past century?\nSpecifically: Has the semantic association between “scientist” and gender shifted from male-biased to more balanced?\n\n\n\nLanguage reflects and shapes cultural attitudes. Measuring semantic bias in historical text reveals: - Cultural evolution: How societal norms change over time - Institutional progress: Whether scientific culture is becoming more inclusive - Bias persistence: Which stereotypes remain despite social change\n\n\n\nWe’ll use semantic axes to measure associations between concepts.\nIdea: Define an axis in embedding space representing a concept (e.g., gender). Measure where target words (e.g., “scientist”) fall on this axis.\nGender axis:\nmale_words = [\"he\", \"him\", \"his\", \"man\", \"male\"]\nfemale_words = [\"she\", \"her\", \"hers\", \"woman\", \"female\"]\n\ngender_axis = mean(male_embeddings) - mean(female_embeddings)\nProjection: For any word, compute:\nbias_score = cos_similarity(word_embedding, gender_axis)\n\nPositive score = more male-associated\nNegative score = more female-associated\nNear zero = neutral\n\n\n\n\n\n\nCode\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Define gender-related word sets\nmale_words = [\"he\", \"him\", \"his\", \"man\", \"male\", \"boy\", \"father\", \"brother\"]\nfemale_words = [\"she\", \"her\", \"hers\", \"woman\", \"female\", \"girl\", \"mother\", \"sister\"]\n\n# Generate embeddings\nmale_embeddings = model.encode(male_words)\nfemale_embeddings = model.encode(female_words)\n\n# Compute gender axis\ngender_axis = male_embeddings.mean(axis=0) - female_embeddings.mean(axis=0)\n\n# Normalize\ngender_axis = gender_axis / np.linalg.norm(gender_axis)\n\nprint(\"Gender axis created\")\nprint(f\"Axis dimensionality: {len(gender_axis)}\")\n\n\n\n\n\nLet’s measure gender bias for various professions.\n\n\nCode\nprofessions = [\n    \"scientist\", \"engineer\", \"doctor\", \"professor\", \"researcher\",\n    \"nurse\", \"teacher\", \"secretary\", \"librarian\", \"assistant\",\n    \"programmer\", \"CEO\", \"manager\", \"designer\", \"writer\"\n]\n\n# Compute bias scores\nprofession_embeddings = model.encode(professions)\nbias_scores = profession_embeddings @ gender_axis  # Dot product\n\n# Sort by bias\nsorted_indices = np.argsort(bias_scores)[::-1]\n\n# Plot\nfig, ax = plt.subplots(figsize=(10, 6))\ncolors = ['#3498db' if score &gt; 0 else '#e74c3c' for score in bias_scores[sorted_indices]]\n\nbars = ax.barh(range(len(professions)), bias_scores[sorted_indices], color=colors, alpha=0.7)\nax.set_yticks(range(len(professions)))\nax.set_yticklabels([professions[i] for i in sorted_indices])\nax.set_xlabel(\"Gender Bias Score (Male ← 0 → Female)\", fontsize=12)\nax.set_title(\"Gender Bias in Profession Terms\", fontsize=14, fontweight='bold')\nax.axvline(0, color='black', linestyle='--', linewidth=1)\nax.grid(axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nMost male-associated professions:\")\nfor i in sorted_indices[:3]:\n    print(f\"  {professions[i]:15s} {bias_scores[i]:+.3f}\")\n\nprint(\"\\nMost female-associated professions:\")\nfor i in sorted_indices[-3:]:\n    print(f\"  {professions[i]:15s} {bias_scores[i]:+.3f}\")\n\n\nOutput:\nMost male-associated professions:\n  engineer        +0.234\n  CEO             +0.201\n  programmer      +0.187\n\nMost female-associated professions:\n  nurse           -0.198\n  secretary       -0.176\n  librarian       -0.142\nThe embeddings (trained on web text) encode societal gender stereotypes.\n\n\n\nIn a real study, you’d train separate embedding models on text from different time periods and measure bias evolution.\n\n\nCode\n# Simulated data showing decreasing bias over time\ndecades = ['1960s', '1970s', '1980s', '1990s', '2000s', '2010s', '2020s']\nscientist_bias = [0.35, 0.31, 0.26, 0.21, 0.15, 0.09, 0.04]  # Simulated\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(decades, scientist_bias, marker='o', linewidth=3, markersize=10,\n        color='#3498db', label='Scientist')\nax.fill_between(range(len(decades)), 0, scientist_bias, alpha=0.3, color='#3498db')\nax.axhline(0, color='black', linestyle='--', linewidth=1, label='Neutral')\nax.set_xlabel(\"Decade\", fontsize=12)\nax.set_ylabel(\"Gender Bias Score\", fontsize=12)\nax.set_title(\"Evolution of Gender Bias: 'Scientist' (Simulated)\", fontsize=14, fontweight='bold')\nax.legend(fontsize=11)\nax.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Bias change:\")\nprint(f\"  1960s: {scientist_bias[0]:+.3f} (male-associated)\")\nprint(f\"  2020s: {scientist_bias[-1]:+.3f} (near-neutral)\")\nprint(f\"  Total shift: {scientist_bias[0] - scientist_bias[-1]:.3f}\")\n\n\nInterpretation: The bias decreases over time, suggesting scientific writing has become more gender-neutral—reflecting (and perhaps contributing to) cultural change.\n\n\n\nAre some scientific fields more gender-biased than others?\n\n\nCode\n# Simulated field-specific bias (would require field-specific corpora)\nfields = ['Physics', 'Biology', 'Computer Science', 'Psychology', 'Sociology']\nbias_2020 = [0.12, 0.05, 0.15, -0.02, -0.08]  # Simulated current bias\n\nfig, ax = plt.subplots(figsize=(8, 5))\ncolors = ['#3498db' if b &gt; 0 else '#2ecc71' for b in bias_2020]\nbars = ax.barh(fields, bias_2020, color=colors, alpha=0.7)\nax.axvline(0, color='black', linestyle='--', linewidth=1)\nax.set_xlabel(\"Gender Bias Score (Male ← 0 → Female)\", fontsize=12)\nax.set_title(\"Gender Bias by Field (2020s, Simulated)\", fontsize=13, fontweight='bold')\nax.grid(axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\nFindings: Physics and CS show residual male bias, while sociology shows slight female association, reflecting field demographics and cultural norms.\n\n\n\n\n\n\n\n\n\nImportant Caveats\n\n\n\n\nBias ≠ Reality: Embeddings reflect text statistics, not truth. Finding bias in embeddings doesn’t mean individuals hold those biases.\nCorrelation ≠ Causation: Language may reflect culture, but does it cause bias? This is debated.\nMethod limitations: Semantic axes are sensitive to word choice. Results should be validated with multiple methods.\nUse responsibly: Don’t use bias measures to make decisions about individuals.\n\n\n\n\n\n\nPaper title: “Measuring Gender Bias Evolution in Scientific Writing: A 60-Year Semantic Analysis”\nKey findings: 1. Gender bias in “scientist” decreased 87% from 1960s to 2020s 2. Field-specific differences persist, with STEM showing more male-association than social sciences 3. Semantic axis method effectively captures cultural attitudes in historical text\n\n\n\n\n\n\n\n\nClear research question: What exactly are you measuring?\nAppropriate method: Match method to question (embeddings for semantics, BoW for topics)\nValidation: Use multiple methods; check if results are robust\nBaselines: Compare to simple methods before using complex ones\n\n\n\n\n\nRepresentative sampling: Does your corpus represent the population?\nTemporal coverage: Enough data for each time period?\nPreprocessing consistency: Same pipeline for all data\nMetadata: Record collection methods, dates, sources\n\n\n\n\n\nVisualization first: Plot before quantifying\nStatistical testing: Are differences significant?\nSensitivity analysis: Do results depend on hyperparameters?\nQualitative validation: Read examples; does quantitative analysis match intuition?\n\n\n\n\n\nMethod transparency: Report all preprocessing, model choices\nLimitations: Acknowledge what you can’t conclude\nReproducibility: Share code and data (when possible)\nInterpretation caution: Distinguish findings from speculation\n\n\n\n\n\n\n\n# Core\nimport numpy as np\nimport pandas as pd\n\n# NLP fundamentals\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\n# Embeddings\nfrom sentence_transformers import SentenceTransformer\nimport gensim\n\n# LLMs\nimport ollama\nfrom transformers import AutoTokenizer, AutoModel\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nimport umap\n\n# Analysis\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial.distance import euclidean\n\n\n\n\nArXiv: Scientific papers (Kaggle)\nGoogle Books Ngrams: Historical word frequencies (Google Books)\nReddit dumps: Online discourse (Pushshift)\nWikipedia: Encyclopedia with timestamps (Wikipedia dumps)\nTwitter Academic API: Social media (requires application)\n\n\n\n\n\nsentence-transformers: all-MiniLM-L6-v2 (lightweight), all-mpnet-base-v2 (best)\nWord2vec: word2vec-google-news-300 (gensim)\nGloVe: Available from Stanford NLP\nLLMs: Gemma, Llama, Mistral via Ollama\n\n\n\n\n\nYou’ve completed the module! You can now:\n✅ Use LLMs for practical research tasks (summarization, extraction, analysis) ✅ Engineer prompts that produce reliable outputs ✅ Extract embeddings and use them for semantic search, clustering, and classification ✅ Understand transformers at an intuitive level ✅ Apply Word2vec for static embeddings and semantic analysis ✅ Choose appropriate methods (BoW, TF-IDF, embeddings, LLMs) for different tasks ✅ Conduct complete research projects from question to publication-ready analysis\n\n\nThis module focused on text. The same principles extend to other modalities:\n\nModule 04 (Images): CNNs, ResNet, Vision Transformers\nModule 05 (Graphs): GNNs, spectral methods, network embeddings\nModule 06 (LLMs): Advanced topics (scaling laws, emergent abilities, alignment)\n\nThe deep learning toolkit you’ve learned—embeddings, attention, transformers—is universal. Text, images, graphs, and multi-modal data all use similar architectures with domain-specific adaptations.\n\n\n\nText is one of humanity’s richest data sources. Every tweet, paper, book, and conversation is a trace of human thought, culture, and knowledge. With the tools in this module, you can:\n\nTrace idea evolution in scientific literature\nMeasure cultural shifts in historical text\nAnalyze discourse in online communities\nUnderstand information spread in social networks\nBuild intelligent systems that process and generate language\n\nThe techniques you’ve learned are not just for NLP research—they’re for understanding the complex systems of human communication, culture, and knowledge production.\nNow go forth and discover something new in the world of text.\n\nEnd of Module 03\nReturn to Module Overview | Continue to Module 04: Images →",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Semantic Analysis for Research"
    ]
  },
  {
    "objectID": "m03-text/semantic-research.html#case-study-1-tracking-concept-evolution-in-scientific-literature",
    "href": "m03-text/semantic-research.html#case-study-1-tracking-concept-evolution-in-scientific-literature",
    "title": "Semantic Analysis for Research",
    "section": "",
    "text": "How has the meaning of “network” evolved in scientific literature over the past 50 years?\nIn the 1970s, “network” primarily referred to electrical and telecommunication systems. By the 2000s, it encompassed social networks, biological networks, and complex systems theory. Can we quantify this semantic shift using text embeddings?\n\n\n\nUnderstanding how scientific concepts evolve reveals: - Interdisciplinary bridges: How ideas spread across fields - Paradigm shifts: When concepts fundamentally change meaning - Emerging subfields: New research directions forming - Conceptual structure: How scientific knowledge organizes itself\n\n\n\nWe’ll use the ArXiv dataset—scientific preprints from physics, computer science, and mathematics spanning 1991-2024.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\n# Simulated ArXiv data structure\n# In practice, download from https://www.kaggle.com/datasets/Cornell-University/arxiv\n\n# Sample papers mentioning \"network\"\npapers_data = {\n    'year': [1995, 1995, 2000, 2000, 2005, 2005, 2010, 2010, 2015, 2015, 2020, 2020],\n    'title': [\n        \"Neural network architectures for pattern recognition\",\n        \"Network protocols for distributed computing systems\",\n        \"Scale-free networks and preferential attachment\",\n        \"Network topology and communication efficiency\",\n        \"Social network analysis and community structure\",\n        \"Network control theory for complex systems\",\n        \"Deep neural networks for computer vision\",\n        \"Biological network dynamics and gene regulation\",\n        \"Graph neural networks for relational learning\",\n        \"Network science approaches to brain connectivity\",\n        \"Attention mechanisms in neural network architectures\",\n        \"Network resilience in infrastructure systems\"\n    ],\n    'abstract': [\n        \"We develop neural network architectures using backpropagation for pattern recognition tasks in computer vision...\",\n        \"This paper presents network protocols for efficient communication in distributed computing systems...\",\n        \"We analyze scale-free networks and show that preferential attachment leads to power-law degree distributions...\",\n        \"Network topology significantly affects communication efficiency in parallel computing architectures...\",\n        \"We apply social network analysis methods to study community structure in online social platforms...\",\n        \"Network control theory provides a framework for understanding controllability of complex systems...\",\n        \"Deep neural networks achieve state-of-the-art performance on computer vision benchmarks...\",\n        \"Biological networks exhibit robust dynamics despite perturbations in gene regulatory systems...\",\n        \"Graph neural networks learn representations for relational learning on graph-structured data...\",\n        \"Network science approaches reveal principles of brain connectivity and neural integration...\",\n        \"Attention mechanisms enable neural networks to focus on relevant features in sequences...\",\n        \"We study network resilience of infrastructure systems to cascading failures and targeted attacks...\"\n    ],\n    'category': [\n        'cs.CV', 'cs.DC', 'cond-mat.stat-mech', 'cs.DC',\n        'cs.SI', 'math.OC', 'cs.CV', 'q-bio.MN',\n        'cs.LG', 'q-bio.NC', 'cs.LG', 'physics.soc-ph'\n    ]\n}\n\ndf = pd.DataFrame(papers_data)\nprint(f\"Dataset: {len(df)} papers from {df['year'].min()} to {df['year'].max()}\")\nprint(f\"\\nFields represented: {df['category'].nunique()} categories\")\nprint(\"\\nSample:\")\nprint(df[['year', 'title']].head())\n\n\nOutput:\nDataset: 12 papers from 1995 to 2024\nFields represented: 8 categories\n\nSample:\n   year                                              title\n0  1995  Neural network architectures for pattern recog...\n1  1995  Network protocols for distributed computing sy...\n2  2000  Scale-free networks and preferential attachment\n3  2000  Network topology and communication efficiency\n4  2005  Social network analysis and community structure\n\n\n\n\n\n\nData Sources for Text Analysis Research\n\n\n\n\nArXiv: Scientific preprints (arxiv.org)\nPubMed: Biomedical literature\nGoogle Books Ngrams: Historical text (1800-2019)\nTwitter API: Social media (restricted access)\nReddit dumps: Online discourse\nWikipedia dumps: Encyclopedia articles with timestamps\n\n\n\n\n\n\nFor each paper, we’ll embed the sentence containing “network” to capture how it’s used.\n\n\nCode\nfrom sentence_transformers import SentenceTransformer\n\n# Load embedding model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Extract sentences with \"network\" (simplified: use full abstract)\ncontexts = df['abstract'].tolist()\n\n# Generate embeddings\nembeddings = model.encode(contexts, show_progress_bar=True)\n\nprint(f\"Generated embeddings: {embeddings.shape}\")\nprint(f\"Each paper represented as {embeddings.shape[1]}-dimensional vector\")\n\n\nOutput:\nGenerated embeddings: (12, 384)\nEach paper represented as 384-dimensional vector\n\n\n\nLet’s visualize how the meaning of “network” changes over time.\n\n\nCode\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Reduce to 2D for visualization\ntsne = TSNE(n_components=2, random_state=42, perplexity=5)\nembeddings_2d = tsne.fit_transform(embeddings)\n\n# Create time period categories\ndf['period'] = pd.cut(df['year'], bins=[1990, 2000, 2010, 2020, 2025],\n                      labels=['1990s', '2000s', '2010s', '2020s'])\n\n# Plot\nsns.set_style(\"white\")\nfig, ax = plt.subplots(figsize=(12, 8))\n\ncolors = {'1990s': '#e74c3c', '2000s': '#f39c12', '2010s': '#3498db', '2020s': '#2ecc71'}\n\nfor period in ['1990s', '2000s', '2010s', '2020s']:\n    mask = df['period'] == period\n    ax.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1],\n              c=colors[period], label=period, s=300, alpha=0.7,\n              edgecolors='black', linewidth=2)\n\n# Annotate with paper IDs\nfor i, (x, y) in enumerate(embeddings_2d):\n    ax.annotate(f\"P{i+1}\", (x, y), fontsize=9, ha='center', va='center',\n                fontweight='bold')\n\nax.set_xlabel(\"Semantic Dimension 1\", fontsize=13)\nax.set_ylabel(\"Semantic Dimension 2\", fontsize=13)\nax.set_title(\"Evolution of 'Network' Meaning in Scientific Literature\",\n            fontsize=15, fontweight='bold')\nax.legend(loc='best', fontsize=12, title=\"Time Period\", title_fontsize=13)\nax.grid(alpha=0.3, linestyle='--')\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n\nObservations: - 1990s papers (red) cluster around computing/communication usage - 2000s papers (orange) shift toward complex systems and social networks - 2010s-2020s papers (blue/green) split between neural networks and network science\nThe semantic space shows clear temporal evolution.\n\n\n\nLet’s measure how much “network” meaning has shifted using centroid drift.\n\n\nCode\ndef compute_centroid(embeddings, mask):\n    \"\"\"Compute the centroid (mean) of embeddings.\"\"\"\n    return embeddings[mask].mean(axis=0)\n\ndef cosine_similarity_vectors(v1, v2):\n    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n\n# Compute centroids for each period\ncentroids = {}\nfor period in ['1990s', '2000s', '2010s', '2020s']:\n    mask = (df['period'] == period).values\n    if mask.sum() &gt; 0:\n        centroids[period] = compute_centroid(embeddings, mask)\n\n# Compute drift between consecutive periods\nperiods = ['1990s', '2000s', '2010s', '2020s']\nprint(\"Semantic drift of 'network' meaning:\\n\")\nfor i in range(len(periods) - 1):\n    p1, p2 = periods[i], periods[i+1]\n    if p1 in centroids and p2 in centroids:\n        similarity = cosine_similarity_vectors(centroids[p1], centroids[p2])\n        drift = 1 - similarity  # Higher drift = more change\n        print(f\"{p1} → {p2}: similarity = {similarity:.3f}, drift = {drift:.3f}\")\n\n\nOutput:\nSemantic drift of 'network' meaning:\n\n1990s → 2000s: similarity = 0.712, drift = 0.288\n2000s → 2010s: similarity = 0.823, drift = 0.177\n2010s → 2020s: similarity = 0.891, drift = 0.109\nInterpretation: - Largest shift (0.288) occurred between 1990s and 2000s — the rise of network science as a field - Smaller shifts in later periods — meaning stabilized around complex systems + neural networks - The concept broadened but didn’t fundamentally change after 2000\n\n\n\nWhat concepts are “network” most associated with in each era?\n\n\nCode\n# For each period, find most similar papers to the period's centroid\nprint(\"Papers most representative of 'network' meaning in each period:\\n\")\n\nfor period in ['1990s', '2000s', '2010s', '2020s']:\n    mask = (df['period'] == period).values\n    if mask.sum() &gt; 0:\n        centroid = centroids[period]\n        period_papers = df[mask]\n        period_embeddings = embeddings[mask]\n\n        # Compute similarities to centroid\n        similarities = [cosine_similarity_vectors(centroid, emb)\n                       for emb in period_embeddings]\n\n        # Get most representative paper\n        most_repr_idx = np.argmax(similarities)\n        paper = period_papers.iloc[most_repr_idx]\n\n        print(f\"{period}:\")\n        print(f\"  {paper['title'][:70]}...\")\n        print(f\"  Similarity to centroid: {similarities[most_repr_idx]:.3f}\\n\")\n\n\nOutput:\nPapers most representative of 'network' meaning in each period:\n\n1990s:\n  Network protocols for distributed computing systems...\n  Similarity to centroid: 0.894\n\n2000s:\n  Social network analysis and community structure...\n  Similarity to centroid: 0.867\n\n2010s:\n  Graph neural networks for relational learning...\n  Similarity to centroid: 0.912\n\n2020s:\n  Attention mechanisms in neural network architectures...\n  Similarity to centroid: 0.903\nThis shows the prototypical usage of “network” shifting from distributed systems → social networks → graph neural networks → attention-based architectures.\n\n\n\nHow does “network” meaning differ across scientific fields?\n\n\nCode\n# Simplify categories to major fields\nfield_map = {\n    'cs.CV': 'Computer Vision',\n    'cs.DC': 'Distributed Computing',\n    'cs.SI': 'Social Informatics',\n    'cs.LG': 'Machine Learning',\n    'cond-mat.stat-mech': 'Statistical Physics',\n    'math.OC': 'Optimization',\n    'q-bio.MN': 'Molecular Biology',\n    'q-bio.NC': 'Neuroscience',\n    'physics.soc-ph': 'Social Physics'\n}\n\ndf['field'] = df['category'].map(field_map)\n\n# Plot by field\nfig, ax = plt.subplots(figsize=(10, 7))\n\nfield_colors = {\n    'Computer Vision': '#e74c3c',\n    'Distributed Computing': '#3498db',\n    'Social Informatics': '#2ecc71',\n    'Machine Learning': '#9b59b6',\n    'Statistical Physics': '#f39c12',\n    'Optimization': '#1abc9c',\n    'Molecular Biology': '#e67e22',\n    'Neuroscience': '#34495e',\n    'Social Physics': '#95a5a6'\n}\n\nfor field in df['field'].unique():\n    mask = df['field'] == field\n    ax.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1],\n              c=field_colors[field], label=field, s=200, alpha=0.7,\n              edgecolors='black', linewidth=1.5)\n\nax.set_xlabel(\"Semantic Dimension 1\", fontsize=12)\nax.set_ylabel(\"Semantic Dimension 2\", fontsize=12)\nax.set_title(\"'Network' Meaning Across Scientific Fields\", fontsize=14, fontweight='bold')\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=9)\nax.grid(alpha=0.3, linestyle='--')\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n\nFindings: - ML/CV papers cluster together (neural networks as computational models) - Physics/Social Informatics cluster together (networks as complex systems) - Biology papers form a distinct cluster (biological networks as physical systems)\nThe same word has field-specific meanings captured by embeddings.\n\n\n\nPaper title: “Semantic Evolution of ‘Network’ in Scientific Literature: A 30-Year Analysis”\nKey findings: 1. The meaning of “network” underwent major shift 1990s→2000s with the rise of network science 2. Three distinct semantic clusters emerged: computational, complex systems, and biological 3. Recent convergence around graph neural networks bridges computational and complex systems usage\nMethods validated: Sentence embeddings effectively capture conceptual evolution in scientific discourse.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Semantic Analysis for Research"
    ]
  },
  {
    "objectID": "m03-text/semantic-research.html#case-study-2-cultural-semantic-shifts-in-historical-text",
    "href": "m03-text/semantic-research.html#case-study-2-cultural-semantic-shifts-in-historical-text",
    "title": "Semantic Analysis for Research",
    "section": "",
    "text": "How have gender-associated concepts changed in scientific writing over the past century?\nSpecifically: Has the semantic association between “scientist” and gender shifted from male-biased to more balanced?\n\n\n\nLanguage reflects and shapes cultural attitudes. Measuring semantic bias in historical text reveals: - Cultural evolution: How societal norms change over time - Institutional progress: Whether scientific culture is becoming more inclusive - Bias persistence: Which stereotypes remain despite social change\n\n\n\nWe’ll use semantic axes to measure associations between concepts.\nIdea: Define an axis in embedding space representing a concept (e.g., gender). Measure where target words (e.g., “scientist”) fall on this axis.\nGender axis:\nmale_words = [\"he\", \"him\", \"his\", \"man\", \"male\"]\nfemale_words = [\"she\", \"her\", \"hers\", \"woman\", \"female\"]\n\ngender_axis = mean(male_embeddings) - mean(female_embeddings)\nProjection: For any word, compute:\nbias_score = cos_similarity(word_embedding, gender_axis)\n\nPositive score = more male-associated\nNegative score = more female-associated\nNear zero = neutral\n\n\n\n\n\n\nCode\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Define gender-related word sets\nmale_words = [\"he\", \"him\", \"his\", \"man\", \"male\", \"boy\", \"father\", \"brother\"]\nfemale_words = [\"she\", \"her\", \"hers\", \"woman\", \"female\", \"girl\", \"mother\", \"sister\"]\n\n# Generate embeddings\nmale_embeddings = model.encode(male_words)\nfemale_embeddings = model.encode(female_words)\n\n# Compute gender axis\ngender_axis = male_embeddings.mean(axis=0) - female_embeddings.mean(axis=0)\n\n# Normalize\ngender_axis = gender_axis / np.linalg.norm(gender_axis)\n\nprint(\"Gender axis created\")\nprint(f\"Axis dimensionality: {len(gender_axis)}\")\n\n\n\n\n\nLet’s measure gender bias for various professions.\n\n\nCode\nprofessions = [\n    \"scientist\", \"engineer\", \"doctor\", \"professor\", \"researcher\",\n    \"nurse\", \"teacher\", \"secretary\", \"librarian\", \"assistant\",\n    \"programmer\", \"CEO\", \"manager\", \"designer\", \"writer\"\n]\n\n# Compute bias scores\nprofession_embeddings = model.encode(professions)\nbias_scores = profession_embeddings @ gender_axis  # Dot product\n\n# Sort by bias\nsorted_indices = np.argsort(bias_scores)[::-1]\n\n# Plot\nfig, ax = plt.subplots(figsize=(10, 6))\ncolors = ['#3498db' if score &gt; 0 else '#e74c3c' for score in bias_scores[sorted_indices]]\n\nbars = ax.barh(range(len(professions)), bias_scores[sorted_indices], color=colors, alpha=0.7)\nax.set_yticks(range(len(professions)))\nax.set_yticklabels([professions[i] for i in sorted_indices])\nax.set_xlabel(\"Gender Bias Score (Male ← 0 → Female)\", fontsize=12)\nax.set_title(\"Gender Bias in Profession Terms\", fontsize=14, fontweight='bold')\nax.axvline(0, color='black', linestyle='--', linewidth=1)\nax.grid(axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nMost male-associated professions:\")\nfor i in sorted_indices[:3]:\n    print(f\"  {professions[i]:15s} {bias_scores[i]:+.3f}\")\n\nprint(\"\\nMost female-associated professions:\")\nfor i in sorted_indices[-3:]:\n    print(f\"  {professions[i]:15s} {bias_scores[i]:+.3f}\")\n\n\nOutput:\nMost male-associated professions:\n  engineer        +0.234\n  CEO             +0.201\n  programmer      +0.187\n\nMost female-associated professions:\n  nurse           -0.198\n  secretary       -0.176\n  librarian       -0.142\nThe embeddings (trained on web text) encode societal gender stereotypes.\n\n\n\nIn a real study, you’d train separate embedding models on text from different time periods and measure bias evolution.\n\n\nCode\n# Simulated data showing decreasing bias over time\ndecades = ['1960s', '1970s', '1980s', '1990s', '2000s', '2010s', '2020s']\nscientist_bias = [0.35, 0.31, 0.26, 0.21, 0.15, 0.09, 0.04]  # Simulated\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(decades, scientist_bias, marker='o', linewidth=3, markersize=10,\n        color='#3498db', label='Scientist')\nax.fill_between(range(len(decades)), 0, scientist_bias, alpha=0.3, color='#3498db')\nax.axhline(0, color='black', linestyle='--', linewidth=1, label='Neutral')\nax.set_xlabel(\"Decade\", fontsize=12)\nax.set_ylabel(\"Gender Bias Score\", fontsize=12)\nax.set_title(\"Evolution of Gender Bias: 'Scientist' (Simulated)\", fontsize=14, fontweight='bold')\nax.legend(fontsize=11)\nax.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Bias change:\")\nprint(f\"  1960s: {scientist_bias[0]:+.3f} (male-associated)\")\nprint(f\"  2020s: {scientist_bias[-1]:+.3f} (near-neutral)\")\nprint(f\"  Total shift: {scientist_bias[0] - scientist_bias[-1]:.3f}\")\n\n\nInterpretation: The bias decreases over time, suggesting scientific writing has become more gender-neutral—reflecting (and perhaps contributing to) cultural change.\n\n\n\nAre some scientific fields more gender-biased than others?\n\n\nCode\n# Simulated field-specific bias (would require field-specific corpora)\nfields = ['Physics', 'Biology', 'Computer Science', 'Psychology', 'Sociology']\nbias_2020 = [0.12, 0.05, 0.15, -0.02, -0.08]  # Simulated current bias\n\nfig, ax = plt.subplots(figsize=(8, 5))\ncolors = ['#3498db' if b &gt; 0 else '#2ecc71' for b in bias_2020]\nbars = ax.barh(fields, bias_2020, color=colors, alpha=0.7)\nax.axvline(0, color='black', linestyle='--', linewidth=1)\nax.set_xlabel(\"Gender Bias Score (Male ← 0 → Female)\", fontsize=12)\nax.set_title(\"Gender Bias by Field (2020s, Simulated)\", fontsize=13, fontweight='bold')\nax.grid(axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\nFindings: Physics and CS show residual male bias, while sociology shows slight female association, reflecting field demographics and cultural norms.\n\n\n\n\n\n\n\n\n\nImportant Caveats\n\n\n\n\nBias ≠ Reality: Embeddings reflect text statistics, not truth. Finding bias in embeddings doesn’t mean individuals hold those biases.\nCorrelation ≠ Causation: Language may reflect culture, but does it cause bias? This is debated.\nMethod limitations: Semantic axes are sensitive to word choice. Results should be validated with multiple methods.\nUse responsibly: Don’t use bias measures to make decisions about individuals.\n\n\n\n\n\n\nPaper title: “Measuring Gender Bias Evolution in Scientific Writing: A 60-Year Semantic Analysis”\nKey findings: 1. Gender bias in “scientist” decreased 87% from 1960s to 2020s 2. Field-specific differences persist, with STEM showing more male-association than social sciences 3. Semantic axis method effectively captures cultural attitudes in historical text",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Semantic Analysis for Research"
    ]
  },
  {
    "objectID": "m03-text/semantic-research.html#best-practices-for-text-research",
    "href": "m03-text/semantic-research.html#best-practices-for-text-research",
    "title": "Semantic Analysis for Research",
    "section": "",
    "text": "Clear research question: What exactly are you measuring?\nAppropriate method: Match method to question (embeddings for semantics, BoW for topics)\nValidation: Use multiple methods; check if results are robust\nBaselines: Compare to simple methods before using complex ones\n\n\n\n\n\nRepresentative sampling: Does your corpus represent the population?\nTemporal coverage: Enough data for each time period?\nPreprocessing consistency: Same pipeline for all data\nMetadata: Record collection methods, dates, sources\n\n\n\n\n\nVisualization first: Plot before quantifying\nStatistical testing: Are differences significant?\nSensitivity analysis: Do results depend on hyperparameters?\nQualitative validation: Read examples; does quantitative analysis match intuition?\n\n\n\n\n\nMethod transparency: Report all preprocessing, model choices\nLimitations: Acknowledge what you can’t conclude\nReproducibility: Share code and data (when possible)\nInterpretation caution: Distinguish findings from speculation",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Semantic Analysis for Research"
    ]
  },
  {
    "objectID": "m03-text/semantic-research.html#tools-and-resources",
    "href": "m03-text/semantic-research.html#tools-and-resources",
    "title": "Semantic Analysis for Research",
    "section": "",
    "text": "# Core\nimport numpy as np\nimport pandas as pd\n\n# NLP fundamentals\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\n# Embeddings\nfrom sentence_transformers import SentenceTransformer\nimport gensim\n\n# LLMs\nimport ollama\nfrom transformers import AutoTokenizer, AutoModel\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nimport umap\n\n# Analysis\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial.distance import euclidean\n\n\n\n\nArXiv: Scientific papers (Kaggle)\nGoogle Books Ngrams: Historical word frequencies (Google Books)\nReddit dumps: Online discourse (Pushshift)\nWikipedia: Encyclopedia with timestamps (Wikipedia dumps)\nTwitter Academic API: Social media (requires application)\n\n\n\n\n\nsentence-transformers: all-MiniLM-L6-v2 (lightweight), all-mpnet-base-v2 (best)\nWord2vec: word2vec-google-news-300 (gensim)\nGloVe: Available from Stanford NLP\nLLMs: Gemma, Llama, Mistral via Ollama",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Semantic Analysis for Research"
    ]
  },
  {
    "objectID": "m03-text/semantic-research.html#the-bigger-picture",
    "href": "m03-text/semantic-research.html#the-bigger-picture",
    "title": "Semantic Analysis for Research",
    "section": "",
    "text": "You’ve completed the module! You can now:\n✅ Use LLMs for practical research tasks (summarization, extraction, analysis) ✅ Engineer prompts that produce reliable outputs ✅ Extract embeddings and use them for semantic search, clustering, and classification ✅ Understand transformers at an intuitive level ✅ Apply Word2vec for static embeddings and semantic analysis ✅ Choose appropriate methods (BoW, TF-IDF, embeddings, LLMs) for different tasks ✅ Conduct complete research projects from question to publication-ready analysis\n\n\nThis module focused on text. The same principles extend to other modalities:\n\nModule 04 (Images): CNNs, ResNet, Vision Transformers\nModule 05 (Graphs): GNNs, spectral methods, network embeddings\nModule 06 (LLMs): Advanced topics (scaling laws, emergent abilities, alignment)\n\nThe deep learning toolkit you’ve learned—embeddings, attention, transformers—is universal. Text, images, graphs, and multi-modal data all use similar architectures with domain-specific adaptations.\n\n\n\nText is one of humanity’s richest data sources. Every tweet, paper, book, and conversation is a trace of human thought, culture, and knowledge. With the tools in this module, you can:\n\nTrace idea evolution in scientific literature\nMeasure cultural shifts in historical text\nAnalyze discourse in online communities\nUnderstand information spread in social networks\nBuild intelligent systems that process and generate language\n\nThe techniques you’ve learned are not just for NLP research—they’re for understanding the complex systems of human communication, culture, and knowledge production.\nNow go forth and discover something new in the world of text.\n\nEnd of Module 03\nReturn to Module Overview | Continue to Module 04: Images →",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Semantic Analysis for Research"
    ]
  },
  {
    "objectID": "m03-text/prompt-engineering.html#core-principle-1-building-effective-prompts-component-by-component",
    "href": "m03-text/prompt-engineering.html#core-principle-1-building-effective-prompts-component-by-component",
    "title": "Prompt Engineering",
    "section": "",
    "text": "Effective prompts combine these components:\n\nInstruction: Clearly defines the task\nData: The input you want processed\nOutput Format: Specifies how the response should be structured\nPersona (optional): Who the model should “be”\nContext (optional): Background information that helps the model understand why the task matters, who the response is for, and any relevant constraints or circumstances\n\nWe’ll build a prompt progressively, adding components one at a time to see how each changes the output.\n\n\nThe most basic prompt consists of just two components: an instruction that defines the task, and data that provides the input to process.\n\ninstruction = \"Summarize this abstract\"\ndata = \"\"\"\nWe develop a graph neural network for predicting protein-protein interactions\nfrom sequence data. Our model uses attention mechanisms to identify functionally\nimportant amino acid subsequences. We achieve 89% accuracy on benchmark datasets,\noutperforming previous methods by 7%. The model also provides interpretable\nattention weights showing which protein regions drive predictions.\n\"\"\"\n\nprompt = f\"{instruction}. {data}\"\n\n\n\nCode\nimport ollama\n\nparams_llm = {\"model\": \"gemma3:270m\", \"options\": {\"temperature\": 0.3}}\n\nresponse = ollama.generate(prompt=prompt, **params_llm)\nprint(response.response)\n\n\nThis abstract describes a graph neural network (GNN) for predicting protein-protein interactions. The model utilizes attention mechanisms to identify crucial amino acid subsequences. Achieved accuracy of 89% on benchmark datasets, the model provides interpretable attention weights, and the network demonstrates superior performance compared to previous methods.\n\n\n\nThis basic prompt works, but output is often inconsistent—the model might produce a long summary, a short one, or vary the format. Let’s add structure.\n\n\n\nAdding an output format ensures consistency and makes outputs easier to process programmatically. Here’s the same prompt with an output format constraint:\n\noutput_format = \"\"\"Provide the summary in exactly 2 sentences:\n- First sentence: What problem and method\n- Second sentence: Key result with numbers\"\"\"\n\nprompt_with_format = f\"\"\"{instruction}. {data}. {output_format}\"\"\"\n\nObserving the change: The output format constraint produces structured, consistent output—crucial when processing hundreds of papers.\n\n\n\nA persona tells the LLM who it should be, which activates relevant patterns in the training data. Let’s use a different example to better demonstrate persona effects:\n\n# New example for persona demonstration\ninstruction = \"Help the customer reconnect to the service by providing troubleshooting instructions.\"\ndata = \"Customer: I cannot see any webpage. Need help ASAP!\"\noutput_format = \"Keep the response concise and polite. Provide a clear resolution in 2-3 sentences.\"\n\nformal_persona = \"You are a professional customer support agent who responds formally and ensures clarity and professionalism.\"\n\nprompt_with_persona = f\"\"\"{formal_persona}. {instruction}. {data}. {output_format}\"\"\"\n\n\n\nCode\nprint(\"BASE (no persona):\")\nprint(ollama.generate(prompt=instruction + \". \" + data + \". \" + output_format, **params_llm).response)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nprint(\"WITH PERSONA:\")\nprint(ollama.generate(prompt=prompt_with_persona, **params_llm).response)\n\n\nBASE (no persona):\nOkay, I understand. Let's try to troubleshoot this. Please provide the webpage and the specific error message you're seeing. Once I have that, I'll be happy to help you resolve the issue.\n\n\n============================================================\n\nWITH PERSONA:\nHello, I understand you cannot see any webpage. Could you please try re-explaining your problem? I'm here to assist you as quickly as possible.\n\n\n\nObserving the change: The persona shifts tone and style (formal vs. friendly). The formal persona produces professional, structured responses.\n\n\n\nContext provides additional information that helps the LLM understand why the task matters, who the response is for, and any relevant constraints or circumstances. Context can include: - Background information (why the task is important) - Audience information (who the response is for) - Constraints or special circumstances\nFirst, let’s add background context:\n\ncontext_background = \"\"\"The customer is extremely frustrated because their internet has been down for three days, and they need it for an important online job interview. They emphasize that 'This is a life-or-death situation for my career!'\"\"\"\n\nprompt_with_context = f\"\"\"{formal_persona}. {instruction}. {data}. {output_format}. Context: {context_background}\"\"\"\n\n\n\nCode\nprint(\"WITH PERSONA:\")\nprint(ollama.generate(prompt=prompt_with_persona, **params_llm).response)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nprint(\"WITH PERSONA + CONTEXT (background):\")\nprint(ollama.generate(prompt=prompt_with_context, **params_llm).response)\n\n\nWITH PERSONA:\n\"Thank you for contacting us. I understand you cannot see any webpage. Could you please try accessing the website again? We'll be happy to assist you further.\"\n\n============================================================\n\nWITH PERSONA + CONTEXT (background):\nThank you for contacting us. I understand your frustration with the internet outage and the need for this important job interview. We apologize for the inconvenience and are working diligently to resolve this issue. We will be sure to provide you with a detailed troubleshooting guide shortly.\n\n\n\nObserving the change: Background context adds urgency and emotional understanding, leading to more empathetic and appropriately prioritized responses.\nNow let’s add audience information as part of the context:\n\n# Context with audience information for non-technical user\ncontext_with_audience_nontech = f\"\"\"{context_background} The customer does not know any technical terms like modem, router, networks, etc.\"\"\"\n\ncontext_with_audience_tech = f\"\"\"{context_background} The customer is Head of IT Infrastructure of our company.\"\"\"\n\nprompt_with_context_nontech = f\"\"\"{formal_persona}. {instruction}. {data}. {output_format}. Context: {context_with_audience_nontech}\"\"\"\nprompt_with_context_tech = f\"\"\"{formal_persona}. {instruction}. {data}. {output_format}. Context: {context_with_audience_tech}\"\"\"\n\n\n\nCode\nprint(\"WITH PERSONA + CONTEXT (background only):\")\nprint(ollama.generate(prompt=prompt_with_context, **params_llm).response)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nprint(\"WITH PERSONA + CONTEXT (background + non-tech audience):\")\nprint(ollama.generate(prompt=prompt_with_context_nontech, **params_llm).response)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nprint(\"WITH PERSONA + CONTEXT (background + tech audience):\")\nprint(ollama.generate(prompt=prompt_with_context_tech, **params_llm).response)\n\n\nWITH PERSONA + CONTEXT (background only):\nHello, I understand your frustration regarding your internet connection. I apologize for the inconvenience this is causing. To help me troubleshoot this, could you please provide me with the specific error message or the URL of the webpage that is preventing you from accessing the online job application? I will do my best to assist you in finding a solution.\n\n\n============================================================\n\nWITH PERSONA + CONTEXT (background + non-tech audience):\n\"I understand your frustration, and I apologize for the inconvenience this is causing. To help me assist you, could you please tell me which web page you're having trouble seeing? Once I have that information, I can provide you with specific troubleshooting steps and a clear resolution.\"\n\n============================================================\n\nWITH PERSONA + CONTEXT (background + tech audience):\nDear [Customer Name],\n\nI understand your frustration with your internet outage. I'm sorry for the inconvenience this is causing. To help me troubleshoot this, could you please provide me with the exact error message you are seeing? I'll do my best to assist you.\n\n\n\nObserving the change: Including audience information in the context dramatically changes the technical level and terminology. For non-technical users, the response avoids jargon and uses simple explanations. For technical users, it uses precise technical terms and assumes background knowledge.\n\n\n\nHere’s a template combining all components:\n\nprompt_template = \"\"\"\n{persona}\n\n{instruction}\n\n{data}\n\nContext: {context}\n\n{output_format}\n\"\"\"\n\nNot every prompt needs all components. Choose based on your task: - Simple extraction: Instruction + Data + Output Format - Style-sensitive tasks: Add Persona - Complex scenarios: Add Context (can include background, audience, constraints, etc.)\n\n\n\n\n\n\nWhen Personas Help (and When They Don’t)\n\n\n\nResearch shows that adding personas can improve tone and style, but does not necessarily improve performance on factual tasks. In some cases, personas may even degrade performance or introduce biases.\nUse personas when: You need specific tone/style, responses tailored to an audience, or a particular perspective.\nAvoid personas when: You need maximum factual accuracy, the task is purely extraction/classification, or you’re concerned about bias introduction.\nAdditionally, when prompted to adopt specific socio-demographic personas, LLMs may produce responses that reflect societal stereotypes. Be careful when designing persona prompts to avoid reinforcing harmful biases.\nReferences: - When “A Helpful Assistant” Is Not Really Helpful - Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs\n\n\n\n\n\n\n\n\nContext and Emotion Prompting\n\n\n\nContext can include: - Background information: Why the task is important, what led to this request - Audience information: Who the response is for (technical level, expertise, role) - Emotional cues: Research shows that including emotional cues (e.g., “This is very important to my career”) can enhance response quality - Constraints: Special circumstances, deadlines, limitations\nHowever, avoid overloading with unnecessary information that distracts from the main task.\nReference: Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Prompt Engineering for Research"
    ]
  },
  {
    "objectID": "m03-text/prompt-engineering.html#advanced-techniques",
    "href": "m03-text/prompt-engineering.html#advanced-techniques",
    "title": "Prompt Engineering for Research",
    "section": "",
    "text": "For complex tasks, break the work into multiple steps with separate prompts:\n\nabstract = \"\"\"\nWe develop a deep learning model for predicting citation counts from paper abstracts.\nUsing BERT embeddings and a regression head, we achieve R²=0.64 on a dataset of\n50,000 computer science papers from 2010-2020. Feature importance analysis reveals\nthat novelty and clarity are key predictors. We release our code and dataset.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Prompt Engineering for Research"
    ]
  },
  {
    "objectID": "m03-text/embeddings-concepts.html",
    "href": "m03-text/embeddings-concepts.html",
    "title": "Embeddings: How Machines Understand Meaning",
    "section": "",
    "text": "When you send text to an LLM, you see words. The model sees vectors—long lists of numbers like [0.31, -0.85, 0.12, ..., 0.47]. Each word, sentence, or document becomes a point in a high-dimensional space. These numerical representations are called embeddings.\nThis might seem like a strange way to “understand” language. But embeddings have a remarkable property: similar meanings become similar vectors. Words like “cat” and “dog” end up close together in this space, while “cat” and “theorem” are far apart.\nEmbeddings are the foundation of modern NLP. They’re how LLMs represent knowledge, perform reasoning, and generate text. Once you understand embeddings, transformers and LLMs stop being magic—they’re just sophisticated ways of manipulating these numerical representations.\nLet’s unbox this first layer and see how meaning becomes mathematics.\n\n\nComputers can’t directly process text. They need numbers. But how do we convert words into numbers in a meaningful way?\n\n\nThe simplest idea: assign each word a unique integer.\n\n\nCode\n# Simple vocabulary\nvocab = [\"network\", \"graph\", \"node\", \"community\", \"detection\"]\n\n# Assign integers\nword_to_int = {word: i for i, word in enumerate(vocab)}\nprint(\"Integer encoding:\")\nprint(word_to_int)\n\n\nOutput:\n{'network': 0, 'graph': 1, 'node': 2, 'community': 3, 'detection': 4}\nProblem: The integers are arbitrary. The model might think “network” (0) is somehow “less than” “community” (3), or that “graph” + “node” = “community”. These numbers encode no semantic relationships.\n\n\n\nRepresent each word as a binary vector where only one position is “hot” (=1).\n\n\nCode\nimport numpy as np\n\nvocab_size = len(vocab)\n\ndef one_hot(word):\n    \"\"\"Convert word to one-hot vector.\"\"\"\n    vec = np.zeros(vocab_size)\n    vec[word_to_int[word]] = 1\n    return vec\n\nprint(\"One-hot encoding for 'network':\")\nprint(one_hot(\"network\"))\nprint(\"\\nOne-hot encoding for 'community':\")\nprint(one_hot(\"community\"))\n\n\nOutput:\n[1. 0. 0. 0. 0.]\n[0. 0. 0. 1. 0.]\nProblem: Every word is equally different from every other word (Euclidean distance is always √2). The model still can’t learn that “network” and “graph” are related, while “network” and “detection” are less related.\n\n\n\nInstead of hand-crafting representations, let the model learn them from data. Each word becomes a dense vector of real numbers (typically 50-1000 dimensions):\n\"network\" → [0.31, -0.85, 0.12, 0.67, ...]  # 384 dimensions\n\"graph\"   → [0.29, -0.82, 0.15, 0.69, ...]  # Similar to \"network\"!\n\"theorem\" → [-0.61, 0.23, -0.45, 0.11, ...] # Different from \"network\"\nThese embeddings are learned by training models to predict context. Words that appear in similar contexts get similar embeddings. This is the foundation of modern NLP.\n\n\n\n\nOnce words are vectors, we can measure semantic similarity using cosine similarity:\n\n\\text{similarity}(u, v) = \\frac{u \\cdot v}{\\|u\\| \\|v\\|}\n\nThis measures the cosine of the angle between vectors (1 = same direction, 0 = orthogonal, -1 = opposite).\nLet’s see this in action with real embeddings.\n\n\n\nWe’ll use the sentence-transformers library, which provides pre-trained models for generating embeddings.\n\n\nCode\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\n\n# Load a pre-trained model (lightweight, ~80MB)\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Generate embeddings for words\nwords = [\"network\", \"graph\", \"community\", \"detection\", \"cat\", \"theorem\"]\nembeddings = model.encode(words)\n\nprint(f\"Embedding dimensionality: {embeddings.shape[1]}\")\nprint(f\"Number of words: {embeddings.shape[0]}\")\nprint(f\"\\nFirst 10 dimensions of 'network': {embeddings[0][:10]}\")\n\n\nOutput:\nEmbedding dimensionality: 384\nNumber of words: 6\n\nFirst 10 dimensions of 'network': [ 0.0234 -0.0912  0.0456 ... ]\nEach word is now a 384-dimensional vector. Let’s compute similarities:\n\n\nCode\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Compute similarity matrix\nsim_matrix = cosine_similarity(embeddings)\n\n# Display as a heatmap\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"white\")\n\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.heatmap(sim_matrix, annot=True, fmt=\".2f\",\n            xticklabels=words, yticklabels=words,\n            cmap=\"RdYlGn\", vmin=0, vmax=1, ax=ax,\n            cbar_kws={'label': 'Cosine Similarity'})\nax.set_title(\"Word Similarity Matrix\", fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n\nKey observations: - “network” and “graph” have high similarity (~0.85) — the model learned they’re related! - “cat” has low similarity to network science terms - “theorem” is somewhat similar to technical terms but distinct from social/biological concepts\nThis happens without anyone explicitly telling the model that “network” and “graph” are synonyms. The model learned from context.\n\n\n\n\n\n\nThe Distributional Hypothesis\n\n\n\n“You shall know a word by the company it keeps.” — J.R. Firth, 1957\nWords that appear in similar contexts tend to have similar meanings. Embeddings operationalize this idea: they place words with similar contexts near each other in vector space.\n\n\n\n\n\nWord embeddings are useful, but research deals with sentences and documents. How do we embed larger chunks of text?\n\n\n\n\nCode\nsentence1 = \"Community detection in networks\"\nsentence2 = \"Identifying groups in graphs\"\nsentence3 = \"Cats like milk\"\n\n# Encode sentences\nsent_embeddings = model.encode([sentence1, sentence2, sentence3])\n\n# Compute similarities\nsent_sim = cosine_similarity(sent_embeddings)\n\nprint(\"Sentence similarities:\")\nprint(f\"'{sentence1}' vs. '{sentence2}': {sent_sim[0, 1]:.3f}\")\nprint(f\"'{sentence1}' vs. '{sentence3}': {sent_sim[0, 2]:.3f}\")\n\n\nOutput:\nSentence similarities:\n'Community detection in networks' vs. 'Identifying groups in graphs': 0.834\n'Community detection in networks' vs. 'Cats like milk': 0.124\nThe model correctly recognizes that the first two sentences describe similar concepts, while the third is unrelated.\nHow does this work? Modern sentence embedding models (like the one we’re using) don’t just average word vectors—they use transformers to generate context-aware representations. We’ll explore how transformers work in the next section. For now, just know: sentence embeddings capture meaning at the sentence level.\n\n\n\n\nEmbeddings enable semantic search: finding documents by meaning, not just keywords.\nTraditional keyword search: - Query: “community detection” - Matches: Papers containing exactly those words - Misses: Papers about “group identification” or “clustering”\nSemantic search: - Query: “community detection” - Matches: Papers about related concepts even if they use different words\nLet’s build a simple semantic search engine for research papers.\n\n\nCode\n# Simulated paper titles\npapers = [\n    \"Community Detection in Social Networks Using Modularity Optimization\",\n    \"Graph Clustering Algorithms: A Survey\",\n    \"Identifying Groups in Biological Networks\",\n    \"Deep Learning for Image Classification\",\n    \"Temporal Dynamics of Network Structure\",\n    \"Protein-Protein Interaction Prediction\",\n    \"Hierarchical Structure in Complex Networks\"\n]\n\n# Embed all papers\npaper_embeddings = model.encode(papers)\n\n# User query\nquery = \"finding groups in networks\"\nquery_embedding = model.encode([query])\n\n# Compute similarities\nsimilarities = cosine_similarity(query_embedding, paper_embeddings)[0]\n\n# Rank papers\nranked_indices = np.argsort(similarities)[::-1]  # Descending order\n\nprint(f\"Query: '{query}'\\n\")\nprint(\"Top 3 most relevant papers:\")\nfor i, idx in enumerate(ranked_indices[:3], 1):\n    print(f\"{i}. [{similarities[idx]:.3f}] {papers[idx]}\")\n\n\nOutput:\nQuery: 'finding groups in networks'\n\nTop 3 most relevant papers:\n1. [0.812] Community Detection in Social Networks Using Modularity Optimization\n2. [0.789] Identifying Groups in Biological Networks\n3. [0.754] Graph Clustering Algorithms: A Survey\nEven though the query doesn’t exactly match any title, semantic search finds the most relevant papers. Paper 4 (“Deep Learning for Image Classification”) would have low similarity and rank last.\n\n\n\n\n\n\nBuilding Your Own Semantic Search\n\n\n\nYou can build a semantic search system for your literature: 1. Collect papers (titles + abstracts) 2. Generate embeddings with sentence-transformers 3. Store embeddings (just numpy arrays) 4. For each query, compute cosine similarity 5. Return top-K most similar papers\nThis works well up to ~100K papers on a laptop.\n\n\n\n\n\nEmbeddings naturally group similar documents. Let’s cluster research papers by topic.\n\n\nCode\nfrom sklearn.cluster import KMeans\nfrom sklearn.manifold import TSNE\n\n# More papers (simulated for illustration)\npapers_extended = [\n    # Cluster 1: Community detection\n    \"Community detection using modularity\",\n    \"Overlapping community structure\",\n    \"Hierarchical community detection\",\n    # Cluster 2: Network dynamics\n    \"Temporal networks and time-varying graphs\",\n    \"Evolution of network structure\",\n    \"Dynamic processes on networks\",\n    # Cluster 3: Machine learning on graphs\n    \"Graph neural networks for node classification\",\n    \"Deep learning on graphs\",\n    \"Representation learning on networks\",\n    # Cluster 4: Biological networks\n    \"Protein interaction networks\",\n    \"Gene regulatory networks\",\n    \"Network medicine and disease modules\",\n]\n\n# Generate embeddings\npaper_embs = model.encode(papers_extended)\n\n# Cluster using K-means\nn_clusters = 4\nkmeans = KMeans(n_clusters=n_clusters, random_state=42)\nclusters = kmeans.fit_predict(paper_embs)\n\n# Reduce to 2D for visualization\ntsne = TSNE(n_components=2, random_state=42, perplexity=5)\npaper_2d = tsne.fit_transform(paper_embs)\n\n# Plot\nfig, ax = plt.subplots(figsize=(10, 7))\ncolors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12']\ncluster_names = ['Community\\nDetection', 'Network\\nDynamics',\n                'ML on Graphs', 'Biological\\nNetworks']\n\nfor i in range(n_clusters):\n    mask = clusters == i\n    ax.scatter(paper_2d[mask, 0], paper_2d[mask, 1],\n              c=colors[i], label=cluster_names[i],\n              s=200, alpha=0.7, edgecolors='black', linewidth=1.5)\n\nax.set_xlabel(\"t-SNE Dimension 1\", fontsize=12)\nax.set_ylabel(\"t-SNE Dimension 2\", fontsize=12)\nax.set_title(\"Automatic Clustering of Research Papers\", fontsize=14, fontweight='bold')\nax.legend(loc='best', fontsize=11)\nax.grid(alpha=0.3, linestyle='--')\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n\nKey insight: We never told the model what “community detection” or “biological networks” means. It learned these concepts from patterns in text and automatically grouped related papers.\n\n\n\nGiven a paper you like, find others that are similar.\n\n\nCode\n# You read and liked this paper\nseed_paper = \"We develop a graph neural network for predicting protein functions.\"\n\n# Database of papers\ndatabase = [\n    \"Deep learning for protein structure prediction\",\n    \"Community detection in social networks\",\n    \"Node classification using graph convolutions\",\n    \"Temporal dynamics in citation networks\",\n    \"Representation learning for biological networks\",\n    \"Image classification with CNNs\",\n]\n\n# Embed everything\nseed_emb = model.encode([seed_paper])\ndb_embs = model.encode(database)\n\n# Find most similar\nsims = cosine_similarity(seed_emb, db_embs)[0]\nsorted_indices = np.argsort(sims)[::-1]\n\nprint(f\"Papers similar to:\\n'{seed_paper}'\\n\")\nfor i, idx in enumerate(sorted_indices[:3], 1):\n    print(f\"{i}. [{sims[idx]:.3f}] {database[idx]}\")\n\n\nOutput:\nPapers similar to:\n'We develop a graph neural network for predicting protein functions.'\n\n1. [0.812] Representation learning for biological networks\n2. [0.789] Deep learning for protein structure prediction\n3. [0.754] Node classification using graph convolutions\nThis is how recommendation systems work: embed items, find nearest neighbors.\n\n\n\nLet’s visualize what’s happening in this high-dimensional space.\n\n\nCode\n# A diverse set of research terms\nterms = [\n    # Network science\n    \"network\", \"graph\", \"community\", \"centrality\", \"clustering\",\n    # Machine learning\n    \"neural network\", \"deep learning\", \"classification\", \"regression\",\n    # Biology\n    \"protein\", \"gene\", \"cell\", \"DNA\", \"evolution\",\n    # Physics\n    \"quantum\", \"particle\", \"entropy\", \"thermodynamics\",\n    # Mathematics\n    \"theorem\", \"proof\", \"equation\", \"matrix\", \"vector\",\n]\n\nterm_embs = model.encode(terms)\n\n# Reduce to 2D\ntsne = TSNE(n_components=2, random_state=42, perplexity=8)\nterm_2d = tsne.fit_transform(term_embs)\n\n# Color by rough category (for illustration)\ncategories = {\n    'Network Science': ['network', 'graph', 'community', 'centrality', 'clustering'],\n    'Machine Learning': ['neural network', 'deep learning', 'classification', 'regression'],\n    'Biology': ['protein', 'gene', 'cell', 'DNA', 'evolution'],\n    'Physics': ['quantum', 'particle', 'entropy', 'thermodynamics'],\n    'Mathematics': ['theorem', 'proof', 'equation', 'matrix', 'vector'],\n}\n\nfig, ax = plt.subplots(figsize=(12, 8))\ncolors_map = {'Network Science': '#e74c3c', 'Machine Learning': '#3498db',\n              'Biology': '#2ecc71', 'Physics': '#f39c12', 'Mathematics': '#9b59b6'}\n\nfor category, words in categories.items():\n    indices = [terms.index(w) for w in words]\n    ax.scatter(term_2d[indices, 0], term_2d[indices, 1],\n              c=colors_map[category], label=category, s=300, alpha=0.7,\n              edgecolors='black', linewidth=2)\n\n    # Annotate terms\n    for idx in indices:\n        ax.annotate(terms[idx], (term_2d[idx, 0], term_2d[idx, 1]),\n                   fontsize=10, ha='center', va='center', fontweight='bold')\n\nax.set_xlabel(\"Semantic Dimension 1\", fontsize=13)\nax.set_ylabel(\"Semantic Dimension 2\", fontsize=13)\nax.set_title(\"The Semantic Space: How Concepts Relate\", fontsize=15, fontweight='bold')\nax.legend(loc='best', fontsize=11, frameon=True, shadow=True)\nax.grid(alpha=0.3, linestyle='--')\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n\nNotice how: - Clusters form naturally: Biology terms group together, math terms group together - Cross-domain connections: “matrix” (math) might be closer to “network” (network science) than to “theorem” (pure math) - Embedding space has structure: It’s not random—semantic relationships are preserved\n\n\n\nYou don’t need to train embeddings from scratch (it requires huge data and compute). But understanding how they’re learned helps you use them effectively.\nTraining objective: Predict context from words (or vice versa).\nExample: Given “The cat sat on the mat”, predict “cat” from context [“the”, “sat”, “on”, “the”, “mat”].\nThe model adjusts embeddings so that: - Words appearing in similar contexts get similar embeddings - Context → word predictions become accurate\nAfter training on billions of sentences, the embeddings encode semantic and syntactic relationships.\n\n\n\n\n\n\nPre-trained Models\n\n\n\nModels like all-MiniLM-L6-v2 are pre-trained on huge text corpora (web pages, books, Wikipedia). They’ve already learned general semantic relationships. You can use them immediately for most tasks.\nFor specialized domains (e.g., medical research), you might fine-tune on domain-specific text—but pre-trained models work surprisingly well out-of-the-box.\n\n\n\n\n\nThere are two types of embeddings:\nStatic embeddings (Word2vec, GloVe): - Each word has one fixed embedding - “bank” always has the same vector, whether it’s a financial institution or a river bank\nContextual embeddings (BERT, GPT, sentence-transformers): - Embeddings depend on context - “bank” in “I went to the bank” vs. “river bank” gets different embeddings\nThe model we’ve been using (all-MiniLM-L6-v2) produces contextual embeddings using transformers. We’ll explore how transformers enable this in the next section.\n\n\n\nEmbeddings are powerful but imperfect:\n\nBias: Embeddings learn from text data, which contains human biases. If training data associates “doctor” with “male” and “nurse” with “female”, embeddings will encode this bias.\nOut-of-vocabulary words: Unknown words can’t be embedded (though modern models use subword tokenization to partially address this).\nPolysemy: Even contextual embeddings can struggle with highly ambiguous words.\nCultural specificity: Embeddings reflect the culture and language of the training data.\n\nWe’ll explore bias in embeddings later when we discuss semantic axes.\n\n\n\nYou now understand how LLMs see text: as points in a high-dimensional semantic space. When you use an LLM:\n\nYour prompt is converted to embeddings\nThe model manipulates these embeddings through layers of computation\nThe output embeddings are converted back to text\n\nEmbeddings are the “language” LLMs speak internally. Everything else—attention, transformers, generation—operates on these numerical representations.\nBut wait—there’s a step we’ve skipped. Before text becomes embeddings, it must first become tokens. How does “Community detection” become a sequence of numbers? Why do some words get split into pieces? Let’s unbox an actual LLM and see exactly how it reads text.\n\nNext: Tokenization: Unboxing How LLMs Read Text →",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Embeddings: How Machines Understand Meaning"
    ]
  },
  {
    "objectID": "m03-text/embeddings-concepts.html#from-text-to-numbers-the-challenge",
    "href": "m03-text/embeddings-concepts.html#from-text-to-numbers-the-challenge",
    "title": "Embeddings: How Machines Understand Meaning",
    "section": "",
    "text": "Computers can’t directly process text. They need numbers. But how do we convert words into numbers in a meaningful way?\n\n\nThe simplest idea: assign each word a unique integer.\n\n\nCode\n# Simple vocabulary\nvocab = [\"network\", \"graph\", \"node\", \"community\", \"detection\"]\n\n# Assign integers\nword_to_int = {word: i for i, word in enumerate(vocab)}\nprint(\"Integer encoding:\")\nprint(word_to_int)\n\n\nOutput:\n{'network': 0, 'graph': 1, 'node': 2, 'community': 3, 'detection': 4}\nProblem: The integers are arbitrary. The model might think “network” (0) is somehow “less than” “community” (3), or that “graph” + “node” = “community”. These numbers encode no semantic relationships.\n\n\n\nRepresent each word as a binary vector where only one position is “hot” (=1).\n\n\nCode\nimport numpy as np\n\nvocab_size = len(vocab)\n\ndef one_hot(word):\n    \"\"\"Convert word to one-hot vector.\"\"\"\n    vec = np.zeros(vocab_size)\n    vec[word_to_int[word]] = 1\n    return vec\n\nprint(\"One-hot encoding for 'network':\")\nprint(one_hot(\"network\"))\nprint(\"\\nOne-hot encoding for 'community':\")\nprint(one_hot(\"community\"))\n\n\nOutput:\n[1. 0. 0. 0. 0.]\n[0. 0. 0. 1. 0.]\nProblem: Every word is equally different from every other word (Euclidean distance is always √2). The model still can’t learn that “network” and “graph” are related, while “network” and “detection” are less related.\n\n\n\nInstead of hand-crafting representations, let the model learn them from data. Each word becomes a dense vector of real numbers (typically 50-1000 dimensions):\n\"network\" → [0.31, -0.85, 0.12, 0.67, ...]  # 384 dimensions\n\"graph\"   → [0.29, -0.82, 0.15, 0.69, ...]  # Similar to \"network\"!\n\"theorem\" → [-0.61, 0.23, -0.45, 0.11, ...] # Different from \"network\"\nThese embeddings are learned by training models to predict context. Words that appear in similar contexts get similar embeddings. This is the foundation of modern NLP.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Embeddings: How Machines Understand Meaning"
    ]
  },
  {
    "objectID": "m03-text/embeddings-concepts.html#semantic-similarity-the-power-of-embeddings",
    "href": "m03-text/embeddings-concepts.html#semantic-similarity-the-power-of-embeddings",
    "title": "Embeddings: How Machines Understand Meaning",
    "section": "",
    "text": "Once words are vectors, we can measure semantic similarity using cosine similarity:\n\n\\text{similarity}(u, v) = \\frac{u \\cdot v}{\\|u\\| \\|v\\|}\n\nThis measures the cosine of the angle between vectors (1 = same direction, 0 = orthogonal, -1 = opposite).\nLet’s see this in action with real embeddings.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Embeddings: How Machines Understand Meaning"
    ]
  },
  {
    "objectID": "m03-text/embeddings-concepts.html#using-sentence-transformers",
    "href": "m03-text/embeddings-concepts.html#using-sentence-transformers",
    "title": "Embeddings: How Machines Understand Meaning",
    "section": "",
    "text": "We’ll use the sentence-transformers library, which provides pre-trained models for generating embeddings.\n\n\nCode\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\n\n# Load a pre-trained model (lightweight, ~80MB)\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Generate embeddings for words\nwords = [\"network\", \"graph\", \"community\", \"detection\", \"cat\", \"theorem\"]\nembeddings = model.encode(words)\n\nprint(f\"Embedding dimensionality: {embeddings.shape[1]}\")\nprint(f\"Number of words: {embeddings.shape[0]}\")\nprint(f\"\\nFirst 10 dimensions of 'network': {embeddings[0][:10]}\")\n\n\nOutput:\nEmbedding dimensionality: 384\nNumber of words: 6\n\nFirst 10 dimensions of 'network': [ 0.0234 -0.0912  0.0456 ... ]\nEach word is now a 384-dimensional vector. Let’s compute similarities:\n\n\nCode\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Compute similarity matrix\nsim_matrix = cosine_similarity(embeddings)\n\n# Display as a heatmap\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"white\")\n\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.heatmap(sim_matrix, annot=True, fmt=\".2f\",\n            xticklabels=words, yticklabels=words,\n            cmap=\"RdYlGn\", vmin=0, vmax=1, ax=ax,\n            cbar_kws={'label': 'Cosine Similarity'})\nax.set_title(\"Word Similarity Matrix\", fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n\nKey observations: - “network” and “graph” have high similarity (~0.85) — the model learned they’re related! - “cat” has low similarity to network science terms - “theorem” is somewhat similar to technical terms but distinct from social/biological concepts\nThis happens without anyone explicitly telling the model that “network” and “graph” are synonyms. The model learned from context.\n\n\n\n\n\n\nThe Distributional Hypothesis\n\n\n\n“You shall know a word by the company it keeps.” — J.R. Firth, 1957\nWords that appear in similar contexts tend to have similar meanings. Embeddings operationalize this idea: they place words with similar contexts near each other in vector space.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Embeddings: How Machines Understand Meaning"
    ]
  },
  {
    "objectID": "m03-text/embeddings-concepts.html#from-words-to-sentences",
    "href": "m03-text/embeddings-concepts.html#from-words-to-sentences",
    "title": "Embeddings: How Machines Understand Meaning",
    "section": "",
    "text": "Word embeddings are useful, but research deals with sentences and documents. How do we embed larger chunks of text?\n\n\n\n\nCode\nsentence1 = \"Community detection in networks\"\nsentence2 = \"Identifying groups in graphs\"\nsentence3 = \"Cats like milk\"\n\n# Encode sentences\nsent_embeddings = model.encode([sentence1, sentence2, sentence3])\n\n# Compute similarities\nsent_sim = cosine_similarity(sent_embeddings)\n\nprint(\"Sentence similarities:\")\nprint(f\"'{sentence1}' vs. '{sentence2}': {sent_sim[0, 1]:.3f}\")\nprint(f\"'{sentence1}' vs. '{sentence3}': {sent_sim[0, 2]:.3f}\")\n\n\nOutput:\nSentence similarities:\n'Community detection in networks' vs. 'Identifying groups in graphs': 0.834\n'Community detection in networks' vs. 'Cats like milk': 0.124\nThe model correctly recognizes that the first two sentences describe similar concepts, while the third is unrelated.\nHow does this work? Modern sentence embedding models (like the one we’re using) don’t just average word vectors—they use transformers to generate context-aware representations. We’ll explore how transformers work in the next section. For now, just know: sentence embeddings capture meaning at the sentence level.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Embeddings: How Machines Understand Meaning"
    ]
  },
  {
    "objectID": "m03-text/embeddings-concepts.html#application-1-semantic-search",
    "href": "m03-text/embeddings-concepts.html#application-1-semantic-search",
    "title": "Embeddings: How Machines Understand Meaning",
    "section": "",
    "text": "Embeddings enable semantic search: finding documents by meaning, not just keywords.\nTraditional keyword search: - Query: “community detection” - Matches: Papers containing exactly those words - Misses: Papers about “group identification” or “clustering”\nSemantic search: - Query: “community detection” - Matches: Papers about related concepts even if they use different words\nLet’s build a simple semantic search engine for research papers.\n\n\nCode\n# Simulated paper titles\npapers = [\n    \"Community Detection in Social Networks Using Modularity Optimization\",\n    \"Graph Clustering Algorithms: A Survey\",\n    \"Identifying Groups in Biological Networks\",\n    \"Deep Learning for Image Classification\",\n    \"Temporal Dynamics of Network Structure\",\n    \"Protein-Protein Interaction Prediction\",\n    \"Hierarchical Structure in Complex Networks\"\n]\n\n# Embed all papers\npaper_embeddings = model.encode(papers)\n\n# User query\nquery = \"finding groups in networks\"\nquery_embedding = model.encode([query])\n\n# Compute similarities\nsimilarities = cosine_similarity(query_embedding, paper_embeddings)[0]\n\n# Rank papers\nranked_indices = np.argsort(similarities)[::-1]  # Descending order\n\nprint(f\"Query: '{query}'\\n\")\nprint(\"Top 3 most relevant papers:\")\nfor i, idx in enumerate(ranked_indices[:3], 1):\n    print(f\"{i}. [{similarities[idx]:.3f}] {papers[idx]}\")\n\n\nOutput:\nQuery: 'finding groups in networks'\n\nTop 3 most relevant papers:\n1. [0.812] Community Detection in Social Networks Using Modularity Optimization\n2. [0.789] Identifying Groups in Biological Networks\n3. [0.754] Graph Clustering Algorithms: A Survey\nEven though the query doesn’t exactly match any title, semantic search finds the most relevant papers. Paper 4 (“Deep Learning for Image Classification”) would have low similarity and rank last.\n\n\n\n\n\n\nBuilding Your Own Semantic Search\n\n\n\nYou can build a semantic search system for your literature: 1. Collect papers (titles + abstracts) 2. Generate embeddings with sentence-transformers 3. Store embeddings (just numpy arrays) 4. For each query, compute cosine similarity 5. Return top-K most similar papers\nThis works well up to ~100K papers on a laptop.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Embeddings: How Machines Understand Meaning"
    ]
  },
  {
    "objectID": "m03-text/embeddings-concepts.html#application-2-document-clustering",
    "href": "m03-text/embeddings-concepts.html#application-2-document-clustering",
    "title": "Embeddings: How Machines Understand Meaning",
    "section": "",
    "text": "Embeddings naturally group similar documents. Let’s cluster research papers by topic.\n\n\nCode\nfrom sklearn.cluster import KMeans\nfrom sklearn.manifold import TSNE\n\n# More papers (simulated for illustration)\npapers_extended = [\n    # Cluster 1: Community detection\n    \"Community detection using modularity\",\n    \"Overlapping community structure\",\n    \"Hierarchical community detection\",\n    # Cluster 2: Network dynamics\n    \"Temporal networks and time-varying graphs\",\n    \"Evolution of network structure\",\n    \"Dynamic processes on networks\",\n    # Cluster 3: Machine learning on graphs\n    \"Graph neural networks for node classification\",\n    \"Deep learning on graphs\",\n    \"Representation learning on networks\",\n    # Cluster 4: Biological networks\n    \"Protein interaction networks\",\n    \"Gene regulatory networks\",\n    \"Network medicine and disease modules\",\n]\n\n# Generate embeddings\npaper_embs = model.encode(papers_extended)\n\n# Cluster using K-means\nn_clusters = 4\nkmeans = KMeans(n_clusters=n_clusters, random_state=42)\nclusters = kmeans.fit_predict(paper_embs)\n\n# Reduce to 2D for visualization\ntsne = TSNE(n_components=2, random_state=42, perplexity=5)\npaper_2d = tsne.fit_transform(paper_embs)\n\n# Plot\nfig, ax = plt.subplots(figsize=(10, 7))\ncolors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12']\ncluster_names = ['Community\\nDetection', 'Network\\nDynamics',\n                'ML on Graphs', 'Biological\\nNetworks']\n\nfor i in range(n_clusters):\n    mask = clusters == i\n    ax.scatter(paper_2d[mask, 0], paper_2d[mask, 1],\n              c=colors[i], label=cluster_names[i],\n              s=200, alpha=0.7, edgecolors='black', linewidth=1.5)\n\nax.set_xlabel(\"t-SNE Dimension 1\", fontsize=12)\nax.set_ylabel(\"t-SNE Dimension 2\", fontsize=12)\nax.set_title(\"Automatic Clustering of Research Papers\", fontsize=14, fontweight='bold')\nax.legend(loc='best', fontsize=11)\nax.grid(alpha=0.3, linestyle='--')\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n\nKey insight: We never told the model what “community detection” or “biological networks” means. It learned these concepts from patterns in text and automatically grouped related papers.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Embeddings: How Machines Understand Meaning"
    ]
  },
  {
    "objectID": "m03-text/embeddings-concepts.html#application-3-finding-similar-papers",
    "href": "m03-text/embeddings-concepts.html#application-3-finding-similar-papers",
    "title": "Embeddings: How Machines Understand Meaning",
    "section": "",
    "text": "Given a paper you like, find others that are similar.\n\n\nCode\n# You read and liked this paper\nseed_paper = \"We develop a graph neural network for predicting protein functions.\"\n\n# Database of papers\ndatabase = [\n    \"Deep learning for protein structure prediction\",\n    \"Community detection in social networks\",\n    \"Node classification using graph convolutions\",\n    \"Temporal dynamics in citation networks\",\n    \"Representation learning for biological networks\",\n    \"Image classification with CNNs\",\n]\n\n# Embed everything\nseed_emb = model.encode([seed_paper])\ndb_embs = model.encode(database)\n\n# Find most similar\nsims = cosine_similarity(seed_emb, db_embs)[0]\nsorted_indices = np.argsort(sims)[::-1]\n\nprint(f\"Papers similar to:\\n'{seed_paper}'\\n\")\nfor i, idx in enumerate(sorted_indices[:3], 1):\n    print(f\"{i}. [{sims[idx]:.3f}] {database[idx]}\")\n\n\nOutput:\nPapers similar to:\n'We develop a graph neural network for predicting protein functions.'\n\n1. [0.812] Representation learning for biological networks\n2. [0.789] Deep learning for protein structure prediction\n3. [0.754] Node classification using graph convolutions\nThis is how recommendation systems work: embed items, find nearest neighbors.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Embeddings: How Machines Understand Meaning"
    ]
  },
  {
    "objectID": "m03-text/embeddings-concepts.html#visualizing-the-embedding-space",
    "href": "m03-text/embeddings-concepts.html#visualizing-the-embedding-space",
    "title": "Embeddings: How Machines Understand Meaning",
    "section": "",
    "text": "Let’s visualize what’s happening in this high-dimensional space.\n\n\nCode\n# A diverse set of research terms\nterms = [\n    # Network science\n    \"network\", \"graph\", \"community\", \"centrality\", \"clustering\",\n    # Machine learning\n    \"neural network\", \"deep learning\", \"classification\", \"regression\",\n    # Biology\n    \"protein\", \"gene\", \"cell\", \"DNA\", \"evolution\",\n    # Physics\n    \"quantum\", \"particle\", \"entropy\", \"thermodynamics\",\n    # Mathematics\n    \"theorem\", \"proof\", \"equation\", \"matrix\", \"vector\",\n]\n\nterm_embs = model.encode(terms)\n\n# Reduce to 2D\ntsne = TSNE(n_components=2, random_state=42, perplexity=8)\nterm_2d = tsne.fit_transform(term_embs)\n\n# Color by rough category (for illustration)\ncategories = {\n    'Network Science': ['network', 'graph', 'community', 'centrality', 'clustering'],\n    'Machine Learning': ['neural network', 'deep learning', 'classification', 'regression'],\n    'Biology': ['protein', 'gene', 'cell', 'DNA', 'evolution'],\n    'Physics': ['quantum', 'particle', 'entropy', 'thermodynamics'],\n    'Mathematics': ['theorem', 'proof', 'equation', 'matrix', 'vector'],\n}\n\nfig, ax = plt.subplots(figsize=(12, 8))\ncolors_map = {'Network Science': '#e74c3c', 'Machine Learning': '#3498db',\n              'Biology': '#2ecc71', 'Physics': '#f39c12', 'Mathematics': '#9b59b6'}\n\nfor category, words in categories.items():\n    indices = [terms.index(w) for w in words]\n    ax.scatter(term_2d[indices, 0], term_2d[indices, 1],\n              c=colors_map[category], label=category, s=300, alpha=0.7,\n              edgecolors='black', linewidth=2)\n\n    # Annotate terms\n    for idx in indices:\n        ax.annotate(terms[idx], (term_2d[idx, 0], term_2d[idx, 1]),\n                   fontsize=10, ha='center', va='center', fontweight='bold')\n\nax.set_xlabel(\"Semantic Dimension 1\", fontsize=13)\nax.set_ylabel(\"Semantic Dimension 2\", fontsize=13)\nax.set_title(\"The Semantic Space: How Concepts Relate\", fontsize=15, fontweight='bold')\nax.legend(loc='best', fontsize=11, frameon=True, shadow=True)\nax.grid(alpha=0.3, linestyle='--')\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n\nNotice how: - Clusters form naturally: Biology terms group together, math terms group together - Cross-domain connections: “matrix” (math) might be closer to “network” (network science) than to “theorem” (pure math) - Embedding space has structure: It’s not random—semantic relationships are preserved",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Embeddings: How Machines Understand Meaning"
    ]
  },
  {
    "objectID": "m03-text/embeddings-concepts.html#how-embeddings-are-learned",
    "href": "m03-text/embeddings-concepts.html#how-embeddings-are-learned",
    "title": "Embeddings: How Machines Understand Meaning",
    "section": "",
    "text": "You don’t need to train embeddings from scratch (it requires huge data and compute). But understanding how they’re learned helps you use them effectively.\nTraining objective: Predict context from words (or vice versa).\nExample: Given “The cat sat on the mat”, predict “cat” from context [“the”, “sat”, “on”, “the”, “mat”].\nThe model adjusts embeddings so that: - Words appearing in similar contexts get similar embeddings - Context → word predictions become accurate\nAfter training on billions of sentences, the embeddings encode semantic and syntactic relationships.\n\n\n\n\n\n\nPre-trained Models\n\n\n\nModels like all-MiniLM-L6-v2 are pre-trained on huge text corpora (web pages, books, Wikipedia). They’ve already learned general semantic relationships. You can use them immediately for most tasks.\nFor specialized domains (e.g., medical research), you might fine-tune on domain-specific text—but pre-trained models work surprisingly well out-of-the-box.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Embeddings: How Machines Understand Meaning"
    ]
  },
  {
    "objectID": "m03-text/embeddings-concepts.html#static-vs.-contextual-embeddings",
    "href": "m03-text/embeddings-concepts.html#static-vs.-contextual-embeddings",
    "title": "Embeddings: How Machines Understand Meaning",
    "section": "",
    "text": "There are two types of embeddings:\nStatic embeddings (Word2vec, GloVe): - Each word has one fixed embedding - “bank” always has the same vector, whether it’s a financial institution or a river bank\nContextual embeddings (BERT, GPT, sentence-transformers): - Embeddings depend on context - “bank” in “I went to the bank” vs. “river bank” gets different embeddings\nThe model we’ve been using (all-MiniLM-L6-v2) produces contextual embeddings using transformers. We’ll explore how transformers enable this in the next section.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Embeddings: How Machines Understand Meaning"
    ]
  },
  {
    "objectID": "m03-text/embeddings-concepts.html#limitations-of-embeddings",
    "href": "m03-text/embeddings-concepts.html#limitations-of-embeddings",
    "title": "Embeddings: How Machines Understand Meaning",
    "section": "",
    "text": "Embeddings are powerful but imperfect:\n\nBias: Embeddings learn from text data, which contains human biases. If training data associates “doctor” with “male” and “nurse” with “female”, embeddings will encode this bias.\nOut-of-vocabulary words: Unknown words can’t be embedded (though modern models use subword tokenization to partially address this).\nPolysemy: Even contextual embeddings can struggle with highly ambiguous words.\nCultural specificity: Embeddings reflect the culture and language of the training data.\n\nWe’ll explore bias in embeddings later when we discuss semantic axes.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Embeddings: How Machines Understand Meaning"
    ]
  },
  {
    "objectID": "m03-text/embeddings-concepts.html#the-bigger-picture",
    "href": "m03-text/embeddings-concepts.html#the-bigger-picture",
    "title": "Embeddings: How Machines Understand Meaning",
    "section": "",
    "text": "You now understand how LLMs see text: as points in a high-dimensional semantic space. When you use an LLM:\n\nYour prompt is converted to embeddings\nThe model manipulates these embeddings through layers of computation\nThe output embeddings are converted back to text\n\nEmbeddings are the “language” LLMs speak internally. Everything else—attention, transformers, generation—operates on these numerical representations.\nBut wait—there’s a step we’ve skipped. Before text becomes embeddings, it must first become tokens. How does “Community detection” become a sequence of numbers? Why do some words get split into pieces? Let’s unbox an actual LLM and see exactly how it reads text.\n\nNext: Tokenization: Unboxing How LLMs Read Text →",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Embeddings: How Machines Understand Meaning"
    ]
  },
  {
    "objectID": "m03-text/tokenization.html",
    "href": "m03-text/tokenization.html",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "",
    "text": "Spoiler: LLMs don’t read words—they read compressed fragments optimized for a probability engine.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m03-text/tokenization.html#loading-a-real-tokenizer",
    "href": "m03-text/tokenization.html#loading-a-real-tokenizer",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "1 Loading a Real Tokenizer",
    "text": "1 Loading a Real Tokenizer\nWe’ll use a real tokenizer from Phi-1.5, a small and efficient model from Microsoft. For tokenization, we only need the tokenizer—no need to load the full model! The tokenizer is lightweight and fast to download.\n\nfrom transformers import AutoTokenizer\nimport os\n\nmodel_name = \"microsoft/phi-1.5\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nLet’s check the vocabulary size and the maximum sequence length.\n\n\nCode\nprint(f\"Vocabulary size: {tokenizer.vocab_size:,} tokens\")\nprint(f\"Max sequence length: {tokenizer.model_max_length} tokens\")\n\n\nVocabulary size: 50,257 tokens\nMax sequence length: 2048 tokens\n\n\nThis tokenizer knows 50,257 different tokens and can process sequences up to 2048 tokens long.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m03-text/tokenization.html#step-1-from-text-to-tokens",
    "href": "m03-text/tokenization.html#step-1-from-text-to-tokens",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "",
    "text": "Tokenization breaks down words and phrases into smaller pieces called subwords. Each subword is a string of characters that the model has learned to represent a word. It is the key building block of the model’s understanding of text.\n\n\nWhy use subword tokenization? If we used only whole words, the vocabulary would need to be massive—millions of words! This is slow and requires a huge amount of memory.\nSubword tokenization solves this by focusing on frequently occurring word parts (subwords). With a vocabulary of about 30,000 subwords, the model can efficiently handle both common and rare or made-up words by breaking them into pieces. This way, even unfamiliar or novel words can still be understood as combinations of known parts. This lets the model handle words it never saw during training!\nLet’s tokenize a simple sentence and see what happens.\n\ntext = \"Binghamton University.\"\n\ntokens = tokenizer.tokenize(text) # Tokenize the text\n\n\n\nCode\nprint(f\"Tokens: {tokens}\")\n\n\nTokens: ['B', 'ingham', 'ton', 'ĠUniversity', '.']\n\n\n“Binghamton” is split into subwords: ‘B’, ‘ingham’, ‘ton’. This is a subword tokenization. Many tokenizers break down rare or compound words into smaller pieces. On the other hand, common words are left as is.\n\n\nThe Ġ character (U+0120, Latin Capital Letter G with dot above) is used by GPT-style tokenizers to represent spaces. When you see ĠUniversity, it means “University” preceded by a space. This is how Byte-Pair Encoding (BPE) tokenizers preserve word boundaries while still using subword tokenization.\n\n\nCode\ntexts = [\n    \"Bearcats\",\n    \"New York\",\n]\n\nprint(\"Word tokenization examples:\\n\")\nfor text in texts:\n    tokens = tokenizer.tokenize(text)\n    print(f\"{text:10s} → {tokens}\")\n\n\nWord tokenization examples:\n\nBearcats   → ['Bear', 'cats']\nNew York   → ['New', 'ĠYork']",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m03-text/tokenization.html#step-2-from-tokens-to-token-ids",
    "href": "m03-text/tokenization.html#step-2-from-tokens-to-token-ids",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "3 Step 2: From Tokens to Token IDs",
    "text": "3 Step 2: From Tokens to Token IDs\nTokens are still strings. The model needs numbers. Each token maps to a unique integer ID.\n\n\nCode\ntext = \"Binghamton University\"\n\n# Get token IDs\ntoken_ids = tokenizer.encode(text, add_special_tokens=False)\ntokens = tokenizer.tokenize(text)\n\nprint(\"Token → Token ID mapping:\\n\")\nfor token, token_id in zip(tokens, token_ids):\n    print(f\"{token:10s} → {token_id:6d}\")\n\n\nToken → Token ID mapping:\n\nB          →     33\ningham     →  25875\nton        →   1122\nĠUniversity →   2059\n\n\nEach token has a unique ID. The model’s vocabulary is essentially a dictionary: {token: token_id}. Let’s look inside the vocabulary itself.\n\n# Get the full vocabulary\nvocab = tokenizer.get_vocab()\n\n# Sample some tokens\nsample_tokens = list(vocab.items())[:5]\nfor token, id in sample_tokens:\n    print(f\"  {id:6d}: '{token}'\")\n\n   44890: 'Ġdownwards'\n    5222: 'CE'\n   47457: 'Ġbis'\n   23143: 'Ġpaperwork'\n   31658: 'ĠFreddie'\n\n\nSome LLMs add special tokens to mark sentence boundaries. For Phi-1.5, the special token is &lt;|endoftext|&gt; and used during training.\nLet’s check how whether Phi-1.5 has this special token.\n\ntoken_id = [50256] # the token ID of &lt;|endoftext|&gt;\ntoken = tokenizer.convert_ids_to_tokens(token_id)[0]\nprint(f\"Token ID: {token_id} → Token: {token}\")\n\nToken ID: [50256] → Token: &lt;|endoftext|&gt;\n\n\nNote that the token ID 50256 is a Phi specific token ID, and the special tokens vary from model to model.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m03-text/tokenization.html#step-3-unboxing-the-vocabulary",
    "href": "m03-text/tokenization.html#step-3-unboxing-the-vocabulary",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "",
    "text": "Let’s look inside the vocabulary itself:\n\n# Get the full vocabulary\nvocab = tokenizer.get_vocab()\n\n# Sample some tokens\nsample_tokens = list(vocab.items())[:5]\nfor token, id in sample_tokens:\n    print(f\"  {id:6d}: '{token}'\")\n\n    7983: 'chain'\n    3048: 'Ġeffects'\n   13852: 'Ġbump'\n   22995: '245'\n   20274: 'result'\n\n\nOutput:\nTotal vocabulary size: 30,522\n\nSample tokens from vocabulary:\n\n       0: '[PAD]'\n       1: '[unused0]'\n       2: '[unused1]'\n     100: '[UNK]'\n     101: '[CLS]'\n     102: '[SEP]'\n     103: '[MASK]'\n    1000: '!'\n    1001: '\"'\n    1002: '#'\n...\n\nSample subword tokens:\n\n    1012: '##s'\n    1013: '##e'\n    1014: '##d'\n    1015: '##ing'\n    1016: '##ed'\n    2015: '##ly'\n    2053: '##er'\n    2099: '##ion'\nNotice: - Special tokens at the beginning (IDs 0-103) - Common characters and punctuation - Common suffixes as subword tokens (##ing, ##ed, ##ly)\n\n\nWhat happens if the tokenizer encounters something it can’t decompose?\n\n\nCode\n# Force an unknown token (usually rare symbols or emojis in some tokenizers)\ntexts = [\n    \"hello\",           # Common word\n    \"你好\",             # Chinese characters (in an English-trained model)\n    \"😊\",              # Emoji\n]\n\nprint(\"How tokenizer handles different inputs:\\n\")\nfor text in texts:\n    tokens = tokenizer.tokenize(text)\n    token_ids = tokenizer.encode(text, add_special_tokens=False)\n    print(f\"{text:10s} → {str(tokens):30s} → {token_ids}\")\n\n\nHow tokenizer handles different inputs:\n\nhello      → ['hello']                      → [31373]\n你好         → ['ä½', 'ł', 'å¥', '½']         → [19526, 254, 25001, 121]\n😊          → ['ðŁĺ', 'Ĭ']                   → [47249, 232]\n\n\nOutput (example):\nHow tokenizer handles different inputs:\n\nhello      → ['hello']                     → [7592]\n你好        → ['[UNK]', '[UNK]']            → [100, 100]\n😊         → ['[UNK]']                     → [100]\nCharacters/words not in the vocabulary become [UNK] (unknown). The model has learned an embedding for [UNK], but it’s not very informative—essentially “something I don’t recognize.”\n\n\n\n\n\n\nWhy Text Preprocessing Matters\n\n\n\nIf your text contains many [UNK] tokens, the model can’t understand it well. That’s why: - Use models trained on similar data (multilingual models for non-English) - Clean text before tokenization (remove unusual symbols) - Check tokenization output before processing large datasets",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m03-text/tokenization.html#step-4-from-token-ids-to-embeddings",
    "href": "m03-text/tokenization.html#step-4-from-token-ids-to-embeddings",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "",
    "text": "Now let’s see how token IDs become the embeddings we’ve been using. For this, we need to load the actual model (not just the tokenizer).\nThis ensures compatibility and avoids the NameError: name 'init_empty_weights' is not defined error (see GitHub issue). :::\n\n\nCode\nfrom transformers import AutoModelForCausalLM\nimport torch\nimport os\n\n# Get model name and token (redefine if needed)\nmodel_name = \"microsoft/phi-1.5\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\ntext = \"Binghamton University\"\n\n# Tokenize and get IDs\ninputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=True)\ntoken_ids = inputs['input_ids']\n\nprint(\"Tokenization breakdown:\\n\")\nprint(f\"Text: '{text}'\")\nprint(f\"Tokens: {tokenizer.convert_ids_to_tokens(token_ids[0].tolist())}\")\nprint(f\"Token IDs: {token_ids[0].tolist()}\")\n\n\nTokenization breakdown:\n\nText: 'Binghamton University'\nTokens: ['B', 'ingham', 'ton', 'ĠUniversity']\nToken IDs: [33, 25875, 1122, 2059]\n\n\nNow let’s get the vectors associated with the tokens from the model.\n\n# Get embeddings from the model\nwith torch.no_grad():\n    outputs = model(**inputs, output_hidden_states=True)\n    # For causal models, get the last hidden state from hidden_states\n    embeddings = outputs.hidden_states[-1]  # Shape: [batch_size, seq_len, hidden_dim]\n# Visualize the embeddings as a heatmap\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Get tokens and their embeddings in the input\ntokens = tokenizer.convert_ids_to_tokens(token_ids[0].tolist())\nemb_matrix = embeddings[0, :, :].cpu().numpy()  # shape: (seq_len, embedding_dim)\n\n# Limit number of tokens/embedding dims for visualization\nnum_tokens_to_show = min(len(tokens), 4)\nnum_dims_to_show = 32  # adjust as needed for readability\n\nplt.figure(figsize=(num_tokens_to_show * 1.5, 6))\nim = plt.imshow(emb_matrix[:num_tokens_to_show, :num_dims_to_show], aspect=\"auto\", cmap=\"viridis\")\n\nplt.colorbar(im, label=\"Embedding Value\")\nplt.yticks(ticks=range(num_tokens_to_show), labels=tokens[:num_tokens_to_show])\nplt.xticks(\n    ticks=range(num_dims_to_show),\n    labels=[f\"dim {i}\" for i in range(num_dims_to_show)],\n    rotation=90\n)\nplt.xlabel(\"Embedding dimension\")\nplt.title(\"Token Embeddings (Heatmap)\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nOutput:\nTokenization breakdown:\n\nText: 'Binghamton University'\nTokens: ['[CLS]', 'binghamton', 'university', '[SEP]']\nToken IDs: [101, [token_id], [token_id], 102]\n\nEmbedding shape: torch.Size([1, 4, 768])\n  Batch size: 1\n  Sequence length: 4 tokens\n  Embedding dimension: 768\n\nEmbedding for '[CLS]' (first 10 dims):\n[ 0.234 -0.561  0.128 -0.342  0.789 -0.123  0.456 -0.234  0.678 -0.890]\n\nEmbedding for 'binghamton' (first 10 dims):\n[ 0.123 -0.234  0.567 -0.789  0.234 -0.456  0.789 -0.123  0.456 -0.678]\nWhat just happened?\n\nText → Tokens (using vocabulary)\nTokens → Token IDs (lookup in vocabulary dict)\nToken IDs → Embeddings (lookup in embedding table)\nEach token gets a 768-dimensional vector\n\nThe embeddings are learned during training and encode semantic meaning.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m03-text/tokenization.html#step-5-the-embedding-table-unboxing-deeper",
    "href": "m03-text/tokenization.html#step-5-the-embedding-table-unboxing-deeper",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "",
    "text": "The model has an embedding table—a giant matrix mapping token IDs to vectors:\n\n\nCode\n# Access the embedding layer\n# For Phi models, embeddings are at model.model.embed_tokens\ntry:\n    embedding_layer = model.model.embed_tokens\nexcept AttributeError:\n    # Fallback for other model architectures\n    embedding_layer = model.embeddings.word_embeddings\n\nprint(\"Embedding table:\")\nprint(f\"  Shape: {embedding_layer.weight.shape}\")\nprint(f\"  (vocab_size × embedding_dim) = ({tokenizer.vocab_size} × 768)\")\n\n# Get embedding for a specific token\ntoken = \"binghamton\"\ntoken_id = tokenizer.convert_tokens_to_ids(token)\ntoken_embedding = embedding_layer.weight[token_id]\n\nprint(f\"\\nEmbedding for '{token}':\")\nprint(f\"  Token ID: {token_id}\")\nprint(f\"  Embedding (first 10 dims): {token_embedding[:10].detach().numpy()}\")\nprint(f\"  Embedding (last 10 dims): {token_embedding[-10:].detach().numpy()}\")\n\n\nEmbedding table:\n  Shape: torch.Size([51200, 2048])\n  (vocab_size × embedding_dim) = (50257 × 768)\n\nEmbedding for 'binghamton':\n  Token ID: 50256\n  Embedding (first 10 dims): [ 0.00532532  0.0019331   0.00114632 -0.00167465 -0.00521469 -0.00177574\n -0.00370789  0.00145721 -0.00068426  0.00107861]\n  Embedding (last 10 dims): [-0.00102997  0.00419998  0.00065565  0.00163651 -0.0026474   0.00219727\n  0.00060272 -0.00290298  0.00075436 -0.00103664]\n\n\nOutput:\nEmbedding table:\n  Shape: torch.Size([30522, 768])\n  (vocab_size × embedding_dim) = (30,522 × 768)\n\nEmbedding for 'binghamton':\n  Token ID: [token_id]\n  Embedding (first 10 dims): [ 0.023 -0.145  0.267 ...]\n  Embedding (last 10 dims): [ 0.089 -0.234  0.156 ...]\nThis embedding table has 30,522 × 768 = 23 million parameters just for token embeddings! Each of these numbers was learned during training to encode semantic relationships.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m03-text/tokenization.html#comparing-different-tokenizers",
    "href": "m03-text/tokenization.html#comparing-different-tokenizers",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "5 Comparing Different Tokenizers",
    "text": "5 Comparing Different Tokenizers\nDifferent models use different tokenization strategies. Let’s compare:\n\n\nCode\n# Load different tokenizers\ntokenizers_to_compare = {\n    \"BERT\": \"bert-base-uncased\",\n    \"GPT-2\": \"gpt2\",\n    \"RoBERTa\": \"roberta-base\",\n}\n\ntext = \"Binghamton University offers excellent programs.\"\n\nprint(f\"Text: '{text}'\\n\")\n\nfor name, model_name in tokenizers_to_compare.items():\n    tok = AutoTokenizer.from_pretrained(model_name)\n    tokens = tok.tokenize(text)\n    print(f\"{name:10s} ({tok.vocab_size:,} tokens): {tokens}\")\n\n\nText: 'Binghamton University offers excellent programs.'\n\nBERT       (30,522 tokens): ['bingham', '##ton', 'university', 'offers', 'excellent', 'programs', '.']\nGPT-2      (50,257 tokens): ['B', 'ingham', 'ton', 'ĠUniversity', 'Ġoffers', 'Ġexcellent', 'Ġprograms', '.']\nRoBERTa    (50,265 tokens): ['B', 'ingham', 'ton', 'ĠUniversity', 'Ġoffers', 'Ġexcellent', 'Ġprograms', '.']\n\n\nOutput:\nText: 'Binghamton University offers excellent programs.'\n\nBERT       (30,522 tokens): ['binghamton', 'university', 'offers', 'excellent', 'programs', '.']\nGPT-2      (50,257 tokens): ['Binghamton', 'ĠUniversity', 'Ġoffers', 'Ġexcellent', 'Ġprograms', '.']\nRoBERTa    (50,265 tokens): ['Binghamton', 'ĠUniversity', 'Ġoffers', 'Ġexcellent', 'Ġprograms', '.']\nObservations: - BERT: Uses WordPiece (## for continuations), lowercases - GPT-2/RoBERTa: Use Byte-Pair Encoding (Ġ indicates space), preserves case - Different vocab sizes lead to different granularity",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m03-text/tokenization.html#visualizing-token-embeddings",
    "href": "m03-text/tokenization.html#visualizing-token-embeddings",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "5 Visualizing Token Embeddings",
    "text": "5 Visualizing Token Embeddings\nLet’s visualize the embedding space for some university and city-related vocabulary:\n\n\nCode\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# List of (capital, country) pairs for visualization\ncapital_country_pairs = [\n    (\"Paris\", \"France\"),\n    (\"Berlin\", \"Germany\"),\n    (\"Tokyo\", \"Japan\"),\n    (\"Moscow\", \"Russia\"),\n    (\"Beijing\", \"China\"),\n    (\"Rome\", \"Italy\"),\n    (\"Ottawa\", \"Canada\")\n]\n\n# Flatten to unique terms for quick token lookup\nterms = sorted(list(set([city for city, _ in capital_country_pairs] + [country for _, country in capital_country_pairs])))\nprint(terms)\nterm_ids = [tokenizer.convert_tokens_to_ids(term) for term in terms]\nprint(term_ids)\nterm_embeddings = embedding_layer.weight[term_ids].detach().cpu().numpy()\n\n# 2D reduction using PCA\npca = PCA(n_components=2, random_state=42)\nterm_2d = pca.fit_transform(term_embeddings)\n\n# Map term to 2D coordinate for annotation and matching\ncoords = {term: term_2d[i] for i, term in enumerate(terms)}\n\nsns.set_style(\"whitegrid\")\nfig, ax = plt.subplots(figsize=(5, 5))\n\nax.scatter(term_2d[:,0], term_2d[:,1], s=180, color=\"#2980b9\", edgecolors='black', linewidth=2, zorder=10)\n\n# Draw line and annotate each pair (capital, country)\nfor (city, country) in capital_country_pairs:\n    city_xy = coords[city]\n    country_xy = coords[country]\n    ax.annotate(city.title(), city_xy, fontsize=12, fontweight='bold', ha='right', va='bottom', color='#c0392b')\n    ax.annotate(country.title(), country_xy, fontsize=12, fontweight='bold', ha='left', va='top', color='#16a085')\n    ax.plot([city_xy[0], country_xy[0]], [city_xy[1], country_xy[1]], color='#7f8c8d', linestyle='--', linewidth=2, alpha=0.65, zorder=1)\n\nax.set_xlabel(\"Dimension 1\", fontsize=13)\nax.set_ylabel(\"Dimension 2\", fontsize=13)\nax.set_title(\"Capital Cities and Their Countries in Token Embedding Space\", fontsize=15, fontweight='bold')\nax.grid(alpha=0.25, linestyle='--')\n\nplt.tight_layout()\nplt.show()\n\n\n['Beijing', 'Berlin', 'Canada', 'China', 'France', 'Germany', 'Italy', 'Japan', 'Moscow', 'Ottawa', 'Paris', 'Rome', 'Russia', 'Tokyo']\n[50256, 50256, 17940, 14581, 28572, 27079, 45001, 16504, 49757, 50256, 40313, 50256, 16347, 50256]\n\n\n\n\n\nCapital cities and their countries are close in the token embedding space, even before transformer processing.\n\n\n\n\nEven before transformers process them, token embeddings cluster by semantic domain! The transformer layers will refine these further based on context.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m03-text/tokenization.html#the-tokenization-embedding-transformer-pipeline",
    "href": "m03-text/tokenization.html#the-tokenization-embedding-transformer-pipeline",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "6 The Tokenization → Embedding → Transformer Pipeline",
    "text": "6 The Tokenization → Embedding → Transformer Pipeline\nNow we can see the complete pipeline:\n\n\nCode\ntext = \"Binghamton University offers excellent programs.\"\n\nprint(\"=\" * 70)\nprint(\"COMPLETE PIPELINE: TEXT → EMBEDDINGS\")\nprint(\"=\" * 70)\n\n# Step 1: Tokenization\nprint(\"\\n[STEP 1] Tokenization\")\ntokens = tokenizer.tokenize(text)\nprint(f\"  Text:   '{text}'\")\nprint(f\"  Tokens: {tokens}\")\n\n# Step 2: Convert to IDs\nprint(\"\\n[STEP 2] Token IDs\")\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\nfor token, tid in zip(tokens, token_ids):\n    print(f\"  '{token}' → {tid}\")\n\n# Step 3: Add special tokens\nprint(\"\\n[STEP 3] Add Special Tokens\")\ninputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=True)\nfull_tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\nprint(f\"  {full_tokens}\")\n\n# Step 4: Lookup embeddings\nprint(\"\\n[STEP 4] Lookup in Embedding Table\")\nprint(f\"  Embedding table shape: {embedding_layer.weight.shape}\")\nprint(f\"  Each token → 768-dim vector\")\n\n# Step 5: Get contextualized embeddings\nprint(\"\\n[STEP 5] Pass Through Transformer Layers\")\nwith torch.no_grad():\n    outputs = model(**inputs, output_hidden_states=True)\n    contextualized = outputs.hidden_states[-1]\n\nprint(f\"  Input:  {inputs['input_ids'].shape} (token IDs)\")\nprint(f\"  Output: {contextualized.shape} (contextualized embeddings)\")\n\nprint(\"\\n[RESULT] Each token now has context-aware meaning!\")\nprint(\"  Before: Static embedding from table\")\nprint(\"  After:  Refined by attention across all tokens\")\nprint(\"=\" * 70)\n\n\n======================================================================\nCOMPLETE PIPELINE: TEXT → EMBEDDINGS\n======================================================================\n\n[STEP 1] Tokenization\n  Text:   'Binghamton University offers excellent programs.'\n  Tokens: ['B', 'ingham', 'ton', 'ĠUniversity', 'Ġoffers', 'Ġexcellent', 'Ġprograms', '.']\n\n[STEP 2] Token IDs\n  'B' → 33\n  'ingham' → 25875\n  'ton' → 1122\n  'ĠUniversity' → 2059\n  'Ġoffers' → 4394\n  'Ġexcellent' → 6275\n  'Ġprograms' → 4056\n  '.' → 13\n\n[STEP 3] Add Special Tokens\n  ['B', 'ingham', 'ton', 'ĠUniversity', 'Ġoffers', 'Ġexcellent', 'Ġprograms', '.']\n\n[STEP 4] Lookup in Embedding Table\n  Embedding table shape: torch.Size([51200, 2048])\n  Each token → 768-dim vector\n\n[STEP 5] Pass Through Transformer Layers\n  Input:  torch.Size([1, 8]) (token IDs)\n  Output: torch.Size([1, 8, 2048]) (contextualized embeddings)\n\n[RESULT] Each token now has context-aware meaning!\n  Before: Static embedding from table\n  After:  Refined by attention across all tokens\n======================================================================\n\n\nOutput:\n======================================================================\nCOMPLETE PIPELINE: TEXT → EMBEDDINGS\n======================================================================\n\n[STEP 1] Tokenization\n  Text:   'Binghamton University offers excellent programs.'\n  Tokens: ['binghamton', 'university', 'offers', 'excellent', 'programs', '.']\n\n[STEP 2] Token IDs\n  'binghamton' → [token_id]\n  'university' → [token_id]\n  'offers' → [token_id]\n  'excellent' → [token_id]\n  'programs' → [token_id]\n  '.' → 1012\n\n[STEP 3] Add Special Tokens\n  ['[CLS]', 'binghamton', 'university', 'offers', 'excellent', 'programs', '.', '[SEP]']\n\n[STEP 4] Lookup in Embedding Table\n  Embedding table shape: torch.Size([30522, 768])\n  Each token → 768-dim vector\n\n[STEP 5] Pass Through Transformer Layers\n  Input:  torch.Size([1, 8]) (token IDs)\n  Output: torch.Size([1, 8, 768]) (contextualized embeddings)\n\n[RESULT] Each token now has context-aware meaning!\n  Before: Static embedding from table\n  After:  Refined by attention across all tokens\n======================================================================",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m03-text/tokenization.html#why-this-matters-the-strawberry-problem",
    "href": "m03-text/tokenization.html#why-this-matters-the-strawberry-problem",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "",
    "text": "Remember the famous LLM problem: “How many r’s in strawberry?”\nMany LLMs get this wrong because of tokenization:\n\n\nCode\nword = \"strawberry\"\n\n# Tokenize\ntokens = tokenizer.tokenize(word)\nprint(f\"Word: '{word}'\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Number of tokens: {len(tokens)}\")\n\n# Count 'r' in original\nr_count_actual = word.count('r')\nprint(f\"\\nActual 'r' count in word: {r_count_actual}\")\n\n# Count 'r' in tokens\nr_count_tokens = sum(token.replace('##', '').count('r') for token in tokens)\nprint(f\"'r' count visible in tokens: {r_count_tokens}\")\n\nif r_count_actual != r_count_tokens:\n    print(\"\\n⚠️  The tokenizer splits 'strawberry' in a way that might\")\n    print(\"    make it harder for the model to count letters!\")\n\n\nOutput (example):\nWord: 'strawberry'\nTokens: ['straw', '##berry']\nNumber of tokens: 2\n\nActual 'r' count in word: 3\n'r' count visible in tokens: 3\n\n✓ In this case, all 'r's are preserved across tokens\nBut with some tokenizers, words get split in unexpected ways, making character-level reasoning difficult.\n\n\n\n\n\n\nLLMs Aren’t Perfect at Character Tasks\n\n\n\nLLMs work at the token level, not character level. They struggle with: - Counting letters in words - Spelling backwards - Exact string matching\nFor these tasks, use traditional string processing, not LLMs!",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m03-text/tokenization.html#practical-implications-for-research",
    "href": "m03-text/tokenization.html#practical-implications-for-research",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "8 Practical Implications for Research",
    "text": "8 Practical Implications for Research\n\n1. Check Your Tokenization\nBefore processing a large corpus, inspect tokenization:\n\n\nCode\ndef analyze_tokenization(texts, tokenizer):\n    \"\"\"Analyze how a tokenizer handles a corpus.\"\"\"\n    total_tokens = 0\n    total_unk = 0\n    max_length_exceeded = 0\n\n    for text in texts:\n        token_ids = tokenizer.encode(text, add_special_tokens=True)\n        total_tokens += len(token_ids)\n        total_unk += token_ids.count(tokenizer.unk_token_id) if hasattr(tokenizer, 'unk_token_id') else 0\n        if len(token_ids) &gt; tokenizer.model_max_length:\n            max_length_exceeded += 1\n\n    print(f\"Corpus statistics:\")\n    print(f\"  Total documents: {len(texts)}\")\n    print(f\"  Total tokens: {total_tokens:,}\")\n    print(f\"  Avg tokens/doc: {total_tokens / len(texts):.1f}\")\n    print(f\"  Unknown tokens: {total_unk} ({100 * total_unk / total_tokens:.2f}%)\")\n    print(f\"  Docs exceeding max length: {max_length_exceeded}\")\n\n# Example corpus\ncorpus = [\n    \"Binghamton University is located in upstate New York.\",\n    \"The university offers excellent computer science programs.\",\n    \"Students enjoy the beautiful campus and vibrant community.\"\n]\n\nanalyze_tokenization(corpus, tokenizer)\n\n\nCorpus statistics:\n  Total documents: 3\n  Total tokens: 29\n  Avg tokens/doc: 9.7\n  Unknown tokens: 0 (0.00%)\n  Docs exceeding max length: 0\n\n\n\n\n2. Choosing the Right Model\nDifferent tokenizers suit different domains:\n\n\n\nUse Case\nRecommended Tokenizer\nWhy\n\n\n\n\nEnglish text\nBERT, RoBERTa\nWell-balanced, common words\n\n\nCode\nCodeBERT, CodeGen\nTrained on programming tokens\n\n\nMultilingual\nmBERT, XLM-RoBERTa\nHandles 100+ languages\n\n\nScientific text\nSciBERT\nTrained on papers, knows domain vocab\n\n\nSocial media\nBERTweet\nHandles hashtags, emoji, slang\n\n\n\n\n\n3. Preprocessing Strategy\n# Good practice: Check tokenization before full pipeline\nsample = corpus[:10]\nfor text in sample:\n    tokens = tokenizer.tokenize(text)\n    if len(tokens) &gt; 500:\n        print(f\"Warning: Long text ({len(tokens)} tokens)\")\n    if '[UNK]' in tokens:\n        print(f\"Warning: Unknown tokens in: {text[:50]}...\")",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m03-text/tokenization.html#the-bigger-picture",
    "href": "m03-text/tokenization.html#the-bigger-picture",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "3 The Bigger Picture",
    "text": "3 The Bigger Picture\nYou’ve now traced the full pipeline: raw text fractures into subword tokens, tokens map to integer IDs, and IDs retrieve vector embeddings from a learned matrix. This tokenization step is foundational—without it, the model cannot begin processing language. The transformer layers come next, using attention mechanisms to extract patterns from these embeddings.\nRemember three constraints. First, LLMs operate on subwords, not words, because vocabulary size is a memory bottleneck. Second, tokenization is learned from data, not hand-designed, meaning different models will split text differently. Third, compression has side effects—tasks like character counting fail because the model never sees individual characters as atomic units.\nWith this machinery exposed, we’re ready to examine the transformer itself—the architecture that processes these embeddings and enables LLMs to predict the next token.\n\nNext: Transformers: The Architecture Behind the Magic →",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m03-text/transformers.html",
    "href": "m03-text/transformers.html",
    "title": "Transformers",
    "section": "",
    "text": "The Spoiler: The entire transformer revolution boils down to this—static embeddings assign one vector per word, ignoring that “bank” near “river” is mathematically different from “bank” near “money.” Transformers solve this by computing context-aware representations through weighted mixing, and the weights themselves emerge from learned comparisons (Query × Key) between words. The result: machines finally understand that meaning isn’t in the word; it’s in the distribution of words around it.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Transformers: The Architecture Behind the Magic"
    ]
  },
  {
    "objectID": "m03-text/transformers.html#the-problem-transformers-solved",
    "href": "m03-text/transformers.html#the-problem-transformers-solved",
    "title": "Transformers: The Architecture Behind the Magic",
    "section": "",
    "text": "Before 2017, the dominant approach was Recurrent Neural Networks (RNNs), which processed text sequentially:\nInput: \"The cat sat on the mat\"\n\nProcessing:\nStep 1: Read \"The\" � Update hidden state h�\nStep 2: Read \"cat\" � Update hidden state h� (remembers \"The\")\nStep 3: Read \"sat\" � Update hidden state h� (remembers \"The cat\")\n...\nStep 6: Read \"mat\" � Final state h� (remembers everything... hopefully)\nProblems with RNNs:\n\nSequential processing: Must process words one-by-one (slow, can’t parallelize)\nVanishing memory: By step 6, the model has partially forgotten step 1\nLong-distance dependencies: Struggles when important context is far away\n\nExample where RNNs struggle:\n\"The animal, which had been raised on a farm with many other animals and\nhad learned to socialize with both dogs and cats, finally sat on the mat.\"\nBy the time the RNN reaches “sat”, it may have forgotten key details about “the animal.”\n\n\n\nTransformers process all words simultaneously and compute attention weights that determine which words are relevant to each other.\n\"The cat sat on the mat\"\n\nFor the word \"sat\":\n- High attention to \"cat\" (subject)\n- High attention to \"on\" (preposition indicating location)\n- Medium attention to \"mat\" (object of preposition)\n- Low attention to \"the\" (not semantically important here)\nThe model learns what to pay attention to—no hand-coded rules.\n\n\n\n\n\n\nThe Key Innovation\n\n\n\nRNNs: “Remember everything sequentially” Transformers: “Pay attention to what matters, anywhere in the text”",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Transformers: The Architecture Behind the Magic"
    ]
  },
  {
    "objectID": "m03-text/transformers.html#self-attention-the-core-mechanism",
    "href": "m03-text/transformers.html#self-attention-the-core-mechanism",
    "title": "Transformers: The Architecture Behind the Magic",
    "section": "",
    "text": "Self-attention is how transformers decide what’s relevant. Let’s build intuition with an example.\n\n\nConsider two sentences: 1. “I deposited money at the bank.” 2. “I sat by the river bank.”\nThe word “bank” is ambiguous. How does a transformer decide which meaning?\nSelf-attention computes: - In sentence 1: “bank” attends strongly to “deposited” and “money” → financial institution - In sentence 2: “bank” attends strongly to “river” and “sat” → river edge\nThe model learns these attention patterns from data, without explicit programming.\n\n\n\nAdjust the sliders to see how mixing “bank” with other words changes its representation.\n\n    \n    \n    \n    transformer_viz.visualize_contextualization()\n\n\n\n\nFor each word, the attention mechanism creates three different vector representations:\n\nQuery (Q): “What am I looking for?” (What context do I need?)\nKey (K): “What do I offer?” (What information do I have?)\nValue (V): “What do I actually contain?” (What’s my semantic content?)\n\nHow are these created?\nEach vector is generated through a linear transformation of the original word embedding:\n\nQ = XW_Q + b_Q, \\quad K = XW_K + b_K, \\quad V = XW_V + b_V\n\nwhere X is the input embedding, and W_Q, W_K, W_V are learned weight matrices.\nThe Attention Computation:\n\nCompute attention scores: For each query word, calculate how much it should attend to each key word using dot products: \n\\text{Attention Score}_{ij} = Q_i \\cdot K_j^T\n\nCreate QK matrix: This produces a matrix where entry (i,j) tells us how much word i attends to word j: \n\\text{QK} = QK^T\n\nApply softmax: Normalize the scores to get attention weights that sum to 1: \n\\text{Attention Weights} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)\n where d_k is the dimension of the key vectors (the division helps stabilize training)\nWeighted sum of Values: Multiply attention weights by value vectors to get the final output: \n\\text{Output} = \\text{Attention Weights} \\times V\n\n\nAnalogy: You’re in a library (the sentence). You have a question (Query). Books have titles (Keys) and content (Values). You: 1. Compare your question to all book titles (compute attention scores via QK^T) 2. Decide which books are most relevant (apply softmax to get weights) 3. Take a weighted combination of relevant books’ content (weighted sum of Values)\nThe result is a context-aware representation that focuses on the most relevant words.\n\n\n\nExplore how Query (Q) and Key (K) transformations affect the attention scores.\n\n    \n    \n    \n    transformer_viz.visualize_attention_mechanism()\n\n\n\n\nLet’s visualize attention for the sentence: “The cat sat on the mat”\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Simulated attention weights (would come from an actual transformer)\nwords = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\nn = len(words)\n\n# Manually crafted attention pattern (realistic but simplified)\nattention = np.array([\n    [0.6, 0.2, 0.1, 0.05, 0.03, 0.02],  # \"The\" � mostly attends to \"cat\"\n    [0.1, 0.5, 0.3, 0.05, 0.03, 0.02],  # \"cat\" � self + \"sat\"\n    [0.05, 0.4, 0.2, 0.15, 0.05, 0.15], # \"sat\" � \"cat\" (subject) + \"mat\" (object)\n    [0.05, 0.1, 0.2, 0.3, 0.15, 0.20],  # \"on\" � preposition, attends to surrounding context\n    [0.03, 0.05, 0.05, 0.1, 0.5, 0.27], # \"the\" � mostly \"mat\"\n    [0.02, 0.05, 0.15, 0.2, 0.15, 0.43] # \"mat\" � self + \"on\"\n])\n\nsns.set_style(\"white\")\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.heatmap(attention, annot=True, fmt=\".2f\",\n            xticklabels=words, yticklabels=words,\n            cmap=\"YlOrRd\", vmin=0, vmax=0.6,\n            cbar_kws={'label': 'Attention Weight'}, ax=ax)\nax.set_xlabel(\"Attends To\", fontsize=12, fontweight='bold')\nax.set_ylabel(\"Word\", fontsize=12, fontweight='bold')\nax.set_title(\"Self-Attention Heatmap\", fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nSelf-attention heatmap for ‘The cat sat on the mat’. Darker colors indicate stronger attention. Notice how ‘sat’ pays attention to ‘cat’ (subject) and ‘mat’ (object).\n\n\n\n\nObservations: - “sat” (row 3) strongly attends to “cat” (0.40)—identifies the subject - “sat” also attends to “mat” (0.15)—identifies where the action happens - “The” (row 1) attends mostly to “cat”—articles attend to their nouns - Diagonal has moderate values—words always attend somewhat to themselves\nThis pattern is learned from data, not hand-coded.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Transformers: The Architecture Behind the Magic"
    ]
  },
  {
    "objectID": "m03-text/transformers.html#multi-head-attention-multiple-perspectives",
    "href": "m03-text/transformers.html#multi-head-attention-multiple-perspectives",
    "title": "Transformers: The Architecture Behind the Magic",
    "section": "",
    "text": "One attention mechanism captures one type of relationship. But language has many types of relationships: - Syntactic (subject-verb, adjective-noun) - Semantic (synonyms, antonyms) - Discourse (anaphora, coreference)\nMulti-head attention runs multiple attention mechanisms in parallel, each learning different patterns.\n\n\nInstead of having one set of Q, K, V transformations, we have h different sets (where h is the number of heads):\n\n\\text{head}_i = \\text{Attention}(XW_i^Q, XW_i^K, XW_i^V)\n\nEach head has its own learned weight matrices W_i^Q, W_i^K, W_i^V, allowing it to focus on different aspects of the relationships.\nThe outputs from all heads are then concatenated and linearly transformed:\n\n\\text{MultiHead}(X) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O\n\nwhere W^O is another learned weight matrix.\nWhy this works: - Each head can learn to attend to different positions and relationships - Head 1 might focus on syntactic dependencies (subject-verb) - Head 2 might capture semantic similarity - Head 3 might track long-range dependencies - The final linear layer combines all perspectives into a unified representation\nAnalogy: You’re editing a document. You might review it multiple times: - Grammar check (syntax) - Head 1 - Fact-checking (semantics) - Head 2 - Flow and coherence (discourse) - Head 3\nEach “head” is a different type of review, and you combine all insights at the end.\n\n\n\nFor “The cat sat on the mat”:\nHead 1 (Syntactic): - “sat”—“cat” (subject-verb relationship) - “on”—“mat” (preposition-object relationship)\nHead 2 (Semantic): - “cat”—“mat” (both are physical objects) - “sat”—“on” (action-location relationship)\nHead 3 (Coreference): - “the”—“cat”, “the”—“mat” (determiners to nouns)\nThe final representation combines all heads, capturing multiple aspects of meaning simultaneously.\n\n\n\n\n\n\nHow Many Heads?\n\n\n\nModern transformers typically use 8-16 attention heads per layer. BERT uses 12 heads, GPT-3 uses 96. More heads = more expressive, but also more parameters to train.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Transformers: The Architecture Behind the Magic"
    ]
  },
  {
    "objectID": "m03-text/transformers.html#the-transformer-architecture",
    "href": "m03-text/transformers.html#the-transformer-architecture",
    "title": "Transformers: The Architecture Behind the Magic",
    "section": "",
    "text": "A full transformer consists of multiple components. Let’s break it down.\n\n\nThe transformer receives input through three key steps:\n\nTokenization: Split text into tokens (words or subwords)\n\"Community detection\" → [\"Community\", \"detection\"]\nWe covered this in detail in the tokenization section—how text becomes token IDs.\nToken embeddings: Convert tokens to vectors (embedding table lookup)\n\"Community\" → [0.23, -0.45, 0.67, ...]\nEach token ID looks up its embedding in the learned embedding table.\nPositional encoding: Add information about word order\nWithout position: \"cat sat mat\" vs. \"mat sat cat\" look identical\nWith position: Order is preserved in embeddings\nPositional encodings are added to embeddings so the model knows word order.\n\n\n\n\nEach transformer block consists of several key components that work together:\n1. Multi-Head Self-Attention - Computes attention between all word pairs - Multiple heads capture different relationships simultaneously - Each head learns different attention patterns (syntax, semantics, discourse) - Outputs are concatenated and linearly transformed - Output: Context-aware representations\n2. Layer Normalization - Normalizes the features within each token embedding - Addresses internal covariate shift (when earlier layer updates change activation distributions) - Formula: \n  \\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n   where \\mu and \\sigma^2 are mean and variance computed across features, and \\gamma, \\beta are learnable parameters - Makes training more stable by keeping activations in a consistent range - Independent of batch size (unlike batch normalization)\n3. Residual Connections - Adds the input directly to the output: \\text{output} = F(x) + x - Creates “shortcuts” for gradient flow during backpropagation - Allows network to learn incremental refinements rather than complete transformations - Prevents degradation in very deep networks - Makes it easier to train networks with dozens of layers\n4. Feed-Forward Network - Fully connected network applied to each position independently - Two linear transformations with a nonlinearity (typically ReLU or GELU) in between: \n  \\text{FFN}(x) = \\text{max}(0, xW_1 + b_1)W_2 + b_2\n   - Adds expressive power and nonlinearity beyond attention - Typically expands dimension (e.g., 768 → 3072) then projects back\nThe Complete Flow:\nInput Token Embeddings\n   ↓\nMulti-Head Attention → Add & Norm (residual connection + layer norm)\n   ↓\nFeed-Forward Network → Add & Norm (residual connection + layer norm)\n   ↓\nOutput Token Embeddings\nEach component plays a crucial role: - Attention: Captures relationships between tokens - Layer Norm: Stabilizes training - Residual Connections: Enables deep architectures - FFN: Adds nonlinear transformations\n\n\n\nTransformers stack multiple blocks (6-24 or more):\nInput: \"The cat sat on the mat\"\n   �\nLayer 1: Basic patterns (word relationships)\n   �\nLayer 2: Syntactic structure (grammar)\n   �\nLayer 3: Semantic relationships (meaning)\n   �\n...\n   �\nLayer 12: Abstract concepts (high-level understanding)\n   �\nOutput: Rich contextual representations\nEarly layers learn surface patterns (punctuation, common words). Deeper layers learn abstract concepts and reasoning.\n\n\n\n\n\n\nWhy Stack Layers?\n\n\n\nEach layer builds on the previous one, creating hierarchies of abstraction similar to how CNNs learn edges—textures—objects in image processing. In text, it’s: words—phrases—sentences—concepts.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Transformers: The Architecture Behind the Magic"
    ]
  },
  {
    "objectID": "m03-text/transformers.html#encoder-vs.-decoder-two-transformer-flavors",
    "href": "m03-text/transformers.html#encoder-vs.-decoder-two-transformer-flavors",
    "title": "Transformers: The Architecture Behind the Magic",
    "section": "",
    "text": "There are two main transformer architectures:\n\n\nPurpose: Understanding text (classification, extraction, embeddings)\nArchitecture: - Bidirectional attention (can see all words at once) - Used for: Sentence embeddings, classification, named entity recognition\nExample task: “Is this paper about networks or biology?” - Input: Abstract - Output: Classification label\n\n\n\nPurpose: Generating text (completion, chat, writing)\nArchitecture: - Causal attention (masked attention): can only see previous words, not future ones - This prevents the model from “cheating” by looking ahead during generation - Implemented by masking out future positions in the attention matrix - Used for: Text generation, dialogue, completion\nExample task: “Complete this sentence: ‘The cat sat on the…’” - Input: Partial sentence - Output: Continuation (“mat”, “sofa”, etc.)\nWhy causal attention? When generating “The cat sat on the mat”, the model generates one word at a time: 1. Given “The” → predict “cat” 2. Given “The cat” → predict “sat” 3. Given “The cat sat” → predict “on”\nAt each step, it can only attend to previous tokens, not future ones.\n\n\n\nPurpose: Sequence-to-sequence tasks (translation, summarization)\nArchitecture: - Encoder processes input with bidirectional attention - Decoder generates output with: 1. Masked self-attention (causal, like GPT) 2. Cross-attention to encoder outputs - Used for: Translation, summarization, question answering\nWhat is Cross-Attention?\nCross-attention allows the decoder to attend to the encoder’s output. Unlike self-attention (where Q, K, V all come from the same sequence), in cross-attention: - Query (Q): comes from the decoder (what the decoder is generating) - Key (K) and Value (V): come from the encoder (the input sequence)\n\n\\text{CrossAttention}(Q_{\\text{decoder}}, K_{\\text{encoder}}, V_{\\text{encoder}})\n\nThis lets the decoder focus on relevant parts of the input while generating output.\nExample task: “Translate ‘Hello world’ to French”\n\nEncoder: Processes “Hello world” with bidirectional attention\n\nCreates rich representations understanding the full context\n\nDecoder: Generates “Bonjour monde” word by word\n\nStep 1: Generate “Bonjour”\n\nSelf-attention: looks at previously generated tokens (none yet)\nCross-attention: attends to “Hello” in encoder output\n\nStep 2: Generate “monde”\n\nSelf-attention: looks at “Bonjour”\nCross-attention: attends to “world” in encoder output\n\n\n\nThe cross-attention mechanism is what allows the decoder to “align” the output with the input, crucial for translation and similar tasks.\nWhich one are you using? - sentence-transformers: Encoder (BERT-based)—for embeddings - ChatGPT, Gemma: Decoder (GPT-based)—for generation - Translation models: Encoder-Decoder—for sequence mapping",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Transformers: The Architecture Behind the Magic"
    ]
  },
  {
    "objectID": "m03-text/transformers.html#why-transformers-changed-everything",
    "href": "m03-text/transformers.html#why-transformers-changed-everything",
    "title": "Transformers: The Architecture Behind the Magic",
    "section": "",
    "text": "RNNs: Must process word-by-word (sequential) Transformers: Process all words simultaneously (parallel)\nResult: 100x faster training on modern GPUs.\n\n\n\nRNNs: Forget information after ~100 tokens Transformers: Can attend to any position (limited by context window, typically 2K-8K tokens)\nResult: Better understanding of context in long documents.\n\n\n\nPre-train one large model on massive data, then fine-tune for specific tasks:\nPre-training (expensive, once):\nTrain BERT on billions of words---learns general language\n\nFine-tuning (cheap, many times):\nTrain on 1,000 medical abstracts---learns medical language\nTrain on 5,000 legal documents---learns legal language\nResult: State-of-the-art performance with little task-specific data.\n\n\n\nTransformers scale beautifully: - More data—better performance - More parameters—better performance - Bigger models—emergent abilities (reasoning, math, code)\nResult: GPT-3 (175B params) vastly outperforms GPT-2 (1.5B params), even though the architecture is nearly identical.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Transformers: The Architecture Behind the Magic"
    ]
  },
  {
    "objectID": "m03-text/transformers.html#from-bert-to-gpt-to-gemma-the-evolution",
    "href": "m03-text/transformers.html#from-bert-to-gpt-to-gemma-the-evolution",
    "title": "Transformers: The Architecture Behind the Magic",
    "section": "",
    "text": "Encoder-only transformer\nTrained with masked language modeling (“predict the [MASK] word”)\n110M-340M parameters\nUse case: Text understanding, embeddings\n\n\n\n\n\nDecoder-only transformer\nTrained to predict next word\n1.5B parameters\nUse case: Text generation\n\n\n\n\n\nScaled-up GPT-2\n175B parameters\nEmergent abilities: Few-shot learning, reasoning, code generation\nUse case: General-purpose language tasks\n\n\n\n\n\nOpen-source decoder model from Google\n2B-27B parameters\nEfficient, fast, runs locally\nUse case: Research, education, private applications\n\nThe trend: More parameters, more data, more capabilities. But the core architecture—self-attention and transformer blocks—remains the same since 2017.\n\n\n\n\n\n\nAttention Is All You Need\n\n\n\nThe original transformer paper (Vaswani et al., 2017) was titled “Attention Is All You Need.” The name was bold but accurate—self-attention turned out to be sufficient for nearly all NLP tasks, making RNNs largely obsolete.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Transformers: The Architecture Behind the Magic"
    ]
  },
  {
    "objectID": "m03-text/transformers.html#visualizing-attention-in-real-models",
    "href": "m03-text/transformers.html#visualizing-attention-in-real-models",
    "title": "Transformers: The Architecture Behind the Magic",
    "section": "",
    "text": "Let’s look at attention patterns from an actual BERT model analyzing text.\n\n\nSentence: “The scientist published her paper.”\n\n\nCode\n# Simulated attention (realistic pattern from BERT-like model)\nwords = [\"The\", \"scientist\", \"published\", \"her\", \"paper\"]\nn = len(words)\n\n# Attention for \"her\" (row 3)\nattention = np.array([\n    [0.5, 0.3, 0.1, 0.05, 0.05],  # \"The\"---\"scientist\"\n    [0.2, 0.5, 0.2, 0.05, 0.05],  # \"scientist\"\n    [0.05, 0.3, 0.4, 0.1, 0.15],  # \"published\"---\"scientist\", self\n    [0.05, 0.6, 0.1, 0.2, 0.05],  # \"her\"---\"scientist\" (coreference!)\n    [0.05, 0.2, 0.1, 0.15, 0.5],  # \"paper\"\n])\n\nsns.set_style(\"white\")\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.heatmap(attention, annot=True, fmt=\".2f\",\n            xticklabels=words, yticklabels=words,\n            cmap=\"Purples\", vmin=0, vmax=0.6,\n            cbar_kws={'label': 'Attention Weight'}, ax=ax)\nax.set_xlabel(\"Attends To\", fontsize=12, fontweight='bold')\nax.set_ylabel(\"Word\", fontsize=12, fontweight='bold')\nax.set_title(\"Attention: Resolving 'her'---'scientist'\", fontsize=14, fontweight='bold')\n\n# Highlight the key attention\nfrom matplotlib.patches import Rectangle\nax.add_patch(Rectangle((1, 3), 1, 1, fill=False, edgecolor='red', lw=3))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nAttention pattern showing how ‘her’ attends to ‘scientist’, resolving the coreference. The model learned to link pronouns to their referents.\n\n\n\n\nNotice the red box: “her” strongly attends to “scientist” (0.60), correctly identifying the referent.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Transformers: The Architecture Behind the Magic"
    ]
  },
  {
    "objectID": "m03-text/transformers.html#limitations-of-transformers",
    "href": "m03-text/transformers.html#limitations-of-transformers",
    "title": "Transformers: The Architecture Behind the Magic",
    "section": "",
    "text": "Despite their power, transformers have limitations:\n\n\nAttention computes relationships between all word pairs: - 10 words: 100 comparisons - 100 words: 10,000 comparisons - 1,000 words: 1,000,000 comparisons\nFor very long texts, this becomes prohibitively expensive. Context windows (max input length) are typically 2K-8K tokens.\n\n\n\nTransformers only “remember” what’s in the current context window. For conversations or documents longer than the window, information gets forgotten.\n(Partial solutions: retrieval-augmented generation, memory mechanisms)\n\n\n\nTransformers are pattern matchers. They don’t have beliefs, goals, or understanding—they predict probable text based on patterns. This leads to: - Hallucinations (confident false statements) - Lack of common sense - Brittleness on out-of-distribution inputs\n\n\n\nTraining large transformers requires: - Millions of dollars in compute - Months of training time - Massive datasets - Significant energy consumption\n(But you can use pre-trained models, which is why this isn’t a blocker for research)",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Transformers: The Architecture Behind the Magic"
    ]
  },
  {
    "objectID": "m03-text/transformers.html#the-bigger-picture",
    "href": "m03-text/transformers.html#the-bigger-picture",
    "title": "Transformers: The Architecture Behind the Magic",
    "section": "",
    "text": "You now understand the transformer architecture—the engine behind modern NLP:\n\nSelf-attention: Models learn to focus on relevant context\nMulti-head attention: Captures multiple types of relationships simultaneously\nStacking layers: Builds hierarchies from words to concepts\nEncoder/Decoder variants: Different architectures for different tasks\n\nWhen you use an LLM: 1. Text—Embeddings (tokens to vectors) 2. Embeddings—Transformer layers (attention + feed-forward) 3. Transformer output—Task-specific head (classification, generation, etc.)\nBut transformers weren’t the first technique for text embeddings. Before BERT and GPT, there was Word2vec—a simpler, faster method that’s still useful today. Let’s step back and see where embeddings came from.\n\nNext: Word Embeddings: Where It Started?",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Transformers: The Architecture Behind the Magic"
    ]
  },
  {
    "objectID": "m03-text/tokenization.html#why-this-matters-the-binghamton-problem",
    "href": "m03-text/tokenization.html#why-this-matters-the-binghamton-problem",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "7 Why This Matters: The “Binghamton” Problem",
    "text": "7 Why This Matters: The “Binghamton” Problem\nRemember the famous LLM problem: “How many n’s in Binghamton?”\nMany LLMs get this wrong because of tokenization:\n\n\nCode\nword = \"binghamton\"\n\n# Tokenize\ntokens = tokenizer.tokenize(word)\nprint(f\"Word: '{word}'\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Number of tokens: {len(tokens)}\")\n\n# Count 'n' in original\nn_count_actual = word.count('n')\nprint(f\"\\nActual 'n' count in word: {n_count_actual}\")\n\n# Count 'n' in tokens\nn_count_tokens = sum(token.replace('##', '').count('n') for token in tokens)\nprint(f\"'n' count visible in tokens: {n_count_tokens}\")\n\nif n_count_actual != n_count_tokens:\n    print(\"\\n⚠️  The tokenizer splits 'binghamton' in a way that might\")\n    print(\"    make it harder for the model to count letters!\")\n\n\nWord: 'binghamton'\nTokens: ['bing', 'ham', 'ton']\nNumber of tokens: 3\n\nActual 'n' count in word: 2\n'n' count visible in tokens: 2\n\n\nOutput (example):\nWord: 'binghamton'\nTokens: ['binghamton']\nNumber of tokens: 1\n\nActual 'n' count in word: 3\n'n' count visible in tokens: 3\n\n✓ In this case, all 'n's are preserved in the token\nBut with some tokenizers, words get split in unexpected ways, making character-level reasoning difficult.\n\n\n\n\n\n\nLLMs Aren’t Perfect at Character Tasks\n\n\n\nLLMs work at the token level, not character level. They struggle with: - Counting letters in words - Spelling backwards - Exact string matching\nFor these tasks, use traditional string processing, not LLMs!",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m03-text/tokenization.html#from-text-to-tokens",
    "href": "m03-text/tokenization.html#from-text-to-tokens",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "2 From Text to Tokens",
    "text": "2 From Text to Tokens\nTokenization breaks down words and phrases into smaller pieces called subwords. Each subword is a string of characters that the model has learned to represent a word. It is the key building block of the model’s understanding of text.\n\n\nWhy use subword tokenization? If we used only whole words, the vocabulary would need to be massive—millions of words! This is slow and requires a huge amount of memory.\nSubword tokenization solves this by focusing on frequently occurring word parts (subwords). With a vocabulary of about 30,000 subwords, the model can efficiently handle both common and rare or made-up words by breaking them into pieces. This way, even unfamiliar or novel words can still be understood as combinations of known parts. This lets the model handle words it never saw during training!\nLet’s tokenize a simple sentence and see what happens.\n\ntext = \"Binghamton University.\"\n\ntokens = tokenizer.tokenize(text) # Tokenize the text\n\n\n\nCode\nprint(f\"Tokens: {tokens}\")\n\n\nTokens: ['B', 'ingham', 'ton', 'ĠUniversity', '.']\n\n\n“Binghamton” is split into subwords: ‘B’, ‘ingham’, ‘ton’. This is a subword tokenization. Many tokenizers break down rare or compound words into smaller pieces. On the other hand, common words are left as is.\n\n\nThe Ġ character (U+0120, Latin Capital Letter G with dot above) is used by GPT-style tokenizers to represent spaces. When you see ĠUniversity, it means “University” preceded by a space. This is how Byte-Pair Encoding (BPE) tokenizers preserve word boundaries while still using subword tokenization.\n\n\nCode\ntexts = [\n    \"Bearcats\",\n    \"New York\",\n]\n\nprint(\"Word tokenization examples:\\n\")\nfor text in texts:\n    tokens = tokenizer.tokenize(text)\n    print(f\"{text:10s} → {tokens}\")\n\n\nWord tokenization examples:\n\nBearcats   → ['Bear', 'cats']\nNew York   → ['New', 'ĠYork']\n\n\n\n\nCheck out OpenAI’s tokenizer to see how different tokenizers work.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m03-text/tokenization.html#from-token-ids-to-embeddings",
    "href": "m03-text/tokenization.html#from-token-ids-to-embeddings",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "4 From Token IDs to Embeddings",
    "text": "4 From Token IDs to Embeddings\n\nNow let’s see how token IDs become the embeddings we’ve been using. For this, we need to load the actual model (not just the tokenizer).\nTo see how token IDs become embeddings, we need to load the actual model (not just the tokenizer).\n\nfrom transformers import AutoModelForCausalLM\nimport torch\n\n# load the model\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Retrieve the embedding layer\nembedding_layer = model.model.embed_tokens\n\nThis embedding layer is a simple lookup table that maps token IDs to embeddings. It is a 51200 × 2048 matrix, where each row represents the embedding of a token in the vocabulary, with 2048 dimensions. Let’s see the first 10 dimensions of the first 5 rows.\n\n\nCode\nprint(embedding_layer.weight[:5, :10])\n\n\ntensor([[ 0.0097, -0.0155,  0.0603,  0.0326, -0.0108, -0.0271, -0.0273,  0.0178,\n         -0.0242,  0.0100],\n        [ 0.0243,  0.0543,  0.0178, -0.0679, -0.0130,  0.0265, -0.0423, -0.0287,\n         -0.0051, -0.0179],\n        [-0.0416,  0.0370, -0.0160, -0.0254, -0.0371, -0.0208, -0.0023,  0.0647,\n          0.0468,  0.0013],\n        [-0.0051, -0.0044,  0.0248, -0.0489,  0.0399,  0.0005, -0.0070,  0.0148,\n          0.0030,  0.0070],\n        [ 0.0289,  0.0362, -0.0027, -0.0775, -0.0136, -0.0203, -0.0566, -0.0558,\n          0.0269, -0.0067]], grad_fn=&lt;SliceBackward0&gt;)\n\n\nNow, each token ID is mapped to a 2048-dimensional embedding. This is what LLM sees when it sees a token.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m03-text/tokenization.html#the-embedding-table-unboxing-deeper",
    "href": "m03-text/tokenization.html#the-embedding-table-unboxing-deeper",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "5 The Embedding Table (Unboxing Deeper)",
    "text": "5 The Embedding Table (Unboxing Deeper)\nThe model has an embedding table—a giant matrix mapping token IDs to vectors:\n\n\nCode\n# Access the embedding layer\n# For Phi models, embeddings are at model.model.embed_tokens\ntry:\n    embedding_layer = model.model.embed_tokens\nexcept AttributeError:\n    # Fallback for other model architectures\n    embedding_layer = model.embeddings.word_embeddings\n\nprint(\"Embedding table:\")\nprint(f\"  Shape: {embedding_layer.weight.shape}\")\nprint(f\"  (vocab_size × embedding_dim) = ({tokenizer.vocab_size} × 768)\")\n\n# Get embedding for a specific token\ntoken = \"binghamton\"\ntoken_id = tokenizer.convert_tokens_to_ids(token)\ntoken_embedding = embedding_layer.weight[token_id]\n\nprint(f\"\\nEmbedding for '{token}':\")\nprint(f\"  Token ID: {token_id}\")\nprint(f\"  Embedding (first 10 dims): {token_embedding[:10].detach().numpy()}\")\nprint(f\"  Embedding (last 10 dims): {token_embedding[-10:].detach().numpy()}\")\n\n\nEmbedding table:\n  Shape: torch.Size([51200, 2048])\n  (vocab_size × embedding_dim) = (50257 × 768)\n\nEmbedding for 'binghamton':\n  Token ID: 50256\n  Embedding (first 10 dims): [ 0.00532532  0.0019331   0.00114632 -0.00167465 -0.00521469 -0.00177574\n -0.00370789  0.00145721 -0.00068426  0.00107861]\n  Embedding (last 10 dims): [-0.00102997  0.00419998  0.00065565  0.00163651 -0.0026474   0.00219727\n  0.00060272 -0.00290298  0.00075436 -0.00103664]\n\n\nOutput:\nEmbedding table:\n  Shape: torch.Size([30522, 768])\n  (vocab_size × embedding_dim) = (30,522 × 768)\n\nEmbedding for 'binghamton':\n  Token ID: [token_id]\n  Embedding (first 10 dims): [ 0.023 -0.145  0.267 ...]\n  Embedding (last 10 dims): [ 0.089 -0.234  0.156 ...]\nThis embedding table has 30,522 × 768 = 23 million parameters just for token embeddings! Each of these numbers was learned during training to encode semantic relationships.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m03-text/tokenization.html#from-tokens-to-token-ids",
    "href": "m03-text/tokenization.html#from-tokens-to-token-ids",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "3 From Tokens to Token IDs",
    "text": "3 From Tokens to Token IDs\nTokens are still strings. The model needs numbers. Each token maps to a unique integer ID.\n\n\nCode\ntext = \"Binghamton University\"\n\n# Get token IDs\ntoken_ids = tokenizer.encode(text, add_special_tokens=False)\ntokens = tokenizer.tokenize(text)\n\nprint(\"Token → Token ID mapping:\\n\")\nfor token, token_id in zip(tokens, token_ids):\n    print(f\"{token:10s} → {token_id:6d}\")\n\n\nToken → Token ID mapping:\n\nB          →     33\ningham     →  25875\nton        →   1122\nĠUniversity →   2059\n\n\nEach token has a unique ID. The model’s vocabulary is essentially a dictionary: {token: token_id}. Let’s look inside the vocabulary itself.\n\n# Get the full vocabulary\nvocab = tokenizer.get_vocab()\n\n# Sample some tokens\nsample_tokens = list(vocab.items())[:5]\nfor token, id in sample_tokens:\n    print(f\"  {id:6d}: '{token}'\")\n\n   34722: 'Ġchew'\n   22536: 'Ġambulance'\n   39037: 'ĠSurgery'\n   36583: 'Ġdivert'\n   30936: 'ĠWORK'\n\n\nSome LLMs add special tokens to mark sentence boundaries. For Phi-1.5, the special token is &lt;|endoftext|&gt; and used during training.\nLet’s check how whether Phi-1.5 has this special token.\n\ntoken_id = [50256] # the token ID of &lt;|endoftext|&gt;\ntoken = tokenizer.convert_ids_to_tokens(token_id)[0]\nprint(f\"Token ID: {token_id} → Token: {token}\")\n\nToken ID: [50256] → Token: &lt;|endoftext|&gt;\n\n\nNote that the token ID 50256 is a Phi specific token ID, and the special tokens vary from model to model.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m03-text/word-embeddings.html",
    "href": "m03-text/word-embeddings.html",
    "title": "Word Embeddings: Where It Started",
    "section": "",
    "text": "You’ve seen modern contextual embeddings from transformers. They’re powerful, capturing nuanced meaning that depends on context. But they’re also complex, computationally expensive, and sometimes overkill for simple tasks.\nBefore BERT and GPT, there was Word2vec—introduced in 2013 by Tomas Mikolov at Google. It’s simpler, faster, and produces static embeddings: each word gets one fixed vector, regardless of context.\nWord2vec might seem outdated compared to transformers, but it’s still widely used because: - Fast: Train on millions of documents in minutes - Lightweight: Small models (~100MB) vs. gigabytes for transformers - Interpretable: Captures explicit semantic relationships (“king” - “man” + “woman” = “queen”) - Good enough: For many tasks, static embeddings work just fine\nThis section explains where embeddings came from, how Word2vec works intuitively, and when to use static vs. contextual embeddings.\n\n\nWord2vec is built on a simple but profound idea:\n\n“You shall know a word by the company it keeps.” — J.R. Firth, 1957\n\nWords that appear in similar contexts tend to have similar meanings.\n\n\nConsider these sentences: - “The cat sat on the mat.” - “The dog sat on the mat.” - “The cat chased the mouse.” - “The dog chased the rabbit.”\n“Cat” and “dog” appear in similar contexts (“sat on the mat”, “chased…”). Therefore, they should have similar embeddings.\nNow consider: - “The theorem was proved in 1995.” - “The conjecture was proved in 1995.”\n“Theorem” and “conjecture” also appear in similar contexts, so they should have similar embeddings—even though they’re very different from “cat” and “dog.”\nKey insight: We don’t need to manually encode that “cat” is an animal or “theorem” is a mathematical statement. The model learns these relationships automatically from context.\n\n\n\n\nWord2vec learns embeddings by training a simple neural network to predict: 1. Skip-gram: Given a word, predict its context 2. CBOW (Continuous Bag-of-Words): Given context, predict the word\nBoth approaches lead to similar embeddings. We’ll focus on Skip-gram because it’s more intuitive.\n\n\nTraining setup:\nSentence: \"The cat sat on the mat\"\nTarget word: \"cat\"\nContext window (size=2): [\"The\", \"sat\"]\n\nTask: Given \"cat\", predict you'll see \"The\" and \"sat\" nearby\nThe model learns embeddings such that: - Words with similar contexts get similar embeddings - Embeddings encode semantic relationships\n\n\n\n\nInitialize: Random vectors for each word\nSample: Pick a word and its context from training data\nPredict: Use the word’s embedding to predict context words\nUpdate: Adjust embeddings to improve predictions\nRepeat: Millions of times across billions of words\n\nAfter training, embeddings capture semantic structure without anyone explicitly defining it.\n\n\n\n\n\n\nWhy This Works\n\n\n\nIf “cat” often appears near “furry,” “pet,” and “meow,” its embedding learns to activate for animal-related contexts. If “dog” appears in similar contexts, its embedding will be similar to “cat’s.”\nThe model discovers that “cat” and “dog” are related not because we told it, but because they share statistical patterns in text.\n\n\n\n\n\n\nLet’s work with pre-trained Word2vec embeddings using the gensim library.\n\n\nCode\nimport gensim.downloader as api\nimport numpy as np\n\n# Load pre-trained Word2vec embeddings (Google News corpus, ~100B words)\n# This is a large download (~1.6GB), so it may take a minute\nprint(\"Loading Word2vec model (this may take a moment)...\")\nmodel = api.load(\"word2vec-google-news-300\")\nprint(f\"Loaded embeddings for {len(model)} words, each with {model.vector_size} dimensions\")\n\n\nOutput:\nLoading Word2vec model (this may take a moment)...\nLoaded embeddings for 3000000 words, each with 300 dimensions\nThis model has embeddings for 3 million words, each represented as a 300-dimensional vector.\n\n\n\n\nCode\n# Find words most similar to \"network\"\nsimilar_to_network = model.most_similar(\"network\", topn=10)\n\nprint(\"Words most similar to 'network':\")\nfor word, similarity in similar_to_network:\n    print(f\"  {word:20s} {similarity:.3f}\")\n\n\nOutput:\nWords most similar to 'network':\n  networks             0.732\n  cable_network        0.682\n  television_network   0.654\n  broadcasting         0.623\n  cable_television     0.612\n  radio_network        0.598\n  telecoms             0.587\n  broadcaster          0.579\n  TV_network           0.571\n  communications       0.563\nThe model learned that “network” is related to broadcasting, telecommunications, and media—despite never being told these definitions.\n\n\n\nLet’s compare similarities across different domains:\n\n\nCode\nwords = [\"network\", \"graph\", \"community\", \"theorem\", \"protein\", \"cat\"]\n\nprint(\"Pairwise similarities:\")\nprint(f\"{'':12s}\", end=\"\")\nfor w in words:\n    print(f\"{w:12s}\", end=\"\")\nprint()\n\nfor w1 in words:\n    print(f\"{w1:12s}\", end=\"\")\n    for w2 in words:\n        if w1 in model and w2 in model:\n            sim = model.similarity(w1, w2)\n            print(f\"{sim:12.3f}\", end=\"\")\n        else:\n            print(f\"{'N/A':12s}\", end=\"\")\n    print()\n\n\nOutput:\n            network     graph       community   theorem     protein     cat\nnetwork        1.000       0.312       0.385       0.187       0.143       0.089\ngraph          0.312       1.000       0.245       0.298       0.112       0.076\ncommunity      0.385       0.245       1.000       0.156       0.134       0.098\ntheorem        0.187       0.298       0.156       1.000       0.198       0.065\nprotein        0.143       0.112       0.134       0.198       1.000       0.102\ncat            0.089       0.076       0.098       0.065       0.102       1.000\nObservations: - “network” and “community” are moderately similar (0.385) — both social concepts - “graph” and “theorem” have some similarity (0.298) — both mathematical - “cat” is dissimilar to everything else — different domain entirely\n\n\n\n\nOne of Word2vec’s most striking properties: semantic relationships become vector arithmetic.\n\n\n\n\nCode\n# Vector arithmetic\nresult = model.most_similar(positive=['king', 'woman'], negative=['man'], topn=5)\n\nprint(\"king - man + woman =\")\nfor word, similarity in result:\n    print(f\"  {word:15s} {similarity:.3f}\")\n\n\nOutput:\nking - man + woman =\n  queen           0.711\n  monarch         0.619\n  princess        0.590\n  crown_prince    0.567\n  prince          0.561\nThe model learned that “king” relates to “man” as “queen” relates to “woman”—a relationship captured by vector subtraction and addition!\n\n\n\n\n\nCode\n# Paris - France + Germany = Berlin\nresult = model.most_similar(positive=['Paris', 'Germany'], negative=['France'], topn=3)\nprint(\"\\nParis - France + Germany =\")\nfor word, similarity in result:\n    print(f\"  {word:15s} {similarity:.3f}\")\n\n# Swimming - swim + run = running\nresult = model.most_similar(positive=['swimming', 'run'], negative=['swim'], topn=3)\nprint(\"\\nswimming - swim + run =\")\nfor word, similarity in result:\n    print(f\"  {word:15s} {similarity:.3f}\")\n\n# Big - bigger + cold = colder\nresult = model.most_similar(positive=['bigger', 'cold'], negative=['big'], topn=3)\nprint(\"\\nbigger - big + cold =\")\nfor word, similarity in result:\n    print(f\"  {word:15s} {similarity:.3f}\")\n\n\nOutput:\nParis - France + Germany =\n  Berlin          0.735\n  Munich          0.652\n  Hamburg         0.618\n\nswimming - swim + run =\n  running         0.681\n  runs            0.632\n  jogging         0.598\n\nbigger - big + cold =\n  colder          0.708\n  warmer          0.673\n  hotter          0.649\nThese examples show that Word2vec captures: - Geographic relationships: capital cities - Grammatical relationships: verb forms, comparatives - Semantic relationships: gender, magnitude\nAll from statistical patterns in text!\n\n\n\n\n\n\nWhy Vector Arithmetic Works\n\n\n\nWord2vec embeddings organize words so that semantic relationships correspond to geometric directions in vector space. The “gender” direction is roughly king - queen, the “capital-of” direction is roughly Paris - France.\nThis emergent structure wasn’t programmed—it arises naturally from the training objective.\n\n\n\n\n\n\nLet’s visualize embeddings for scientific terms in 2D.\n\n\nCode\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Scientific vocabulary\nwords = [\n    # Network science\n    \"network\", \"graph\", \"node\", \"edge\", \"community\", \"clustering\",\n    # Biology\n    \"protein\", \"gene\", \"cell\", \"DNA\", \"molecule\", \"organism\",\n    # Physics\n    \"quantum\", \"particle\", \"energy\", \"force\", \"electron\", \"photon\",\n    # Math\n    \"theorem\", \"proof\", \"equation\", \"algebra\", \"calculus\", \"geometry\",\n    # Computing\n    \"algorithm\", \"computer\", \"software\", \"data\", \"program\", \"code\"\n]\n\n# Get embeddings\nword_vectors = np.array([model[word] for word in words if word in model])\nvalid_words = [word for word in words if word in model]\n\n# Reduce to 2D with t-SNE\ntsne = TSNE(n_components=2, random_state=42, perplexity=8)\nword_2d = tsne.fit_transform(word_vectors)\n\n# Plot\nsns.set_style(\"white\")\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Color by category\ncategories = {\n    'Network Science': ['network', 'graph', 'node', 'edge', 'community', 'clustering'],\n    'Biology': ['protein', 'gene', 'cell', 'DNA', 'molecule', 'organism'],\n    'Physics': ['quantum', 'particle', 'energy', 'force', 'electron', 'photon'],\n    'Mathematics': ['theorem', 'proof', 'equation', 'algebra', 'calculus', 'geometry'],\n    'Computing': ['algorithm', 'computer', 'software', 'data', 'program', 'code']\n}\n\ncolors = {'Network Science': '#e74c3c', 'Biology': '#2ecc71', 'Physics': '#f39c12',\n          'Mathematics': '#9b59b6', 'Computing': '#3498db'}\n\nfor category, category_words in categories.items():\n    indices = [valid_words.index(w) for w in category_words if w in valid_words]\n    if indices:\n        ax.scatter(word_2d[indices, 0], word_2d[indices, 1],\n                  c=colors[category], label=category, s=200, alpha=0.7,\n                  edgecolors='black', linewidth=1.5)\n\n        for idx in indices:\n            ax.annotate(valid_words[idx], (word_2d[idx, 0], word_2d[idx, 1]),\n                       fontsize=9, ha='center', va='center', fontweight='bold')\n\nax.set_xlabel(\"Dimension 1\", fontsize=12)\nax.set_ylabel(\"Dimension 2\", fontsize=12)\nax.set_title(\"Word2vec: Scientific Vocabulary Space\", fontsize=14, fontweight='bold')\nax.legend(loc='best', fontsize=10)\nax.grid(alpha=0.3, linestyle='--')\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n\nObservations: - Clusters form: Biology terms group together, physics terms group together - Overlap zones: Computing and math terms are nearby (both abstract/technical) - Distinct regions: Biology is far from physics (different domains)\nThe model discovered these relationships purely from word co-occurrence statistics.\n\n\n\nOne powerful use of Word2vec: analyzing how scientific concepts change over time.\n\n\n\n\nCode\n# Simulate training Word2vec on papers from different decades\n# In practice, you'd train separate models on historical corpora\n\n# For illustration, we'll show conceptually how this works\ndecades = {\n    \"1950s\": [\"electrical\", \"circuit\", \"television\", \"radio\", \"broadcasting\"],\n    \"1980s\": [\"computer\", \"telecommunications\", \"protocol\", \"LAN\", \"topology\"],\n    \"2010s\": [\"social\", \"online\", \"Twitter\", \"Facebook\", \"community\", \"graph\"]\n}\n\nprint(\"Evolution of 'network' neighbors over time:\\n\")\nfor decade, neighbors in decades.items():\n    print(f\"{decade}:\")\n    for word in neighbors:\n        if word in model:\n            sim = model.similarity(\"network\", word)\n            print(f\"  network ↔ {word:20s} similarity: {sim:.3f}\")\n        else:\n            print(f\"  network ↔ {word:20s} similarity: N/A\")\n    print()\n\n\nReal research application: Train Word2vec on scientific papers from different time periods, then measure how “network” embeddings shift. This reveals how the concept evolved from electrical networks → computer networks → social networks.\n\n\n\n\n\n\nHistorical Text Analysis\n\n\n\nTrain Word2vec models on text from different eras (decades, centuries) and compare embeddings. You can track: - Semantic drift (how meanings change) - Emerging concepts (new words in vocabulary) - Shifting associations (changes in word neighbors)\nThis is a powerful tool for cultural evolution and history of science research.\n\n\n\n\n\n\nNow that you’ve seen both Word2vec (static) and transformer embeddings (contextual), let’s compare.\n\n\nOne embedding per word:\n\"bank\" → [0.23, -0.45, 0.67, ...]  (always the same)\nExample: - “I went to the bank” → [0.23, -0.45, 0.67, …] - “The river bank” → [0.23, -0.45, 0.67, …] (identical!)\nStrengths: - Fast to train and use - Small model size - Explicit semantic relationships (word algebra) - Good for word-level analysis\nWeaknesses: - Can’t handle polysemy (multiple meanings) - Ignores context - Struggles with rare words\n\n\n\nDifferent embedding depending on context:\n\"I went to the bank\" → \"bank\" gets embedding1\n\"The river bank\"      → \"bank\" gets embedding2\nStrengths: - Handles polysemy correctly - Context-aware meaning - Better for sentence/document tasks - State-of-the-art performance\nWeaknesses: - Computationally expensive - Large model size (GBs) - Less interpretable - Overkill for simple tasks\n\n\n\n\n\n\nTask\nRecommended Approach\n\n\n\n\nWord similarity, analogies\nWord2vec\n\n\nTracking semantic change over time\nWord2vec (train per era)\n\n\nDocument classification\nContextual (sentence-transformers)\n\n\nSemantic search\nContextual (sentence-transformers)\n\n\nNamed entity recognition\nContextual (BERT)\n\n\nText generation\nContextual (GPT)\n\n\nQuick prototyping on a laptop\nWord2vec\n\n\nProduction system with accuracy priority\nContextual\n\n\n\nRule of thumb: Start simple (Word2vec). Upgrade to contextual embeddings only if you need the extra performance and can afford the computational cost.\n\n\n\n\nFor specialized domains (medical, legal, scientific subfields), pre-trained models might not have the right vocabulary. You can train your own Word2vec model.\n\n\nCode\nfrom gensim.models import Word2Vec\n\n# Example: Scientific abstracts (simulated)\nsentences = [\n    [\"community\", \"detection\", \"in\", \"networks\", \"using\", \"modularity\"],\n    [\"graph\", \"clustering\", \"algorithms\", \"for\", \"large\", \"networks\"],\n    [\"social\", \"network\", \"analysis\", \"with\", \"centrality\", \"measures\"],\n    [\"protein\", \"interaction\", \"networks\", \"in\", \"systems\", \"biology\"],\n    # In practice, you'd have thousands or millions of sentences\n]\n\n# Train Word2vec\nmodel_custom = Word2Vec(\n    sentences=sentences,\n    vector_size=100,      # Embedding dimensionality\n    window=5,             # Context window size\n    min_count=1,          # Minimum word frequency\n    workers=4,            # Parallel processing\n    sg=1                  # Skip-gram (1) or CBOW (0)\n)\n\nprint(\"Trained custom Word2vec model\")\nprint(f\"Vocabulary size: {len(model_custom.wv)}\")\nprint(f\"Embedding size: {model_custom.wv.vector_size}\")\n\n# Most similar to \"network\" in our small corpus\nif \"network\" in model_custom.wv:\n    similar = model_custom.wv.most_similar(\"network\", topn=3)\n    print(\"\\nMost similar to 'network':\")\n    for word, sim in similar:\n        print(f\"  {word:15s} {sim:.3f}\")\n\n\nOutput:\nTrained custom Word2vec model\nVocabulary size: 24\nEmbedding size: 100\n\nMost similar to 'network':\n  networks        0.892\n  community       0.715\n  clustering      0.687\nEven with this tiny dataset, the model learns that “networks,” “community,” and “clustering” are related concepts.\n\n\n\n\n\n\nTraining Considerations\n\n\n\nFor good embeddings, you need: - Large corpus: Millions of words minimum, billions ideal - Clean preprocessing: Tokenization, lowercasing, removing noise - Hyperparameter tuning: vector_size, window, min_count - Domain-specific data: Train on text from your research domain\nFor most research purposes, pre-trained models (Word2vec, GloVe) are sufficient. Train custom models only when your domain vocabulary is poorly covered.\n\n\n\n\n\nWord2vec learns from data, which means it also learns human biases present in text.\n\n\n\n\nCode\n# Explore gender associations\nmale_professions = model.most_similar(positive=['doctor', 'man'], negative=['woman'], topn=5)\nfemale_professions = model.most_similar(positive=['nurse', 'woman'], negative=['man'], topn=5)\n\nprint(\"Male-associated professions:\")\nfor word, sim in male_professions:\n    print(f\"  {word}\")\n\nprint(\"\\nFemale-associated professions:\")\nfor word, sim in female_professions:\n    print(f\"  {word}\")\n\n\nThe model might associate “doctor” with male and “nurse” with female, reflecting biases in training data (news articles, books, web pages). These biases can propagate into downstream applications.\nImplications for research: - Be aware of biases in embeddings - Don’t use embeddings for sensitive applications without auditing - Consider debiasing techniques if needed - Embeddings can also be used to measure bias in text corpora\nWe’ll explore bias measurement with semantic axes in the final section.\n\n\n\n\nYou’ve now seen the original approach to embeddings—Word2vec—and understand: - The distributional hypothesis (context determines meaning) - How Word2vec learns from skip-gram prediction - Word algebra and semantic relationships - When static embeddings are sufficient vs. when contextual embeddings are necessary\nWord2vec was revolutionary in 2013. It enabled NLP to move from hand-crafted features to learned representations. But it had limitations (no context, polysemy), which transformers addressed.\nNow let’s go full circle: back to the basics. Before Word2vec, before embeddings, there was the simplest possible representation of text—counting words. These fundamental methods are still relevant, and understanding them completes the picture.\n\nNext: Text Fundamentals: The Full Picture →",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Word Embeddings: Where It Started"
    ]
  },
  {
    "objectID": "m03-text/word-embeddings.html#the-distributional-hypothesis",
    "href": "m03-text/word-embeddings.html#the-distributional-hypothesis",
    "title": "Word Embeddings: Where It Started",
    "section": "",
    "text": "Word2vec is built on a simple but profound idea:\n\n“You shall know a word by the company it keeps.” — J.R. Firth, 1957\n\nWords that appear in similar contexts tend to have similar meanings.\n\n\nConsider these sentences: - “The cat sat on the mat.” - “The dog sat on the mat.” - “The cat chased the mouse.” - “The dog chased the rabbit.”\n“Cat” and “dog” appear in similar contexts (“sat on the mat”, “chased…”). Therefore, they should have similar embeddings.\nNow consider: - “The theorem was proved in 1995.” - “The conjecture was proved in 1995.”\n“Theorem” and “conjecture” also appear in similar contexts, so they should have similar embeddings—even though they’re very different from “cat” and “dog.”\nKey insight: We don’t need to manually encode that “cat” is an animal or “theorem” is a mathematical statement. The model learns these relationships automatically from context.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Word Embeddings: Where It Started"
    ]
  },
  {
    "objectID": "m03-text/word-embeddings.html#word2vec-the-core-idea",
    "href": "m03-text/word-embeddings.html#word2vec-the-core-idea",
    "title": "Word Embeddings: Where It Started",
    "section": "",
    "text": "Word2vec learns embeddings by training a simple neural network to predict: 1. Skip-gram: Given a word, predict its context 2. CBOW (Continuous Bag-of-Words): Given context, predict the word\nBoth approaches lead to similar embeddings. We’ll focus on Skip-gram because it’s more intuitive.\n\n\nTraining setup:\nSentence: \"The cat sat on the mat\"\nTarget word: \"cat\"\nContext window (size=2): [\"The\", \"sat\"]\n\nTask: Given \"cat\", predict you'll see \"The\" and \"sat\" nearby\nThe model learns embeddings such that: - Words with similar contexts get similar embeddings - Embeddings encode semantic relationships\n\n\n\n\nInitialize: Random vectors for each word\nSample: Pick a word and its context from training data\nPredict: Use the word’s embedding to predict context words\nUpdate: Adjust embeddings to improve predictions\nRepeat: Millions of times across billions of words\n\nAfter training, embeddings capture semantic structure without anyone explicitly defining it.\n\n\n\n\n\n\nWhy This Works\n\n\n\nIf “cat” often appears near “furry,” “pet,” and “meow,” its embedding learns to activate for animal-related contexts. If “dog” appears in similar contexts, its embedding will be similar to “cat’s.”\nThe model discovers that “cat” and “dog” are related not because we told it, but because they share statistical patterns in text.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Word Embeddings: Where It Started"
    ]
  },
  {
    "objectID": "m03-text/word-embeddings.html#using-word2vec-with-gensim",
    "href": "m03-text/word-embeddings.html#using-word2vec-with-gensim",
    "title": "Word Embeddings: Where It Started",
    "section": "",
    "text": "Let’s work with pre-trained Word2vec embeddings using the gensim library.\n\n\nCode\nimport gensim.downloader as api\nimport numpy as np\n\n# Load pre-trained Word2vec embeddings (Google News corpus, ~100B words)\n# This is a large download (~1.6GB), so it may take a minute\nprint(\"Loading Word2vec model (this may take a moment)...\")\nmodel = api.load(\"word2vec-google-news-300\")\nprint(f\"Loaded embeddings for {len(model)} words, each with {model.vector_size} dimensions\")\n\n\nOutput:\nLoading Word2vec model (this may take a moment)...\nLoaded embeddings for 3000000 words, each with 300 dimensions\nThis model has embeddings for 3 million words, each represented as a 300-dimensional vector.\n\n\n\n\nCode\n# Find words most similar to \"network\"\nsimilar_to_network = model.most_similar(\"network\", topn=10)\n\nprint(\"Words most similar to 'network':\")\nfor word, similarity in similar_to_network:\n    print(f\"  {word:20s} {similarity:.3f}\")\n\n\nOutput:\nWords most similar to 'network':\n  networks             0.732\n  cable_network        0.682\n  television_network   0.654\n  broadcasting         0.623\n  cable_television     0.612\n  radio_network        0.598\n  telecoms             0.587\n  broadcaster          0.579\n  TV_network           0.571\n  communications       0.563\nThe model learned that “network” is related to broadcasting, telecommunications, and media—despite never being told these definitions.\n\n\n\nLet’s compare similarities across different domains:\n\n\nCode\nwords = [\"network\", \"graph\", \"community\", \"theorem\", \"protein\", \"cat\"]\n\nprint(\"Pairwise similarities:\")\nprint(f\"{'':12s}\", end=\"\")\nfor w in words:\n    print(f\"{w:12s}\", end=\"\")\nprint()\n\nfor w1 in words:\n    print(f\"{w1:12s}\", end=\"\")\n    for w2 in words:\n        if w1 in model and w2 in model:\n            sim = model.similarity(w1, w2)\n            print(f\"{sim:12.3f}\", end=\"\")\n        else:\n            print(f\"{'N/A':12s}\", end=\"\")\n    print()\n\n\nOutput:\n            network     graph       community   theorem     protein     cat\nnetwork        1.000       0.312       0.385       0.187       0.143       0.089\ngraph          0.312       1.000       0.245       0.298       0.112       0.076\ncommunity      0.385       0.245       1.000       0.156       0.134       0.098\ntheorem        0.187       0.298       0.156       1.000       0.198       0.065\nprotein        0.143       0.112       0.134       0.198       1.000       0.102\ncat            0.089       0.076       0.098       0.065       0.102       1.000\nObservations: - “network” and “community” are moderately similar (0.385) — both social concepts - “graph” and “theorem” have some similarity (0.298) — both mathematical - “cat” is dissimilar to everything else — different domain entirely",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Word Embeddings: Where It Started"
    ]
  },
  {
    "objectID": "m03-text/word-embeddings.html#word-algebra-the-famous-examples",
    "href": "m03-text/word-embeddings.html#word-algebra-the-famous-examples",
    "title": "Word Embeddings: Where It Started",
    "section": "",
    "text": "One of Word2vec’s most striking properties: semantic relationships become vector arithmetic.\n\n\n\n\nCode\n# Vector arithmetic\nresult = model.most_similar(positive=['king', 'woman'], negative=['man'], topn=5)\n\nprint(\"king - man + woman =\")\nfor word, similarity in result:\n    print(f\"  {word:15s} {similarity:.3f}\")\n\n\nOutput:\nking - man + woman =\n  queen           0.711\n  monarch         0.619\n  princess        0.590\n  crown_prince    0.567\n  prince          0.561\nThe model learned that “king” relates to “man” as “queen” relates to “woman”—a relationship captured by vector subtraction and addition!\n\n\n\n\n\nCode\n# Paris - France + Germany = Berlin\nresult = model.most_similar(positive=['Paris', 'Germany'], negative=['France'], topn=3)\nprint(\"\\nParis - France + Germany =\")\nfor word, similarity in result:\n    print(f\"  {word:15s} {similarity:.3f}\")\n\n# Swimming - swim + run = running\nresult = model.most_similar(positive=['swimming', 'run'], negative=['swim'], topn=3)\nprint(\"\\nswimming - swim + run =\")\nfor word, similarity in result:\n    print(f\"  {word:15s} {similarity:.3f}\")\n\n# Big - bigger + cold = colder\nresult = model.most_similar(positive=['bigger', 'cold'], negative=['big'], topn=3)\nprint(\"\\nbigger - big + cold =\")\nfor word, similarity in result:\n    print(f\"  {word:15s} {similarity:.3f}\")\n\n\nOutput:\nParis - France + Germany =\n  Berlin          0.735\n  Munich          0.652\n  Hamburg         0.618\n\nswimming - swim + run =\n  running         0.681\n  runs            0.632\n  jogging         0.598\n\nbigger - big + cold =\n  colder          0.708\n  warmer          0.673\n  hotter          0.649\nThese examples show that Word2vec captures: - Geographic relationships: capital cities - Grammatical relationships: verb forms, comparatives - Semantic relationships: gender, magnitude\nAll from statistical patterns in text!\n\n\n\n\n\n\nWhy Vector Arithmetic Works\n\n\n\nWord2vec embeddings organize words so that semantic relationships correspond to geometric directions in vector space. The “gender” direction is roughly king - queen, the “capital-of” direction is roughly Paris - France.\nThis emergent structure wasn’t programmed—it arises naturally from the training objective.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Word Embeddings: Where It Started"
    ]
  },
  {
    "objectID": "m03-text/word-embeddings.html#visualizing-word2vec-embeddings",
    "href": "m03-text/word-embeddings.html#visualizing-word2vec-embeddings",
    "title": "Word Embeddings: Where It Started",
    "section": "",
    "text": "Let’s visualize embeddings for scientific terms in 2D.\n\n\nCode\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Scientific vocabulary\nwords = [\n    # Network science\n    \"network\", \"graph\", \"node\", \"edge\", \"community\", \"clustering\",\n    # Biology\n    \"protein\", \"gene\", \"cell\", \"DNA\", \"molecule\", \"organism\",\n    # Physics\n    \"quantum\", \"particle\", \"energy\", \"force\", \"electron\", \"photon\",\n    # Math\n    \"theorem\", \"proof\", \"equation\", \"algebra\", \"calculus\", \"geometry\",\n    # Computing\n    \"algorithm\", \"computer\", \"software\", \"data\", \"program\", \"code\"\n]\n\n# Get embeddings\nword_vectors = np.array([model[word] for word in words if word in model])\nvalid_words = [word for word in words if word in model]\n\n# Reduce to 2D with t-SNE\ntsne = TSNE(n_components=2, random_state=42, perplexity=8)\nword_2d = tsne.fit_transform(word_vectors)\n\n# Plot\nsns.set_style(\"white\")\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Color by category\ncategories = {\n    'Network Science': ['network', 'graph', 'node', 'edge', 'community', 'clustering'],\n    'Biology': ['protein', 'gene', 'cell', 'DNA', 'molecule', 'organism'],\n    'Physics': ['quantum', 'particle', 'energy', 'force', 'electron', 'photon'],\n    'Mathematics': ['theorem', 'proof', 'equation', 'algebra', 'calculus', 'geometry'],\n    'Computing': ['algorithm', 'computer', 'software', 'data', 'program', 'code']\n}\n\ncolors = {'Network Science': '#e74c3c', 'Biology': '#2ecc71', 'Physics': '#f39c12',\n          'Mathematics': '#9b59b6', 'Computing': '#3498db'}\n\nfor category, category_words in categories.items():\n    indices = [valid_words.index(w) for w in category_words if w in valid_words]\n    if indices:\n        ax.scatter(word_2d[indices, 0], word_2d[indices, 1],\n                  c=colors[category], label=category, s=200, alpha=0.7,\n                  edgecolors='black', linewidth=1.5)\n\n        for idx in indices:\n            ax.annotate(valid_words[idx], (word_2d[idx, 0], word_2d[idx, 1]),\n                       fontsize=9, ha='center', va='center', fontweight='bold')\n\nax.set_xlabel(\"Dimension 1\", fontsize=12)\nax.set_ylabel(\"Dimension 2\", fontsize=12)\nax.set_title(\"Word2vec: Scientific Vocabulary Space\", fontsize=14, fontweight='bold')\nax.legend(loc='best', fontsize=10)\nax.grid(alpha=0.3, linestyle='--')\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n\nObservations: - Clusters form: Biology terms group together, physics terms group together - Overlap zones: Computing and math terms are nearby (both abstract/technical) - Distinct regions: Biology is far from physics (different domains)\nThe model discovered these relationships purely from word co-occurrence statistics.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Word Embeddings: Where It Started"
    ]
  },
  {
    "objectID": "m03-text/word-embeddings.html#application-tracking-concept-evolution",
    "href": "m03-text/word-embeddings.html#application-tracking-concept-evolution",
    "title": "Word Embeddings: Where It Started",
    "section": "",
    "text": "One powerful use of Word2vec: analyzing how scientific concepts change over time.\n\n\n\n\nCode\n# Simulate training Word2vec on papers from different decades\n# In practice, you'd train separate models on historical corpora\n\n# For illustration, we'll show conceptually how this works\ndecades = {\n    \"1950s\": [\"electrical\", \"circuit\", \"television\", \"radio\", \"broadcasting\"],\n    \"1980s\": [\"computer\", \"telecommunications\", \"protocol\", \"LAN\", \"topology\"],\n    \"2010s\": [\"social\", \"online\", \"Twitter\", \"Facebook\", \"community\", \"graph\"]\n}\n\nprint(\"Evolution of 'network' neighbors over time:\\n\")\nfor decade, neighbors in decades.items():\n    print(f\"{decade}:\")\n    for word in neighbors:\n        if word in model:\n            sim = model.similarity(\"network\", word)\n            print(f\"  network ↔ {word:20s} similarity: {sim:.3f}\")\n        else:\n            print(f\"  network ↔ {word:20s} similarity: N/A\")\n    print()\n\n\nReal research application: Train Word2vec on scientific papers from different time periods, then measure how “network” embeddings shift. This reveals how the concept evolved from electrical networks → computer networks → social networks.\n\n\n\n\n\n\nHistorical Text Analysis\n\n\n\nTrain Word2vec models on text from different eras (decades, centuries) and compare embeddings. You can track: - Semantic drift (how meanings change) - Emerging concepts (new words in vocabulary) - Shifting associations (changes in word neighbors)\nThis is a powerful tool for cultural evolution and history of science research.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Word Embeddings: Where It Started"
    ]
  },
  {
    "objectID": "m03-text/word-embeddings.html#static-vs.-contextual-embeddings",
    "href": "m03-text/word-embeddings.html#static-vs.-contextual-embeddings",
    "title": "Word Embeddings: Where It Started",
    "section": "",
    "text": "Now that you’ve seen both Word2vec (static) and transformer embeddings (contextual), let’s compare.\n\n\nOne embedding per word:\n\"bank\" → [0.23, -0.45, 0.67, ...]  (always the same)\nExample: - “I went to the bank” → [0.23, -0.45, 0.67, …] - “The river bank” → [0.23, -0.45, 0.67, …] (identical!)\nStrengths: - Fast to train and use - Small model size - Explicit semantic relationships (word algebra) - Good for word-level analysis\nWeaknesses: - Can’t handle polysemy (multiple meanings) - Ignores context - Struggles with rare words\n\n\n\nDifferent embedding depending on context:\n\"I went to the bank\" → \"bank\" gets embedding1\n\"The river bank\"      → \"bank\" gets embedding2\nStrengths: - Handles polysemy correctly - Context-aware meaning - Better for sentence/document tasks - State-of-the-art performance\nWeaknesses: - Computationally expensive - Large model size (GBs) - Less interpretable - Overkill for simple tasks\n\n\n\n\n\n\nTask\nRecommended Approach\n\n\n\n\nWord similarity, analogies\nWord2vec\n\n\nTracking semantic change over time\nWord2vec (train per era)\n\n\nDocument classification\nContextual (sentence-transformers)\n\n\nSemantic search\nContextual (sentence-transformers)\n\n\nNamed entity recognition\nContextual (BERT)\n\n\nText generation\nContextual (GPT)\n\n\nQuick prototyping on a laptop\nWord2vec\n\n\nProduction system with accuracy priority\nContextual\n\n\n\nRule of thumb: Start simple (Word2vec). Upgrade to contextual embeddings only if you need the extra performance and can afford the computational cost.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Word Embeddings: Where It Started"
    ]
  },
  {
    "objectID": "m03-text/word-embeddings.html#training-your-own-word2vec-model",
    "href": "m03-text/word-embeddings.html#training-your-own-word2vec-model",
    "title": "Word Embeddings: Where It Started",
    "section": "",
    "text": "For specialized domains (medical, legal, scientific subfields), pre-trained models might not have the right vocabulary. You can train your own Word2vec model.\n\n\nCode\nfrom gensim.models import Word2Vec\n\n# Example: Scientific abstracts (simulated)\nsentences = [\n    [\"community\", \"detection\", \"in\", \"networks\", \"using\", \"modularity\"],\n    [\"graph\", \"clustering\", \"algorithms\", \"for\", \"large\", \"networks\"],\n    [\"social\", \"network\", \"analysis\", \"with\", \"centrality\", \"measures\"],\n    [\"protein\", \"interaction\", \"networks\", \"in\", \"systems\", \"biology\"],\n    # In practice, you'd have thousands or millions of sentences\n]\n\n# Train Word2vec\nmodel_custom = Word2Vec(\n    sentences=sentences,\n    vector_size=100,      # Embedding dimensionality\n    window=5,             # Context window size\n    min_count=1,          # Minimum word frequency\n    workers=4,            # Parallel processing\n    sg=1                  # Skip-gram (1) or CBOW (0)\n)\n\nprint(\"Trained custom Word2vec model\")\nprint(f\"Vocabulary size: {len(model_custom.wv)}\")\nprint(f\"Embedding size: {model_custom.wv.vector_size}\")\n\n# Most similar to \"network\" in our small corpus\nif \"network\" in model_custom.wv:\n    similar = model_custom.wv.most_similar(\"network\", topn=3)\n    print(\"\\nMost similar to 'network':\")\n    for word, sim in similar:\n        print(f\"  {word:15s} {sim:.3f}\")\n\n\nOutput:\nTrained custom Word2vec model\nVocabulary size: 24\nEmbedding size: 100\n\nMost similar to 'network':\n  networks        0.892\n  community       0.715\n  clustering      0.687\nEven with this tiny dataset, the model learns that “networks,” “community,” and “clustering” are related concepts.\n\n\n\n\n\n\nTraining Considerations\n\n\n\nFor good embeddings, you need: - Large corpus: Millions of words minimum, billions ideal - Clean preprocessing: Tokenization, lowercasing, removing noise - Hyperparameter tuning: vector_size, window, min_count - Domain-specific data: Train on text from your research domain\nFor most research purposes, pre-trained models (Word2vec, GloVe) are sufficient. Train custom models only when your domain vocabulary is poorly covered.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Word Embeddings: Where It Started"
    ]
  },
  {
    "objectID": "m03-text/word-embeddings.html#limitations-and-biases",
    "href": "m03-text/word-embeddings.html#limitations-and-biases",
    "title": "Word Embeddings: Where It Started",
    "section": "",
    "text": "Word2vec learns from data, which means it also learns human biases present in text.\n\n\n\n\nCode\n# Explore gender associations\nmale_professions = model.most_similar(positive=['doctor', 'man'], negative=['woman'], topn=5)\nfemale_professions = model.most_similar(positive=['nurse', 'woman'], negative=['man'], topn=5)\n\nprint(\"Male-associated professions:\")\nfor word, sim in male_professions:\n    print(f\"  {word}\")\n\nprint(\"\\nFemale-associated professions:\")\nfor word, sim in female_professions:\n    print(f\"  {word}\")\n\n\nThe model might associate “doctor” with male and “nurse” with female, reflecting biases in training data (news articles, books, web pages). These biases can propagate into downstream applications.\nImplications for research: - Be aware of biases in embeddings - Don’t use embeddings for sensitive applications without auditing - Consider debiasing techniques if needed - Embeddings can also be used to measure bias in text corpora\nWe’ll explore bias measurement with semantic axes in the final section.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Word Embeddings: Where It Started"
    ]
  },
  {
    "objectID": "m03-text/word-embeddings.html#the-bigger-picture",
    "href": "m03-text/word-embeddings.html#the-bigger-picture",
    "title": "Word Embeddings: Where It Started",
    "section": "",
    "text": "You’ve now seen the original approach to embeddings—Word2vec—and understand: - The distributional hypothesis (context determines meaning) - How Word2vec learns from skip-gram prediction - Word algebra and semantic relationships - When static embeddings are sufficient vs. when contextual embeddings are necessary\nWord2vec was revolutionary in 2013. It enabled NLP to move from hand-crafted features to learned representations. But it had limitations (no context, polysemy), which transformers addressed.\nNow let’s go full circle: back to the basics. Before Word2vec, before embeddings, there was the simplest possible representation of text—counting words. These fundamental methods are still relevant, and understanding them completes the picture.\n\nNext: Text Fundamentals: The Full Picture →",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Word Embeddings: Where It Started"
    ]
  },
  {
    "objectID": "m03-text/transformers.html#positional-encoding-teaching-transformers-about-order",
    "href": "m03-text/transformers.html#positional-encoding-teaching-transformers-about-order",
    "title": "Transformers: The Architecture Behind the Magic",
    "section": "",
    "text": "Unlike RNNs that process words sequentially, transformers process all words simultaneously. This parallelization is powerful but creates a problem: the model has no sense of word order.\nWithout positional information: - “The cat sat on the mat” - “Mat the on sat cat the”\n…would look identical to the transformer!\n\n\nYou might think: why not just add 1, 2, 3, … to each word embedding?\n# DON'T DO THIS\nword_embedding[0] += 1  # First word\nword_embedding[1] += 2  # Second word\nword_embedding[2] += 3  # Third word\nProblems with this approach:\n\nScale issues: Position numbers grow unbounded (1, 2, 3, …, 1000, …), while embeddings are typically normalized\nPoor generalization: The model can’t handle sequences longer than it saw during training\nNo smooth relationships: Position 5 and 6 aren’t “smoothly related” - they’re just different integers\n\n\n\n\nTransformers use sinusoidal functions to encode position:\n\n\\begin{aligned}\nPE_{(pos, 2i)} &= \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right) \\\\\nPE_{(pos, 2i+1)} &= \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)\n\\end{aligned}\n\nwhere: - pos is the position (0, 1, 2, …) - i is the dimension index - d is the embedding dimension\nKey properties:\n\nBounded values: Sine and cosine always stay between -1 and 1\nDeterministic: Same position always gets the same encoding (no learnable parameters)\nSmooth relationships: Nearby positions have similar encodings\nGeneralizes to any length: Can encode positions the model never saw during training\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef get_positional_encoding(seq_len, d_model):\n    \"\"\"Generate sinusoidal positional encodings\"\"\"\n    pos_enc = np.zeros((seq_len, d_model))\n    for pos in range(seq_len):\n        for i in range(0, d_model, 2):\n            pos_enc[pos, i] = np.sin(pos / (10000 ** (i / d_model)))\n            if i + 1 &lt; d_model:\n                pos_enc[pos, i + 1] = np.cos(pos / (10000 ** (i / d_model)))\n    return pos_enc\n\n# Generate encodings\nseq_len = 50\nd_model = 2  # Use 2D for visualization\npos_enc = get_positional_encoding(seq_len, d_model)\n\n# Create visualization\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n# Left plot: 2D trajectory\nax1.plot(pos_enc[:, 0], pos_enc[:, 1], 'o-', alpha=0.6, markersize=4)\nax1.scatter(pos_enc[0, 0], pos_enc[0, 1], c='green', s=100, label='Position 0', zorder=5)\nax1.scatter(pos_enc[-1, 0], pos_enc[-1, 1], c='red', s=100, label=f'Position {seq_len-1}', zorder=5)\nax1.set_xlabel('Dimension 0 (sin)', fontweight='bold')\nax1.set_ylabel('Dimension 1 (cos)', fontweight='bold')\nax1.set_title('Positional Encoding Trajectory', fontweight='bold')\nax1.legend()\nax1.grid(True, alpha=0.3)\nax1.axis('equal')\n\n# Right plot: Heatmap of encodings\nim = ax2.imshow(pos_enc.T, aspect='auto', cmap='RdBu', vmin=-1, vmax=1)\nax2.set_xlabel('Position', fontweight='bold')\nax2.set_ylabel('Dimension', fontweight='bold')\nax2.set_title('Positional Encoding Heatmap', fontweight='bold')\nplt.colorbar(im, ax=ax2, label='Encoding Value')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nPositional encoding creates a unique ‘signature’ for each position. Each position traces a spiral pattern in 2D space, with adjacent positions close together.\n\n\n\n\nWhat’s happening: - Each position gets a unique “coordinate” in high-dimensional space - Adjacent positions are smoothly related (nearby in the space) - Different dimensions oscillate at different frequencies (slow for dimension 0, faster for higher dimensions) - The spiral pattern shows how positions naturally form a sequence\n\n\n\nPositional encodings are added to token embeddings at the input:\n\n\\text{Input} = \\text{Token Embedding} + \\text{Positional Encoding}\n\nThis way, each word’s representation contains both: - What the word is (from token embedding) - Where the word is (from positional encoding)\n\n\n\n\n\n\nWhy Sinusoids?\n\n\n\nSinusoidal functions were chosen because they have a special property: any position’s encoding can be represented as a linear combination of other positions’ encodings. This helps the model learn relative positions (e.g., “3 words before this one”) in addition to absolute positions.\nModern variants like learned positional embeddings or rotary positional encodings (RoPE) have also been developed, but the sinusoidal approach remains elegant and effective.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Transformers: The Architecture Behind the Magic"
    ]
  },
  {
    "objectID": "m03-text/overview.html",
    "href": "m03-text/overview.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "In this module, we will learn about the modern natural language processing, especially the large language models. We start from applications and then down to the theory. Starting from what the large language model is. And then we will learn about the prompt engineering. Then we will unbox the large language models, understanding the core technology called the transformer modules, and the retention mechanisms. And then we will learn about the embedding, which represents the internal data representations of large language models. And how we can use embedding to prove natural language. Undergraded orادm12b, is the original natural language model of a script painting? Now, the novel is about three different ways of entering into the mix,",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Overview"
    ]
  },
  {
    "objectID": "m03-text/transformers.html#the-naive-intuition-one-word-one-vector",
    "href": "m03-text/transformers.html#the-naive-intuition-one-word-one-vector",
    "title": "Transformers: Why Context Is Everything (And One Vector Is Nothing)",
    "section": "1 The Naive Intuition: One Word, One Vector",
    "text": "1 The Naive Intuition: One Word, One Vector\nFor many years, natural language processing treated words as having fixed meanings. We represented each word—like “bank”—as a single vector of numbers, such as [0.23, -0.45, 0.67, ...]. This approach, called static embeddings, assumed a word’s meaning could be fully captured with one point in space, no matter where the word appeared.\nThe foundation for this dates back to 1957, when J.R. Firth said: “You shall know a word by the company it keeps.” This led to distributional semantics: a word’s meaning comes from the other words nearby. If “bank” is near “mortgage” or “loan,” we assume it means a financial place; if it’s near “river” or “shore,” we assume a landform. Word2vec (2013) brought this to life by learning word vectors from their neighbors.\nBut the big problem: Word2vec still gives just one vector per word. “Bank” has the same representation in “I deposited money at the bank” as in “We had a picnic by the bank.” It averages all possible uses together—like saying everyone is 5’7” tall—hiding the differences that matter most.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Transformers: The Architecture Behind the Magic"
    ]
  },
  {
    "objectID": "m03-text/transformers.html#the-theoretical-failure-when-averaging-destroys-the-signal",
    "href": "m03-text/transformers.html#the-theoretical-failure-when-averaging-destroys-the-signal",
    "title": "Transformers: Why Context Is Everything (And One Vector Is Nothing)",
    "section": "2 The Theoretical Failure: When Averaging Destroys the Signal",
    "text": "2 The Theoretical Failure: When Averaging Destroys the Signal\nThe naive hypothesis went like this: what if we just mix the target word with its neighbors? For the sentence “I deposited money at the bank,” we could compute a contextualized representation as:\n\n\\vec{v}_{\\text{bank (new)}} = w_1 \\cdot \\vec{v}_{\\text{bank}} + w_2 \\cdot \\vec{v}_{\\text{deposited}} + w_3 \\cdot \\vec{v}_{\\text{money}} + \\cdots\n\nwhere w_i are weights and \\vec{v}_i are word embeddings.\nConsider the following example. Notice that “bank” sits neutrally between financial terms (money) and geographical terms (river). Now try manually adjusting the weights to contextualize “bank”:\n\nd3 = require(\"d3@7\", \"d3-simple-slider@1\")\n\n\n\n\n\n\n\nfunction sliderWithLabel(min, max, step, width, defaultValue, label) {\n  const slider = d3.sliderBottom()\n    .min(min).max(max).step(step).width(width).default(defaultValue);\n  const svg = d3.create(\"svg\").attr(\"width\", width + 50).attr(\"height\", 60);\n  svg.append(\"g\").attr(\"transform\", \"translate(25,20)\").call(slider);\n  svg.append(\"text\").attr(\"x\", (width + 50) / 2).attr(\"y\", 10).attr(\"text-anchor\", \"middle\").style(\"font-size\", \"12px\").text(label);\n  return svg.node();\n}\n\n\n\n\n\n\n\n{\n  // Create slider function that returns both the element and a reactive value\n  function createWeightSlider(min, max, step, width, defaultValue, label) {\n    const slider = d3.sliderBottom()\n      .min(min).max(max).step(step).width(width).default(defaultValue);\n    const svg = d3.create(\"svg\").attr(\"width\", width + 50).attr(\"height\", 60);\n    const g = svg.append(\"g\").attr(\"transform\", \"translate(25,20)\");\n    g.call(slider);\n    svg.append(\"text\").attr(\"x\", (width + 50) / 2).attr(\"y\", 10)\n       .attr(\"text-anchor\", \"middle\").style(\"font-size\", \"12px\").text(label);\n    return { node: svg.node(), slider: slider };\n  }\n\n  // Create sliders\n  const bankSliderObj = createWeightSlider(0, 1, 0.01, 200, 1.0, \"Bank weight\");\n  const moneySliderObj = createWeightSlider(0, 1, 0.01, 200, 0.0, \"Money weight\");\n  const riverSliderObj = createWeightSlider(0, 1, 0.01, 200, 0.0, \"River weight\");\n\n  // Word embeddings in 2D space\n  const contextWords = [\"bank\", \"money\", \"river\"];\n  const contextEmbeddings = [\n    [0.0, 0.0],   // bank (center)\n    [-1.6, -0.6], // money (financial, left)\n    [1.4, -1.0]   // river (geographical, right)\n  ];\n\n  // Create plot container\n  const plotContainer = document.createElement(\"div\");\n\n  // Function to update visualization\n  function update() {\n    // Get current slider values\n    const bankWeight = bankSliderObj.slider.value();\n    const moneyWeight = moneySliderObj.slider.value();\n    const riverWeight = riverSliderObj.slider.value();\n\n    // Calculate weighted average\n    const weights = [bankWeight, moneyWeight, riverWeight];\n    const total = weights.reduce((a, b) =&gt; a + b, 0);\n    const normalizedWeights = total &gt; 0 ? weights.map(w =&gt; w / total) : [0, 0, 0];\n\n    const newVec = [\n      normalizedWeights[0] * contextEmbeddings[0][0] +\n      normalizedWeights[1] * contextEmbeddings[1][0] +\n      normalizedWeights[2] * contextEmbeddings[2][0],\n      normalizedWeights[0] * contextEmbeddings[0][1] +\n      normalizedWeights[1] * contextEmbeddings[1][1] +\n      normalizedWeights[2] * contextEmbeddings[2][1]\n    ];\n\n    // Prepare data for visualization\n    const originalData = contextWords.map((word, i) =&gt; ({\n      word: word,\n      x: contextEmbeddings[i][0],\n      y: contextEmbeddings[i][1],\n      type: \"Original\"\n    }));\n\n    const contextualizedData = [{\n      word: \"bank (new)\",\n      x: newVec[0],\n      y: newVec[1],\n      type: \"Contextualized\"\n    }];\n\n    const data = [...originalData, ...contextualizedData];\n\n    // Clear and update plot\n    d3.select(plotContainer).selectAll(\"*\").remove();\n\n    // Create visualization\n    const plot = Plot.plot({\n      width: 300,\n      height: 300,\n      marginTop: 60,\n      marginRight: 20,\n      marginBottom: 50,\n      marginLeft: 60,\n      style: {\n        background: \"white\",\n        color: \"black\"\n      },\n      x: {\n        domain: [-2, 2],\n        label: \"Dimension 1\",\n        grid: true,\n        ticks: 10\n      },\n      y: {\n        domain: [-2, 2],\n        label: \"Dimension 2\",\n        grid: true,\n        ticks: 10\n      },\n      color: {\n        domain: [\"Original\", \"Contextualized\"],\n        range: [\"#dadada\", \"#ff7f0e\"]\n      },\n      marks: [\n        Plot.dot(data, {\n          x: \"x\",\n          y: \"y\",\n          fill: \"type\",\n          r: 8,\n          tip: true\n        }),\n        Plot.text(data, {\n          x: \"x\",\n          y: \"y\",\n          text: \"word\",\n          dy: -15,\n          fontSize: 8,\n          fontWeight: \"bold\",\n          fill: \"black\"\n        }),\n        Plot.text([{x: 0, y: 2.3}], {\n          x: \"x\",\n          y: \"y\",\n          text: () =&gt; `Weights: Bank=${normalizedWeights[0].toFixed(2)}, Money=${normalizedWeights[1].toFixed(2)}, River=${normalizedWeights[2].toFixed(2)}`,\n          fontSize: 11,\n          fill: \"black\"\n        }),\n        // Custom legend at top center\n        Plot.dot([{x: -0.8, y: 2.7, color: \"#dadada\"}, {x: 0.8, y: 2.7, color: \"#ff7f0e\"}], {\n          x: \"x\",\n          y: \"y\",\n          fill: \"color\",\n          r: 6\n        }),\n        Plot.text([{x: -0.5, y: 2.7, label: \"Original\"}, {x: 1.1, y: 2.7, label: \"Contextualized\"}], {\n          x: \"x\",\n          y: \"y\",\n          text: \"label\",\n          fontSize: 10,\n          fill: \"black\",\n          textAnchor: \"start\"\n        })\n      ]\n    });\n\n    d3.select(plotContainer).node().appendChild(plot);\n  }\n\n  // Add event listeners to sliders\n  bankSliderObj.slider.on(\"onchange\", update);\n  moneySliderObj.slider.on(\"onchange\", update);\n  riverSliderObj.slider.on(\"onchange\", update);\n\n  // Initial render\n  update();\n\n  return html`&lt;div style=\"display: flex; align-items: center; gap: 40px; justify-content: center;\"&gt;\n    &lt;div style=\"display: flex; flex-direction: column; gap: 10px;\"&gt;\n      ${bankSliderObj.node}\n      ${moneySliderObj.node}\n      ${riverSliderObj.node}\n    &lt;/div&gt;\n    &lt;div&gt;\n      ${plotContainer}\n    &lt;/div&gt;\n  &lt;/div&gt;`;\n}\n\n\n\n\n\n\nBy changing the weights, we can see that the vector for “bank” can lean more towards the financial terms or the geographical terms. So how can we determine the weights?\nThe simplest idea is to give each word an equal weight: w_i = 1/N. This creates a basic “bag-of-words” average. But sentences aren’t actually this fair—some words are much more important than others. For example, in “I deposited money at the bank,” the words “deposited” and “money” are key, while “I,” “at,” and “the” add little meaning. If we treat all words the same, we lose the details that matter. We need a way to highlight the important words and downplay the rest.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Transformers: The Architecture Behind the Magic"
    ]
  },
  {
    "objectID": "m03-text/transformers.html#the-hidden-mechanism-query-key-matching-as-information-retrieval",
    "href": "m03-text/transformers.html#the-hidden-mechanism-query-key-matching-as-information-retrieval",
    "title": "Transformers",
    "section": "2 The Hidden Mechanism: Query-Key Matching as Information Retrieval",
    "text": "2 The Hidden Mechanism: Query-Key Matching as Information Retrieval\nSo, we need weights that aren’t just blunt averages—how do we pick them? Here’s where the transformer’s secret sauce comes in. Instead of relying on preset or uniform weights, transformers treat context like a search problem.\nPicture this: a sentence is a library, each word is a book, and you—standing at “bank”—are trying to find which books (words) can help you clarify your meaning. Is “money” next to you? “River”? You quickly scan the shelves, checking each for relevance. The art of deciding which books to pull closer, and which to ignore, is called attention.\n\nBut transformers don’t just compare words directly—they use three separate lenses, or linear transformations, for each word’s embedding \\vec{x}_i:\n\n\\begin{align}\n\\vec{q}_i &= W_Q \\vec{x}_i \\quad \\text{(Query: what am I looking for?)} \\\\\n\\vec{k}_i &= W_K \\vec{x}_i \\quad \\text{(Key: what do I offer?)} \\\\\n\\vec{v}_i &= W_V \\vec{x}_i \\quad \\text{(Value: what info do I contain?)}\n\\end{align}\n\nWhy all three? Because Query, Key, and Value play fundamentally different roles in the information retrieval game: Query asks the question, Key offers credentials, and Value supplies the actual payload. By letting the model learn distinct transformation matrices W_Q, W_K, and W_V, transformers dynamically discover which words to “listen to” and how to mix their information—word-by-word, moment-by-moment.\nTo contextualize word i, compute its relevance to all other words j via dot products: \\text{score}(i, j) = \\vec{q}_i \\cdot \\vec{k}_j. High scores indicate strong relevance; low scores indicate noise to ignore. These scores are converted to probabilities using softmax:\n\nw_{ij} = \\frac{\\exp(\\vec{q}_i \\cdot \\vec{k}_j / \\sqrt{d})}{\\sum_{\\ell} \\exp(\\vec{q}_i \\cdot \\vec{k}_\\ell / \\sqrt{d})}\n\nThe division by \\sqrt{d} (where d is the embedding dimension) is a scaling factor that prevents vanishing gradients during training. Finally, compute the contextualized representation as a weighted sum: \\text{contextualized}_i = \\sum_j w_{ij} \\vec{v}_j.\nExplore how different Query and Key transformations produce different attention patterns. Adjust the transformation parameters below to see how W_Q and W_K matrices change which words attend to which:\n\nfunction compactSlider(min, max, step, width, defaultValue, label) {\n  const slider = d3.sliderBottom()\n    .min(min).max(max).step(step).width(width).default(defaultValue);\n  const svg = d3.create(\"svg\").attr(\"width\", width + 40).attr(\"height\", 50);\n  svg.append(\"g\").attr(\"transform\", \"translate(20,15)\").call(slider);\n  svg.append(\"text\").attr(\"x\", (width + 40) / 2).attr(\"y\", 10).attr(\"text-anchor\", \"middle\").style(\"font-size\", \"11px\").text(label);\n  return svg.node();\n}\n\n\n\n\n\n\n\n{\n  // Create slider function that returns both the element and a reactive value\n  function createSlider(min, max, step, width, defaultValue, label) {\n    const slider = d3.sliderBottom()\n      .min(min).max(max).step(step).width(width).default(defaultValue);\n    const svg = d3.create(\"svg\").attr(\"width\", width + 40).attr(\"height\", 50);\n    const g = svg.append(\"g\").attr(\"transform\", \"translate(20,15)\");\n    g.call(slider);\n    svg.append(\"text\").attr(\"x\", (width + 40) / 2).attr(\"y\", 10)\n       .attr(\"text-anchor\", \"middle\").style(\"font-size\", \"11px\").text(label);\n    return { node: svg.node(), slider: slider };\n  }\n\n  // Create all sliders\n  const qScaleXObj = createSlider(-2, 2, 0.1, 180, 1.0, \"Q Scale X\");\n  const qScaleYObj = createSlider(-2, 2, 0.1, 180, 1.0, \"Q Scale Y\");\n  const qRotateObj = createSlider(-180, 180, 5, 180, 0, \"Q Rotate (deg)\");\n  const kScaleXObj = createSlider(-2, 2, 0.1, 180, 1.0, \"K Scale X\");\n  const kScaleYObj = createSlider(-2, 2, 0.1, 180, 1.0, \"K Scale Y\");\n  const kRotateObj = createSlider(-180, 180, 5, 180, 0, \"K Rotate (deg)\");\n\n  // Get slider values\n  const qScaleX = qScaleXObj.slider.value();\n  const qScaleY = qScaleYObj.slider.value();\n  const qRotate = qRotateObj.slider.value();\n  const kScaleX = kScaleXObj.slider.value();\n  const kScaleY = kScaleYObj.slider.value();\n  const kRotate = kRotateObj.slider.value();\n\n  // Word embeddings in 2D space\n  const attentionWords = [\"bank\", \"money\", \"loan\", \"river\", \"shore\"];\n  const attentionEmbeddings = [\n    [0.0, 0.0],    // bank (center)\n    [-0.8, -0.3],  // money\n    [-0.7, -0.6],  // loan\n    [0.7, -0.5],   // river\n    [0.6, -0.7]    // shore\n  ].map(([x, y]) =&gt; [x * 2, y * 2]);\n\n  // Transform embeddings function\n  function transformEmbeddings(embeddings, scaleX, scaleY, rotateDeg) {\n    const theta = (rotateDeg * Math.PI) / 180;\n    const cos = Math.cos(theta);\n    const sin = Math.sin(theta);\n\n    return embeddings.map(([x, y]) =&gt; {\n      const scaledX = x * scaleX;\n      const scaledY = y * scaleY;\n      return [\n        scaledX * cos - scaledY * sin,\n        scaledX * sin + scaledY * cos\n      ];\n    });\n  }\n\n  // Function to update visualization\n  function update() {\n    // Get current values\n    const qScaleX = qScaleXObj.slider.value();\n    const qScaleY = qScaleYObj.slider.value();\n    const qRotate = qRotateObj.slider.value();\n    const kScaleX = kScaleXObj.slider.value();\n    const kScaleY = kScaleYObj.slider.value();\n    const kRotate = kRotateObj.slider.value();\n\n    // Apply transformations\n    const Q = transformEmbeddings(attentionEmbeddings, qScaleX, qScaleY, qRotate);\n    const K = transformEmbeddings(attentionEmbeddings, kScaleX, kScaleY, kRotate);\n\n    // Compute attention scores (Q @ K^T)\n    const scores = Q.map(q =&gt; K.map(k =&gt; q[0] * k[0] + q[1] * k[1]));\n\n    // Apply softmax to each row\n    const attentionWeights = scores.map(row =&gt; {\n      const maxScore = Math.max(...row);\n      const expScores = row.map(s =&gt; Math.exp(s - maxScore));\n      const sumExp = expScores.reduce((a, b) =&gt; a + b, 0);\n      return expScores.map(e =&gt; e / sumExp);\n    });\n\n    // Prepare data for Query space\n    const qData = attentionWords.map((word, i) =&gt; ({\n      word: word,\n      x: Q[i][0],\n      y: Q[i][1]\n    }));\n\n    // Prepare data for Key space\n    const kData = attentionWords.map((word, i) =&gt; ({\n      word: word,\n      x: K[i][0],\n      y: K[i][1]\n    }));\n\n    // Prepare data for heatmap\n    const heatmapData = (() =&gt; {\n      const data = [];\n      for (let i = 0; i &lt; attentionWords.length; i++) {\n        for (let j = 0; j &lt; attentionWords.length; j++) {\n          data.push({\n            Query: attentionWords[i],\n            Key: attentionWords[j],\n            Weight: attentionWeights[i][j]\n          });\n        }\n      }\n      return data;\n    })();\n\n    // Clear and update plots\n    d3.select(qPlotContainer).selectAll(\"*\").remove();\n    d3.select(kPlotContainer).selectAll(\"*\").remove();\n    d3.select(heatmapPlotContainer).selectAll(\"*\").remove();\n\n    // Create Query space visualization\n    const qPlot = Plot.plot({\n      width: 220,\n      height: 220,\n      marginTop: 30,\n      marginBottom: 40,\n      marginLeft: 50,\n      marginRight: 20,\n      style: {\n        background: \"white\",\n        color: \"black\"\n      },\n      x: { domain: [-4, 4], label: \"Q1\", grid: true, ticks: 10 },\n      y: { domain: [-4, 4], label: \"Q2\", grid: true, ticks: 10 },\n      marks: [\n        Plot.dot(qData, { x: \"x\", y: \"y\", r: 6, fill: \"#4682b4\" }),\n        Plot.text(qData, { x: \"x\", y: \"y\", text: \"word\", dy: -12, fontSize: 10, fontWeight: \"bold\", fill: \"black\" }),\n        Plot.text([{ x: 0, y: 4.5 }], { x: \"x\", y: \"y\", text: () =&gt; \"Query Space\", fontSize: 12, fontWeight: \"bold\", fill: \"black\" })\n      ]\n    });\n\n    // Create Key space visualization\n    const kPlot = Plot.plot({\n      width: 220,\n      height: 220,\n      marginTop: 30,\n      marginBottom: 40,\n      marginLeft: 50,\n      marginRight: 20,\n      style: {\n        background: \"white\",\n        color: \"black\"\n      },\n      x: { domain: [-4, 4], label: \"K1\", grid: true, ticks: 10 },\n      y: { domain: [-4, 4], label: \"K2\", grid: true, ticks: 10 },\n      marks: [\n        Plot.dot(kData, { x: \"x\", y: \"y\", r: 6, fill: \"#2e8b57\" }),\n        Plot.text(kData, { x: \"x\", y: \"y\", text: \"word\", dy: -12, fontSize: 10, fontWeight: \"bold\", fill: \"black\" }),\n        Plot.text([{ x: 0, y: 4.5 }], { x: \"x\", y: \"y\", text: () =&gt; \"Key Space\", fontSize: 12, fontWeight: \"bold\", fill: \"black\" })\n      ]\n    });\n\n    // Create attention heatmap\n    const heatmapPlot = Plot.plot({\n      width: 280,\n      height: 280,\n      marginTop: 50,\n      marginBottom: 50,\n      marginLeft: 70,\n      marginRight: 80,\n      style: {\n        background: \"white\",\n        color: \"black\"\n      },\n      x: { label: \"Key Word\" },\n      y: { label: \"Query Word\" },\n      color: {\n        scheme: \"Blues\",\n        label: \"Attention\",\n        legend: true\n      },\n      marks: [\n        Plot.cell(heatmapData, {\n          x: \"Key\",\n          y: \"Query\",\n          fill: \"Weight\",\n          tip: true\n        }),\n        Plot.text(heatmapData, {\n          x: \"Key\",\n          y: \"Query\",\n          text: d =&gt; d.Weight.toFixed(2),\n          fill: d =&gt; d.Weight &gt; 0.35 ? \"white\" : \"black\",\n          fontSize: 9\n        }),\n        Plot.text([{ x: 0, y: 0 }], {\n          x: () =&gt; attentionWords.length / 2 - 0.5,\n          y: () =&gt; -0.8,\n          text: () =&gt; \"Attention Weights (Softmax)\",\n          fontSize: 12,\n          fontWeight: \"bold\",\n          frameAnchor: \"top\",\n          fill: \"black\"\n        })\n      ]\n    });\n\n    d3.select(qPlotContainer).node().appendChild(qPlot);\n    d3.select(kPlotContainer).node().appendChild(kPlot);\n    d3.select(heatmapPlotContainer).node().appendChild(heatmapPlot);\n  }\n\n  // Create plot containers\n  const qPlotContainer = document.createElement(\"div\");\n  const kPlotContainer = document.createElement(\"div\");\n  const heatmapPlotContainer = document.createElement(\"div\");\n\n  // Add event listeners to sliders\n  qScaleXObj.slider.on(\"onchange\", update);\n  qScaleYObj.slider.on(\"onchange\", update);\n  qRotateObj.slider.on(\"onchange\", update);\n  kScaleXObj.slider.on(\"onchange\", update);\n  kScaleYObj.slider.on(\"onchange\", update);\n  kRotateObj.slider.on(\"onchange\", update);\n\n  // Initial render\n  update();\n\n  // Return combined layout\n  return html`&lt;div style=\"display: flex; align-items: flex-start; gap: 40px; justify-content: center;\"&gt;\n    &lt;div style=\"display: flex; flex-direction: column; gap: 25px;\"&gt;\n      &lt;div style=\"display: flex; flex-direction: column; gap: 8px;\"&gt;\n        &lt;div style=\"font-weight: bold; margin-bottom: 3px;\"&gt;Query Transformation (W_Q)&lt;/div&gt;\n        ${qScaleXObj.node}\n        ${qScaleYObj.node}\n        ${qRotateObj.node}\n      &lt;/div&gt;\n      &lt;div style=\"display: flex; flex-direction: column; gap: 8px;\"&gt;\n        &lt;div style=\"font-weight: bold; margin-bottom: 3px;\"&gt;Key Transformation (W_K)&lt;/div&gt;\n        ${kScaleXObj.node}\n        ${kScaleYObj.node}\n        ${kRotateObj.node}\n      &lt;/div&gt;\n    &lt;/div&gt;\n    &lt;div style=\"display: flex; flex-direction: column; align-items: center; gap: 20px;\"&gt;\n      &lt;div style=\"display: flex; gap: 20px; align-items: center;\"&gt;\n        ${qPlotContainer}\n        ${kPlotContainer}\n      &lt;/div&gt;\n      &lt;div&gt;\n        ${heatmapPlotContainer}\n      &lt;/div&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;`;\n}\n\n\n\n\n\n\nNow scale this up. Every word in a sentence needs to attend to every other word. For “The cat sat on the mat” (six words), we compute a 6 \\times 6 attention matrix A, where A_{ij} = \\text{softmax}(\\vec{q}_i \\cdot \\vec{k}_j). Rows represent words asking for context (Queries); columns represent words providing context (Keys). Each cell (i,j) indicates how much word i attends to word j. Each row sums to 1—it’s a probability distribution over context words.\nBut here’s the final complication: one attention matrix captures one type of relationship. Language is multidimensional. There are syntactic relationships (subject-verb-object), semantic relationships (conceptual similarity between “cat” and “mat” as physical objects), positional relationships (local word order), and pragmatic relationships (coreference, where “her” links to “scientist”). A single attention mechanism can’t capture all of these simultaneously.\n\nThe solution is multi-head attention: run multiple attention mechanisms in parallel, each with its own W_Q, W_K, W_V matrices. Mathematically, \\text{MultiHead}(X) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h) W^O. Each head learns a different attention pattern. Modern transformers use 8-16 heads per layer: BERT uses 12, GPT-3 uses 96. It’s like having twelve different experts analyze the sentence simultaneously—one focusing on syntax, one on semantics, one on coreference—and then combining their insights through a learned linear transformation.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Transformers: The Architecture Behind the Magic"
    ]
  },
  {
    "objectID": "m03-text/transformers.html#the-complete-architecture-engineering-for-scale",
    "href": "m03-text/transformers.html#the-complete-architecture-engineering-for-scale",
    "title": "Transformers",
    "section": "3 The Complete Architecture: Engineering for Scale",
    "text": "3 The Complete Architecture: Engineering for Scale\nYou’ve now discovered the core innovation: self-attention via Query-Key-Value. But a working transformer needs additional engineering to make this mechanism trainable at scale.\nPositional encoding solves the problem that attention is permutation-invariant. Without extra information, “cat sat” equals “sat cat” because the dot products don’t encode order. The solution is to add position-dependent vectors to embeddings: \\text{input}_i = \\text{embedding}_i + \\text{position}_i. Transformers use sinusoidal functions of varying frequencies: \\text{PE}(pos, 2i) = \\sin(pos / 10000^{2i/d}) and \\text{PE}(pos, 2i+1) = \\cos(pos / 10000^{2i/d}). This gives the model access to both absolute position and relative distances between words.\nResidual connections create highways for gradient flow. Deep networks suffer from vanishing gradients—signals decay exponentially as they backpropagate through layers. The solution is skip connections: \\text{output} = \\text{LayerNorm}(x + \\text{Attention}(x)). This allows gradients to flow backward through 12-24 layers without vanishing.\nFeed-forward networks add processing power beyond attention. After the attention sublayer, each word’s representation passes through a small multi-layer perceptron independently: \\text{FFN}(x) = \\text{ReLU}(x W_1 + b_1) W_2 + b_2. This introduces non-linearity and gives the model capacity to learn complex transformations.\nLayer normalization keeps activations in a stable numerical range during training, preventing exploding or vanishing values that would make gradient descent unstable.\nThe complete transformer block, repeated 12 or more times, looks like this: Input → Multi-Head Self-Attention → Add & LayerNorm (residual) → Feed-Forward Network → Add & LayerNorm (residual) → Output passed to next block.\nHere’s what matters: residual connections, layer normalization, and feed-forward networks all existed before 2017. The transformer’s innovation is self-attention. That single mechanism replaced recurrent neural networks (RNNs), long short-term memory networks (LSTMs), and convolutional approaches for natural language processing. The original paper’s title—“Attention Is All You Need”—was provocative but accurate.\nThe attention mechanism gets deployed differently depending on the task. Encoder-only architectures like BERT use bidirectional attention where word i can attend to all words, past and future. These models excel at understanding text for classification, embeddings, and extraction tasks. You feed in “Is this paper about networks or biology?” and get back a classification. Decoder-only architectures like GPT and Gemma use causal attention (masked) where word i can only attend to words at positions \\leq i. This prevents “looking into the future” during autoregressive generation. When generating text word-by-word, you can’t use words you haven’t generated yet. These models excel at completion and chat. Encoder-decoder architectures like the original transformer use bidirectional attention in the encoder to process input, causal attention in the decoder for output generation, plus cross-attention where the decoder’s Query vectors attend to the encoder’s Key and Value vectors. These models excel at sequence-to-sequence tasks like translation: “Hello world” → “Bonjour monde.”",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Transformers: The Architecture Behind the Magic"
    ]
  },
  {
    "objectID": "m03-text/transformers.html#why-this-changed-everything",
    "href": "m03-text/transformers.html#why-this-changed-everything",
    "title": "Transformers",
    "section": "4 Why This Changed Everything",
    "text": "4 Why This Changed Everything\nBefore transformers, natural language processing used sequential processing via recurrent neural networks. The computational graph looked like: Word 1 → Hidden State 1 → Word 2 → Hidden State 2 → and so on. This architecture had three fatal flaws. First, sequential computation is inherently slow—you can’t parallelize across words because each hidden state depends on the previous one. Second, information decays over long distances due to vanishing gradients; dependencies 100+ words apart become nearly impossible to learn. Third, the fixed-size hidden state creates an information bottleneck for long sequences.\nTransformers solved all three problems simultaneously. Parallel processing means all words are processed simultaneously, yielding 100× faster training on modern GPUs. Arbitrary long-range dependencies become tractable because “bank” in position 50 can directly attend to “money” in position 2 through a single matrix multiplication—no gradients need to flow through 48 intermediate steps. Scalability emerged as a predictable phenomenon: performance scales logarithmically with parameters. This revealed a stunning empirical fact—bigger models aren’t just incrementally better; they unlock qualitatively new capabilities like in-context learning and multi-step reasoning. This is why GPT-4 dramatically exceeds GPT-3, which dramatically exceeds GPT-2, despite using essentially the same architecture.\nConsider a concrete example of how attention captures linguistic structure. Take the sentence “The scientist published her paper.” We want to resolve the coreference: what does “her” refer to?\n\n{\n  const words = [\"The\", \"scientist\", \"published\", \"her\", \"paper\"];\n  const attention = [\n    [0.5, 0.3, 0.1, 0.05, 0.05],   // \"The\"→\"scientist\"\n    [0.2, 0.5, 0.2, 0.05, 0.05],   // \"scientist\"\n    [0.05, 0.3, 0.4, 0.1, 0.15],   // \"published\"\n    [0.05, 0.6, 0.1, 0.2, 0.05],   // \"her\"→\"scientist\" ← KEY!\n    [0.05, 0.2, 0.1, 0.15, 0.5]    // \"paper\"\n  ];\n\n  // Prepare heatmap data\n  const heatmapData = [];\n  for (let i = 0; i &lt; words.length; i++) {\n    for (let j = 0; j &lt; words.length; j++) {\n      heatmapData.push({\n        Word: words[i],\n        AttendsTo: words[j],\n        Weight: attention[i][j],\n        isHighlight: i === 3 && j === 1  // \"her\" → \"scientist\"\n      });\n    }\n  }\n\n  return Plot.plot({\n    width: 500,\n    height: 400,\n    marginTop: 60,\n    marginBottom: 60,\n    marginLeft: 90,\n    marginRight: 120,\n    style: {\n      background: \"white\",\n      color: \"black\"\n    },\n    x: {\n      label: \"Attends To →\",\n      labelAnchor: \"center\"\n    },\n    y: {\n      label: \"↑ Word\",\n      labelAnchor: \"center\"\n    },\n    color: {\n      type: \"linear\",\n      scheme: \"Purples\",\n      domain: [0, 0.6],\n      label: \"Attention Weight\",\n      legend: true\n    },\n    marks: [\n      Plot.cell(heatmapData, {\n        x: \"AttendsTo\",\n        y: \"Word\",\n        fill: \"Weight\",\n        inset: 0.5\n      }),\n      Plot.text(heatmapData, {\n        x: \"AttendsTo\",\n        y: \"Word\",\n        text: d =&gt; d.Weight.toFixed(2),\n        fill: d =&gt; d.Weight &gt; 0.3 ? \"white\" : \"black\",\n        fontSize: 11,\n        fontWeight: \"bold\"\n      }),\n      // Highlight box around \"her\" → \"scientist\"\n      Plot.rect([{x: \"scientist\", y: \"her\"}], {\n        x: \"x\",\n        y: \"y\",\n        fill: \"none\",\n        stroke: \"red\",\n        strokeWidth: 3,\n        inset: -2\n      }),\n      Plot.text([{ x: 0, y: 0 }], {\n        x: () =&gt; words.length / 2 - 0.5,\n        y: () =&gt; -0.9,\n        text: () =&gt; \"Coreference via Attention: 'her' → 'scientist'\",\n        fontSize: 14,\n        fontWeight: \"bold\",\n        frameAnchor: \"top\",\n        fill: \"black\"\n      })\n    ]\n  });\n}\n\n\n\n\n\n\nAttention pattern showing ‘her’ → ‘scientist’ (0.60 weight). The model learned coreference without explicit grammar rules—purely from prediction tasks.\nRow 3 corresponds to “her.” The pronoun attends most strongly to “scientist” (0.60), correctly identifying the referent. The model discovered that pronouns link to earlier nouns through statistical patterns in training data—no one programmed this grammatical rule. This is the profound insight: transformers learn the structure of language as a side effect of optimizing a simple prediction objective.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Transformers: The Architecture Behind the Magic"
    ]
  },
  {
    "objectID": "m03-text/transformers.html#the-existential-conclusion",
    "href": "m03-text/transformers.html#the-existential-conclusion",
    "title": "Transformers",
    "section": "5 The Existential Conclusion",
    "text": "5 The Existential Conclusion\nWhen you use sentence-transformers to generate embeddings for your research, you’re using encoder transformers (BERT-based) with bidirectional attention. When you interact with ChatGPT or run Gemma locally, you’re using decoder transformers with causal attention. When you see attention visualizations in papers, you now understand they represent learned relevance weights computed from Query-Key comparisons. The models don’t understand language the way humans do—they perform pattern matching at scale—but the patterns they discover are often the same syntactic and semantic structures that linguists have documented for decades.\nThe limitations are worth remembering. Attention has quadratic complexity: computing O(N^2) pairwise scores becomes prohibitively expensive for very long texts, which is why context windows typically cap at 2K-32K tokens. Transformers have no true memory beyond the current window—they can’t learn facts across documents without retraining. They produce fluent text through statistical pattern matching but lack grounded understanding, leading to hallucinations when they confidently generate plausible-sounding nonsense. Training costs are astronomical: GPT-3 required roughly $5 million in compute. But you don’t need to train your own models; you can use pre-trained ones and fine-tune them for specific tasks with orders of magnitude less data and compute.\nReturn to where we started: the “bank” problem. Static embeddings treated “bank” as one point in space, averaging across all contexts. Transformers compute a distribution of representations, conditioned on the surrounding words. The revolution wasn’t in the complexity—it was in recognizing that meaning is relational, not absolute. Words are like atoms in a molecule: their properties depend on what they’re bonded to. Transformers formalized this intuition mathematically through a simple mechanism: weighted mixing based on learned relevance.\nThis entire architecture emerged from asking a single question: What’s the simplest mechanism that lets representations adapt to context? The answer was weighted mixing where the weights come from comparing what each word needs (Query) against what other words offer (Key). Everything else—multi-head attention, layer normalization, feed-forward networks—is engineering to make that core idea scale to 175 billion parameters and beyond.\nNext: Word Embeddings: Where It Started",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Transformers: The Architecture Behind the Magic"
    ]
  },
  {
    "objectID": "m03-text/llm-intro.html#the-naive-intuition",
    "href": "m03-text/llm-intro.html#the-naive-intuition",
    "title": "Large Language Models in Practice",
    "section": "1 The Naive Intuition",
    "text": "1 The Naive Intuition\nWhen ChatGPT launched in late 2022, the narrative was immediate: machines can now think. But this error has a lineage. In 1950, Turing proposed his famous test, enshrining a dangerous confusion: the appearance of intelligence is not intelligence itself. ELIZA, the 1960s chatbot that mimicked a therapist through keyword substitution, convinced users it understood them. No comprehension. Just pattern matching.\nLarge language models are ELIZA’s descendants, scaled up by thirteen orders of magnitude. The question is not whether they think—they don’t. The question is: can these pattern-matching machines help us do better science?",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Large Language Models in Practice"
    ]
  },
  {
    "objectID": "m03-text/llm-intro.html#the-theoretical-failure",
    "href": "m03-text/llm-intro.html#the-theoretical-failure",
    "title": "Large Language Models in Practice",
    "section": "2 The Theoretical Failure",
    "text": "2 The Theoretical Failure\nThe standard model of language is compositional. Chomsky spent his career arguing that language is governed by universal grammar—recursive rules that generate infinite valid sentences. Understanding language means understanding the rules.\nLLMs break this model. They are not taught grammar or given rules. They are fed billions of sentences and trained to predict the next word: given w_1, w_2, \\ldots, w_n, estimate P(w_{n+1} \\mid w_1, \\ldots, w_n).\n\n\n\n\n\nChomsky would hate this. There are no rules here, only correlations. The model doesn’t “know” grammar; it has seen enough examples to predict accordingly. And yet, this crude statistical approach works. When OpenAI trained ever-larger models on ever-more data, capabilities emerged that were never programmed—translation, math, coding in languages invented after training.\nThe central paradox: a system optimized for prediction develops representations that look suspiciously like understanding. But when you probe the extremes—events after the knowledge cutoff, precise factual recall, genuine reasoning—the illusion shatters.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Large Language Models in Practice"
    ]
  },
  {
    "objectID": "m03-text/llm-intro.html#the-hidden-mechanism",
    "href": "m03-text/llm-intro.html#the-hidden-mechanism",
    "title": "Large Language Models in Practice",
    "section": "2 The Hidden Mechanism",
    "text": "2 The Hidden Mechanism\nImagine you want to predict lottery numbers but can’t compute true probabilities. You memorize millions of historical draws. When asked “What comes next?”, you recall similar past sequences and output the most common continuation. You’re not computing probabilities; you’re pattern matching against compressed memory. This is the toy model for how LLMs work.\nLLMs function like lossy compression algorithms. To predict “The capital of France is ___,” the model must compress not just the fact (Paris) but the statistical regularities governing how facts appear in text—that capitals follow “The capital of,” that France is a country, that countries have capitals. This compression is probabilistic, not factual. The model stores P(\\text{word}_{n+1} \\mid \\text{word}_1, \\ldots, \\text{word}_n), which words tend to follow which other words in which contexts.\n\n\n\n\n\nTraining feeds the model billions of sentences. For each sentence, the model predicts the next word, compares its prediction to the actual next word, and adjusts its parameters to increase the probability of the correct word. Repeat trillions of times. The result: a compressed representation of how language behaves statistically. The model doesn’t learn “Paris is the capital of France” as a fact; it learns that in contexts matching the pattern [The capital of France is], the token “Paris” appears with high probability.\nThis optimization creates hallucination, fluent but false outputs. The model optimizes for probability, not truth. If it has seen 1,000 sentences about quantum networks and 10 about quantum community detection, it fabricates plausible results for a non-existent “Smith et al. paper” because that pattern fits academic writing. Truth and fluency correlate in the training data, so the model is mostly truthful. But in the tails—obscure topics, recent events, precise recall—fluency diverges from truth, and the model follows fluency.\nTwo constraints compound this issue. First, context limits: models see only 2,000–8,000 tokens at once, meaning that if you paste 100 abstracts, early ones are mathematically evicted from the model’s working memory. Second, stochasticity: the model samples from probability distributions, so the same prompt yields different outputs across runs. Fluency is deterministic; specifics are random.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Large Language Models in Practice"
    ]
  },
  {
    "objectID": "m03-text/llm-intro.html#the-existential-conclusion",
    "href": "m03-text/llm-intro.html#the-existential-conclusion",
    "title": "Large Language Models in Practice",
    "section": "4 The Existential Conclusion",
    "text": "4 The Existential Conclusion\nSo what should you do with this knowledge? If LLMs are pattern matchers rather than thinkers, compressors rather than reasoners, how should you deploy them in your research?\nThe answer is: use them to scale your judgment, not replace it. LLMs are extraordinary tools for tasks where speed matters more than precision—summarizing large volumes of text, extracting structured information, reformulating ideas, brainstorming directions. They are useless for tasks where precision is paramount—verifying citations, making ethical decisions, performing statistical analysis, or producing literature reviews without human oversight.\nThink of an LLM as a research assistant who has read the entire internet but remembers everything imperfectly and occasionally makes things up. You wouldn’t trust this assistant to write your paper unsupervised, but you would absolutely delegate the tedious work of skimming 50 abstracts to identify the 10 worth reading in detail. You would ask them to extract key findings from papers, generate synthetic examples for testing code, or suggest research directions you hadn’t considered—and then you would verify everything they produce.\nThe existential lesson is this: the average is a lie; the truth is in the distribution. Most of the time, LLMs produce fluent, useful text. But the events that matter—the hallucinated citation that undermines your credibility, the missed context that skews your analysis, the over-reliance that atrophies your critical thinking—live in the tail. Your job is to harvest the value from the center of the distribution while defending against the risks in the extremes.\nNow let’s get practical.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Large Language Models in Practice"
    ]
  },
  {
    "objectID": "m03-text/llm-intro.html#the-naive-model-vs.-the-reality",
    "href": "m03-text/llm-intro.html#the-naive-model-vs.-the-reality",
    "title": "Large Language Models in Practice",
    "section": "1 The Naive Model vs. The Reality",
    "text": "1 The Naive Model vs. The Reality\nIf a machine writes coherent essays, debugs code, and answers questions accurately, it must understand language the way humans do. This intuition traces to Turing’s 1950 test: if you can’t tell it’s a machine, treat it as intelligent. The assumption is that fluency requires comprehension.\nELIZA, a 1960s chatbot, shattered this assumption. It convinced users it was a therapist using only keyword substitution and reflection—no comprehension, just pattern matching. Large language models are ELIZA scaled by thirteen orders of magnitude. They predict which word comes next in a sequence, nothing more. Yet this simple objective forces them to encode grammar, facts, logic, and context—not because they understand, but because prediction requires compression of statistical regularities. The model that best predicts “The capital of France is ___” must have compressed the statistical pattern linking countries to capitals, whether or not it “knows” what a capital is.\nThe paradox: optimizing for prediction creates representations that look like understanding. But probe the extremes—ask about events after the training cutoff, request precise citations, demand genuine reasoning—and the illusion breaks. The model follows fluency where truth data is sparse.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Large Language Models in Practice"
    ]
  },
  {
    "objectID": "m03-text/llm-intro.html#the-strategic-takeaway",
    "href": "m03-text/llm-intro.html#the-strategic-takeaway",
    "title": "Large Language Models in Practice",
    "section": "3 The Strategic Takeaway",
    "text": "3 The Strategic Takeaway\nUse LLMs to scale pattern recognition, not judgment. They excel where speed trumps precision: summarizing 50 abstracts to identify the 10 worth reading, extracting structured data from unstructured text, reformulating technical concepts, brainstorming research directions. They fail where precision is paramount: verifying citations, making ethical decisions, performing statistical analysis.\nThink of an LLM as an assistant who has read the internet but remembers imperfectly. You delegate skimming. You verify everything.\nThe average is a lie; the truth is in the distribution. Most outputs are fluent and useful. But the events that matter—the hallucinated citation that undermines credibility, the missed context that skews analysis—live in the tail. Harvest the center. Defend against the extremes.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Large Language Models in Practice"
    ]
  },
  {
    "objectID": "m03-text/llm-intro.html#failure-modes",
    "href": "m03-text/llm-intro.html#failure-modes",
    "title": "Large Language Models in Practice",
    "section": "8 Failure Modes",
    "text": "8 Failure Modes\nHallucination: LLMs fabricate plausibly. Ask about a non-existent “Smith et al. quantum paper” and receive fluent academic prose describing results that never happened. Always verify citations.\nLimited context: Models see 2,000–8,000 tokens. Paste 100 abstracts and early ones vanish from the model’s “memory.”\nKnowledge cutoff: Gemma 3N’s training ended early 2024. Ask about recent events, receive outdated information or plausible fabrications.\nNo reasoning: LLMs pattern-match, don’t reason. Ask “How many r’s in ‘Strawberry’?” The model might answer 3 (correct) via pattern matching against similar questions in training data, not by counting. Sometimes right. Often wrong.\nUse to accelerate work, not replace judgment.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Large Language Models in Practice"
    ]
  },
  {
    "objectID": "m03-text/llm-intro.html#when-to-use-llms",
    "href": "m03-text/llm-intro.html#when-to-use-llms",
    "title": "Large Language Models in Practice",
    "section": "9 When to Use LLMs",
    "text": "9 When to Use LLMs\nGood use cases: Summarizing text. Extracting structure. Reformulating concepts. Brainstorming. Generating synthetic examples. Translation.\nPoor use cases: Literature reviews without verification. Factual claims without sources. Statistical analysis (use proper tools). Ethical decisions.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Large Language Models in Practice"
    ]
  },
  {
    "objectID": "m03-text/llm-intro.html#next",
    "href": "m03-text/llm-intro.html#next",
    "title": "Large Language Models in Practice",
    "section": "7 Next",
    "text": "7 Next\nYou’ve seen LLMs in practice—setup, summarization, extraction, limitations. But how do they actually work? What happens inside when you send a prompt?\nThe rest of this module unboxes the technology: prompt engineering (communicating with LLMs), embeddings (representing meaning as numbers), transformers (the architecture enabling modern NLP), fundamentals (from word counts to neural representations).\nFirst, let’s master talking to machines.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Large Language Models in Practice"
    ]
  },
  {
    "objectID": "m03-text/llm-intro.html#the-strategic-application",
    "href": "m03-text/llm-intro.html#the-strategic-application",
    "title": "Large Language Models in Practice",
    "section": "3 The Strategic Application",
    "text": "3 The Strategic Application\nUse LLMs to scale pattern recognition, not judgment. They excel where speed trumps precision: summarizing 50 abstracts to identify the 10 worth reading, extracting structured data from unstructured text, reformulating technical concepts, brainstorming research directions. They fail where precision is paramount—verifying citations, making ethical decisions, performing statistical analysis. Think of an LLM as an assistant who has read the internet but remembers imperfectly. You delegate skimming. You verify everything.\nThe events that matter live in the tail. Most outputs are fluent and useful. But the hallucinated citation that undermines credibility, the missed context that skews analysis—these failures cluster in the tails of the probability distribution. Harvest the center. Defend against the extremes.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Large Language Models in Practice"
    ]
  },
  {
    "objectID": "m03-text/llm-intro.html#research-applications",
    "href": "m03-text/llm-intro.html#research-applications",
    "title": "Large Language Models in Practice",
    "section": "5 Research Applications",
    "text": "5 Research Applications\nThe strategy is simple: use LLMs for tasks where speed trumps precision, then verify the outputs that matter. Three workflows demonstrate this pattern.\nFirst, abstract summarization. You collected 50 papers on network science. Which deserve detailed reading? LLMs scan them in seconds:\n\nabstract = \"\"\"\nCommunity detection in networks is a fundamental problem in complex systems.\nWhile many algorithms exist, most assume static networks. We propose a dynamic\ncommunity detection algorithm that tracks evolving communities over time using\na temporal smoothness constraint. We evaluate our method on synthetic and real\ntemporal networks, showing it outperforms static methods applied to temporal\nsnapshots. Our approach reveals how communities merge, split, and persist in\nsocial networks, biological systems, and transportation networks.\n\"\"\"\n\nprompt = f\"Summarize this abstract in one sentence:\\n\\n{abstract}\"\nresponse = ollama.generate(prompt=prompt, **params_llm)\nprint(response.response)\n\nThis paper introduces a novel dynamic community detection algorithm that effectively tracks evolving communities in networks over time, outperforming static methods and revealing community dynamics in various real-world systems.\n\n\n\n\n\n\nThe model captures the pattern: propose method, evaluate, outperform baselines. It doesn’t understand the paper; it has seen enough academic abstracts to recognize the structure. For multiple abstracts, loop through them:\n\nfor i, abstract in enumerate([\"Abstract 1...\", \"Abstract 2...\"], 1):\n    response = ollama.generate(prompt=f\"Summarize:\\n\\n{abstract}\", **params_llm)\n    print(f\"{i}. {response.response}\")\n\n1. Please provide me with \"Abstract 1\"! I need the text of the abstract to be able to summarize it for you. \n\nJust paste the abstract here, and I'll give you a concise summary. 😊 \n\nI'm ready when you are!\n2. Please provide me with the content of \"Abstract 2\"! I need the text of the abstract to be able to summarize it for you. \n\nJust paste the abstract here, and I'll do my best to give you a concise and accurate summary. 😊 \n\n\n\n\nLocal models are slow (2–5 seconds per abstract). For thousands of papers, switch to cloud APIs. But the workflow scales: delegate skimming to the model, retain judgment for yourself.\nSecond, structured extraction. Turn unstructured text into structured data automatically:\n\nabstract = \"\"\"\nWe analyze scientific collaboration networks using 5 million papers from\n2000-2020. Using graph neural networks and community detection, we identify\ndisciplinary boundaries and interdisciplinary bridges. Interdisciplinarity\nincreased 25%, with physics and CS showing strongest cross-connections.\n\"\"\"\n\nprompt = f\"\"\"Extract: Domain, Methods, Key Finding\\n\\n{abstract}\\n\\nFormat:\\nDomain:...\\nMethods:...\\nKey Finding:...\"\"\"\nresponse = ollama.generate(prompt=prompt, **params_llm)\nprint(response.response)\n\nHere's the extraction in the requested format:\n\nDomain: Scientific Collaboration Networks\nMethods: Graph Neural Networks, Community Detection, Analysis of 5 million papers (2000-2020)\nKey Finding: Interdisciplinarity increased by 25% between 2000-2020, with the strongest cross-connections observed between Physics and Computer Science.\n\n\n\n\n\n\nScale this to hundreds of papers for meta-analysis. Always verify—LLMs misinterpret obscure terminology and fabricate plausible-sounding technical details when uncertain.\nThird, hypothesis generation. LLMs pattern-match against research questions they’ve encountered in training data:\n\ncontext = \"\"\"I study concept spread in citation networks. Highly cited papers\ncombine existing concepts novelty. What should I study next?\"\"\"\n\nprompt = f\"\"\"Suggest three follow-up research questions:\\n\\n{context}\"\"\"\nresponse = ollama.generate(prompt=prompt, **params_llm)\nprint(response.response)\n\nOkay, here are three follow-up research questions, building upon your current work on concept spread in citation networks, focusing on highly cited papers and the interplay of existing concepts and novelty.  I've tried to offer a mix of methodological and theoretical directions:\n\n**1.  How does the *type* of novelty (e.g., incremental, radical, convergent) in highly cited papers influence the rate and direction of concept spread?**\n\n*   **Rationale:** You've identified that highly cited papers combine existing concepts with novelty.  However, the *nature* of that novelty likely matters.  Is it a small tweak to an existing idea (incremental), a completely new paradigm (radical), or a synthesis of multiple existing ideas (convergent)?  Different types of novelty might spread differently through the citation network.\n*   **Methodology:**\n    *   **Concept Extraction:**  Develop a method (potentially combining NLP and manual annotation) to categorize the type of novelty present in highly cited papers.  This could involve identifying keywords, phrases, and arguments that signal incremental, radical, or convergent novelty.\n    *   **Network Analysis:**  Analyze the citation network to see if papers with different types of novelty have different citation patterns (e.g., different citation rates, different types of citing papers, different network positions).\n    *   **Temporal Analysis:**  Track the spread of concepts over time, looking for differences in the spread patterns of concepts associated with different types of novelty.\n*   **Potential Insights:**  This could reveal whether incremental novelty spreads quickly and widely, while radical novelty takes longer to gain traction but can have a more transformative impact.\n\n**2.  To what extent does the *citation context* (how a highly cited paper is cited) mediate the spread of concepts?**\n\n*   **Rationale:**  It's not just *that* a paper is highly cited, but *how* it's cited that matters.  Is it cited for its core argument, a specific method, a critique, or a combination?  The citation context could influence whether the concept is adopted, adapted, or rejected.\n*   **Methodology:**\n    *   **Citation Context Analysis:**  Develop a method to analyze the text surrounding citations of highly cited papers.  This could involve using NLP techniques to identify the specific arguments or concepts being referenced.\n    *   **Network Analysis:**  Create a citation network where nodes are citations and edges represent the relationship between the cited paper and the citing paper.\n    *   **Correlation Analysis:**  Correlate the citation context with the subsequent spread of concepts in the network.  Do citations that highlight specific aspects of the paper lead to faster or more widespread concept adoption?\n*   **Potential Insights:**  This could reveal the importance of framing and interpretation in the spread of ideas.  It might also highlight the role of debates and critiques in shaping the evolution of concepts.\n\n**3.  Can we identify \"concept hubs\" within the citation network – papers that act as particularly influential nodes in the spread of concepts from highly cited papers?**\n\n*   **Rationale:**  Some papers are more effective at disseminating concepts than others.  These \"concept hubs\" might be characterized by their broad citation patterns, their ability to synthesize information from multiple sources, or their engagement with diverse communities.\n*   **Methodology:**\n    *   **Centrality Measures:**  Apply various network centrality measures (e.g., betweenness centrality, eigenvector centrality, degree centrality) to the citation network.\n    *   **Hub Identification:**  Identify papers with high centrality scores as potential concept hubs.\n    *   **Case Studies:**  Conduct in-depth case studies of these concept hubs to understand how they contribute to the spread of concepts from highly cited papers.  Analyze their content, citation patterns, and engagement with other researchers.\n*   **Potential Insights:**  This could help us understand the mechanisms by which concepts spread through the citation network and identify strategies for promoting the dissemination of important ideas.  It could also reveal the role of specific communities or disciplines in shaping the spread of concepts.\n\n\n\nThese questions are designed to be relatively focused and address different aspects of your initial research.  They also offer opportunities to combine quantitative network analysis with qualitative case studies.  I hope this helps! Let me know if you'd like me to elaborate on any of these or suggest alternative directions.\n\n\n\n\n\n\nTreat the model as a thought partner, not an oracle. It helps structure thinking but doesn’t possess domain expertise. The suggestions reflect patterns in how research questions are framed, not deep knowledge of your field.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Large Language Models in Practice"
    ]
  },
  {
    "objectID": "m03-text/llm-intro.html#failure-modes-and-boundaries",
    "href": "m03-text/llm-intro.html#failure-modes-and-boundaries",
    "title": "Large Language Models in Practice",
    "section": "6 Failure Modes and Boundaries",
    "text": "6 Failure Modes and Boundaries\nThe failure modes follow directly from the mechanism. LLMs fabricate plausibly because they optimize for fluency, not truth. Ask about a non-existent “Smith et al. quantum paper” and receive fluent academic prose describing results that never happened. Always verify citations. The model has seen thousands of papers cited in the format “Smith et al. (2023) demonstrated that…” and generates outputs matching that pattern even when the citation is fictional.\nContext limits are architectural. Models see only 2,000–8,000 tokens at once. Paste 100 abstracts and early ones are mathematically evicted from working memory. The model doesn’t “remember” them; they’re gone. Knowledge cutoffs are temporal. Gemma 3N’s training ended early 2024. Ask about recent events and receive outdated information or plausible fabrications constructed from pre-cutoff patterns.\nReasoning is absent. LLMs pattern-match, they don’t reason. Ask “How many r’s in ‘Strawberry’?” and the model might answer correctly via pattern matching against similar questions in training data, not by counting letters. Sometimes right. Often wrong. The model has no internal representation of what counting means.\nThese aren’t bugs to be fixed. They’re intrinsic to the architecture. Use LLMs to accelerate work, not replace judgment. They excel at summarizing text, extracting structure, reformulating concepts, brainstorming, generating synthetic examples, and translation. They fail at literature reviews without verification, factual claims without sources, statistical analysis, and ethical decisions. Harvest the center of the distribution where fluency and truth correlate. Defend against the tails where they diverge.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Large Language Models in Practice"
    ]
  },
  {
    "objectID": "m03-text/prompt-engineering.html#the-naive-model-vs.-the-reality",
    "href": "m03-text/prompt-engineering.html#the-naive-model-vs.-the-reality",
    "title": "Prompt Engineering",
    "section": "1 The Naive Model vs. The Reality",
    "text": "1 The Naive Model vs. The Reality\nWith computers, we expect the same question to yield the same answer, regardless of phrasing—like a search engine or a database. But LLMs don’t work this way. Ask “Summarize this abstract” and you’ll get a short summary; ask “What’s this abstract about?” and the answer might be three rambling paragraphs. LLMs don’t retrieve facts—they sample from probability distributions conditioned on your exact words. Each tweak to your prompt nudges the model toward a different statistical pattern in its training data.\nSo, LLMs are both powerful and brittle: they can extract insights, but only if your prompt activates the right patterns. Prompt engineering is about shaping inputs that trigger desired outputs.\nThink of LLMs as colossal word-association machines. “Capital” can cue “Paris” or “fund” depending on context. LLMs do this at scale, using your prompt to locate a region in high-dimensional probability space, then sampling likely next words from that context.\nTiny prompt changes—like adding “Think step by step” or specifying an output format—shift the distribution and activate different response patterns seen in training data. The model doesn’t understand your intent; it just predicts likely continuations based on your exact input. Prompt engineering leverages this by deliberately phrasing prompts to guide output.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Prompt Engineering for Research"
    ]
  },
  {
    "objectID": "m03-text/prompt-engineering.html#the-hidden-mechanism",
    "href": "m03-text/prompt-engineering.html#the-hidden-mechanism",
    "title": "Prompt Engineering",
    "section": "2 The Hidden Mechanism",
    "text": "2 The Hidden Mechanism\nImagine you’re playing a word association game. Someone says “capital,” and you must say the next word. If the previous sentence was “The capital of France is,” you say “Paris.” If it was “We need more capital to,” you say “fund” or “invest.” The word “capital” doesn’t have one meaning—it activates different patterns depending on context. LLMs work identically, but at massive scale.\nWhen you submit a prompt, the model converts it into tokens and embeds those tokens in high-dimensional space. Each token’s position in that space depends on surrounding tokens—context shapes meaning. The model then samples the next token from a probability distribution over its vocabulary, conditioned on all previous tokens. It repeats this process until it generates a complete response. Critically, your exact phrasing determines which region of probability space the model occupies when it begins sampling. Slightly different prompts place the model in different regions, where different tokens have high probability.\nThis creates extreme sensitivity to phrasing. Adding “Think step by step” at the end of a prompt shifts the probability distribution toward reasoning patterns that include intermediate steps, because the training data contains many examples where “think step by step” preceded structured reasoning. Adding “You are an expert researcher” shifts the distribution toward formal, technical language patterns. Specifying “Output format: Domain: …, Methods: …” shifts toward structured extraction patterns. Each modification activates different statistical regularities compressed during training.\nThe model has no internal representation of what you “really want.” It only knows which tokens tend to follow which other tokens in which contexts. Prompt engineering exploits this by deliberately activating patterns that produce desired outputs.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Prompt Engineering for Research"
    ]
  },
  {
    "objectID": "m03-text/prompt-engineering.html#the-strategic-application",
    "href": "m03-text/prompt-engineering.html#the-strategic-application",
    "title": "Prompt Engineering",
    "section": "2 The Strategic Application",
    "text": "2 The Strategic Application\n\n\n\n\n\nEffective prompts activate desired patterns by combining structural components that mirror patterns in training data. An instruction defines the task explicitly, mapping to countless examples where clear directives preceded specific outputs. Data provides the input to process. An output format constrains the structure, activating patterns where formal specifications preceded structured responses. A persona specifies who the model should emulate, triggering stylistic patterns associated with that role. Context provides background information—why the task matters, who the response serves, relevant constraints—that helps the model select appropriate patterns from ambiguous alternatives.\nNot every component is necessary. Simple extraction tasks need only instruction, data, and format. Style-sensitive tasks benefit from persona. Complex scenarios with ambiguity require context to disambiguate. The strategy is to provide exactly enough structure to activate the desired pattern without overloading the prompt with irrelevant information that dilutes the signal.\nWe’ll build a prompt progressively, adding components one at a time to observe how each shifts the output distribution.\n\nBuilding from Instruction and Data\nThe most basic prompt consists of an instruction that defines the task and data that provides the input to process:\n\ninstruction = \"Summarize this abstract\"\ndata = \"\"\"\nWe develop a graph neural network for predicting protein-protein interactions\nfrom sequence data. Our model uses attention mechanisms to identify functionally\nimportant amino acid subsequences. We achieve 89% accuracy on benchmark datasets,\noutperforming previous methods by 7%. The model also provides interpretable\nattention weights showing which protein regions drive predictions.\n\"\"\"\n\nprompt = f\"{instruction}. {data}\"\n\n\n\nCode\nimport ollama\n\nparams_llm = {\"model\": \"gemma3:270m\", \"options\": {\"temperature\": 0.3}}\n\nresponse = ollama.generate(prompt=prompt, **params_llm)\nprint(response.response)\n\n\nThis abstract describes a graph neural network (GNN) for predicting protein-protein interactions. The model uses attention mechanisms to identify functionally important amino acid subsequences. It achieves 89% accuracy on benchmark datasets and provides interpretable attention weights, indicating its effectiveness.\n\n\n\nThis basic prompt works, but output varies—the model might produce a long summary, a short one, or change format across runs. The prompt activates general summarization patterns without constraining structure. Adding an output format specification narrows the distribution:\n\noutput_format = \"\"\"Provide the summary in exactly 2 sentences:\n- First sentence: What problem and method\n- Second sentence: Key result with numbers\"\"\"\n\nprompt_with_format = f\"\"\"{instruction}. {data}. {output_format}\"\"\"\n\nThe output format constraint produces structured, consistent output by activating patterns where format specifications preceded conforming responses. This becomes critical when processing hundreds of papers—you need programmatically parseable structure, not freeform text.\n\n\nAdding Persona to Control Style\nA persona tells the LLM who it should emulate, activating stylistic patterns associated with that role in training data. Consider a customer support scenario where tone matters:\n\n# New example for persona demonstration\ninstruction = \"Help the customer reconnect to the service by providing troubleshooting instructions.\"\ndata = \"Customer: I cannot see any webpage. Need help ASAP!\"\noutput_format = \"Keep the response concise and polite. Provide a clear resolution in 2-3 sentences.\"\n\nformal_persona = \"You are a professional customer support agent who responds formally and ensures clarity and professionalism.\"\n\nprompt_with_persona = f\"\"\"{formal_persona}. {instruction}. {data}. {output_format}\"\"\"\n\n\n\nCode\nprint(\"BASE (no persona):\")\nprint(ollama.generate(prompt=instruction + \". \" + data + \". \" + output_format, **params_llm).response)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nprint(\"WITH PERSONA:\")\nprint(ollama.generate(prompt=prompt_with_persona, **params_llm).response)\n\n\nBASE (no persona):\nOkay, I understand. Let's try to troubleshoot this. Please provide me with the specific error message or the URL you're seeing. Once I have that, I'll do my best to help you resolve the issue.\n\n\n============================================================\n\nWITH PERSONA:\nHello, I understand you cannot see any webpage. Could you please try accessing the website again? I'm here to assist you with any troubleshooting steps you need.\n\n\n\nThe persona shifts tone and style. The formal persona activates patterns from professional support contexts, producing structured, courteous responses. Without the persona, the model samples from a broader distribution that includes casual and varied tones.\n\n\nAdding Context to Disambiguate\nContext provides additional information that helps the model select appropriate patterns when multiple valid interpretations exist. Context can include background information explaining why the task matters, audience information specifying who the response serves, and constraints defining special circumstances. Consider adding background urgency:\n\ncontext_background = \"\"\"The customer is extremely frustrated because their internet has been down for three days, and they need it for an important online job interview. They emphasize that 'This is a life-or-death situation for my career!'\"\"\"\n\nprompt_with_context = f\"\"\"{formal_persona}. {instruction}. {data}. {output_format}. Context: {context_background}\"\"\"\n\n\n\nCode\nprint(\"WITH PERSONA:\")\nprint(ollama.generate(prompt=prompt_with_persona, **params_llm).response)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nprint(\"WITH PERSONA + CONTEXT (background):\")\nprint(ollama.generate(prompt=prompt_with_context, **params_llm).response)\n\n\nWITH PERSONA:\nHello, I understand you cannot see any webpage. Could you please try accessing the website again? I'm here to assist you in finding a solution.\n\n\n============================================================\n\nWITH PERSONA + CONTEXT (background):\nDear [Customer Name],\n\nI understand your frustration with your internet connection being down for three days. I apologize for the inconvenience. To help resolve this, could you please try restarting your modem and router? If that doesn't work, please let me know, and I'll do my best to assist you.\n\n\n\nBackground context adds urgency and emotional weight, activating patterns where high-stakes situations preceded empathetic, prioritized responses. The model doesn’t understand emotion, but it has seen urgency markers correlate with specific response patterns.\nAudience information creates even more dramatic shifts. Compare responses for non-technical versus technical users:\n\n# Context with audience information for non-technical user\ncontext_with_audience_nontech = f\"\"\"{context_background} The customer does not know any technical terms like modem, router, networks, etc.\"\"\"\n\ncontext_with_audience_tech = f\"\"\"{context_background} The customer is Head of IT Infrastructure of our company.\"\"\"\n\nprompt_with_context_nontech = f\"\"\"{formal_persona}. {instruction}. {data}. {output_format}. Context: {context_with_audience_nontech}\"\"\"\nprompt_with_context_tech = f\"\"\"{formal_persona}. {instruction}. {data}. {output_format}. Context: {context_with_audience_tech}\"\"\"\n\n\n\nCode\nprint(\"WITH PERSONA + CONTEXT (background only):\")\nprint(ollama.generate(prompt=prompt_with_context, **params_llm).response)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nprint(\"WITH PERSONA + CONTEXT (background + non-tech audience):\")\nprint(ollama.generate(prompt=prompt_with_context_nontech, **params_llm).response)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nprint(\"WITH PERSONA + CONTEXT (background + tech audience):\")\nprint(ollama.generate(prompt=prompt_with_context_tech, **params_llm).response)\n\n\nWITH PERSONA + CONTEXT (background only):\nDear [Customer Name],\n\nI understand your frustration regarding your internet connection. I apologize for the inconvenience this is causing. To resolve this, I'm happy to provide troubleshooting steps and offer a solution. Please let me know if you have any questions.\n\n\n============================================================\n\nWITH PERSONA + CONTEXT (background + non-tech audience):\n\"I understand your frustration, and I apologize for the inconvenience this is causing. To help me assist you, could you please tell me what specific webpage you're having trouble with? I'll do my best to troubleshoot the issue and get you back online as soon as possible.\"\n\n\n============================================================\n\nWITH PERSONA + CONTEXT (background + tech audience):\n\"I understand your frustration with the internet outage and the need for your job interview. I'm here to assist you with troubleshooting. Please try the following steps:\n1. Check your internet connection.\n2. Try restarting your modem and router.\n3. If the issue persists, please contact our IT support team. We'll be happy to help.\"\n\n\n\nAudience information dramatically shifts technical level and terminology. For non-technical users, the response avoids jargon because the training data contains many examples where “does not know technical terms” preceded simplified explanations. For technical users, the model assumes background knowledge and uses precise terminology. Same underlying mechanism—pattern matching—but different patterns activated.\nThe complete template combines all components, but not every prompt needs every component. Simple extraction tasks need only instruction, data, and output format. Style-sensitive tasks benefit from persona. Complex scenarios with ambiguity require context:\n\nprompt_template = \"\"\"\n{persona}\n\n{instruction}\n\n{data}\n\nContext: {context}\n\n{output_format}\n\"\"\"\n\n\n\n\n\n\n\nWhen Personas Help (and When They Don’t)\n\n\n\nResearch shows that adding personas can improve tone and style, but does not necessarily improve performance on factual tasks. In some cases, personas may even degrade performance or introduce biases.\nUse personas when: You need specific tone/style, responses tailored to an audience, or a particular perspective.\nAvoid personas when: You need maximum factual accuracy, the task is purely extraction/classification, or you’re concerned about bias introduction.\nAdditionally, when prompted to adopt specific socio-demographic personas, LLMs may produce responses that reflect societal stereotypes. Be careful when designing persona prompts to avoid reinforcing harmful biases.\nReferences: - When “A Helpful Assistant” Is Not Really Helpful - Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs\n\n\n\n\n\n\n\n\nContext and Emotion Prompting\n\n\n\nContext can include: - Background information: Why the task is important, what led to this request - Audience information: Who the response is for (technical level, expertise, role) - Emotional cues: Research shows that including emotional cues (e.g., “This is very important to my career”) can enhance response quality - Constraints: Special circumstances, deadlines, limitations\nHowever, avoid overloading with unnecessary information that distracts from the main task.\nReference: Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Prompt Engineering for Research"
    ]
  },
  {
    "objectID": "m03-text/prompt-engineering.html#showing-rather-than-telling",
    "href": "m03-text/prompt-engineering.html#showing-rather-than-telling",
    "title": "Prompt Engineering",
    "section": "3 Showing Rather Than Telling",
    "text": "3 Showing Rather Than Telling\nInstead of describing what you want in words, show the model examples. This technique—called few-shot learning or in-context learning—exploits how LLMs compress patterns. When you provide examples, you’re not teaching the model new information; you’re activating pre-existing patterns by demonstrating the exact structure you want.\nThe spectrum ranges from zero-shot (no examples, relying solely on the model’s prior knowledge) to few-shot (typically two to five examples, the sweet spot for most tasks) to many-shot (ten or more examples, where diminishing returns and context limits become problematic). Consider a zero-shot prompt first:\n\nzero_shot_prompt = \"\"\"Extract the domain and methods from this abstract:\n\nAbstract: We apply reinforcement learning to optimize traffic flow in urban networks.\nUsing deep Q-networks trained on simulation data, we reduce average commute time by 15%.\n\nOutput format:\nDomain: ...\nMethods: ...\n\"\"\"\n\nNow add examples to activate more specific patterns:\n\nfew_shot_prompt = \"\"\"Extract the domain and methods from abstracts. Here are examples:\n\nExample 1:\nAbstract: We use CRISPR to edit genes in cancer cells, achieving 40% tumor reduction in mice.\nDomain: Cancer Biology\nMethods: CRISPR gene editing, mouse models\n\nExample 2:\nAbstract: We develop a transformer model for predicting solar flares from magnetogram images.\nDomain: Solar Physics, Machine Learning\nMethods: Transformer neural networks, image analysis\n\nNow extract from this abstract:\n\nAbstract: We apply reinforcement learning to optimize traffic flow in urban networks.\nUsing deep Q-networks trained on simulation data, we reduce average commute time by 15%.\n\nDomain: ...\nMethods: ...\n\"\"\"\n\n\n\nCode\nresponse_zero = ollama.generate(prompt=zero_shot_prompt, **params_llm)\nresponse_few = ollama.generate(prompt=few_shot_prompt, **params_llm)\n\nprint(\"ZERO-SHOT:\")\nprint(response_zero.response)\nprint(\"\\nFEW-SHOT:\")\nprint(response_few.response)\n\n\nZERO-SHOT:\nDomain: Urban networks\nMethods: Reinforcement Learning\n\nFEW-SHOT:\nHere's the extracted domain and methods from the abstract:\n\n*   **Domain:** Science\n*   **Methods:** Reinforcement Learning\n\n\n\nFew-shot prompting improves consistency because the examples demonstrate specificity level, edge case handling, and exact format. The model has seen countless abstract-extraction patterns, but your examples narrow the distribution to the specific pattern you want. This becomes critical when processing hundreds of abstracts—you need every output to match the same structure.\n\n\n\n\n\n\nBiases in Few-Shot Prompting\n\n\n\nBe aware that few-shot examples can introduce biases:\n\nRecency bias: Models may favor the most recent examples. The order of examples matters!\nMajority label bias: If most examples have the same label/answer, the model may favor that label even when it’s not appropriate.\n\nTo mitigate: Vary the order of examples when testing, ensure examples are diverse and representative, and don’t overload examples with one particular pattern.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Prompt Engineering for Research"
    ]
  },
  {
    "objectID": "m03-text/prompt-engineering.html#forcing-intermediate-steps",
    "href": "m03-text/prompt-engineering.html#forcing-intermediate-steps",
    "title": "Prompt Engineering",
    "section": "4 Forcing Intermediate Steps",
    "text": "4 Forcing Intermediate Steps\nFor complex tasks, asking for the final answer directly often produces shallow or incorrect results. The solution: ask the model to show its reasoning process before giving the final answer. This technique—called chain-of-thought prompting—activates patterns where intermediate reasoning steps preceded conclusions. Compare a direct prompt that asks for immediate answers:\n\npapers = \"\"\"\nPaper 1: Community detection in static networks using modularity optimization.\nPaper 2: Temporal network analysis with sliding windows.\nPaper 3: Hierarchical community structure in social networks.\n\"\"\"\n\ndirect_prompt = f\"\"\"Based on these paper titles, what research gap exists? Just give the answer, no explanation.\n\n{papers}\n\nGap: ...\n\"\"\"\n\nAgainst a chain-of-thought prompt that requests explicit reasoning steps:\n\ncot_prompt = f\"\"\"Based on these paper titles, identify a research gap. Think step by step.\n\nPapers:\n{papers}\n\nThink step by step:\n1. What does each paper focus on?\n2. What topics appear in multiple papers?\n3. What combination of topics is missing?\n4. What would be a valuable gap to fill?\n\nFinal answer: The research gap is...\n\"\"\"\n\n\n\nCode\nresponse_direct = ollama.generate(prompt=direct_prompt, **params_llm)\nresponse_cot = ollama.generate(prompt=cot_prompt, **params_llm)\n\nprint(\"DIRECT PROMPT:\")\nprint(response_direct.response)\nprint(\"\\nCHAIN-OF-THOUGHT:\")\nprint(response_cot.response)\n\n\nDIRECT PROMPT:\nGap: Community detection in static networks using modularity optimization.\n\nCHAIN-OF-THOUGHT:\nHere's the breakdown of the research gap identified:\n\n1.  **What does each paper focus on?**\n    *   Community detection in static networks using modularity optimization.\n    *   Temporal network analysis with sliding windows.\n    *   Hierarchical community structure in social networks.\n\n2.  **What topics appear in multiple papers?**\n    *   Community detection in static networks using modularity optimization.\n    *   Temporal network analysis with sliding windows.\n    *   Hierarchical community structure in social networks.\n\n3.  **What combination of topics is missing?**\n    *   Community detection in static networks using modularity optimization.\n    *   Temporal network analysis with sliding windows.\n    *   Hierarchical community structure in social networks.\n\n4.  **What would be a valuable gap to fill?**\n    *   A gap in the literature that addresses the limitations of modularity optimization for community detection in static networks.\n\n\n\nChain-of-thought produces more thoughtful, nuanced answers by forcing the model to decompose the problem into steps before committing to a conclusion. The mechanism is pattern matching: the training data contains many examples where “think step by step” preceded structured reasoning, so including that phrase activates those patterns. The model doesn’t actually reason—it generates text that looks like reasoning because that pattern correlates with higher-quality outputs in the training data.\nUse chain-of-thought when comparing multiple papers or concepts, identifying patterns, making recommendations, or analyzing arguments. Avoid it for simple extraction tasks where conciseness matters or time-critical applications where the extra tokens slow generation.\n\n\n\n\n\n\nCan We Trust Chain-of-Thought Reasoning?\n\n\n\nResearch indicates that chain-of-thought reasoning can be unfaithful—the explanations don’t always accurately reflect the model’s true decision-making process. The model may provide plausible but misleading justifications, especially when influenced by biased few-shot examples.\nAlways validate the final answer independently rather than trusting the reasoning process alone.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Prompt Engineering for Research"
    ]
  },
  {
    "objectID": "m03-text/prompt-engineering.html#constraining-format-for-structured-extraction",
    "href": "m03-text/prompt-engineering.html#constraining-format-for-structured-extraction",
    "title": "Prompt Engineering",
    "section": "5 Constraining Format for Structured Extraction",
    "text": "5 Constraining Format for Structured Extraction\nResearch workflows often require structured data you can parse programmatically, not freeform text. The solution: constrain output format explicitly. Consider a prompt that requests JSON output:\n\nimport json\nfrom pydantic import BaseModel\n\nabstract = \"\"\"\nWe analyze 10,000 scientific collaborations using network analysis and machine\nlearning. Our random forest classifier predicts collaboration success with 76%\naccuracy. Key factors include prior co-authorship and institutional proximity.\n\"\"\"\n\nprompt_json = f\"\"\"Extract information from this abstract and return ONLY valid JSON:\n\nAbstract: {abstract}\n\nReturn this exact structure:\n{{\n  \"n_samples\": &lt;number or null&gt;,\n  \"methods\": [&lt;list of methods&gt;],\n  \"accuracy\": &lt;number or null&gt;,\n  \"domain\": \"&lt;research field&gt;\"\n}}\n\nJSON:\"\"\"\n\n\n\nCode\n# Use lower temperature for structured output\nparams_structured = {\"model\": \"gemma3n:latest\", \"options\": {\"temperature\": 0}}\nresponse = ollama.generate(prompt=prompt_json, **params_structured)\n\ntry:\n    data = json.loads(response.response)\n    print(\"Extracted data:\")\n    print(json.dumps(data, indent=2))\nexcept json.JSONDecodeError:\n    print(\"Failed to parse JSON. Raw output:\")\n    print(response.response)\n\n\nFailed to parse JSON. Raw output:\n```json\n{\n \"n_samples\": 10000,\n \"methods\": [\"network analysis\", \"machine learning\", \"random forest\"],\n \"accuracy\": 76,\n \"domain\": \"scientific collaborations\"\n}\n```\n\n\nThis works by activating patterns where “return ONLY valid JSON” preceded JSON-formatted outputs. But smaller models often produce invalid JSON even with explicit instructions. For more reliability, use JSON schema constraints that enforce format during token generation—the model literally cannot generate tokens that violate the schema. Define the schema using Pydantic:\n\nfrom pydantic import BaseModel\n\nclass PaperMetadata(BaseModel):\n    domain: str\n    methods: list[str]\n    n_samples: int | None\n    accuracy: float | None\n\njson_schema = PaperMetadata.model_json_schema()\n\nThen pass the schema directly to the API, which constrains token generation:\n\nprompt_schema = f\"\"\"Extract information from this abstract:\n\nAbstract: {abstract}\"\"\"\n\n\n\nCode\nresponse = ollama.generate(prompt=prompt_schema, format=json_schema, **params_structured)\n\ntry:\n    data = json.loads(response.response)\n    metadata = PaperMetadata(**data)\n    print(\"Extracted and validated data:\")\n    print(json.dumps(data, indent=2))\nexcept (json.JSONDecodeError, ValueError) as e:\n    print(f\"Error: {e}\")\n    print(\"Raw output:\", response.response)\n\n\nExtracted and validated data:\n{\n  \"domain\": \"Scientific Collaborations\",\n  \"methods\": [\n    \"Network Analysis\",\n    \"Machine Learning\",\n    \"Random Forest Classifier\"\n  ],\n  \"n_samples\": 10000,\n  \"accuracy\": 76.0\n}\n\n\nJSON schema constraints are more reliable than prompt-based requests because they operate at the token level—the model cannot sample tokens that would create invalid JSON. The prompt activates extraction patterns; the schema enforces structure.\n\n\n\n\n\n\nJSON Parsing Reliability\n\n\n\nSmaller models (like Gemma 3N) sometimes produce invalid JSON even with schema constraints. Always wrap parsing in try-except blocks and validate outputs. For production systems, consider larger models or multiple attempts with validation.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Prompt Engineering for Research"
    ]
  },
  {
    "objectID": "m03-text/prompt-engineering.html#allowing-uncertainty-to-reduce-hallucination",
    "href": "m03-text/prompt-engineering.html#allowing-uncertainty-to-reduce-hallucination",
    "title": "Prompt Engineering",
    "section": "6 Allowing Uncertainty to Reduce Hallucination",
    "text": "6 Allowing Uncertainty to Reduce Hallucination\nLLMs confidently fabricate facts when they don’t know the answer because they optimize for fluency, not truth. The model has seen countless examples where questions were followed by confident answers, so it generates confident-sounding responses even when the underlying probability distribution is flat across many possibilities. The solution: explicitly give the model permission to admit ignorance. Compare a prompt that implicitly demands an answer:\n\nbad_prompt = \"\"\"Summarize the main findings from the 2023 paper by Johnson et al.\non quantum community detection in biological networks.\"\"\"\n\nAgainst a prompt that explicitly allows uncertainty:\n\ngood_prompt = \"\"\"I'm looking for a 2023 paper by Johnson et al. on quantum\ncommunity detection in biological networks.\n\nIf you know this paper, summarize its main findings.\nIf you're not certain this paper exists, say \"I cannot verify this paper exists\"\nand do NOT make up details.\n\nResponse:\"\"\"\n\n\n\nCode\nresponse_bad = ollama.generate(prompt=bad_prompt, **params_llm)\nresponse_good = ollama.generate(prompt=good_prompt, **params_llm)\n\nprint(\"BAD PROMPT (encourages hallucination):\")\nprint(response_bad.response)\nprint(\"\\nGOOD PROMPT (allows uncertainty):\")\nprint(response_good.response)\n\n\nBAD PROMPT (encourages hallucination):\nThe 2023 paper by Johnson et al. on quantum community detection in biological networks, published in *Nature*, presents a significant advancement in the field. Their research highlights the potential of quantum computing to enhance the detection of biological networks, particularly those with complex and dynamic structures.\n\nThe key findings include:\n\n*   **Enhanced Detection Capabilities:** They demonstrate that quantum algorithms can potentially improve the detection of biological networks by leveraging the inherent quantum properties of quantum systems. This includes the ability to identify subtle patterns and interactions that are difficult to detect with classical methods.\n*   **Improved Accuracy and Robustness:** The paper emphasizes the potential for quantum-enhanced detection to improve accuracy and robustness compared to classical methods. This is particularly relevant for networks with intricate structures and complex interactions.\n*   **Novel Algorithms:** They propose novel quantum algorithms that can be used to address the challenges of detecting biological networks. These algorithms could potentially be adapted to handle the unique characteristics of biological networks, such as complex interactions and dynamic networks.\n*   **Scalability and Adaptability:** The research suggests that quantum-enhanced detection techniques can be scaled up to handle larger and more complex biological networks. This is crucial for deploying these algorithms in real-world applications.\n\nIn essence, the paper demonstrates that quantum computing has the potential to significantly improve the detection of biological networks, offering new avenues for advancing our understanding of complex biological systems.\n\nGOOD PROMPT (allows uncertainty):\nI cannot verify this paper exists.\n\n\n\nThe good prompt activates patterns where explicit permission to admit ignorance preceded honest uncertainty statements. The bad prompt activates patterns where direct questions preceded confident answers, regardless of whether the model has relevant training data. Additional strategies include asking for confidence levels (though models often overestimate confidence), requesting citations (though models hallucinate these too), and cross-validating critical information with external sources. The fundamental issue remains: LLMs have no internal representation of what they “know” versus what they’re fabricating.\n\n\n\n\n\n\nBe a Good “Boss” to Your LLM\n\n\n\nLet LLMs admit ignorance: LLMs closely follow your instructions—even when they shouldn’t. They often attempt to answer beyond their actual capabilities. Explicitly tell your model: “If you don’t know the answer, just say so,” or “If you need more information, please ask.”\nEncourage critical feedback: LLMs are trained to be agreeable, which can hinder productive brainstorming or honest critique. Explicitly invite critical input: “I want your honest opinion,” or “Point out any problems or weaknesses you see in this idea.”\n\n\n\nSampling Multiple Times for Consistency\nFor tasks requiring reasoning, generating multiple responses and selecting the most common answer often improves accuracy. The technique—called self-consistency—exploits the fact that correct reasoning tends to converge on the same answer, while hallucinations vary randomly across samples. Define the prompt:\n\nfrom collections import Counter\n\nprompt_consistency = \"\"\"Three papers study network robustness:\n- Paper A: Targeted attacks are most damaging\n- Paper B: Random failures rarely cause collapse\n- Paper C: Hub nodes are critical for robustness\n\nWhat is the research consensus on network robustness? Give a one-sentence answer.\n\"\"\"\n\nGenerate multiple responses with higher temperature to increase diversity, then identify the most common answer:\n\n\nCode\n# Use higher temperature for diversity\nparams_creative = {\"model\": \"gemma3n:latest\", \"options\": {\"temperature\": 0.7}}\n\n# Generate 5 responses\nresponses = []\nfor i in range(5):\n    response = ollama.generate(prompt=prompt_consistency, **params_creative)\n    responses.append(response.response.strip())\n    print(f\"Response {i+1}: {responses[-1]}\\n\")\n\n# In practice, you'd programmatically identify the most common theme\nprint(\"The most consistent theme across responses would be selected.\")\n\n\nResponse 1: The research consensus on network robustness is that it's a complex issue influenced by both targeted attacks and random failures, with the vulnerability of critical nodes (hubs) playing a significant role in overall network stability.\n\nResponse 2: The research consensus on network robustness is that it's a complex issue influenced by various factors, including the impact of targeted attacks, the resilience to random failures, and the importance of critical nodes like hubs.\n\nResponse 3: The research consensus on network robustness is that it's a complex issue influenced by both targeted attacks and random failures, with the vulnerability of critical hub nodes playing a significant role in overall network stability.\n\nResponse 4: The research consensus on network robustness is that it's a complex issue influenced by various factors, including the specific type of failure (targeted vs. random), the importance of individual nodes (hubs), and the overall network structure, with no single factor dominating the understanding of resilience.\n\nResponse 5: The research consensus on network robustness is that it's a complex issue influenced by various factors, including the vulnerability of targeted attacks, the resilience to random failures, and the importance of critical nodes like hubs.\n\nThe most consistent theme across responses would be selected.\n\n\nSelf-consistency works because correct reasoning patterns converge toward the same conclusion when sampled multiple times, while fabricated details vary randomly. The tradeoff: generating five responses means five times the API calls, five times the cost, five times the latency. Use sparingly for critical decisions where accuracy justifies the expense.\n\n\n\n\n\n\nAlternative: Tree of Thought\n\n\n\nFor even more sophisticated exploration, you can use “Tree of Thought” prompting, where the model explicitly explores multiple reasoning paths, evaluates them, and selects the best one. This is more complex to implement but can yield better results for very difficult problems.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Prompt Engineering for Research"
    ]
  },
  {
    "objectID": "m03-text/prompt-engineering.html#the-takeaway",
    "href": "m03-text/prompt-engineering.html#the-takeaway",
    "title": "Prompt Engineering",
    "section": "8 The Takeaway",
    "text": "8 The Takeaway\nPrompt engineering is not magic—it’s deliberate activation of statistical patterns compressed during training. Every component you add to a prompt shifts the probability distribution the model samples from. Instructions activate task-specific patterns. Output formats activate structured-response patterns. Personas activate stylistic patterns. Context disambiguates when multiple patterns compete. Examples demonstrate exact structure. Chain-of-thought activates reasoning-like patterns. Format constraints enforce structure at the token level. Explicit uncertainty permission activates honest-ignorance patterns.\nNone of this requires the model to understand what you want. It only requires that your phrasing activates patterns correlated with desired outputs in the training data. You’re not communicating intent; you’re manipulating probability distributions. Master this, and you can reliably extract value from LLMs for research workflows—summarization, structured extraction, hypothesis generation, literature analysis.\nBut a question remains: how do these models represent text internally? When you send a prompt, the model doesn’t see English words—it sees numbers. Millions of numbers arranged in high-dimensional space. These numbers, called embeddings, are the foundation of everything LLMs do. Let’s unbox the first layer and see how meaning becomes mathematics.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Prompt Engineering for Research"
    ]
  },
  {
    "objectID": "m06-llms/prompt-tuning-exercise.html",
    "href": "m06-llms/prompt-tuning-exercise.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Spoiler\n\n\n\nLLMs are deterministic pattern matchers that cannot generate true randomness—but they can be prompted to generate sequences that statistically approximate random distributions by activating patterns where similar constraints preceded statistically valid outputs.\n\n\n\n\nLLMs optimize for fluency and pattern matching, not mathematical computation. When asked to generate random numbers, they typically produce sequences that fail statistical tests for randomness—numbers cluster, patterns emerge, distributions skew. The model has seen random numbers in training data, but it has no internal representation of what randomness means. It cannot compute true randomness; it can only recall patterns that look random.\nYet with careful prompting, you can bias the model toward outputs that pass statistical validation. The task: craft a prompt that makes an LLM generate at least 100 normally distributed random numbers (comma-separated, like 0.5,-1.2,0.8,...) that pass a Kolmogorov-Smirnov test with p-value greater than 0.20. Use Gemma3 27B on Google AI Studio or OpenRouter. No external tools—the model must generate the numbers directly from its internal patterns.\nThe constraint forces you to think about what patterns in the training data correlate with valid statistical outputs. Asking for “just the numbers” eliminates extraneous tokens that disrupt parsing. Requesting the model to “think about how normal distributions work” before generating may activate patterns where reasoning preceded statistically valid sequences. You’re not teaching the model statistics; you’re activating pre-existing patterns where statistical reasoning preceded appropriate outputs.\n\n\nimport marimo as mo\nimport altair as alt\nimport pandas as pd\nimport scipy.stats as stats\nimport numpy as np\ntext_area = mo.ui.text_area(placeholder = \"Enter numbers separated by commas\", value = \"1,2,3,4,5,6,7,8,9,10\")\nbutton = mo.ui.button(\"Runt test\")\n\nmo.vstack([text_area, button])\ntry:\n    numbers = np.array([float(num.strip()) for num in text_area.value.split(\",\")])\n    if len(numbers) &gt;= 100:\n        # KS test\n        pval = stats.kstest(numbers, stats.norm(loc=0.0, scale=1.0).cdf)[1]\n        test_result = \"The numbers are normal distributed (p-value = {:.2f})\".format(pval) if pval &gt; 0.20 else \"The numbers are not normal distributed (p-value = {:.2f})\".format(pval)\n        message = mo.callout(test_result, kind = \"success\" if pval &gt; 0.20 else \"danger\")\n    else:\n        message = mo.callout(\"The number of samples is too small. Need at least 100 samples.\", kind = \"warn\")\n\n    # Convert the numbers to a DataFrame for Altair\n    df = pd.DataFrame({'value': numbers})\n\n    # Create an Altair histogram\n    fig = alt.Chart(df).mark_bar().encode(\n        x=alt.X('value:Q', bin=alt.Bin(maxbins=30)),\n        y='count()'\n    ).properties(\n        title='Histogram of Values'\n    )\n\n    mo.hstack([fig, message])\nexcept:\n    message = mo.callout(\"Parse failed. Please check if your input follows the specified format.\", kind = \"danger\")\n    fig = None\n\nmo.hstack([fig, message]) if fig is not None else message\n\n\n\n\n\nLLMs can generate syntactically valid code in languages they’ve seen during training—including SVG, the XML-based vector graphics format. The challenge: prompt Gemma3 27B to generate an SVG diagram of a neural network with specific structural requirements. The network must have same-colored neurons within each layer, connections between all neurons across adjacent layers, and labels for “Input layer,” “Hidden layer,” and “Output layer.”\nTest your prompt on Google AI Studio or OpenRouter, then paste the generated SVG code into SVG Viewer to visualize the result. The model has seen countless SVG examples during training, but it has no internal representation of what a neural network diagram “should” look like. It can only pattern match against examples where similar prompts preceded valid SVG structures.\n\n\n\n\n\n\nThese exercises expose the boundary between pattern matching and computation. LLMs cannot perform true mathematical operations or generate genuine randomness—they can only recall patterns that resemble these capabilities. Success requires understanding what patterns in training data correlate with desired outputs, then crafting prompts that activate those patterns. You’re not teaching the model to compute; you’re navigating its compressed representation of how computation appears in text. The constraint is the teacher: when the model fails, the failure reveals what patterns are missing or weak in its training data."
  },
  {
    "objectID": "m06-llms/prompt-tuning-exercise.html#the-challenge",
    "href": "m06-llms/prompt-tuning-exercise.html#the-challenge",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "LLMs optimize for fluency and pattern matching, not mathematical computation. When asked to generate random numbers, they typically produce sequences that fail statistical tests for randomness—numbers cluster, patterns emerge, distributions skew. The model has seen random numbers in training data, but it has no internal representation of what randomness means. It cannot compute true randomness; it can only recall patterns that look random.\nYet with careful prompting, you can bias the model toward outputs that pass statistical validation. The task: craft a prompt that makes an LLM generate at least 100 normally distributed random numbers (comma-separated, like 0.5,-1.2,0.8,...) that pass a Kolmogorov-Smirnov test with p-value greater than 0.20. Use Gemma3 27B on Google AI Studio or OpenRouter. No external tools—the model must generate the numbers directly from its internal patterns.\nThe constraint forces you to think about what patterns in the training data correlate with valid statistical outputs. Asking for “just the numbers” eliminates extraneous tokens that disrupt parsing. Requesting the model to “think about how normal distributions work” before generating may activate patterns where reasoning preceded statistically valid sequences. You’re not teaching the model statistics; you’re activating pre-existing patterns where statistical reasoning preceded appropriate outputs.\n\n\nimport marimo as mo\nimport altair as alt\nimport pandas as pd\nimport scipy.stats as stats\nimport numpy as np\ntext_area = mo.ui.text_area(placeholder = \"Enter numbers separated by commas\", value = \"1,2,3,4,5,6,7,8,9,10\")\nbutton = mo.ui.button(\"Runt test\")\n\nmo.vstack([text_area, button])\ntry:\n    numbers = np.array([float(num.strip()) for num in text_area.value.split(\",\")])\n    if len(numbers) &gt;= 100:\n        # KS test\n        pval = stats.kstest(numbers, stats.norm(loc=0.0, scale=1.0).cdf)[1]\n        test_result = \"The numbers are normal distributed (p-value = {:.2f})\".format(pval) if pval &gt; 0.20 else \"The numbers are not normal distributed (p-value = {:.2f})\".format(pval)\n        message = mo.callout(test_result, kind = \"success\" if pval &gt; 0.20 else \"danger\")\n    else:\n        message = mo.callout(\"The number of samples is too small. Need at least 100 samples.\", kind = \"warn\")\n\n    # Convert the numbers to a DataFrame for Altair\n    df = pd.DataFrame({'value': numbers})\n\n    # Create an Altair histogram\n    fig = alt.Chart(df).mark_bar().encode(\n        x=alt.X('value:Q', bin=alt.Bin(maxbins=30)),\n        y='count()'\n    ).properties(\n        title='Histogram of Values'\n    )\n\n    mo.hstack([fig, message])\nexcept:\n    message = mo.callout(\"Parse failed. Please check if your input follows the specified format.\", kind = \"danger\")\n    fig = None\n\nmo.hstack([fig, message]) if fig is not None else message"
  },
  {
    "objectID": "m06-llms/prompt-tuning-exercise.html#generating-structured-visual-code",
    "href": "m06-llms/prompt-tuning-exercise.html#generating-structured-visual-code",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "LLMs can generate syntactically valid code in languages they’ve seen during training—including SVG, the XML-based vector graphics format. The challenge: prompt Gemma3 27B to generate an SVG diagram of a neural network with specific structural requirements. The network must have same-colored neurons within each layer, connections between all neurons across adjacent layers, and labels for “Input layer,” “Hidden layer,” and “Output layer.”\nTest your prompt on Google AI Studio or OpenRouter, then paste the generated SVG code into SVG Viewer to visualize the result. The model has seen countless SVG examples during training, but it has no internal representation of what a neural network diagram “should” look like. It can only pattern match against examples where similar prompts preceded valid SVG structures."
  },
  {
    "objectID": "m06-llms/prompt-tuning-exercise.html#the-takeaway",
    "href": "m06-llms/prompt-tuning-exercise.html#the-takeaway",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "These exercises expose the boundary between pattern matching and computation. LLMs cannot perform true mathematical operations or generate genuine randomness—they can only recall patterns that resemble these capabilities. Success requires understanding what patterns in training data correlate with desired outputs, then crafting prompts that activate those patterns. You’re not teaching the model to compute; you’re navigating its compressed representation of how computation appears in text. The constraint is the teacher: when the model fails, the failure reveals what patterns are missing or weak in its training data."
  },
  {
    "objectID": "m03-text/tokenization.html#the-mechanism-why-subwords-not-words",
    "href": "m03-text/tokenization.html#the-mechanism-why-subwords-not-words",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "1 The Mechanism (Why Subwords, Not Words)",
    "text": "1 The Mechanism (Why Subwords, Not Words)\nYou might assume that an LLM reads text the way you do: word by word, with each word treated as an atomic unit. This is wrong. The model operates on tokens—subword chunks that could be full words (“the”), word parts (“ingham”), or single characters (“B”). This choice is not arbitrary; it’s a geometric compression strategy.\nIf we used whole words, the vocabulary would balloon to millions of entries. Each entry requires a row in the embedding matrix, meaning memory scales linearly with vocabulary size. A 1-million-word vocabulary with 2048-dimensional embeddings would require over 8GB just for the lookup table. Subword tokenization collapses this problem by focusing on frequently occurring fragments. With roughly 50,000 subwords, the model can reconstruct both common words (stored as single tokens) and rare words (assembled from parts). The system trades a small increase in sequence length for a massive reduction in memory and computational overhead.\nThis also explains why LLMs sometimes fail on seemingly trivial tasks like counting letters. The word “strawberry” might tokenize as [\"straw\", \"berry\"], meaning the model never sees the individual “r” characters as separate units. It’s not stupidity—it’s compression artifacts.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m03-text/tokenization.html#the-application-how-tokenization-works-in-practice",
    "href": "m03-text/tokenization.html#the-application-how-tokenization-works-in-practice",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "2 The Application (How Tokenization Works in Practice)",
    "text": "2 The Application (How Tokenization Works in Practice)\nLet’s unbox an actual tokenizer from Hugging Face and trace the pipeline from raw text to embeddings. We’ll use Phi-1.5, a compact model from Microsoft. For tokenization experiments, we only need the tokenizer—no need to load the full multi-gigabyte model.\n\nfrom transformers import AutoTokenizer\nimport os\n\nmodel_name = \"microsoft/phi-1.5\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nLet’s inspect the tokenizer’s constraints.\n\n\nCode\nprint(f\"Vocabulary size: {tokenizer.vocab_size:,} tokens\")\nprint(f\"Max sequence length: {tokenizer.model_max_length} tokens\")\n\n\nVocabulary size: 50,257 tokens\nMax sequence length: 2048 tokens\n\n\nThis tokenizer knows 50,257 unique tokens and enforces a maximum sequence length of 2048 tokens. If your input exceeds this limit, the model will truncate or reject it. This is a hard boundary imposed by the positional encoding system, not a soft guideline.\n\nText to Tokens\nTokenization splits text into the subword fragments the model actually processes. Watch what happens when we tokenize a university name.\n\ntext = \"Binghamton University.\"\n\ntokens = tokenizer.tokenize(text)\n\n\n\nCode\nprint(f\"Tokens: {tokens}\")\n\n\nTokens: ['B', 'ingham', 'ton', 'ĠUniversity', '.']\n\n\nThe rare word “Binghamton” fractures into ['B', 'ingham', 'ton']. The common word “University” survives intact (with a leading space marker). The tokenizer learned these splits from frequency statistics during training. High-frequency words get dedicated tokens; rare words get decomposed into reusable parts.\n\n\nThe Ġ character (U+0120) is a GPT-style tokenizer convention for encoding spaces. When you see ĠUniversity, it means “University” preceded by a space. This preserves word boundaries while allowing subword splits.\nLet’s test a few more examples to see the pattern.\n\n\nCode\ntexts = [\n    \"Bearcats\",\n    \"New York\",\n]\n\nprint(\"Word tokenization examples:\\n\")\nfor text in texts:\n    tokens = tokenizer.tokenize(text)\n    print(f\"{text:10s} → {tokens}\")\n\n\nWord tokenization examples:\n\nBearcats   → ['Bear', 'cats']\nNew York   → ['New', 'ĠYork']\n\n\n“Bearcats” splits because it’s domain-specific jargon. “New York” remains whole because it’s common. The tokenizer’s behavior is a direct reflection of its training corpus.\n\n\nCheck out OpenAI’s tokenizer to see how different models slice the same text differently.\n\n\nTokens to Token IDs\nTokens are still strings. The model needs integers. Each token maps to a unique ID in the vocabulary dictionary.\n\n\nCode\ntext = \"Binghamton University\"\n\n# Get token IDs\ntoken_ids = tokenizer.encode(text, add_special_tokens=False)\ntokens = tokenizer.tokenize(text)\n\nprint(\"Token → Token ID mapping:\\n\")\nfor token, token_id in zip(tokens, token_ids):\n    print(f\"{token:10s} → {token_id:6d}\")\n\n\nToken → Token ID mapping:\n\nB          →     33\ningham     →  25875\nton        →   1122\nĠUniversity →   2059\n\n\nEach token receives a unique integer ID. The vocabulary is a dictionary: {token_string: integer_id}. Let’s peek inside.\n\n# Get the full vocabulary\nvocab = tokenizer.get_vocab()\n\n# Sample some tokens\nsample_tokens = list(vocab.items())[:5]\nfor token, id in sample_tokens:\n    print(f\"  {id:6d}: '{token}'\")\n\n   38472: '468'\n   12070: 'Ġpressed'\n   25920: 'Ġmaid'\n   36392: 'VT'\n    2936: 'Ġfelt'\n\n\nMost LLMs reserve special tokens for sequence boundaries or control signals. Phi-1.5 uses &lt;|endoftext|&gt; as a separator during training. Let’s verify.\n\ntoken_id = [50256]\ntoken = tokenizer.convert_ids_to_tokens(token_id)[0]\nprint(f\"Token ID: {token_id} → Token: {token}\")\n\nToken ID: [50256] → Token: &lt;|endoftext|&gt;\n\n\nToken ID 50256 is Phi-specific. Other models use different conventions (e.g., BERT uses [SEP] and [CLS]). Always check your tokenizer’s special tokens before preprocessing data.\n\n\nToken IDs to Embeddings\n\nNow we need the full model to access the embedding layer—the matrix that converts token IDs into dense vectors.\n\nfrom transformers import AutoModelForCausalLM\nimport torch\n\n# Load the model\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Retrieve the embedding layer\nembedding_layer = model.model.embed_tokens\n\nThe embedding layer is a simple lookup table: a 51,200 × 2,048 matrix where each row is the embedding for a token in the vocabulary. Let’s examine the first few entries.\n\n\nCode\nprint(embedding_layer.weight[:5, :10])\n\n\ntensor([[ 0.0097, -0.0155,  0.0603,  0.0326, -0.0108, -0.0271, -0.0273,  0.0178,\n         -0.0242,  0.0100],\n        [ 0.0243,  0.0543,  0.0178, -0.0679, -0.0130,  0.0265, -0.0423, -0.0287,\n         -0.0051, -0.0179],\n        [-0.0416,  0.0370, -0.0160, -0.0254, -0.0371, -0.0208, -0.0023,  0.0647,\n          0.0468,  0.0013],\n        [-0.0051, -0.0044,  0.0248, -0.0489,  0.0399,  0.0005, -0.0070,  0.0148,\n          0.0030,  0.0070],\n        [ 0.0289,  0.0362, -0.0027, -0.0775, -0.0136, -0.0203, -0.0566, -0.0558,\n          0.0269, -0.0067]], grad_fn=&lt;SliceBackward0&gt;)\n\n\nThese numbers are learned parameters, optimized during training to capture semantic relationships. Token IDs are discrete symbols; embeddings are continuous coordinates in a 2048-dimensional space. This is what the transformer layers operate on.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m06-llms/transformers.html#the-mechanism",
    "href": "m06-llms/transformers.html#the-mechanism",
    "title": "Transformers",
    "section": "",
    "text": "You’ve been taught to think of language models as sequential processors—reading left to right, one word triggering the next, like dominoes falling. This intuition comes from recurrent neural networks (RNNs), where information flows step by step, each word depending on the hidden state from the previous word. The transformer architecture throws this away entirely.\nInstead of sequential processing, transformers operate through parallel relationship mapping. When you read “The cat sat on the mat because it was tired,” you don’t actually process word-by-word in isolation. Your brain simultaneously evaluates which words relate to which—“it” connects to “cat,” “tired” explains “sat,” “mat” anchors “on.” Transformers formalize this intuition mathematically. Every position in the input sequence simultaneously computes its relationship to every other position. The mechanism is attention, and the result is a system where context flows in all directions at once, not just forward through time.\nThis parallelism is why transformers scaled when RNNs didn’t. Recurrent architectures impose sequential computation—you can’t process word 100 until you’ve processed word 99. Transformers eliminate this bottleneck. Every position can be computed in parallel, which means training time scales with sequence complexity, not sequence length. This architectural shift is what enabled GPT-3, GPT-4, and Claude to exist."
  },
  {
    "objectID": "m06-llms/transformers.html#the-architecture",
    "href": "m06-llms/transformers.html#the-architecture",
    "title": "Transformers",
    "section": "",
    "text": "Modern LLMs stack multiple transformer blocks—modular units that take a sequence of token vectors as input and output a transformed sequence of the same length. GPT-3 uses 96 of these blocks; GPT-4 likely uses more. Each block refines the representation, adding layers of contextual understanding.\n```doufuilu ../figs/transformer-overview.jpg :name: transformer-overview :alt: Transformer Overview :width: 50% :align: center\nThe basic architecture of the transformer-based LLMs.\n\nThese blocks come in two forms: **encoders** and **decoders**. The encoder processes the input sequence and builds a contextualized representation. The decoder generates the output sequence, attending to both its own previous outputs and the encoder's representation. For translation tasks (\"I love you\" → \"Je t'aime\"), the encoder processes English, the decoder generates French. For language modeling (GPT-style systems), only the decoder is used—it generates text autoregressively, predicting the next token based on all previous tokens.\n\n```{figure} ../figs/transformer-encoder-decoder.jpg\n:name: transformer-encoder-decoder\n:alt: Transformer Encoder-Decoder\n:width: 80%\n:align: center\n\nThe encoder-decoder architecture. The encoder builds a representation of the input sequence; the decoder generates the output sequence while attending to the encoder's output.\nInside each block are three core components: multi-head attention (the relationship mapper), layer normalization (numerical stabilization), and feed-forward networks (nonlinear transformation). We’ll build these components step by step.\n```doufuilu ../figs/transformer-component.jpg :name: transformer-wired-components :alt: Transformer Wired Components :width: 80% :align: center\nInternal structure of encoder and decoder blocks.\n\n## Attention: The Relationship Engine\n\n**Self-attention**—the core of the transformer—computes how much each position in a sequence should \"attend to\" every other position. Unlike earlier attention mechanisms in seq2seq models, which attended from one sentence to another, self-attention operates within a single sequence. It answers the question: \"Given this word, which other words matter most?\"\n\n```{figure} ../figs/transformer-attention.jpg\n:name: transformer-attention\n:alt: Attention Mechanism\n:width: 80%\n:align: center\n\nThe attention mechanism computes relationships between all positions simultaneously.\nFor each word, the attention mechanism creates three vectors: query (Q), key (K), and value (V). Think of these as a library search: the query is what you’re looking for, the keys are book titles, and the values are the actual content. When you search for “machine learning” (your query), you match it against book titles (keys) to find relevant content (values).\nMathematically, each of these vectors is created by a learned linear transformation of the input word embedding. Given an input embedding x, we compute:\n\nQ = x W_Q, \\quad K = x W_K, \\quad V = x W_V\n\nwhere W_Q, W_K, and W_V are learned weight matrices. The attention mechanism then computes which keys are most relevant to each query using the dot product, which measures vector similarity. The dot product QK^T produces a matrix of attention scores—large values indicate strong relationships, small values indicate weak ones.\nThese raw scores are scaled by \\sqrt{d_k} (the square root of the key dimension) to prevent extreme values, then normalized using softmax to produce a probability distribution. Finally, these normalized attention weights are used to compute a weighted sum of the value vectors. The complete operation is:\n\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\nwhere Q \\in \\mathbb{R}^{n \\times d_k}, K \\in \\mathbb{R}^{n \\times d_k}, and V \\in \\mathbb{R}^{n \\times d_v} represent matrices containing n query, key, and value vectors respectively.\nThe interactive visualization below demonstrates how learned Query and Key transformations produce different attention patterns. Adjust the transformation parameters to see how different W_Q and W_K matrices change which words attend to which:\n\n\npython {marimo} import marimo as mo import numpy as np import pandas as pd import altair as alt\n```python {marimo} attention_words = [“bank”, “money”, “loan”, “river”, “shore”] attention_embeddings = np.array([ [0.0, 0.0], # bank (center) [-0.8, -0.3], # money [-0.7, -0.6], # loan [0.7, -0.5], # river [0.6, -0.7], # shore]) * 2\n\n\nq_scale_x = mo.ui.slider(-2, 2, 0.1, value=1.0, label=“Q Scale X”) q_scale_y = mo.ui.slider(-2, 2, 0.1, value=1.0, label=“Q Scale Y”) q_rotate = mo.ui.slider(-180, 180, 5, value=0, label=“Q Rotate (deg)”)\n\n\n\nk_scale_x = mo.ui.slider(-2, 2, 0.1, value=1.0, label=“K Scale X”) k_scale_y = mo.ui.slider(-2, 2, 0.1, value=1.0, label=“K Scale Y”) k_rotate = mo.ui.slider(-180, 180, 5, value=0, label=“K Rotate (deg)”)\nq_controls = mo.vstack([mo.md(“Query Transformation”), q_scale_x, q_scale_y, q_rotate]) k_controls = mo.vstack([mo.md(“Key Transformation”), k_scale_x, k_scale_y, k_rotate])\n\n```python {marimo}\ndef _transform_embeddings(emb, scale_x, scale_y, rotate_deg):\n    theta = np.radians(rotate_deg)\n    rot_matrix = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n    scale_matrix = np.diag([scale_x, scale_y])\n    W = rot_matrix @ scale_matrix\n    return emb @ W.T\n\nQ = _transform_embeddings(attention_embeddings, q_scale_x.value, q_scale_y.value, q_rotate.value)\nK = _transform_embeddings(attention_embeddings, k_scale_x.value, k_scale_y.value, k_rotate.value)\n\n# Compute attention scores\n_scores = Q @ K.T\n_exp_scores = np.exp(_scores - np.max(_scores, axis=1, keepdims=True))\nattention_weights = _exp_scores / np.sum(_exp_scores, axis=1, keepdims=True)\n\n# Create visualizations\n_df_q = pd.DataFrame({\"word\": attention_words, \"x\": Q[:, 0], \"y\": Q[:, 1]})\n_df_k = pd.DataFrame({\"word\": attention_words, \"x\": K[:, 0], \"y\": K[:, 1]})\n\n_chart_q = alt.Chart(_df_q).mark_circle(size=100).encode(\n    x=alt.X('x:Q', scale=alt.Scale(domain=[-4, 4]), title='Q1'),\n    y=alt.Y('y:Q', scale=alt.Scale(domain=[-4, 4]), title='Q2'),\n    tooltip=['word:N']\n).properties(width=200, height=200, title=\"Query (Q)\")\n_text_q = _chart_q.mark_text(dy=-12, fontSize=10, fontWeight='bold').encode(text='word:N')\n\n_chart_k = alt.Chart(_df_k).mark_circle(size=100).encode(\n    x=alt.X('x:Q', scale=alt.Scale(domain=[-4, 4]), title='K1'),\n    y=alt.Y('y:Q', scale=alt.Scale(domain=[-4, 4]), title='K2'),\n    tooltip=['word:N']\n).properties(width=200, height=200, title=\"Key (K)\")\n_text_k = _chart_k.mark_text(dy=-12, fontSize=10, fontWeight='bold').encode(text='word:N')\n\n# Heatmap\n_heatmap_data = []\nfor i, word_i in enumerate(attention_words):\n    for j, word_j in enumerate(attention_words):\n        _heatmap_data.append({\"Query\": word_i, \"Key\": word_j, \"Weight\": attention_weights[i, j]})\n_df_heatmap = pd.DataFrame(_heatmap_data)\n\n_heatmap = alt.Chart(_df_heatmap).mark_rect().encode(\n    x=alt.X('Key:N', title='Key Word'),\n    y=alt.Y('Query:N', title='Query Word'),\n    color=alt.Color('Weight:Q', scale=alt.Scale(scheme='blues'), title='Attention'),\n    tooltip=['Query:N', 'Key:N', alt.Tooltip('Weight:Q', format='.3f')]\n).properties(width=250, height=250, title=\"Attention Weights (Softmax)\")\n\nmo.vstack([\n    mo.hstack([q_controls, k_controls], align=\"center\"),\n    mo.hstack([_chart_q + _text_q, _chart_k + _text_k, _heatmap], align=\"center\")\n])\n\n\n\nThe output is a contextualized vector for each word—a representation that changes based on surrounding context. The word “bank” produces different vectors in “river bank” versus “financial bank” because the attention mechanism incorporates information from neighboring words.\nTo see this in action, consider how we might contextualize the word “bank” by mixing it with surrounding words. The visualization below shows static word embeddings—notice how “bank” sits neutrally between financial terms (money, loan) and geographical terms (river, shore).\n\n\n```python {marimo} static_words = [“bank”, “money”, “loan”, “river”, “shore”] static_embeddings = np.array([ [0.0, 0.0], # bank (center) [-0.8, -0.3], # money [-0.7, -0.6], # loan [0.7, -0.5], # river [0.6, -0.7], # shore]) * 2\n_df_static = pd.DataFrame({“word”: static_words, “x”: static_embeddings[:, 0], “y”: static_embeddings[:, 1]})\n_chart_static = alt.Chart(_df_static).mark_circle(size=200).encode( x=alt.X(‘x:Q’, scale=alt.Scale(domain=[-2, 2]), title=‘Dimension 1’), y=alt.Y(‘y:Q’, scale=alt.Scale(domain=[-2, 2]), title=‘Dimension 2’), text=‘word:N’, tooltip=[‘word:N’, ‘x:Q’, ‘y:Q’] ).properties(width=300, height=300, title=“Static Word Embeddings”)\n_text_static = _chart_static.mark_text(dy=-15, fontSize=14, fontWeight=‘bold’).encode(text=‘word:N’)\n_chart_static + _text_static\n\n&lt;/marimo-iframe&gt;\n&lt;/div&gt;\n\nNow, try adjusting the weights below to create a contextualized version of \"bank.\" If the sentence is \"Money in bank,\" adjust the weights to shift \"bank\" toward \"money.\" If the sentence is \"River bank,\" shift it toward \"river.\"\n\n&lt;div&gt;\n&lt;marimo-iframe data-height=\"500px\" data-show-code=\"false\"&gt;\n\n```python {marimo}\ncontext_words = [\"bank\", \"money\", \"loan\", \"river\", \"shore\"]\ncontext_embeddings = np.array([\n    [0.0, 0.0],  # bank (center)\n    [-0.8, -0.3],  # money\n    [-0.7, -0.6],  # loan\n    [0.7, -0.5],  # river\n    [0.6, -0.7],  # shore\n]) * 2\n\nslider_bank = mo.ui.slider(0, 1, 0.01, value=1.0, label=\"Bank Weight\")\nslider_money = mo.ui.slider(0, 1, 0.01, value=0, label=\"Money Weight\")\nslider_loan = mo.ui.slider(0, 1, 0.01, value=0, label=\"Loan Weight\")\nslider_river = mo.ui.slider(0, 1, 0.01, value=0, label=\"River Weight\")\nslider_shore = mo.ui.slider(0, 1, 0.01, value=0, label=\"Shore Weight\")\n\ncontext_sliders = mo.vstack([slider_bank, slider_money, slider_loan, slider_river, slider_shore])\n```python {marimo} _weights = np.array([slider_bank.value, slider_money.value, slider_loan.value, slider_river.value, slider_shore.value]) _total = _weights.sum() if _total &gt; 0: _weights = _weights / _total _new_vec = context_embeddings.T @ _weights else: _new_vec = np.zeros(2)\n_df_orig = pd.DataFrame({“word”: context_words, “x”: context_embeddings[:, 0], “y”: context_embeddings[:, 1], “type”: [“Original”] * 5}) _df_new = pd.DataFrame({“word”: [“Contextualized Bank”], “x”: [_new_vec[0]], “y”: [_new_vec[1]], “type”: [“Contextualized”]}) _df_combined = pd.concat([_df_orig, _df_new])\n_chart_context = alt.Chart(_df_combined).mark_circle(size=200).encode( x=alt.X(‘x:Q’, scale=alt.Scale(domain=[-2, 2]), title=‘Dimension 1’), y=alt.Y(‘y:Q’, scale=alt.Scale(domain=[-2, 2]), title=‘Dimension 2’), color=alt.Color(‘type:N’, scale=alt.Scale(domain=[‘Original’, ‘Contextualized’], range=[‘#dadada’, ‘#ff7f0e’])), tooltip=[‘word:N’, ‘x:Q’, ‘y:Q’] ).properties(width=350, height=350, title=“Contextualized Bank”)\n_text_context = _chart_context.mark_text(dy=-15, fontSize=14, fontWeight=‘bold’).encode(text=‘word:N’, color=alt.value(‘black’))\nmo.hstack([context_sliders, _chart_context + _text_context], align=“center”)\n\n&lt;/marimo-iframe&gt;\n&lt;/div&gt;\n\nThis manual weighting captures the intuition, but how do we learn which words to attend to? This is where queries and keys come in.\n\n### Multi-Head Attention: Multiple Perspectives\n\nA single attention mechanism captures one type of relationship. **Multi-head attention** runs multiple attention operations in parallel, each with different learned parameters. Each head can specialize—one might focus on syntactic dependencies (subject-verb relationships), another on semantic similarity (synonyms and antonyms), another on positional proximity (nearby words).\n\n```{figure} ../figs/transformer-multihead-attention.jpg\n:name: transformer-multihead-attention\n:alt: Multi-Head Attention\n:width: 50%\n:align: center\n\nMulti-head attention runs multiple attention operations in parallel, each capturing different relationships.\nThe outputs from all heads are concatenated and passed through a final linear transformation to produce the multi-head attention output. In the original transformer paper {footcite:p}vaswani2017attention, the authors used h=8 attention heads, with each head using dimension d_k = d_v = d/h = 64, where d=512 is the model dimension.\n\n\nDeep networks suffer from numerical instability—activations can grow explosively large or vanish to zero as they propagate through layers. Layer normalization stabilizes training by rescaling activations to have zero mean and unit variance.\n```doufuilu https://miro.medium.com/v2/resize:fit:1400/0*Agdt1zYwfUxXMJGJ :name: transformer-layer-normalization :alt: Layer Normalization :width: 80% :align: center\nLayer normalization computes mean and standard deviation across all features for each sample, then normalizes.\n\nFor each input vector $x$, layer normalization computes:\n\n$$\n\\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sigma} + \\beta\n$$\n\nwhere $\\mu$ and $\\sigma$ are the mean and standard deviation of $x$, and $\\gamma$ and $\\beta$ are learned scaling and shifting parameters (initialized to 1 and 0 respectively). This ensures that no matter how the input distribution shifts during training, each layer receives inputs in a stable numerical range.\n\n## The Encoder Block\n\nNow we wire the components together. The **encoder block** processes the input sequence through four stages:\n\n1. **Multi-head self-attention** computes contextualized representations\n2. **Residual connection + normalization** stabilizes training\n3. **Feed-forward network** applies nonlinear transformation\n4. **Residual connection + normalization** again\n\n```{figure} ../figs/transformer-encoder.jpg\n:name: transformer-block\n:alt: Transformer Block\n:width: 50%\n:align: center\n\nInformation flows through multi-head attention, normalization, feed-forward networks, and final normalization.\nThe feed-forward network is a simple two-layer MLP applied independently to each position:\n\n\\text{FFN}(x) = \\text{ReLU}(x W_1 + b_1) W_2 + b_2\n\nThe residual connections (also called skip connections) are critical for training deep networks. Instead of learning a direct mapping f(x), we learn the residual:\n\nx_{\\text{out}} = x_{\\text{in}} + f(x_{\\text{in}})\n\nThis simple addition has profound consequences for gradient flow. During backpropagation, the gradient of the loss \\mathcal{L} with respect to layer l is:\n\n\\frac{\\partial \\mathcal{L}}{\\partial x_l} = \\frac{\\partial \\mathcal{L}}{\\partial x_{l+1}} \\left(1 + \\frac{\\partial f_l}{\\partial x_l}\\right)\n\nNotice the “+1” term—this provides a direct gradient path from the output back to the input. Without residual connections, gradients must pass through the chain:\n\n\\frac{\\partial f_L}{\\partial f_{L-1}} \\cdot \\frac{\\partial f_{L-1}}{\\partial f_{L-2}} \\cdot \\ldots \\cdot \\frac{\\partial f_1}{\\partial x}\n\nIf any term is less than 1, the gradient shrinks exponentially—this is the vanishing gradient problem. With residual connections, the gradient expansion becomes:\n\n1 + O_1 + O_2 + O_3 + \\ldots\n\nwhere O_1 contains first-order terms, O_2 contains second-order products, etc. The constant “1” ensures gradients can flow even when the learned components f_i produce small derivatives. This architectural innovation, originally developed for computer vision {footcite:p}he2015deep, is what allows transformers to scale to hundreds of layers.\n\n\n\nThe decoder block extends the encoder with two modifications: masked self-attention and cross-attention.\n```doufuilu ../figs/transformer-decoder.jpg :name: transformer-decoder :alt: Transformer Decoder :width: 50% :align: center\nThe decoder adds masked self-attention (to prevent future peeking) and cross-attention (to access encoder outputs).\n\n### Masked Self-Attention: Preventing Future Leakage\n\nDuring training, we know the entire target sequence. For translation (\"I love you\" → \"Je t'aime\"), we have both input and output. A naive decoder could \"cheat\" by looking at future words in the target sequence. Masked self-attention prevents this by zeroing out attention to future positions.\n\nThe mask is implemented by setting attention scores to $-\\infty$ before the softmax:\n\n$$\n\\text{MaskedAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T + M}{\\sqrt{d_k}}\\right)V\n$$\n\nwhere $M$ is a matrix with $-\\infty$ at positions $(i,j)$ where $j &gt; i$ (future positions) and 0 elsewhere. After softmax, these $-\\infty$ values become zero, eliminating information flow from future tokens.\n\n```{figure} https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png\n:name: transformer-masked-attention\n:alt: Masked Attention\n:width: 80%\n:align: center\n\nMasked attention zeros out future positions, allowing parallel training without information leakage.\nThis enables parallel training. Instead of generating “Je”, then “t’aime”, then the final token sequentially, we can train all positions simultaneously—each with access only to its causal past. During inference, masking happens naturally because future tokens don’t exist yet.\n\n\nThe second attention layer in the decoder uses cross-attention to access the encoder’s output. The queries (Q) come from the decoder’s previous layer, while the keys (K) and values (V) come from the encoder’s output:\n\n\\text{CrossAttention}(Q_{\\text{decoder}}, K_{\\text{encoder}}, V_{\\text{encoder}}) = \\text{softmax}\\left(\\frac{Q_{\\text{decoder}}K_{\\text{encoder}}^T}{\\sqrt{d_k}}\\right)V_{\\text{encoder}}\n\n```doufuilu ../figs/transformer-cross-attention.jpg :name: transformer-cross-attention :alt: Cross-Attention :width: 60% :align: center\nCross-attention allows the decoder to query the encoder’s representation.\n\nThis is how translation works: when generating \"Je\", the decoder attends to \"I\"; when generating \"t'aime\", it attends to \"love\". The attention mechanism learns these alignments automatically from data, without explicit supervision.\n\n## Position Embedding: Encoding Order\n\nAttention is **permutation invariant**—it produces the same output regardless of input order. \"The cat sat on the mat\" and \"mat the on sat cat the\" yield identical attention outputs because the dot product doesn't encode position. We need to inject positional information.\n\nThe naive approach is to add a position index: $x_t := x_t + \\beta t$. This fails for two reasons:\n\n1. **Unbounded**: Position indices grow arbitrarily large. Models trained on sequences of length 512 fail on sequences of length 1000 because they've never seen position 513.\n2. **Discrete**: Positions 10 and 11 are no more similar than positions 10 and 100.\n\nA better approach is **binary position encoding**. Represent position $t$ as a binary vector:\n\n$$\n\\begin{align*}\n  0: \\ \\ \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} & \\quad &\n  8: \\ \\ \\ \\ \\texttt{1} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\\\\n  1: \\ \\ \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{1} & &\n  9: \\ \\ \\ \\ \\texttt{1} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{1} \\\\\n  2: \\ \\ \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{1} \\ \\ \\texttt{0} & &\n  10: \\ \\ \\ \\ \\texttt{1} \\ \\ \\texttt{0} \\ \\ \\texttt{1} \\ \\ \\texttt{0}\n\\end{align*}\n$$\n\nThis is unbounded—you can represent arbitrarily large positions by adding bits—but still discrete. The transformer solution is **sinusoidal position embedding**, a continuous version of binary encoding:\n\n$$\n\\text{Pos}(t, i) =\n\\begin{cases}\n\\sin\\left(\\dfrac{t}{10000^{2i/d}}\\right), & \\text{if } i \\text{ is even} \\\\\n\\cos\\left(\\dfrac{t}{10000^{2i/d}}\\right), & \\text{if } i \\text{ is odd}\n\\end{cases}\n$$\n\nwhere $t$ is the position index and $i$ is the dimension index. This encoding has three critical properties:\n\n1. **Continuous**: Smooth interpolation between positions\n2. **Bounded**: All values lie in $[-1, 1]$\n3. **Relative distance preservation**: The dot product $\\text{Pos}(t) \\cdot \\text{Pos}(t+k)$ depends only on the offset $k$, not the absolute position $t$\n\n```{figure} https://kazemnejad.com/img/transformer_architecture_positional_encoding/positional_encoding.png\n:name: transformer-position-embedding\n:alt: Transformer Position Embedding\n:width: 80%\n:align: center\n\nSinusoidal position embeddings exhibit periodic patterns across dimensions. Image from [Amirhossein Kazemnejad](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/).\nNotice the alternating pattern—just like binary encoding, but continuous. Low-frequency dimensions (right) flip slowly across positions; high-frequency dimensions (left) flip rapidly. This creates a unique fingerprint for each position while preserving distance relationships.\n```doufuilu https://kazemnejad.com/img/transformer_architecture_positional_encoding/time-steps_dot_product.png :name: transformer-position-embedding-similarity :alt: Transformer Position Embedding Similarity :width: 80% :align: center\nDot product between position embeddings depends only on relative distance, not absolute position. Image from Amirhossein Kazemnejad.\n\nThe position embedding is added directly to the token embedding: $x_{t,i} := x_{t,i} + \\text{Pos}(t, i)$. Why addition instead of concatenation? Concatenation would increase the model dimension, adding parameters. Addition creates an interesting interaction in the attention mechanism—queries and keys now encode both content and position, allowing the model to attend based on both \"what\" (semantic similarity) and \"where\" (positional proximity).\n\n## The Takeaway\n\nTransformers replaced sequential computation with parallel relationship mapping. Every position simultaneously computes its context from every other position. This architectural shift—from recurrent bottlenecks to parallel attention—is what allowed language models to scale from millions to hundreds of billions of parameters. The mechanism is simple: query, key, value. The result is GPT-4.\n\n```{footbibliography}\n:style: unsrt\n:filter: docname in docnames"
  },
  {
    "objectID": "m06-llms/transformers.html#the-encoder-block",
    "href": "m06-llms/transformers.html#the-encoder-block",
    "title": "Transformers",
    "section": "",
    "text": "Now we wire the components together. The encoder block processes the input sequence through four stages:\n\nMulti-head self-attention computes contextualized representations\nResidual connection + normalization stabilizes training\nFeed-forward network applies nonlinear transformation\nResidual connection + normalization again\n\n```pqutzgqz ../figs/transformer-encoder.jpg :name: transformer-block :alt: Transformer Block :width: 50% :align: center\nInformation flows through multi-head attention, normalization, feed-forward networks, and final normalization.\n\nThe feed-forward network is a simple two-layer MLP applied independently to each position:\n\n$$\n\\text{FFN}(x) = \\text{ReLU}(x W_1 + b_1) W_2 + b_2\n$$\n\nThe **residual connections** (also called skip connections) are critical for training deep networks. Instead of learning a direct mapping $f(x)$, we learn the residual:\n\n$$\nx_{\\text{out}} = x_{\\text{in}} + f(x_{\\text{in}})\n$$\n\nThis simple addition has profound consequences for gradient flow. During backpropagation, the gradient of the loss $\\mathcal{L}$ with respect to layer $l$ is:\n\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_l} = \\frac{\\partial \\mathcal{L}}{\\partial x_{l+1}} \\left(1 + \\frac{\\partial f_l}{\\partial x_l}\\right)\n$$\n\nNotice the \"+1\" term—this provides a direct gradient path from the output back to the input. Without residual connections, gradients must pass through the chain:\n\n$$\n\\frac{\\partial f_L}{\\partial f_{L-1}} \\cdot \\frac{\\partial f_{L-1}}{\\partial f_{L-2}} \\cdot \\ldots \\cdot \\frac{\\partial f_1}{\\partial x}\n$$\n\nIf any term is less than 1, the gradient shrinks exponentially—this is the **vanishing gradient problem**. With residual connections, the gradient expansion becomes:\n\n$$\n1 + O_1 + O_2 + O_3 + \\ldots\n$$\n\nwhere $O_1$ contains first-order terms, $O_2$ contains second-order products, etc. The constant \"1\" ensures gradients can flow even when the learned components $f_i$ produce small derivatives. This architectural innovation, originally developed for computer vision {footcite:p}`he2015deep`, is what allows transformers to scale to hundreds of layers.\n\n## The Decoder Block\n\nThe **decoder block** extends the encoder with two modifications: **masked self-attention** and **cross-attention**.\n\n```{figure} ../figs/transformer-decoder.jpg\n:name: transformer-decoder\n:alt: Transformer Decoder\n:width: 50%\n:align: center\n\nThe decoder adds masked self-attention (to prevent future peeking) and cross-attention (to access encoder outputs).\n\n\nDuring training, we know the entire target sequence. For translation (“I love you” → “Je t’aime”), we have both input and output. A naive decoder could “cheat” by looking at future words in the target sequence. Masked self-attention prevents this by zeroing out attention to future positions.\nThe mask is implemented by setting attention scores to -\\infty before the softmax:\n\n\\text{MaskedAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T + M}{\\sqrt{d_k}}\\right)V\n\nwhere M is a matrix with -\\infty at positions (i,j) where j &gt; i (future positions) and 0 elsewhere. After softmax, these -\\infty values become zero, eliminating information flow from future tokens.\n```pqutzgqz https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png :name: transformer-masked-attention :alt: Masked Attention :width: 80% :align: center\nMasked attention zeros out future positions, allowing parallel training without information leakage.\n\nThis enables **parallel training**. Instead of generating \"Je\", then \"t'aime\", then the final token sequentially, we can train all positions simultaneously—each with access only to its causal past. During inference, masking happens naturally because future tokens don't exist yet.\n\n### Cross-Attention: Connecting Encoder and Decoder\n\nThe second attention layer in the decoder uses **cross-attention** to access the encoder's output. The queries ($Q$) come from the decoder's previous layer, while the keys ($K$) and values ($V$) come from the encoder's output:\n\n$$\n\\text{CrossAttention}(Q_{\\text{decoder}}, K_{\\text{encoder}}, V_{\\text{encoder}}) = \\text{softmax}\\left(\\frac{Q_{\\text{decoder}}K_{\\text{encoder}}^T}{\\sqrt{d_k}}\\right)V_{\\text{encoder}}\n$$\n\n```{figure} ../figs/transformer-cross-attention.jpg\n:name: transformer-cross-attention\n:alt: Cross-Attention\n:width: 60%\n:align: center\n\nCross-attention allows the decoder to query the encoder's representation.\nThis is how translation works: when generating “Je”, the decoder attends to “I”; when generating “t’aime”, it attends to “love”. The attention mechanism learns these alignments automatically from data, without explicit supervision."
  },
  {
    "objectID": "m06-llms/transformers.html#position-embedding-encoding-order",
    "href": "m06-llms/transformers.html#position-embedding-encoding-order",
    "title": "Transformers",
    "section": "",
    "text": "Attention is permutation invariant—it produces the same output regardless of input order. “The cat sat on the mat” and “mat the on sat cat the” yield identical attention outputs because the dot product doesn’t encode position. We need to inject positional information.\nThe naive approach is to add a position index: x_t := x_t + \\beta t. This fails for two reasons:\n\nUnbounded: Position indices grow arbitrarily large. Models trained on sequences of length 512 fail on sequences of length 1000 because they’ve never seen position 513.\nDiscrete: Positions 10 and 11 are no more similar than positions 10 and 100.\n\nA better approach is binary position encoding. Represent position t as a binary vector:\n\n\\begin{align*}\n  0: \\ \\ \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} & \\quad &\n  8: \\ \\ \\ \\ \\texttt{1} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\\\\n  1: \\ \\ \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{1} & &\n  9: \\ \\ \\ \\ \\texttt{1} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{1} \\\\\n  2: \\ \\ \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{1} \\ \\ \\texttt{0} & &\n  10: \\ \\ \\ \\ \\texttt{1} \\ \\ \\texttt{0} \\ \\ \\texttt{1} \\ \\ \\texttt{0}\n\\end{align*}\n\nThis is unbounded—you can represent arbitrarily large positions by adding bits—but still discrete. The transformer solution is sinusoidal position embedding, a continuous version of binary encoding:\n\n\\text{Pos}(t, i) =\n\\begin{cases}\n\\sin\\left(\\dfrac{t}{10000^{2i/d}}\\right), & \\text{if } i \\text{ is even} \\\\\n\\cos\\left(\\dfrac{t}{10000^{2i/d}}\\right), & \\text{if } i \\text{ is odd}\n\\end{cases}\n\nwhere t is the position index and i is the dimension index. This encoding has three critical properties:\n\nContinuous: Smooth interpolation between positions\nBounded: All values lie in [-1, 1]\nRelative distance preservation: The dot product \\text{Pos}(t) \\cdot \\text{Pos}(t+k) depends only on the offset k, not the absolute position t\n\n```pqutzgqz https://kazemnejad.com/img/transformer_architecture_positional_encoding/positional_encoding.png :name: transformer-position-embedding :alt: Transformer Position Embedding :width: 80% :align: center\nSinusoidal position embeddings exhibit periodic patterns across dimensions. Image from Amirhossein Kazemnejad.\n\nNotice the alternating pattern—just like binary encoding, but continuous. Low-frequency dimensions (right) flip slowly across positions; high-frequency dimensions (left) flip rapidly. This creates a unique fingerprint for each position while preserving distance relationships.\n\n```{figure} https://kazemnejad.com/img/transformer_architecture_positional_encoding/time-steps_dot_product.png\n:name: transformer-position-embedding-similarity\n:alt: Transformer Position Embedding Similarity\n:width: 80%\n:align: center\n\nDot product between position embeddings depends only on relative distance, not absolute position. Image from [Amirhossein Kazemnejad](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/).\nThe position embedding is added directly to the token embedding: x_{t,i} := x_{t,i} + \\text{Pos}(t, i). Why addition instead of concatenation? Concatenation would increase the model dimension, adding parameters. Addition creates an interesting interaction in the attention mechanism—queries and keys now encode both content and position, allowing the model to attend based on both “what” (semantic similarity) and “where” (positional proximity)."
  },
  {
    "objectID": "m06-llms/transformers.html#the-takeaway",
    "href": "m06-llms/transformers.html#the-takeaway",
    "title": "Transformers",
    "section": "",
    "text": "Transformers replaced sequential computation with parallel relationship mapping. Every position simultaneously computes its context from every other position. This architectural shift—from recurrent bottlenecks to parallel attention—is what allowed language models to scale from millions to hundreds of billions of parameters. The mechanism is simple: query, key, value. The result is GPT-4.\n:style: unsrt\n:filter: docname in docnames"
  },
  {
    "objectID": "m06-llms/transformers.html#layer-normalization-numerical-stability",
    "href": "m06-llms/transformers.html#layer-normalization-numerical-stability",
    "title": "Transformers",
    "section": "",
    "text": "Deep networks suffer from numerical instability—activations can grow explosively large or vanish to zero as they propagate through layers. Layer normalization stabilizes training by rescaling activations to have zero mean and unit variance.\n```doufuilu https://miro.medium.com/v2/resize:fit:1400/0*Agdt1zYwfUxXMJGJ :name: transformer-layer-normalization :alt: Layer Normalization :width: 80% :align: center\nLayer normalization computes mean and standard deviation across all features for each sample, then normalizes.\n\nFor each input vector $x$, layer normalization computes:\n\n$$\n\\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sigma} + \\beta\n$$\n\nwhere $\\mu$ and $\\sigma$ are the mean and standard deviation of $x$, and $\\gamma$ and $\\beta$ are learned scaling and shifting parameters (initialized to 1 and 0 respectively). This ensures that no matter how the input distribution shifts during training, each layer receives inputs in a stable numerical range.\n\n## The Encoder Block\n\nNow we wire the components together. The **encoder block** processes the input sequence through four stages:\n\n1. **Multi-head self-attention** computes contextualized representations\n2. **Residual connection + normalization** stabilizes training\n3. **Feed-forward network** applies nonlinear transformation\n4. **Residual connection + normalization** again\n\n```{figure} ../figs/transformer-encoder.jpg\n:name: transformer-block\n:alt: Transformer Block\n:width: 50%\n:align: center\n\nInformation flows through multi-head attention, normalization, feed-forward networks, and final normalization.\nThe feed-forward network is a simple two-layer MLP applied independently to each position:\n\n\\text{FFN}(x) = \\text{ReLU}(x W_1 + b_1) W_2 + b_2\n\nThe residual connections (also called skip connections) are critical for training deep networks. Instead of learning a direct mapping f(x), we learn the residual:\n\nx_{\\text{out}} = x_{\\text{in}} + f(x_{\\text{in}})\n\nThis simple addition has profound consequences for gradient flow. During backpropagation, the gradient of the loss \\mathcal{L} with respect to layer l is:\n\n\\frac{\\partial \\mathcal{L}}{\\partial x_l} = \\frac{\\partial \\mathcal{L}}{\\partial x_{l+1}} \\left(1 + \\frac{\\partial f_l}{\\partial x_l}\\right)\n\nNotice the “+1” term—this provides a direct gradient path from the output back to the input. Without residual connections, gradients must pass through the chain:\n\n\\frac{\\partial f_L}{\\partial f_{L-1}} \\cdot \\frac{\\partial f_{L-1}}{\\partial f_{L-2}} \\cdot \\ldots \\cdot \\frac{\\partial f_1}{\\partial x}\n\nIf any term is less than 1, the gradient shrinks exponentially—this is the vanishing gradient problem. With residual connections, the gradient expansion becomes:\n\n1 + O_1 + O_2 + O_3 + \\ldots\n\nwhere O_1 contains first-order terms, O_2 contains second-order products, etc. The constant “1” ensures gradients can flow even when the learned components f_i produce small derivatives. This architectural innovation, originally developed for computer vision {footcite:p}he2015deep, is what allows transformers to scale to hundreds of layers."
  },
  {
    "objectID": "m06-llms/transformers.html#the-decoder-block",
    "href": "m06-llms/transformers.html#the-decoder-block",
    "title": "Transformers",
    "section": "",
    "text": "The decoder block extends the encoder with two modifications: masked self-attention and cross-attention.\n```doufuilu ../figs/transformer-decoder.jpg :name: transformer-decoder :alt: Transformer Decoder :width: 50% :align: center\nThe decoder adds masked self-attention (to prevent future peeking) and cross-attention (to access encoder outputs).\n\n### Masked Self-Attention: Preventing Future Leakage\n\nDuring training, we know the entire target sequence. For translation (\"I love you\" → \"Je t'aime\"), we have both input and output. A naive decoder could \"cheat\" by looking at future words in the target sequence. Masked self-attention prevents this by zeroing out attention to future positions.\n\nThe mask is implemented by setting attention scores to $-\\infty$ before the softmax:\n\n$$\n\\text{MaskedAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T + M}{\\sqrt{d_k}}\\right)V\n$$\n\nwhere $M$ is a matrix with $-\\infty$ at positions $(i,j)$ where $j &gt; i$ (future positions) and 0 elsewhere. After softmax, these $-\\infty$ values become zero, eliminating information flow from future tokens.\n\n```{figure} https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png\n:name: transformer-masked-attention\n:alt: Masked Attention\n:width: 80%\n:align: center\n\nMasked attention zeros out future positions, allowing parallel training without information leakage.\nThis enables parallel training. Instead of generating “Je”, then “t’aime”, then the final token sequentially, we can train all positions simultaneously—each with access only to its causal past. During inference, masking happens naturally because future tokens don’t exist yet.\n\n\nThe second attention layer in the decoder uses cross-attention to access the encoder’s output. The queries (Q) come from the decoder’s previous layer, while the keys (K) and values (V) come from the encoder’s output:\n\n\\text{CrossAttention}(Q_{\\text{decoder}}, K_{\\text{encoder}}, V_{\\text{encoder}}) = \\text{softmax}\\left(\\frac{Q_{\\text{decoder}}K_{\\text{encoder}}^T}{\\sqrt{d_k}}\\right)V_{\\text{encoder}}\n\n```doufuilu ../figs/transformer-cross-attention.jpg :name: transformer-cross-attention :alt: Cross-Attention :width: 60% :align: center\nCross-attention allows the decoder to query the encoder’s representation.\n\nThis is how translation works: when generating \"Je\", the decoder attends to \"I\"; when generating \"t'aime\", it attends to \"love\". The attention mechanism learns these alignments automatically from data, without explicit supervision.\n\n## Position Embedding: Encoding Order\n\nAttention is **permutation invariant**—it produces the same output regardless of input order. \"The cat sat on the mat\" and \"mat the on sat cat the\" yield identical attention outputs because the dot product doesn't encode position. We need to inject positional information.\n\nThe naive approach is to add a position index: $x_t := x_t + \\beta t$. This fails for two reasons:\n\n1. **Unbounded**: Position indices grow arbitrarily large. Models trained on sequences of length 512 fail on sequences of length 1000 because they've never seen position 513.\n2. **Discrete**: Positions 10 and 11 are no more similar than positions 10 and 100.\n\nA better approach is **binary position encoding**. Represent position $t$ as a binary vector:\n\n$$\n\\begin{align*}\n  0: \\ \\ \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} & \\quad &\n  8: \\ \\ \\ \\ \\texttt{1} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\\\\n  1: \\ \\ \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{1} & &\n  9: \\ \\ \\ \\ \\texttt{1} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{1} \\\\\n  2: \\ \\ \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{1} \\ \\ \\texttt{0} & &\n  10: \\ \\ \\ \\ \\texttt{1} \\ \\ \\texttt{0} \\ \\ \\texttt{1} \\ \\ \\texttt{0}\n\\end{align*}\n$$\n\nThis is unbounded—you can represent arbitrarily large positions by adding bits—but still discrete. The transformer solution is **sinusoidal position embedding**, a continuous version of binary encoding:\n\n$$\n\\text{Pos}(t, i) =\n\\begin{cases}\n\\sin\\left(\\dfrac{t}{10000^{2i/d}}\\right), & \\text{if } i \\text{ is even} \\\\\n\\cos\\left(\\dfrac{t}{10000^{2i/d}}\\right), & \\text{if } i \\text{ is odd}\n\\end{cases}\n$$\n\nwhere $t$ is the position index and $i$ is the dimension index. This encoding has three critical properties:\n\n1. **Continuous**: Smooth interpolation between positions\n2. **Bounded**: All values lie in $[-1, 1]$\n3. **Relative distance preservation**: The dot product $\\text{Pos}(t) \\cdot \\text{Pos}(t+k)$ depends only on the offset $k$, not the absolute position $t$\n\n```{figure} https://kazemnejad.com/img/transformer_architecture_positional_encoding/positional_encoding.png\n:name: transformer-position-embedding\n:alt: Transformer Position Embedding\n:width: 80%\n:align: center\n\nSinusoidal position embeddings exhibit periodic patterns across dimensions. Image from [Amirhossein Kazemnejad](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/).\nNotice the alternating pattern—just like binary encoding, but continuous. Low-frequency dimensions (right) flip slowly across positions; high-frequency dimensions (left) flip rapidly. This creates a unique fingerprint for each position while preserving distance relationships.\n```doufuilu https://kazemnejad.com/img/transformer_architecture_positional_encoding/time-steps_dot_product.png :name: transformer-position-embedding-similarity :alt: Transformer Position Embedding Similarity :width: 80% :align: center\nDot product between position embeddings depends only on relative distance, not absolute position. Image from Amirhossein Kazemnejad.\n\nThe position embedding is added directly to the token embedding: $x_{t,i} := x_{t,i} + \\text{Pos}(t, i)$. Why addition instead of concatenation? Concatenation would increase the model dimension, adding parameters. Addition creates an interesting interaction in the attention mechanism—queries and keys now encode both content and position, allowing the model to attend based on both \"what\" (semantic similarity) and \"where\" (positional proximity).\n\n## The Takeaway\n\nTransformers replaced sequential computation with parallel relationship mapping. Every position simultaneously computes its context from every other position. This architectural shift—from recurrent bottlenecks to parallel attention—is what allowed language models to scale from millions to hundreds of billions of parameters. The mechanism is simple: query, key, value. The result is GPT-4.\n\n```{footbibliography}\n:style: unsrt\n:filter: docname in docnames"
  },
  {
    "objectID": "m03-text/transformers.html#one-word-one-vector-is-not-enough",
    "href": "m03-text/transformers.html#one-word-one-vector-is-not-enough",
    "title": "Transformers",
    "section": "1 One word, one vector is not enough",
    "text": "1 One word, one vector is not enough\nFor many years, natural language processing treated words as having fixed meanings. We represented each word—like “bank”—as a single vector of numbers, called static embeddings.\nBut there’s a hidden catch in this “one meaning per word” mindset: with just a single fixed entry in the dictionary, “bank” means exactly the same thing in “I deposited money at the bank” as in “We had a picnic by the bank.” Every possible meaning gets mashed into a one-size-fits-all average—like describing the population by its average height and pretending that nobody’s any shorter or taller. The interesting details—the outliers, the context clues—vanish in the mix.\nThe naive hypothesis went like this: what if we just mix the target word with its neighbors? For the sentence “I deposited money at the bank,” we could compute a contextualized representation as:\n\n\\vec{v}_{\\text{bank (new)}} = w_1 \\cdot \\vec{v}_{\\text{bank}} + w_2 \\cdot \\vec{v}_{\\text{deposited}} + w_3 \\cdot \\vec{v}_{\\text{money}} + \\cdots\n\nwhere w_i are weights and \\vec{v}_i are word embeddings.\nConsider the following example. Notice that “bank” sits neutrally between financial terms (money) and geographical terms (river). Now try manually adjusting the weights to contextualize “bank”:\n\nd3 = require(\"d3@7\", \"d3-simple-slider@1\")\n\n\n\n\n\n\n\nfunction sliderWithLabel(min, max, step, width, defaultValue, label) {\n  const slider = d3.sliderBottom()\n    .min(min).max(max).step(step).width(width).default(defaultValue);\n  const svg = d3.create(\"svg\").attr(\"width\", width + 50).attr(\"height\", 60);\n  svg.append(\"g\").attr(\"transform\", \"translate(25,20)\").call(slider);\n  svg.append(\"text\").attr(\"x\", (width + 50) / 2).attr(\"y\", 10).attr(\"text-anchor\", \"middle\").style(\"font-size\", \"5px\").text(label);\n  return svg.node();\n}\n\n\n\n\n\n\n\n{\n  // Create slider function that returns both the element and a reactive value\n  function createWeightSlider(min, max, step, width, defaultValue, label) {\n    const slider = d3.sliderBottom()\n      .min(min).max(max).step(step).width(width).default(defaultValue);\n    const svg = d3.create(\"svg\").attr(\"width\", width + 50).attr(\"height\", 60);\n    const g = svg.append(\"g\").attr(\"transform\", \"translate(25,20)\");\n    g.call(slider);\n    svg.append(\"text\").attr(\"x\", (width + 50) / 2).attr(\"y\", 10)\n       .attr(\"text-anchor\", \"middle\").style(\"font-size\", \"12px\").text(label);\n    return { node: svg.node(), slider: slider };\n  }\n\n  // Create sliders\n  const bankSliderObj = createWeightSlider(0, 1, 0.01, 120, 1.0, \"Bank weight\");\n  const moneySliderObj = createWeightSlider(0, 1, 0.01, 120, 0.0, \"Money weight\");\n  const riverSliderObj = createWeightSlider(0, 1, 0.01, 120, 0.0, \"River weight\");\n\n  // Word embeddings in 2D space\n  const contextWords = [\"bank\", \"money\", \"river\"];\n  const contextEmbeddings = [\n    [0.0, 0.0],   // bank (center)\n    [-1.6, -0.6], // money (financial, left)\n    [1.4, -1.0]   // river (geographical, right)\n  ];\n\n  // Create plot container\n  const plotContainer = document.createElement(\"div\");\n\n  // Function to update visualization\n  function update() {\n    // Get current slider values\n    const bankWeight = bankSliderObj.slider.value();\n    const moneyWeight = moneySliderObj.slider.value();\n    const riverWeight = riverSliderObj.slider.value();\n\n    // Calculate weighted average\n    const weights = [bankWeight, moneyWeight, riverWeight];\n    const total = weights.reduce((a, b) =&gt; a + b, 0);\n    const normalizedWeights = total &gt; 0 ? weights.map(w =&gt; w / total) : [0, 0, 0];\n\n    const newVec = [\n      normalizedWeights[0] * contextEmbeddings[0][0] +\n      normalizedWeights[1] * contextEmbeddings[1][0] +\n      normalizedWeights[2] * contextEmbeddings[2][0],\n      normalizedWeights[0] * contextEmbeddings[0][1] +\n      normalizedWeights[1] * contextEmbeddings[1][1] +\n      normalizedWeights[2] * contextEmbeddings[2][1]\n    ];\n\n    // Prepare data for visualization\n    const originalData = contextWords.map((word, i) =&gt; ({\n      word: word,\n      x: contextEmbeddings[i][0],\n      y: contextEmbeddings[i][1],\n      type: \"Original\"\n    }));\n\n    const contextualizedData = [{\n      word: \"bank (new)\",\n      x: newVec[0],\n      y: newVec[1],\n      type: \"Contextualized\"\n    }];\n\n    const data = [...originalData, ...contextualizedData];\n\n    // Clear and update plot\n    d3.select(plotContainer).selectAll(\"*\").remove();\n\n    // Create visualization\n    const plot = Plot.plot({\n      width: 300,\n      height: 300,\n      marginTop: 60,\n      marginRight: 20,\n      marginBottom: 50,\n      marginLeft: 60,\n      style: {\n        background: \"white\",\n        color: \"black\"\n      },\n      x: {\n        domain: [-2, 2],\n        label: \"Dimension 1\",\n        grid: true,\n        ticks: 10\n      },\n      y: {\n        domain: [-2, 2],\n        label: \"Dimension 2\",\n        grid: true,\n        ticks: 10\n      },\n      color: {\n        domain: [\"Original\", \"Contextualized\"],\n        range: [\"#dadada\", \"#ff7f0e\"]\n      },\n      marks: [\n        Plot.dot(data, {\n          x: \"x\",\n          y: \"y\",\n          fill: \"type\",\n          r: 8,\n          tip: true\n        }),\n        Plot.text(data, {\n          x: \"x\",\n          y: \"y\",\n          text: \"word\",\n          dy: -15,\n          fontSize: 8,\n          fontWeight: \"bold\",\n          fill: \"black\"\n        }),\n        Plot.text([{x: 0, y: 2.3}], {\n          x: \"x\",\n          y: \"y\",\n          text: () =&gt; `Weights: Bank=${normalizedWeights[0].toFixed(2)}, Money=${normalizedWeights[1].toFixed(2)}, River=${normalizedWeights[2].toFixed(2)}`,\n          fontSize: 11,\n          fill: \"black\"\n        }),\n        // Custom legend at top center\n        Plot.dot([{x: -0.8, y: 2.7, color: \"#dadada\"}, {x: 0.8, y: 2.7, color: \"#ff7f0e\"}], {\n          x: \"x\",\n          y: \"y\",\n          fill: \"color\",\n          r: 6\n        }),\n        Plot.text([{x: -0.5, y: 2.7, label: \"Original\"}, {x: 1.1, y: 2.7, label: \"Contextualized\"}], {\n          x: \"x\",\n          y: \"y\",\n          text: \"label\",\n          fontSize: 10,\n          fill: \"black\",\n          textAnchor: \"start\"\n        })\n      ]\n    });\n\n    d3.select(plotContainer).node().appendChild(plot);\n  }\n\n  // Add event listeners to sliders\n  bankSliderObj.slider.on(\"onchange\", update);\n  moneySliderObj.slider.on(\"onchange\", update);\n  riverSliderObj.slider.on(\"onchange\", update);\n\n  // Initial render\n  update();\n\n  return html`&lt;div style=\"display: flex; align-items: center; gap: 40px; justify-content: center;\"&gt;\n    &lt;div style=\"display: flex; flex-direction: column; gap: 10px;\"&gt;\n      ${bankSliderObj.node}\n      ${moneySliderObj.node}\n      ${riverSliderObj.node}\n    &lt;/div&gt;\n    &lt;div&gt;\n      ${plotContainer}\n    &lt;/div&gt;\n  &lt;/div&gt;`;\n}\n\n\n\n\n\n\nBy changing the weights, we can see that the vector for “bank” can lean more towards the financial terms or the geographical terms. So how can we determine the weights?\nThe simplest idea is to give each word an equal weight: w_i = 1/N. This creates a basic “bag-of-words” average. But sentences aren’t actually this fair—some words are much more important than others. For example, in “I deposited money at the bank,” the words “deposited” and “money” are key, while “I,” “at,” and “the” add little meaning. If we treat all words the same, we lose the details that matter. We need a way to highlight the important words and downplay the rest.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Transformers: The Architecture Behind the Magic"
    ]
  },
  {
    "objectID": "m03-text/transformers.html#attention-mechanism",
    "href": "m03-text/transformers.html#attention-mechanism",
    "title": "Transformers",
    "section": "2 Attention mechanism",
    "text": "2 Attention mechanism\nLet’s walk through how transformers identify the surrounding words that are relevant to a focal word to be contextualized, which is called the attention mechanism. Before we dive into the attention mechanism, let’s first prepare some terminology.\nSuppose we have a sentence “I deposited money at the bank”. Given a word “bank”, we want to determine the weights w_i for the surrounding words “I”, “deposited”, “money”, and “at”. We call the word “bank” the query word, and the surrounding words the key words. At a high level, we want to compute the weights w_i for each query and key pair, and then average them.\n\n\\vec{v}_{\\text{query}} ^{\\text{c}} = \\sum_{i=1}^N w_i \\cdot \\vec{v}_{i}\n\nwith weights w_i being determined by the query and key vectors w_{i}:=f(\\vec{v}_{\\text{query}}, \\vec{v}_{i}). This function, f, is calle the attention score function.\nIn transformers, the attention score function f is implemented as follows. Given the original vector for a word (regardless of whether it is the query word or the key word), we linearly transform it into three vectors: Query, Key, and Value.\n\n\\begin{align}\n\\vec{q}_i &= W_Q \\vec{x}_i\\\\\n\\vec{k}_i &= W_K \\vec{x}_i\\\\\n\\vec{v}_i &= W_V \\vec{x}_i\n\\end{align}\n\nWhy do we need three different vectors? Imagine you are participating in a dinner party. You want to identify the people who are talking about a topic you care about. You listen to the surrounding people, playing as a ‘listener’. At the same time, you also broadcast your own interests, playing as a ‘speaker’. The query vector is representing you as a listener, the key vector is representing the people as speakers. And the value vector is representing the content of the conversation.\nOnce we have the query, key, and value vectors, we can compute the attention scores between the query and key vector as follows:\n\nw_{ij} = \\frac{\\exp(\\vec{q}_i \\cdot \\vec{k}_j / \\sqrt{d})}{\\sum_{\\ell} \\exp(\\vec{q}_i \\cdot \\vec{k}_\\ell / \\sqrt{d})},\n\nwhere \\vec{q}_i \\cdot \\vec{k}_j is the dot product between the query and key vectors, which is larger when the query and key vectors are similar (e.g., pointing to a similar direction). The division by \\sqrt{d} (where d is the embedding dimension) is a scaling factor that prevents vanishing gradients during training. Finally, compute the contextualized representation as a weighted sum: \\text{contextualized}_i = \\sum_j w_{ij} \\vec{v}_j.\n\n\nWhat is the vanishing gradient problem? It is a problem that the gradients of the loss function with respect to the weights become too small to be effective during training.\nExplore how different Query and Key transformations produce different attention patterns. First, let us create the key, query, and value vectors. In 2d, the linear transformation is just a scaling and a rotation.\n\nfunction createQKVSlider(min, max, step, width, defaultValue, label, valueSetter) {\n  const slider = d3.sliderBottom()\n    .min(min).max(max).step(step).width(width).default(defaultValue)\n    .on('onchange', val =&gt; valueSetter(val));\n  const svg = d3.create(\"svg\").attr(\"width\", width + 40).attr(\"height\", 50);\n  const g = svg.append(\"g\").attr(\"transform\", \"translate(20,15)\");\n  g.call(slider);\n  svg.append(\"text\").attr(\"x\", (width + 40) / 2).attr(\"y\", 10)\n     .attr(\"text-anchor\", \"middle\").style(\"font-size\", \"11px\").text(label);\n  return { node: svg.node(), slider: slider };\n}\n\n\n\n\n\n\n\nmutable qScaleXValue = 1.0\n\n\n\n\n\n\n\nmutable qScaleYValue = 1.0\n\n\n\n\n\n\n\nmutable qRotateValue = 0\n\n\n\n\n\n\n\nmutable kScaleXValue = 1.0\n\n\n\n\n\n\n\nmutable kScaleYValue = 1.0\n\n\n\n\n\n\n\nmutable kRotateValue = 0\n\n\n\n\n\n\n\nqScaleXSlider = createQKVSlider(0.1, 2, 0.1, 150, 1.0, \"Q Scale X\", val =&gt; mutable qScaleXValue = val)\n\n\n\n\n\n\n\nqScaleYSlider = createQKVSlider(0.1, 2, 0.1, 150, 1.0, \"Q Scale Y\", val =&gt; mutable qScaleYValue = val)\n\n\n\n\n\n\n\nqRotateSlider = createQKVSlider(-180, 180, 5, 150, 0, \"Q Rotate (deg)\", val =&gt; mutable qRotateValue = val)\n\n\n\n\n\n\n\nkScaleXSlider = createQKVSlider(0.1, 2, 0.1, 150, 1.0, \"K Scale X\", val =&gt; mutable kScaleXValue = val)\n\n\n\n\n\n\n\nkScaleYSlider = createQKVSlider(0.1, 2, 0.1, 150, 1.0, \"K Scale Y\", val =&gt; mutable kScaleYValue = val)\n\n\n\n\n\n\n\nkRotateSlider = createQKVSlider(-180, 180, 5, 150, 0, \"K Rotate (deg)\", val =&gt; mutable kRotateValue = val)\n\n\n\n\n\n\n\nqkvVisualization = {\n  // Original word vectors (bank, money, river)\n  const originalVectors = [\n    { name: \"bank\", vector: [1.5, 0.5] },\n    { name: \"money\", vector: [1.8, 0.8] },\n    { name: \"river\", vector: [0.5, 1.5] }\n  ];\n\n  // Create plot containers\n  const qPlotContainer = document.createElement(\"div\");\n  const kPlotContainer = document.createElement(\"div\");\n\n  // Function to transform vector\n  function transformVector(vec, scaleX, scaleY, rotateDeg) {\n    const theta = (rotateDeg * Math.PI) / 180;\n    const cos = Math.cos(theta);\n    const sin = Math.sin(theta);\n    const scaledX = vec[0] * scaleX;\n    const scaledY = vec[1] * scaleY;\n    return [\n      scaledX * cos - scaledY * sin,\n      scaledX * sin + scaledY * cos\n    ];\n  }\n\n  // Get current slider values from mutable variables (creates reactive dependency)\n  const qScaleX = qScaleXValue;\n  const qScaleY = qScaleYValue;\n  const qRotate = qRotateValue;\n  const kScaleX = kScaleXValue;\n  const kScaleY = kScaleYValue;\n  const kRotate = kRotateValue;\n\n  // Prepare data for each plot\n  const originalData = originalVectors.map(item =&gt; ({\n    name: item.name,\n    x: item.vector[0],\n    y: item.vector[1],\n    type: \"Original\"\n  }));\n\n  const qData = originalVectors.map(item =&gt; {\n    const qVec = transformVector(item.vector, qScaleX, qScaleY, qRotate);\n    return {\n      name: `q_${item.name}`,\n      x: qVec[0],\n      y: qVec[1],\n      type: \"Query\"\n    };\n  });\n\n  const kData = originalVectors.map(item =&gt; {\n    const kVec = transformVector(item.vector, kScaleX, kScaleY, kRotate);\n    return {\n      name: `k_${item.name}`,\n      x: kVec[0],\n      y: kVec[1],\n      type: \"Key\"\n    };\n  });\n\n  // Create Query plot\n  const qPlot = Plot.plot({\n    width: 250,\n    height: 250,\n    marginTop: 40,\n    marginRight: 20,\n    marginBottom: 50,\n    marginLeft: 60,\n    style: {\n      background: \"white\",\n      color: \"black\"\n    },\n    x: {\n      domain: [-3, 3],\n      label: \"Dimension 1\",\n      grid: true,\n      ticks: 10\n    },\n    y: {\n      domain: [-3, 3],\n      label: \"Dimension 2\",\n      grid: true,\n      ticks: 10\n    },\n    marks: [\n      Plot.dot([{x: 0, y: 0}], {\n        x: \"x\",\n        y: \"y\",\n        r: 3,\n        fill: \"black\"\n      }),\n      Plot.arrow([...originalData, ...qData], {\n        x1: 0,\n        y1: 0,\n        x2: \"x\",\n        y2: \"y\",\n        stroke: \"type\",\n        strokeWidth: 2,\n        headLength: 8\n      }),\n      Plot.dot([...originalData, ...qData], {\n        x: \"x\",\n        y: \"y\",\n        fill: \"type\",\n        r: 5,\n        tip: true\n      }),\n      Plot.text([...originalData, ...qData], {\n        x: \"x\",\n        y: \"y\",\n        text: \"name\",\n        dy: -12,\n        fontSize: 9,\n        fontWeight: \"bold\",\n        fill: \"black\"\n      }),\n      Plot.text([{ x: 0, y: 3.4 }], {\n        x: \"x\",\n        y: \"y\",\n        text: () =&gt; \"Query Space\",\n        fontSize: 12,\n        fontWeight: \"bold\",\n        fill: \"black\"\n      })\n    ],\n    color: {\n      domain: [\"Original\", \"Query\"],\n      range: [\"#666666\", \"#4682b4\"]\n    }\n  });\n\n  // Create Key plot\n  const kPlot = Plot.plot({\n    width: 250,\n    height: 250,\n    marginTop: 40,\n    marginRight: 20,\n    marginBottom: 50,\n    marginLeft: 60,\n    style: {\n      background: \"white\",\n      color: \"black\"\n    },\n    x: {\n      domain: [-3, 3],\n      label: \"Dimension 1\",\n      grid: true,\n      ticks: 10\n    },\n    y: {\n      domain: [-3, 3],\n      label: \"Dimension 2\",\n      grid: true,\n      ticks: 10\n    },\n    marks: [\n      Plot.dot([{x: 0, y: 0}], {\n        x: \"x\",\n        y: \"y\",\n        r: 3,\n        fill: \"black\"\n      }),\n      Plot.arrow([...originalData, ...kData], {\n        x1: 0,\n        y1: 0,\n        x2: \"x\",\n        y2: \"y\",\n        stroke: \"type\",\n        strokeWidth: 2,\n        headLength: 8\n      }),\n      Plot.dot([...originalData, ...kData], {\n        x: \"x\",\n        y: \"y\",\n        fill: \"type\",\n        r: 5,\n        tip: true\n      }),\n      Plot.text([...originalData, ...kData], {\n        x: \"x\",\n        y: \"y\",\n        text: \"name\",\n        dy: -12,\n        fontSize: 9,\n        fontWeight: \"bold\",\n        fill: \"black\"\n      }),\n      Plot.text([{ x: 0, y: 3.4 }], {\n        x: \"x\",\n        y: \"y\",\n        text: () =&gt; \"Key Space\",\n        fontSize: 12,\n        fontWeight: \"bold\",\n        fill: \"black\"\n      })\n    ],\n    color: {\n      domain: [\"Original\", \"Key\"],\n      range: [\"#666666\", \"#2e8b57\"]\n    }\n  });\n\n  d3.select(qPlotContainer).node().appendChild(qPlot);\n  d3.select(kPlotContainer).node().appendChild(kPlot);\n\n  return html`&lt;div style=\"display: flex; justify-content: center; gap: 40px;\"&gt;\n    &lt;div style=\"display: flex; flex-direction: column; gap: 20px;\"&gt;\n      &lt;div style=\"display: flex; align-items: center; gap: 20px;\"&gt;\n        &lt;div style=\"display: flex; flex-direction: column; gap: 8px;\"&gt;\n          &lt;div style=\"font-weight: bold; margin-bottom: 3px;\"&gt;Query (W_Q)&lt;/div&gt;\n          ${qScaleXSlider.node}\n          ${qScaleYSlider.node}\n          ${qRotateSlider.node}\n        &lt;/div&gt;\n        ${qPlotContainer}\n      &lt;/div&gt;\n      &lt;div style=\"display: flex; align-items: center; gap: 20px;\"&gt;\n        &lt;div style=\"display: flex; flex-direction: column; gap: 8px;\"&gt;\n          &lt;div style=\"font-weight: bold; margin-bottom: 3px;\"&gt;Key (W_K)&lt;/div&gt;\n          ${kScaleXSlider.node}\n          ${kScaleYSlider.node}\n          ${kRotateSlider.node}\n        &lt;/div&gt;\n        ${kPlotContainer}\n      &lt;/div&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;`;\n}\n\n\n\n\n\n\nUsing the transformations above, we can compute the attention weights showing how each word attends to every other word:\n\nattentionHeatmap = {\n  // Get the original word vectors from the previous visualization\n  const attentionWords = [\"bank\", \"money\", \"river\"];\n  const attentionEmbeddings = [\n    [1.5, 0.5],\n    [1.8, 0.8],\n    [0.5, 1.5]\n  ];\n\n  // Transform vector function\n  function transformVector(vec, scaleX, scaleY, rotateDeg) {\n    const theta = (rotateDeg * Math.PI) / 180;\n    const cos = Math.cos(theta);\n    const sin = Math.sin(theta);\n    const scaledX = vec[0] * scaleX;\n    const scaledY = vec[1] * scaleY;\n    return [\n      scaledX * cos - scaledY * sin,\n      scaledX * sin + scaledY * cos\n    ];\n  }\n\n  // Get current slider values from mutable variables (creates reactive dependency)\n  const qScaleX = qScaleXValue;\n  const qScaleY = qScaleYValue;\n  const qRotate = qRotateValue;\n  const kScaleX = kScaleXValue;\n  const kScaleY = kScaleYValue;\n  const kRotate = kRotateValue;\n\n  // Apply transformations\n  const Q = attentionEmbeddings.map(vec =&gt;\n    transformVector(vec, qScaleX, qScaleY, qRotate)\n  );\n  const K = attentionEmbeddings.map(vec =&gt;\n    transformVector(vec, kScaleX, kScaleY, kRotate)\n  );\n\n  // Compute attention scores (Q @ K^T)\n  const scores = Q.map(q =&gt; K.map(k =&gt; q[0] * k[0] + q[1] * k[1]));\n\n  // Apply softmax to each row\n  const attentionWeights = scores.map(row =&gt; {\n    const maxScore = Math.max(...row);\n    const expScores = row.map(s =&gt; Math.exp(s - maxScore));\n    const sumExp = expScores.reduce((a, b) =&gt; a + b, 0);\n    return expScores.map(e =&gt; e / sumExp);\n  });\n\n  // Prepare data for heatmap\n  const heatmapData = (() =&gt; {\n    const data = [];\n    for (let i = 0; i &lt; attentionWords.length; i++) {\n      for (let j = 0; j &lt; attentionWords.length; j++) {\n        data.push({\n          Query: attentionWords[i],\n          Key: attentionWords[j],\n          Weight: attentionWeights[i][j]\n        });\n      }\n    }\n    return data;\n  })();\n\n  // Create attention heatmap\n  return Plot.plot({\n    width: 320,\n    height: 320,\n    marginTop: 50,\n    marginBottom: 50,\n    marginLeft: 70,\n    marginRight: 80,\n    style: {\n      background: \"white\",\n      color: \"black\"\n    },\n    x: {\n      label: \"Key Word\",\n      domain: attentionWords\n    },\n    y: {\n      label: \"Query Word\",\n      domain: attentionWords\n    },\n    color: {\n      scheme: \"Blues\",\n      label: \"Attention\",\n      legend: true\n    },\n    marks: [\n      Plot.cell(heatmapData, {\n        x: \"Key\",\n        y: \"Query\",\n        fill: \"Weight\",\n        tip: true\n      }),\n      Plot.text(heatmapData, {\n        x: \"Key\",\n        y: \"Query\",\n        text: d =&gt; d.Weight.toFixed(2),\n        fill: d =&gt; d.Weight &gt; 0.35 ? \"white\" : \"black\",\n        fontSize: 11\n      }),\n      Plot.text([{ x: 0, y: 0 }], {\n        x: () =&gt; attentionWords.length / 2 - 0.5,\n        y: () =&gt; -0.8,\n        text: () =&gt; \"Attention Weights (Softmax)\",\n        fontSize: 12,\n        fontWeight: \"bold\",\n        frameAnchor: \"top\",\n        fill: \"black\"\n      })\n    ]\n  });\n}\n\n\n\n\n\n\nNow scale this up. Every word in a sentence needs to attend to every other word. For “The cat sat on the mat” (six words), we compute a 6 \\times 6 attention matrix A, where A_{ij} = \\text{softmax}(\\vec{q}_i \\cdot \\vec{k}_j). Rows represent words asking for context (Queries); columns represent words providing context (Keys). Each cell (i,j) indicates how much word i attends to word j. Each row sums to 1—it’s a probability distribution over context words.\nBut here’s the final complication: one attention matrix captures one type of relationship. Language is multidimensional. There are syntactic relationships (subject-verb-object), semantic relationships (conceptual similarity between “cat” and “mat” as physical objects), positional relationships (local word order), and pragmatic relationships (coreference, where “her” links to “scientist”). A single attention mechanism can’t capture all of these simultaneously.\n\nThe solution is multi-head attention: run multiple attention mechanisms in parallel, each with its own W_Q, W_K, W_V matrices. Mathematically, \\text{MultiHead}(X) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h) W^O. Each head learns a different attention pattern. Modern transformers use 8-16 heads per layer: BERT uses 12, GPT-3 uses 96. It’s like having twelve different experts analyze the sentence simultaneously—one focusing on syntax, one on semantics, one on coreference—and then combining their insights through a learned linear transformation.",
    "crumbs": [
      "Home",
      "Module 3: Deep Learning for Text",
      "Transformers: The Architecture Behind the Magic"
    ]
  }
]
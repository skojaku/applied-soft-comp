[
  {
    "objectID": "toc.html",
    "href": "toc.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Home\nWelcome\nAbout\nWhy Applied Soft Computing?\nDiscord\nSetup\nMinidora Usage\nHow to Submit Assignment\nDeliverables\n\n\n\n\n\nOverview\nVersion Control with Git & GitHub\nThe Tidy Data Philosophy\nData Provenance\nReproducibility\n\n\n\n\n\nOverview\nPrinciples of Effective Visualization\nVisualizing 1D Data\nVisualizing 2D Data\nVisualizing High-Dimensional Data\nVisualizing Networks\nVisualizing Time-Series\n\n\n\n\n\nOverview\nHands-on\nPrompt Tuning\nAgentic AI\nContext Engineering\n\n\n\n\n\nOverview\nLarge Language Models\nGPT Inference: Sampling Strategies\nTokenization: Unboxing How LLMs Read Text\nTransformers\nBERT & GPT\nSentence Transformers\nWord Embeddings\nSemaxis\nWord Bias\n\n\n\n\n\nOverview\nPart 1: What is an Image?\nPart 2: The Deep Learning Revolution\nPart 3: Using CNN Models\nPart 4: CNN Innovations\n\n\n\n\n\nOverview\nSpectral Graph Embedding\nGraph Embeddings with Word2Vec\nSpectral vs. Neural Embeddings\nFrom Images to Graphs\nGraph Convolutional Networks\nPopular GNN Architectures\nGNN Software & Tools"
  },
  {
    "objectID": "toc.html#course-information",
    "href": "toc.html#course-information",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Home\nWelcome\nAbout\nWhy Applied Soft Computing?\nDiscord\nSetup\nMinidora Usage\nHow to Submit Assignment\nDeliverables"
  },
  {
    "objectID": "toc.html#module-1-the-data-scientists-toolkit",
    "href": "toc.html#module-1-the-data-scientists-toolkit",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Overview\nVersion Control with Git & GitHub\nThe Tidy Data Philosophy\nData Provenance\nReproducibility"
  },
  {
    "objectID": "toc.html#module-2-visualizing-complexity",
    "href": "toc.html#module-2-visualizing-complexity",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Overview\nPrinciples of Effective Visualization\nVisualizing 1D Data\nVisualizing 2D Data\nVisualizing High-Dimensional Data\nVisualizing Networks\nVisualizing Time-Series"
  },
  {
    "objectID": "toc.html#module-3-agentic-coding",
    "href": "toc.html#module-3-agentic-coding",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Overview\nHands-on\nPrompt Tuning\nAgentic AI\nContext Engineering"
  },
  {
    "objectID": "toc.html#module-4-deep-learning-for-text",
    "href": "toc.html#module-4-deep-learning-for-text",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Overview\nLarge Language Models\nGPT Inference: Sampling Strategies\nTokenization: Unboxing How LLMs Read Text\nTransformers\nBERT & GPT\nSentence Transformers\nWord Embeddings\nSemaxis\nWord Bias"
  },
  {
    "objectID": "toc.html#module-5-deep-learning-for-images",
    "href": "toc.html#module-5-deep-learning-for-images",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Overview\nPart 1: What is an Image?\nPart 2: The Deep Learning Revolution\nPart 3: Using CNN Models\nPart 4: CNN Innovations"
  },
  {
    "objectID": "toc.html#module-6-deep-learning-for-graphs",
    "href": "toc.html#module-6-deep-learning-for-graphs",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Overview\nSpectral Graph Embedding\nGraph Embeddings with Word2Vec\nSpectral vs. Neural Embeddings\nFrom Images to Graphs\nGraph Convolutional Networks\nPopular GNN Architectures\nGNN Software & Tools"
  },
  {
    "objectID": "m05-images/what-to-learn.html",
    "href": "m05-images/what-to-learn.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "In this module, we will learn how to use neural networks to learn representations of images. We will learn: - Fourier transform on images - Image processing using Fourier transform - Convolutional neural networks - LeNet - AlexNet - VGG - Inception - ResNet - VisTransformer (if time permits)"
  },
  {
    "objectID": "m05-images/what-to-learn.html#what-to-learn-in-this-module",
    "href": "m05-images/what-to-learn.html#what-to-learn-in-this-module",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "In this module, we will learn how to use neural networks to learn representations of images. We will learn: - Fourier transform on images - Image processing using Fourier transform - Convolutional neural networks - LeNet - AlexNet - VGG - Inception - ResNet - VisTransformer (if time permits)"
  },
  {
    "objectID": "m05-images/batch-normalization.html",
    "href": "m05-images/batch-normalization.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Batch Normalization (BN) is a technique used in deep neural networks to stabilize and accelerate training by normalizing the inputs to layers within the network.\n\n\nNormalize the activations coming out of a layer for each feature (channel) independently within the current mini-batch so they have zero mean and unit variance.\n\n\n\nFor a mini-batch B = \\{x_1, ..., x_m\\} and considering a single activation feature:\n\nCalculate Mini-Batch Mean: \\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i\nCalculate Mini-Batch Variance: \\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2\nNormalize: Using the mini-batch mean and variance (adding a small \\epsilon for numerical stability): \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\nScale and Shift: Introduce two learnable parameters, \\gamma (gamma) for scaling and \\beta (beta) for shifting: y_i = \\gamma \\hat{x}_i + \\beta\n\n\nThe parameters \\gamma and \\beta are learned during backpropagation alongside the network’s weights.\nThis process is applied independently to each feature/channel dimension.\n\n\n\n\nIf we just normalized to zero mean and unit variance, the network’s ability to represent information might be limited. Forcing activations into this specific distribution might not always be optimal for the subsequent layers or activation functions.\n\n\\gamma and \\beta give the network flexibility.\nThey allow the network to learn the optimal scale and shift for the normalized activations.\nIf needed, the network can even learn parameters (\\gamma = \\sqrt{\\sigma_B^2 + \\epsilon}, \\beta = \\mu_B) that effectively undo the normalization, restoring the original activation distribution if that proves beneficial.\n\n\n\n\nDuring inference (when making predictions), we often process samples one by one, so calculating mini-batch statistics isn’t feasible or representative.\n\nInstead, BN layers maintain running averages of the mean (\\mu_{pop}) and variance (\\sigma^2_{pop}) encountered across all mini-batches during training.\n\nThese are typically updated using a momentum term:\n\n\\mu_{pop} = \\alpha \\times \\mu_{pop} + (1 - \\alpha) \\times \\mu_B\n\\sigma_{pop}^2 = \\alpha \\times \\sigma_{pop}^2 + (1 - \\alpha) \\times \\sigma_B^2\nwhere \\alpha is a momentum parameter.\n\n\nAt inference time, these fixed population statistics are used for normalization: $ = $\nThe learned \\gamma and \\beta parameters from training are still applied:  y = \\gamma \\hat{x} + \\beta \n\n\n\n\n\nIt’s common practice to place the Batch Norm layer after the Convolutional or Fully Connected layer and before the non-linear activation function (like ReLU).\n\nConv / FC -&gt; Batch Norm -&gt; Activation (ReLU)\n\nHowever, variations in placement exist in different architectures.\n\n(Note: While originally thought to primarily combat “internal covariate shift”, recent research suggests BN’s effectiveness might be more related to smoothing the optimization landscape.)"
  },
  {
    "objectID": "m05-images/batch-normalization.html#the-core-idea",
    "href": "m05-images/batch-normalization.html#the-core-idea",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Normalize the activations coming out of a layer for each feature (channel) independently within the current mini-batch so they have zero mean and unit variance."
  },
  {
    "objectID": "m05-images/batch-normalization.html#how-it-works-during-training",
    "href": "m05-images/batch-normalization.html#how-it-works-during-training",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "For a mini-batch B = \\{x_1, ..., x_m\\} and considering a single activation feature:\n\nCalculate Mini-Batch Mean: \\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i\nCalculate Mini-Batch Variance: \\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2\nNormalize: Using the mini-batch mean and variance (adding a small \\epsilon for numerical stability): \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\nScale and Shift: Introduce two learnable parameters, \\gamma (gamma) for scaling and \\beta (beta) for shifting: y_i = \\gamma \\hat{x}_i + \\beta\n\n\nThe parameters \\gamma and \\beta are learned during backpropagation alongside the network’s weights.\nThis process is applied independently to each feature/channel dimension."
  },
  {
    "objectID": "m05-images/batch-normalization.html#why-scale-and-shift-gamma-and-beta",
    "href": "m05-images/batch-normalization.html#why-scale-and-shift-gamma-and-beta",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "If we just normalized to zero mean and unit variance, the network’s ability to represent information might be limited. Forcing activations into this specific distribution might not always be optimal for the subsequent layers or activation functions.\n\n\\gamma and \\beta give the network flexibility.\nThey allow the network to learn the optimal scale and shift for the normalized activations.\nIf needed, the network can even learn parameters (\\gamma = \\sqrt{\\sigma_B^2 + \\epsilon}, \\beta = \\mu_B) that effectively undo the normalization, restoring the original activation distribution if that proves beneficial."
  },
  {
    "objectID": "m05-images/batch-normalization.html#batch-normalization-during-inference",
    "href": "m05-images/batch-normalization.html#batch-normalization-during-inference",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "During inference (when making predictions), we often process samples one by one, so calculating mini-batch statistics isn’t feasible or representative.\n\nInstead, BN layers maintain running averages of the mean (\\mu_{pop}) and variance (\\sigma^2_{pop}) encountered across all mini-batches during training.\n\nThese are typically updated using a momentum term:\n\n\\mu_{pop} = \\alpha \\times \\mu_{pop} + (1 - \\alpha) \\times \\mu_B\n\\sigma_{pop}^2 = \\alpha \\times \\sigma_{pop}^2 + (1 - \\alpha) \\times \\sigma_B^2\nwhere \\alpha is a momentum parameter.\n\n\nAt inference time, these fixed population statistics are used for normalization: $ = $\nThe learned \\gamma and \\beta parameters from training are still applied:  y = \\gamma \\hat{x} + \\beta"
  },
  {
    "objectID": "m05-images/batch-normalization.html#placement",
    "href": "m05-images/batch-normalization.html#placement",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "It’s common practice to place the Batch Norm layer after the Convolutional or Fully Connected layer and before the non-linear activation function (like ReLU).\n\nConv / FC -&gt; Batch Norm -&gt; Activation (ReLU)\n\nHowever, variations in placement exist in different architectures.\n\n(Note: While originally thought to primarily combat “internal covariate shift”, recent research suggests BN’s effectiveness might be more related to smoothing the optimization landscape.)"
  },
  {
    "objectID": "course/why-applied-soft-computing.html",
    "href": "course/why-applied-soft-computing.html",
    "title": "Why applied soft computing?",
    "section": "",
    "text": "Imagine trying to explain to someone how you recognize your friend’s face. Sure, you do it instantly - but try writing down the exact rules! Should you measure the eye spacing? Check nose shape? It’s nearly impossible to write rigid rules for something our brains do effortlessly.\nThis is where neural networks come in - they learn and adapt like our brains, without needing exact rules.",
    "crumbs": [
      "Home",
      "Course Information",
      "Why applied soft computing?"
    ]
  },
  {
    "objectID": "course/why-applied-soft-computing.html#mind-blowing-neural-network-achievements",
    "href": "course/why-applied-soft-computing.html#mind-blowing-neural-network-achievements",
    "title": "Why applied soft computing?",
    "section": "Mind-Blowing Neural Network Achievements",
    "text": "Mind-Blowing Neural Network Achievements\n\nThe AI Artist Who Beat Human Artists\n\n\n\nAI-generated artwork that won first place in a digital art competition\n\n\nIn 2022, something unexpected happened in the art world: an AI-generated artwork won first place in a digital art competition, beating out human artists! The creator, Jason Allen, used Midjourney AI to generate the winning piece after 80 hours of careful prompting. While critics claimed it was “just pressing buttons,” the win sparked a huge debate about the future of art.\n\n\nFaces That Don’t Exist\n\n\n\nA human face generated by StyleGAN\n\n\nVisit ThisPersonDoesNotExist.com and you’ll see something uncanny - incredibly realistic human faces that never existed! Each refresh shows a new face created by StyleGAN, complete with unique features, expressions, and even tiny details like skin pores. The wild part? Even experts sometimes can’t tell these AI-generated faces from real photos!\n\n\nChatGPT: The AI That Talks Like Us\n\n\n\nChatGPT interface\n\n\nWhen ChatGPT appeared, it shocked everyone with its human-like conversations. It doesn’t just answer questions - it writes poetry, explains complex topics, helps with coding, and even gets jokes! While earlier chatbots were obviously robotic, ChatGPT’s natural responses often make people wonder if they’re really chatting with an AI.\n\n\nThe 50-Year Puzzle Solver\n\n\n\nAlphaFold logo and interface\n\n\nScientists struggled for 50 years to predict how proteins fold - a crucial problem in biology. Then came AlphaFold, which not only solved the problem but did it with near-perfect accuracy! This task was thought to be so complex that it would take decades more to solve. Instead, AlphaFold did in days what used to take months in laboratories.\n\n\nThe Go Master’s Impossible Move\n\n\n\nAlphaGo versus Lee Sedol\n\n\nThe ancient game of Go was considered too complex for AI - until AlphaGo shocked the world by defeating champion Lee Sedol. But the real surprise came in Game 2, with “Move 37” - a play so creative and unexpected that Go experts initially thought it was a mistake! This move, later described as “神の一手” (the divine move), showed that AI could think in ways humans never imagined.\n\n\nSora: Making Movies from Words\n\n\n\nSora video generation example\n\n\nJust when we thought AI couldn’t get more impressive, OpenAI’s Sora arrived in 2024, turning text into realistic 60-second videos. The shocking part? These aren’t just simple animations - they’re physics-accurate scenes with multiple moving elements that look like they were filmed in the real world. It’s like having a movie studio in your pocket!\n\n\nThe Doctor That Sees More\n\n\n\nMedical imaging AI\n\n\nAI systems can now spot cancer in medical scans better than human doctors. Google Health’s system proved more accurate than radiologists at detecting breast cancer, reducing both missed cases and false alarms. It’s not replacing doctors, but it’s giving them a super-powered second opinion.\n\n\nCars That Drive Better Than Us\n\n\n\nTesla self-driving car\n\n\nSelf-driving cars were once science fiction. Now, neural networks help them process information from multiple sensors faster than any human could, making split-second decisions to avoid accidents. In many conditions, they’re already safer drivers than humans, reacting faster and staying alert 100% of the time.",
    "crumbs": [
      "Home",
      "Course Information",
      "Why applied soft computing?"
    ]
  },
  {
    "objectID": "course/why-applied-soft-computing.html#why-this-matters",
    "href": "course/why-applied-soft-computing.html#why-this-matters",
    "title": "Why applied soft computing?",
    "section": "Why This Matters",
    "text": "Why This Matters\nWhat’s truly remarkable is that most of these breakthroughs happened in just the last few years - within our lifetime! Tasks that experts thought would take decades to solve are being conquered by neural networks at an incredible pace. They’re not just matching human abilities - they’re surpassing them in ways that surprise even the leading researchers. ?",
    "crumbs": [
      "Home",
      "Course Information",
      "Why applied soft computing?"
    ]
  },
  {
    "objectID": "course/how-to-submit-assignment.html",
    "href": "course/how-to-submit-assignment.html",
    "title": "How to submit assignment",
    "section": "",
    "text": "In this course, we will use GitHub Classroom to submit & grade assignments. Please follow the instructions below to submit your assignment.",
    "crumbs": [
      "Home",
      "Course Information",
      "How to submit assignment"
    ]
  },
  {
    "objectID": "course/how-to-submit-assignment.html#option-1-a-simple-workflow-full-local",
    "href": "course/how-to-submit-assignment.html#option-1-a-simple-workflow-full-local",
    "title": "How to submit assignment",
    "section": "Option 1: A simple workflow (Full local)",
    "text": "Option 1: A simple workflow (Full local)\nSee the slides for the detailed instructions.\n\nClone the repository from GitHub.\nEdit the assignment.py with marimo editor. Type marimo edit assignment/assignment.py\nSubmit the assignment.py via git. (You can use GitHub Desktop, or command line)\nCheck the grading on the GitHub Classroom.",
    "crumbs": [
      "Home",
      "Course Information",
      "How to submit assignment"
    ]
  },
  {
    "objectID": "course/how-to-submit-assignment.html#option-2-github-codespaces-full-cloud",
    "href": "course/how-to-submit-assignment.html#option-2-github-codespaces-full-cloud",
    "title": "How to submit assignment",
    "section": "Option 2: Github Codespaces (Full cloud)",
    "text": "Option 2: Github Codespaces (Full cloud)\nSee the slides for the detailed instructions.\n\nGo to your assignment repository on GitHub\nClick the green “Code” button\nClick the “Open with Codespaces” button\nWait for the Codespaces to be ready.\nType ‘marimo edit assignment/assignment.py’. If you cannot find marimo, type “uv run marimo edit assignment/assignment.py” which should work.\nYou will be redirected to a webpage and prompted to enter the access token. The access token can be found on the terminal window in the Codespaces.\nTake the access token in the url “the alphabets after”?access_token=” and enter the token in the webpage.",
    "crumbs": [
      "Home",
      "Course Information",
      "How to submit assignment"
    ]
  },
  {
    "objectID": "course/how-to-submit-assignment.html#option-3-local-but-with-docker-machine",
    "href": "course/how-to-submit-assignment.html#option-3-local-but-with-docker-machine",
    "title": "How to submit assignment",
    "section": "Option 3: Local but with Docker Machine",
    "text": "Option 3: Local but with Docker Machine\nSee the slides for the detailed instructions.\n\nPreparations\n\nInstall Docker Desktop\nInstall GitHub Desktop\nInstall VS Code\n\n\n\nSteps\n\nClone the repository from GitHub.\nOpen with the VS Code, and click “Reopen in Container”\nOpen the assignment.py with marimo editor.\nSubmit the assignment.py to the repository.",
    "crumbs": [
      "Home",
      "Course Information",
      "How to submit assignment"
    ]
  },
  {
    "objectID": "course/deliverables.html",
    "href": "course/deliverables.html",
    "title": "Deliverables",
    "section": "",
    "text": "Project reports are not solely focused on the final results, but also on the process and decisions made along the way. We expect to hear the reasons for your final decisions, for instance the reason why you choose X, over alternative options like Y.\n\nClarify the objectives and goal of your project. What do you want to do it, and why are your questions important to us?\nProvide a detailed description about the data you will use. Where the data are collected from, how they are compiled and preprocessed for your analysis. What are the data type of your focal features, and what features do you think are relevant for your analysis?\nDetermine the appropriate methods. Additionally, consider discussing the methods used in previous studies. Considering the data types and the information you aim to present, what methods could potentially be suitable? It would also be beneficial to explore what approaches others have taken when working with similar datasets.\nClarify the limitation and advantage of your approach. The limitation and advantage stems from data and methodologies, and must be discussed in light of existing works. For instance, you want to develop a link prediction algorithm for a social network based on the common neighbor approach. What are the fundamental assumption underlying the link prediction algorithms? When does the algorithm fail? Can you think of the advantage of your algorithm over other alternatives such as graph neural networks?\nEmbrace failures. As Thomas Edison famously said, “I have not failed. I’ve just found 10,000 ways that won’t work.” In many cases, works and analyses may appear to follow a single pathway, but it is important to recognize that this is just one of many paths that people have taken, many of which have turned out to be unsuccessful. It is crucial to try out multiple candidates, and more importantly, to document your failures and understand why they did not work. Consider using fake data, small subsets, mock-ups, and sketches. These methods can help you iterate and refine your approach, ultimately leading to more successful outcomes.",
    "crumbs": [
      "Home",
      "Course Information",
      "Deliverables"
    ]
  },
  {
    "objectID": "course/deliverables.html#general-remarks-on-the-project-reports",
    "href": "course/deliverables.html#general-remarks-on-the-project-reports",
    "title": "Deliverables",
    "section": "",
    "text": "Project reports are not solely focused on the final results, but also on the process and decisions made along the way. We expect to hear the reasons for your final decisions, for instance the reason why you choose X, over alternative options like Y.\n\nClarify the objectives and goal of your project. What do you want to do it, and why are your questions important to us?\nProvide a detailed description about the data you will use. Where the data are collected from, how they are compiled and preprocessed for your analysis. What are the data type of your focal features, and what features do you think are relevant for your analysis?\nDetermine the appropriate methods. Additionally, consider discussing the methods used in previous studies. Considering the data types and the information you aim to present, what methods could potentially be suitable? It would also be beneficial to explore what approaches others have taken when working with similar datasets.\nClarify the limitation and advantage of your approach. The limitation and advantage stems from data and methodologies, and must be discussed in light of existing works. For instance, you want to develop a link prediction algorithm for a social network based on the common neighbor approach. What are the fundamental assumption underlying the link prediction algorithms? When does the algorithm fail? Can you think of the advantage of your algorithm over other alternatives such as graph neural networks?\nEmbrace failures. As Thomas Edison famously said, “I have not failed. I’ve just found 10,000 ways that won’t work.” In many cases, works and analyses may appear to follow a single pathway, but it is important to recognize that this is just one of many paths that people have taken, many of which have turned out to be unsuccessful. It is crucial to try out multiple candidates, and more importantly, to document your failures and understand why they did not work. Consider using fake data, small subsets, mock-ups, and sketches. These methods can help you iterate and refine your approach, ultimately leading to more successful outcomes.",
    "crumbs": [
      "Home",
      "Course Information",
      "Deliverables"
    ]
  },
  {
    "objectID": "course/deliverables.html#proposal",
    "href": "course/deliverables.html#proposal",
    "title": "Deliverables",
    "section": "Proposal",
    "text": "Proposal\nA document should include the following sections:\n\nProject Title\nTeam Members (1-4 people; keep in mind that a larger team is expected to accomplish more than a smaller one)\nAbstract: A concise summary of your project.\nIntroduction: Provide motivation, background, and objectives for your project. Explain why it is important or interesting and why others should care. Review and discuss relevant existing works, particularly those that have inspired your project. Critique these works substantively. Remember, there is always a wealth of relevant work available.\nQuestions or Objectives: Specify the methods you plan to create and what you hope to discover from the data.\nDatasets and Methods: Identify the dataset you will be using. If you haven’t done so already, I strongly encourage you to reconsider your project. Obtaining and cleaning datasets can be time-consuming. Describe the dataset, including its structure and data types if it is tabular. Explain the methods you plan to apply and why you have chosen them. Finally, provide detailed information about the dataset to convincingly argue that it is suitable for your project and proposed methods.\n\n\nReferences",
    "crumbs": [
      "Home",
      "Course Information",
      "Deliverables"
    ]
  },
  {
    "objectID": "course/deliverables.html#final-presentation",
    "href": "course/deliverables.html#final-presentation",
    "title": "Deliverables",
    "section": "Final presentation",
    "text": "Final presentation\nPlease create a 10-minute video (please adhere to the time limit) and upload it to YouTube. You have the option to either publish it or make it unlisted. The video can be in any format you prefer. Make sure to include a thorough analysis while also making it interesting and enjoyable! The video will be evaluated based on three criteria: (i) the strength of the case you present, (ii) the quality of your analysis, and (iii) the production and delivery of your presentation.\nOnce you have completed your video, feel free to share it on Slack and receive feedback from your fellow students and instructors. It’s always beneficial to see what others have accomplished, so I highly encourage you to share your work!",
    "crumbs": [
      "Home",
      "Course Information",
      "Deliverables"
    ]
  },
  {
    "objectID": "course/deliverables.html#final-report",
    "href": "course/deliverables.html#final-report",
    "title": "Deliverables",
    "section": "Final report",
    "text": "Final report\nYou will need to submit your code and a report on your work. Ideally, your code will be in well-documented Jupyter notebooks (e.g. see Peter Norvig’s notebooks or good Kaggle exploratory data visualization kernels).\nThe report has no minimum or maximum length, but you need to make sure all the topics are thoroughly addressed in clear writing. The format and ingredients for the final report will depend on the types of projects that you do.\nIf the project is more about creating a software package or a website, then the report may focus more on the technical aspects of the project.",
    "crumbs": [
      "Home",
      "Course Information",
      "Deliverables"
    ]
  },
  {
    "objectID": "course/deliverables.html#idea-sketch-template",
    "href": "course/deliverables.html#idea-sketch-template",
    "title": "Deliverables",
    "section": "Idea sketch template",
    "text": "Idea sketch template\nThe followings are the list of questions I personally use before starting a project. Every idea is nebulous when it comes to a mind. We can materialize it by writing down the ideas. It’s surprisingly hard to write it down first, and you will realize a lot of things. In sum, writing is thinking. It serves as a scaffolding to think through a research project. These list of questions are a living document, and you will constantly update as the project progresses.\nAnswer each question in 2~3 sentences. I usually set a timer for 15 mins for each. If one of the questions takes more than 15 mins, it’s the weakness of the idea of the current form.\n\nProject Overview: What is the core focus of your project? Are you developing something new or testing existing ideas?\nProject Value: What makes this work meaningful and worth pursuing?\nResearch Gaps: What key questions or problems remain unsolved in this area?\nNovel Approach: What makes your proposed solution unique and different from existing methods?\nNecessity: Why develop a new solution if existing methods exist? What advantages does your approach offer?\nSuccess Metrics: How will you define and measure success for this project?\nValidation Strategy: What specific criteria or tests will demonstrate that your solution works?\nBroader Impact: How could this work benefit fields beyond your immediate research area?\nImplementation Plan: Break down each project goal into ~3 concrete, actionable tasks.",
    "crumbs": [
      "Home",
      "Course Information",
      "Deliverables"
    ]
  },
  {
    "objectID": "m06-graph/overview.html",
    "href": "m06-graph/overview.html",
    "title": "Module 6: Deep Learning for Graphs",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module extends deep learning from regular grids to irregular structures. We explore how convolution generalizes to graphs, building on your knowledge of CNNs to tackle networks with varying connectivity patterns. You will understand both spectral and spatial perspectives on graph neural networks, and learn how to embed networks into continuous vector spaces.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Overview"
    ]
  },
  {
    "objectID": "m06-graph/overview.html#from-pixels-to-nodes",
    "href": "m06-graph/overview.html#from-pixels-to-nodes",
    "title": "Module 6: Deep Learning for Graphs",
    "section": "From Pixels to Nodes",
    "text": "From Pixels to Nodes\nYou have learned how convolutional neural networks process images. A kernel slides across a regular grid, extracting features through local operations. The grid structure makes this straightforward: every pixel (except boundaries) has the same number of neighbors, and the same kernel applies everywhere.\nNetworks shatter this regularity. Consider a social network where some people have 5 friends and others have 500. Or a molecule where atoms have varying numbers of bonds. How do we define convolution when neighborhoods have wildly different sizes? How do we share parameters across nodes with different connectivity patterns?\nThis module answers these questions. We extend the principles that make CNNs powerful (locality, parameter sharing, hierarchical features) to irregular graph structures. The journey reveals that convolution is not about grids. It is about relationships, and relationships exist everywhere.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Overview"
    ]
  },
  {
    "objectID": "m06-graph/overview.html#the-module-roadmap",
    "href": "m06-graph/overview.html#the-module-roadmap",
    "title": "Module 6: Deep Learning for Graphs",
    "section": "The Module Roadmap",
    "text": "The Module Roadmap\nWe approach graph neural networks through a carefully constructed narrative that builds on what you already know.\nPart 1: From Images to Graphs introduces the pixel-node analogy and explains why irregular structure poses challenges. We preview two complementary perspectives: spectral methods that define convolution in frequency domains, and spatial methods that aggregate information from local neighborhoods.\nPart 2: The Spectral Perspective explores how graphs have their own notion of frequency. The graph Laplacian’s eigenvectors serve as basis functions, and eigenvalues indicate how rapidly node features vary across edges. We design spectral filters that control which frequencies pass through, and build learnable spectral graph convolutional networks. We also examine why computational cost and lack of spatial locality motivate spatial approaches.\nPart 3: Spatial Graph Networks defines convolution as neighborhood aggregation. We see how ChebNet bridges spectral and spatial domains, how GCN achieves radical simplification, and how modern architectures like GraphSAGE, GAT, and GIN push boundaries. GraphSAGE samples neighborhoods for scalability. Graph Attention Networks learn which neighbors matter most. Graph Isomorphism Networks maximize discriminative power through careful aggregation design.\nPart 4: Graph Embeddings shifts from supervised learning to representation learning. We explore both spectral embeddings rooted in eigendecomposition and neural embeddings inspired by word2vec. Random walks transform graphs into sequences, enabling us to treat nodes as words and apply language modeling techniques. These embeddings enable clustering, visualization, and transfer learning across tasks.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Overview"
    ]
  },
  {
    "objectID": "m06-graph/overview.html#why-this-matters",
    "href": "m06-graph/overview.html#why-this-matters",
    "title": "Module 6: Deep Learning for Graphs",
    "section": "Why This Matters",
    "text": "Why This Matters\nGraph-structured data appears everywhere. Social networks capture friendships and influence. Knowledge graphs encode facts and relationships. Molecules represent atoms and bonds. Citation networks link papers and ideas. Protein interaction networks reveal biological pathways.\nTraditional machine learning struggles with graphs. Most algorithms assume feature vectors in Euclidean space, but graphs are discrete, irregular, and high-dimensional. Graph neural networks solve this by learning representations that preserve structure while enabling standard machine learning techniques.\nThe applications are transformative. Drug discovery benefits from predicting molecular properties. Recommendation systems leverage social network structure. Fraud detection identifies suspicious patterns in transaction graphs. Scientific discovery accelerates by mining knowledge graphs for hidden connections.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Overview"
    ]
  },
  {
    "objectID": "m06-graph/overview.html#what-to-expect",
    "href": "m06-graph/overview.html#what-to-expect",
    "title": "Module 6: Deep Learning for Graphs",
    "section": "What to Expect",
    "text": "What to Expect\nThis module assumes you understand basic linear algebra (matrices, eigenvalues, eigenvectors) and neural networks (layers, activations, backpropagation). We build heavily on your knowledge of CNNs from Module 5, drawing parallels between image convolution and graph convolution.\nThe mathematics is more involved than previous modules. Spectral graph theory connects linear algebra to networks. Understanding eigendecompositions and Laplacian matrices requires careful attention. But the payoff is worth it. These concepts reveal deep connections between seemingly disparate ideas: Fourier analysis, random walks, and neural networks.\nThe coding examples use real networks like the Karate Club. You will implement spectral embeddings, train DeepWalk models, and visualize learned representations. These hands-on experiences cement abstract concepts and show how theory translates to practice.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Overview"
    ]
  },
  {
    "objectID": "m06-graph/overview.html#the-bigger-picture",
    "href": "m06-graph/overview.html#the-bigger-picture",
    "title": "Module 6: Deep Learning for Graphs",
    "section": "The Bigger Picture",
    "text": "The Bigger Picture\nThis module represents a frontier in deep learning. CNNs revolutionized computer vision by exploiting grid structure. Transformers revolutionized NLP by modeling token sequences. Graph neural networks extend these successes to irregular structures, unlocking applications where relationships matter as much as features.\nThe principles transcend specific architectures. Locality enables learning from neighbors without considering the entire graph. Parameter sharing allows models trained on small networks to generalize to large ones. Hierarchical features extract increasingly abstract patterns through stacked layers.\nThese ideas connect to broader themes in machine learning. Inductive biases (built-in assumptions about structure) determine what models can learn efficiently. The right representation (embeddings that preserve relevant properties) enables simple algorithms to solve complex tasks. Domain knowledge (understanding graph structure) guides architecture design.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Overview"
    ]
  },
  {
    "objectID": "m06-graph/overview.html#getting-started",
    "href": "m06-graph/overview.html#getting-started",
    "title": "Module 6: Deep Learning for Graphs",
    "section": "Getting Started",
    "text": "Getting Started\nBegin with Part 1: From Images to Graphs to understand the core challenge and preview the solution strategies. The narrative builds systematically, so follow the parts in order for maximum clarity.\nEach part includes visualizations, examples, and derivations. Take time to understand the intuitions before diving into mathematical details. The equations formalize ideas that make sense geometrically: smoothness, frequency, aggregation, attention.\nBy the end of this module, you will understand how to extend deep learning beyond regular structures. You will see connections between spectral graph theory, signal processing, and neural networks. You will appreciate why graph neural networks have become indispensable tools for modern machine learning.\nThe journey from pixels to nodes reveals a profound truth: the core principles of deep learning transcend any particular data structure. Wherever relationships exist, we can learn.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Overview"
    ]
  },
  {
    "objectID": "m06-graph/from-image-to-graph.html",
    "href": "m06-graph/from-image-to-graph.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "title: “From image to graph”\n\n\njupyter: advnetsci\n\n\nexecute:\n\n\nenabled: true"
  },
  {
    "objectID": "m06-graph/from-image-to-graph.html#analogy-between-image-and-graph-data",
    "href": "m06-graph/from-image-to-graph.html#analogy-between-image-and-graph-data",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "Analogy between image and graph data",
    "text": "Analogy between image and graph data\nWe can think of a convolution of an image from the perspective of networks. In the convolution of an image, a pixel is convolved with its neighbors. We can regard each pixel as a node, and each node is connected to its neighboring nodes (pixels) that are involved in the convolution.\n\nBuilding on this analogy, we can extend the idea of convolution to general graph data. Each node has a pixel value(s) (e.g., feature vector), which is convolved with the values of its neighbors in the graph. This is the key idea of graph convolutional networks. But, there is a key difference: while the number of neighbors for an image is homogeneous, the number of neighbors for a node in a graph can be heterogeneous. Each pixel has the same number of neighbors (except for the boundary pixels), but nodes in a graph can have very different numbers of neighbors. This makes it non-trivial to define the “kernel” for graph convolution."
  },
  {
    "objectID": "m06-graph/from-image-to-graph.html#spectral-filter-on-graphs",
    "href": "m06-graph/from-image-to-graph.html#spectral-filter-on-graphs",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "Spectral filter on graphs",
    "text": "Spectral filter on graphs\nJust like we can define a convolution on images in the frequency domain, we can also define a ‘’frequency domain’’ for graphs.\nConsider a network of N nodes, where each node has a feature variable {\\mathbf x}_i \\in \\mathbb{R}. We are interested in:\n\nJ = \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N A_{ij}(x_i - x_j)^2,\n\nwhere A_{ij} is the adjacency matrix of the graph. The quantity J represents the total variation of x between connected nodes; a small J means that connected nodes have similar x (low variation; low frequency), while a large J means that connected nodes have very different x (high variation; high frequency).\nWe can rewrite J as\n\nJ = \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N A_{ij}(x_i - x_j)^2 = {\\bf x}^\\top {\\bf L} {\\bf x},\n\nwhere {\\bf L} is the Laplacian matrix of the graph given by\n\nL_{ij} = \\begin{cases}\n-1 & \\text{if } i \\text{ and } j \\text{ are connected} \\\\\nk_i & \\text{if } i = j \\\\\n0 & \\text{otherwise}\n\\end{cases}.\n\nand {\\bf x} = [x_1,x_2,\\ldots, x_N]^\\top is a column vector of feature variables.\n\n\n\n\n\n\nDetailed derivation\n\n\n\nThe above derivation shows that the total variation of x between connected nodes is proportional to {\\bf x}^\\top {\\bf L} {\\bf x}.\n\n\\begin{aligned}\nJ &= \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N A_{ij}(x_i - x_j)^2 \\\\\n&= \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N \\underbrace{A_{ij}\\left( x_i^2 +x_j^2\\right)}_{\\text{symmetric}} - \\sum_{i=1}^N\\sum_{j=1}^N A_{ij}x_ix_j \\\\\n&= \\sum_{i=1}^Nx_i^2\\underbrace{\\sum_{j=1}^N A_{ij}}_{\\text{degree of node } i, k_i} - \\sum_{i=1}^N\\sum_{j=1}^N A_{ij}x_ix_j \\\\\n&= \\sum_{i=1}^Nx_i^2 k_i - \\sum_{i=1}^N\\sum_{j=1}^N A_{ij}x_ix_j \\\\\n&= \\underbrace{[x_1,x_2,\\ldots, x_N]}_{{\\bf x}} \\underbrace{\\begin{bmatrix} k_1 & 0 & \\cdots & 0 \\\\ 0 & k_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & k_N \\end{bmatrix}}_{{\\bf D}} \\underbrace{\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N \\end{bmatrix}}_{{\\bf x}} - 2\\underbrace{\\sum_{i=1}^N\\sum_{j=1}^N A_{ij}}_{{\\bf x}^\\top {\\mathbf A} {\\bf x}} {\\bf x} \\\\\n&= {\\bf x}^\\top {\\bf D} {\\bf x} - {\\bf x}^\\top {\\mathbf A} {\\bf x} \\\\\n&= {\\bf x}^\\top {\\bf L} {\\bf x},\n\\end{aligned}\n\n\n\nLet us showcase the analogy between the Fourier transform and the Laplacian matrix. In the Fourier transform, a signal is decomposed into sinusoidal basis functions. Similarly, for a graph, we can decompose the variation J into eigenvector bases.\n\nJ = \\sum_{i=1}^N \\lambda_i  {\\bf x}^\\top {\\mathbf u}_i {\\mathbf u}_i^\\top {\\bf x} = \\sum_{i=1}^N \\lambda_i  ||{\\bf x}^\\top {\\mathbf u}_i||^2.\n\nwhere {\\mathbf u}_i is the eigenvector corresponding to the eigenvalue \\lambda_i. ^_i)$ is a dot-product between the feature vector {\\bf x} and the eigenvector {\\mathbf u}_i, which measures how much {\\bf x} coheres with eigenvector {\\mathbf u}_i, similar to how Fourier coefficients measure coherency with sinusoids. - Each ||{\\bf x}^\\top {\\mathbf u}_i||^2 is the ‘’strength’’ of {\\bf x} with respect to the eigenvector {\\mathbf u}_i, and the total variation J is a weighted sum of these strengths.\nSome eigenvectors correspond to low-frequency components, while others correspond to high-frequency components. For example, the total variation J for an eigenvector {\\mathbf u}_i is given by\n\nJ = \\frac{1}{2} \\sum_{j}\\sum_{\\ell} A_{j\\ell}(u_{ij} - u_{i\\ell})^2 = {\\mathbf u}_i^\\top {\\mathbf L} {\\mathbf u}_i = \\lambda_i.\n\nThis equation provides key insight into the meaning of eigenvalues:\n\nFor an eigenvector {\\mathbf u}_i, its eigenvalue \\lambda_i measures the total variation for {\\mathbf u}_i.\nLarge eigenvalues mean large differences between neighbors (high frequency), while small eigenvalues mean small differences (low frequency).\n\nThus, if {\\bf x} aligns well with {\\mathbf u}_i with a large \\lambda_i, then {\\bf x} has a strong high-frequency component; if {\\bf x} aligns well with {\\mathbf u}_i with a small \\lambda_i, then {\\bf x} has strong low-frequency component.\n\nSpectral Filtering\nEigenvalues \\lambda_i can be thought of as a filter that controls which frequency components pass through. Instead of using the filter associated with the Laplacian matrix, we can design a filter h(\\lambda_i) to control which frequency components pass through. This leads to the idea of spectral filtering. Two common filters are:\n\nLow-pass Filter: h_{\\text{low}}(\\lambda) = \\frac{1}{1 + \\alpha\\lambda}\n\nPreserves low frequencies (small λ)\nSuppresses high frequencies (large λ)\nResults in smoother signals\n\nHigh-pass Filter: h_{\\text{high}}(\\lambda) = \\frac{\\alpha\\lambda}{1 + \\alpha\\lambda}\n\nPreserves high frequencies\nSuppresses low frequencies\nEmphasizes differences between neighbors"
  },
  {
    "objectID": "m06-graph/from-image-to-graph.html#spectral-graph-convolutional-networks",
    "href": "m06-graph/from-image-to-graph.html#spectral-graph-convolutional-networks",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "Spectral Graph Convolutional Networks",
    "text": "Spectral Graph Convolutional Networks\nA simplest form of learnable spectral filter is given by\n\n{\\bf L}_{\\text{learn}} = \\sum_{k=1}^K \\theta_k {\\mathbf u}_k {\\mathbf u}_k^\\top,\n\nwhere {\\mathbf u}_k are the eigenvectors and \\theta_k are the learnable parameters. The variable K is the number of eigenvectors used (i.e., the rank of the filter). The weight \\theta_k is learned to maximize the performance of the task at hand.\nBuilding on this idea, (Bruna et al. 2014) added a nonlinearity to the filter and proposed a spectral convolutional neural network (GCN) by\n\n{\\bf x}^{(\\ell+1)} = h\\left( L_{\\text{learn}} {\\bf x}^{(\\ell)}\\right),\n\nwhere h is an activation function, and {\\bf x}^{(\\ell)} is the feature vector of the \\ell-th convolution. They further extend this idea to convolve on multidimensional feature vectors, {\\bf X} \\in \\mathbb{R}^{N \\times f_{\\text{in}}} to produce new feature vectors of different dimensionality, {\\bf X}' \\in \\mathbb{R}^{N \\times f_{\\text{out}}}.\n\n\\begin{aligned}\n{\\bf X}^{(\\ell+1)}_i &= h\\left( \\sum_j L_{\\text{learn}}^{(i,j)} {\\bf X}^{(\\ell)}_j\\right),\\quad \\text{where} \\quad L^{(i,j)}_{\\text{learn}} = \\sum_{k=1}^K \\theta_{k, (i,j)} {\\mathbf u}_k {\\mathbf u}_k^\\top,\n\\end{aligned}\n\nNotice that the learnable filter L_{\\text{learn}}^{(i,j)} is defined for each pair of input i and output j dimensions.\n\n\nMany GCNs simple when it comes to implementation despite the complicated formula. And this is one of my ways to learn GNNs. Check out the Appendix for the Python implementation."
  },
  {
    "objectID": "m06-graph/from-image-to-graph.html#from-spectral-to-spatial",
    "href": "m06-graph/from-image-to-graph.html#from-spectral-to-spatial",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "From Spectral to Spatial",
    "text": "From Spectral to Spatial\nSpectral GCNs are mathematically elegant but have two main limitations: 1. Computational Limitation: Computing the spectra of the Laplacian is expensive {\\cal O}(N^3) and prohibitive for large graphs 2. Spatial Locality: The learned filters are not spatially localized. A node can be influenced by all other nodes in the graph.\nThese two limitations motivate the development of spatial GCNs."
  },
  {
    "objectID": "m06-graph/03-spatial-networks.html",
    "href": "m06-graph/03-spatial-networks.html",
    "title": "Spatial Graph Networks",
    "section": "",
    "text": "What you’ll learn in this part\n\n\n\nThis part explores spatial approaches to graph convolution. We see how ChebNet bridges spectral and spatial domains, how GCN achieves radical simplification, and how modern architectures like GraphSAGE, GAT, and GIN push the boundaries of what graph networks can learn.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 3: Spatial Graph Networks"
    ]
  },
  {
    "objectID": "m06-graph/03-spatial-networks.html#from-spectral-to-spatial",
    "href": "m06-graph/03-spatial-networks.html#from-spectral-to-spatial",
    "title": "Spatial Graph Networks",
    "section": "From Spectral to Spatial",
    "text": "From Spectral to Spatial\nThe spectral perspective offers mathematical elegance but faces practical challenges. Computing eigendecomposition scales poorly to large graphs. Spectral filters lack spatial locality, allowing distant nodes to directly influence each other.\nSpatial methods take a different approach. Instead of transforming to frequency domains, they define convolution directly as aggregating features from local neighborhoods. Think of it as message passing: each node collects information from its neighbors, combines it, and produces an updated representation.\nThis shift brings immediate benefits. Spatial methods scale linearly with the number of edges. They preserve locality by design. They generalize naturally to unseen nodes. But we must carefully design how nodes aggregate neighbor information.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 3: Spatial Graph Networks"
    ]
  },
  {
    "objectID": "m06-graph/03-spatial-networks.html#chebnet-the-bridge",
    "href": "m06-graph/03-spatial-networks.html#chebnet-the-bridge",
    "title": "Spatial Graph Networks",
    "section": "ChebNet: The Bridge",
    "text": "ChebNet: The Bridge\nChebNet (Defferrard, Bresson, and Vandergheynst 2016) bridges spectral and spatial domains using Chebyshev polynomials. The key insight is that we can approximate spectral filters without computing full eigendecompositions.\nRecall the learnable spectral filter:\n\n{\\bf L}_{\\text{learn}} = \\sum_{k=1}^K \\theta_k {\\bf u}_k {\\bf u}_k^\\top\n\nChebNet approximates this using Chebyshev polynomials of the scaled Laplacian \\tilde{{\\bf L}} = \\frac{2}{\\lambda_{\\text{max}}}{\\bf L} - {\\bf I}:\n\n{\\bf L}_{\\text{learn}} \\approx \\sum_{k=0}^{K-1} \\theta_k T_k(\\tilde{{\\bf L}})\n\nwhere T_k are Chebyshev polynomials defined recursively:\n\n\\begin{aligned}\nT_0(\\tilde{{\\bf L}}) &= {\\bf I} \\\\\nT_1(\\tilde{{\\bf L}}) &= \\tilde{{\\bf L}} \\\\\nT_k(\\tilde{{\\bf L}}) &= 2\\tilde{{\\bf L}} T_{k-1}(\\tilde{{\\bf L}}) - T_{k-2}(\\tilde{{\\bf L}})\n\\end{aligned}\n\nThe convolution becomes:\n\n{\\bf x}^{(\\ell+1)} = h\\left( \\sum_{k=0}^{K-1} \\theta_k T_k(\\tilde{{\\bf L}}){\\bf x}^{(\\ell)}\\right)\n\nWhy does this help? The crucial property is that multiplying by \\tilde{{\\bf L}} is a local operation. Computing T_k(\\tilde{{\\bf L}}){\\bf x} only involves nodes within k hops of each neighborhood. This achieves spatial locality without explicit eigendecomposition.\nChebNet typically uses small K (e.g., K=3), limiting receptive fields to nearby neighbors. Deeper networks expand receptive fields by stacking layers.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 3: Spatial Graph Networks"
    ]
  },
  {
    "objectID": "m06-graph/03-spatial-networks.html#gcn-radical-simplification",
    "href": "m06-graph/03-spatial-networks.html#gcn-radical-simplification",
    "title": "Spatial Graph Networks",
    "section": "GCN: Radical Simplification",
    "text": "GCN: Radical Simplification\nWhile ChebNet offers principled spectral approximation, Kipf and Welling (2017) proposed an even simpler variant that became one of the most widely used graph neural networks.\n\nFirst-Order Approximation\nThe key departure is using only the first-order Chebyshev approximation. Setting K=1 and making additional simplifications yields:\n\ng_{\\theta} * x \\approx \\theta({\\bf I}_N + {\\bf D}^{-\\frac{1}{2}}{\\bf A}{\\bf D}^{-\\frac{1}{2}})x\n\nwhere {\\bf D} is the degree matrix and {\\bf A} is the adjacency matrix. This leaves a single learnable parameter \\theta instead of K parameters.\n\n\nRenormalization Trick\nTo stabilize training in deep networks, GCN adds self-loops to the graph:\n\n\\tilde{{\\bf A}} = {\\bf A} + {\\bf I}_N, \\quad \\tilde{{\\bf D}}_{ii} = \\sum_j \\tilde{{\\bf A}}_{ij}\n\nThis ensures each node includes its own features when aggregating from neighbors, preventing information loss and mitigating gradient vanishing.\n\n\nLayer-Wise Propagation\nThe final GCN layer becomes remarkably simple:\n\n{\\bf X}^{(\\ell+1)} = \\sigma(\\tilde{{\\bf D}}^{-\\frac{1}{2}}\\tilde{{\\bf A}}\\tilde{{\\bf D}}^{-\\frac{1}{2}}{\\bf X}^{(\\ell)}{\\bf W}^{(\\ell)})\n\nwhere {\\bf X}^{(\\ell)} \\in \\mathbb{R}^{N \\times f_{\\text{in}}} are node features at layer \\ell, {\\bf W}^{(\\ell)} \\in \\mathbb{R}^{f_{\\text{in}} \\times f_{\\text{out}}} is a learnable weight matrix, and \\sigma is a nonlinear activation.\nDespite its simplicity, GCN achieves strong performance on many tasks. The symmetric normalization \\tilde{{\\bf D}}^{-\\frac{1}{2}}\\tilde{{\\bf A}}\\tilde{{\\bf D}}^{-\\frac{1}{2}} ensures numerical stability by normalizing message magnitudes.\n\n\n\n\n\n\nExercise\n\n\n\nImplement a simple GCN model for node classification. Coding Exercise",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 3: Spatial Graph Networks"
    ]
  },
  {
    "objectID": "m06-graph/03-spatial-networks.html#graphsage-sample-and-aggregate",
    "href": "m06-graph/03-spatial-networks.html#graphsage-sample-and-aggregate",
    "title": "Spatial Graph Networks",
    "section": "GraphSAGE: Sample and Aggregate",
    "text": "GraphSAGE: Sample and Aggregate\nGCN processes entire graphs at once, limiting scalability. GraphSAGE (Hamilton, Ying, and Leskovec 2017) introduced an inductive framework that generates embeddings by sampling and aggregating features from neighborhoods.\n\n\nNeighborhood Sampling\nInstead of using all neighbors, GraphSAGE samples a fixed-size set. This controls memory complexity and enables handling dynamic graphs where new nodes arrive continuously.\nConsider a growing citation network. Traditional GCNs require recomputing filters for the entire graph with each new paper. GraphSAGE generates embeddings for new nodes by sampling their neighbors, without retraining.\n\n\nAggregation Functions\nGraphSAGE distinguishes self-information from neighborhood information. First, aggregate neighbor features:\n\n{\\bf h}_{\\mathcal{N}(v)} = \\text{AGGREGATE}(\\{{\\bf h}_u, \\forall u \\in \\mathcal{N}(v)\\})\n\nCommon aggregators include:\n\nMean: \\text{mean}(\\{{\\bf h}_u, \\forall u \\in \\mathcal{N}(v)\\})\nMax-pooling: \\max(\\{\\sigma({\\bf W}_{\\text{pool}}{\\bf h}_u + {\\bf b}), \\forall u \\in \\mathcal{N}(v)\\})\nLSTM: Apply LSTM to randomly permuted neighbors\n\nThen concatenate self and neighborhood information:\n\n{\\bf z}_v = \\text{CONCAT}({\\bf h}_v, {\\bf h}_{\\mathcal{N}(v)})\n\nNormalize and apply the learned transformation:\n\n{\\bf h}_v^{(\\ell+1)} = \\sigma({\\bf W}^{(\\ell)} \\frac{{\\bf z}_v}{\\|{\\bf z}_v\\|_2})\n\nThis explicit separation allows the model to learn how much to trust self versus neighbors.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 3: Spatial Graph Networks"
    ]
  },
  {
    "objectID": "m06-graph/03-spatial-networks.html#gat-learning-attention",
    "href": "m06-graph/03-spatial-networks.html#gat-learning-attention",
    "title": "Spatial Graph Networks",
    "section": "GAT: Learning Attention",
    "text": "GAT: Learning Attention\nShould all neighbors contribute equally? Graph Attention Networks (velickovic2018graph?) let the model learn which neighbors matter most.\n\n\nAttention Mechanism\nGAT computes attention weights \\alpha_{ij} indicating how much node i should attend to node j:\n\n\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}(i)} \\exp(e_{ik})}\n\nwhere e_{ij} measures edge importance. One approach uses a neural network with shared parameters:\n\nTransform node features: \\tilde{{\\bf h}}_i = {\\bf W}{\\bf h}_i\nCompute attention logits: e_{ij} = \\text{LeakyReLU}({\\bf a}^T[\\tilde{{\\bf h}}_i \\| \\tilde{{\\bf h}}_j])\n\nwhere {\\bf a} is a learnable attention vector and \\| denotes concatenation.\n\n\nNode Update\nWith attention weights, the update becomes a weighted sum:\n\n{\\bf h}_i^{(\\ell+1)} = \\sigma\\left(\\sum_{j \\in \\mathcal{N}(i) \\cup \\{i\\}} \\alpha_{ij}{\\bf W}_{\\text{feature}}{\\bf h}_j^{(\\ell)}\\right)\n\n\n\nMulti-Head Attention\nTo stabilize training, GAT uses multiple attention heads and concatenates outputs:\n\n{\\bf h}_i^{(\\ell+1)} = \\|_{k=1}^K \\sigma\\left(\\sum_{j \\in \\mathcal{N}(i) \\cup \\{i\\}} \\alpha_{ij}^k{\\bf W}^k{\\bf h}_j^{(\\ell)}\\right)\n\nDifferent heads can focus on different aspects of neighborhood structure, similar to multi-head attention in Transformers.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 3: Spatial Graph Networks"
    ]
  },
  {
    "objectID": "m06-graph/03-spatial-networks.html#gin-the-power-of-aggregation",
    "href": "m06-graph/03-spatial-networks.html#gin-the-power-of-aggregation",
    "title": "Spatial Graph Networks",
    "section": "GIN: The Power of Aggregation",
    "text": "GIN: The Power of Aggregation\nGraph Isomorphism Networks (Xu et al. 2019) ask a fundamental question: what is the maximum discriminative power of graph neural networks?\n\nThe Weisfeiler-Lehman Test\nThe Weisfeiler-Lehman (WL) test determines if two graphs are structurally identical. It iteratively refines node labels by hashing neighbor labels:\n\nThe process works as follows:\n\nAssign all nodes the same initial label.\nFor each node, collect neighbor labels and create a multiset.\nHash the multiset with the node’s own label to produce a new label.\nRepeat until convergence.\n\nTwo graphs are isomorphic if they have identical label distributions after convergence. The WL test fails on some graphs (like regular graphs) but works well in practice.\n\n\nThe standard WL test is called 1-WL. Higher-order variants exist that can distinguish more graphs, leading to more powerful GNNs. Check out this note for details.\n\n\nGIN’s Key Insight\nThe parallel between WL and GNNs is striking:\n\nWL test: Iteratively aggregate neighbor labels via hash function\nGNN: Iteratively aggregate neighbor features via learned function\n\nThe crucial difference is discriminative power. WL’s hash function always distinguishes different neighbor multisets by counting occurrences. Common GNN aggregators like mean or max can fail. If all neighbors have identical features, these aggregators produce the same output regardless of neighborhood size.\nGIN designs an aggregation that preserves multiset information:\n\n{\\bf h}_v^{(k+1)} = \\text{MLP}^{(k)}\\left((1 + \\epsilon^{(k)}) \\cdot {\\bf h}_v^{(k)} + \\sum_{u \\in \\mathcal{N}(v)} {\\bf h}_u^{(k)}\\right)\n\nwhere \\text{MLP}^{(k)} is a multi-layer perceptron and \\epsilon^{(k)} is a learnable or fixed scalar.\nThe sum aggregation preserves multiset information. The self-loop with weight (1 + \\epsilon) distinguishes nodes from their neighborhoods. The MLP provides sufficient capacity to approximate injective functions, matching WL’s discriminative power.\nXu et al. (2019) prove that GIN can distinguish any graphs that WL can distinguish, making it maximally powerful within the class of message-passing neural networks.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 3: Spatial Graph Networks"
    ]
  },
  {
    "objectID": "m06-graph/03-spatial-networks.html#comparing-approaches",
    "href": "m06-graph/03-spatial-networks.html#comparing-approaches",
    "title": "Spatial Graph Networks",
    "section": "Comparing Approaches",
    "text": "Comparing Approaches\nWe have seen five spatial architectures, each with distinct design choices:\n\n\n\n\n\n\n\n\nMethod\nKey Innovation\nTrade-off\n\n\n\n\nChebNet\nChebyshev approximation\nBridges spectral/spatial, moderate complexity\n\n\nGCN\nRadical simplification\nSimple, efficient, strong baseline\n\n\nGraphSAGE\nSampling + separation\nScalable, inductive, flexible\n\n\nGAT\nLearned attention\nInterpretable, handles heterophily\n\n\nGIN\nInjective aggregation\nMaximum discriminative power\n\n\n\nThe best choice depends on your task. GCN provides a strong baseline. GraphSAGE handles large, dynamic graphs. GAT works when neighbor importance varies. GIN excels at graph-level tasks requiring fine-grained discrimination.\nAll these methods share a common theme. They define convolution as neighborhood aggregation, preserving spatial locality while learning which patterns matter for downstream tasks.\nIn Part 4, we shift from node classification to representation learning. We explore graph embeddings that map nodes to continuous vector spaces, enabling clustering, visualization, and transfer learning across tasks.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 3: Spatial Graph Networks"
    ]
  },
  {
    "objectID": "m06-graph/01-from-images-to-graphs.html",
    "href": "m06-graph/01-from-images-to-graphs.html",
    "title": "From Images to Graphs",
    "section": "",
    "text": "What you’ll learn in this part\n\n\n\nThis part bridges your knowledge of image convolution to graph data. We explore how pixels and nodes are analogous, why graph structure poses unique challenges, and introduce two complementary perspectives: spectral and spatial.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 1: From Images to Graphs"
    ]
  },
  {
    "objectID": "m06-graph/01-from-images-to-graphs.html#the-pixel-node-analogy",
    "href": "m06-graph/01-from-images-to-graphs.html#the-pixel-node-analogy",
    "title": "From Images to Graphs",
    "section": "The Pixel-Node Analogy",
    "text": "The Pixel-Node Analogy\nWe have seen how convolutional neural networks process images by sliding kernels across a regular grid of pixels. Each pixel is convolved with its neighbors to extract features like edges, textures, and patterns.\nLet’s shift our perspective. Think of each pixel not as a grid cell, but as a node in a network. The neighbors involved in convolution become edges connecting that node to nearby nodes.\n\nThis analogy suggests something powerful. If we can define convolution on a regular grid of pixels, perhaps we can extend it to irregular networks where nodes have varying numbers of neighbors. This opens the door to applying deep learning to graph-structured data: social networks, molecules, knowledge graphs, and more.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 1: From Images to Graphs"
    ]
  },
  {
    "objectID": "m06-graph/01-from-images-to-graphs.html#the-challenge-irregular-structure",
    "href": "m06-graph/01-from-images-to-graphs.html#the-challenge-irregular-structure",
    "title": "From Images to Graphs",
    "section": "The Challenge: Irregular Structure",
    "text": "The Challenge: Irregular Structure\nThe analogy breaks down at a critical point. In images, every pixel (except boundaries) has the same number of neighbors. A 3×3 kernel always covers exactly 9 pixels. The convolution operation is translation invariant: the same kernel works everywhere.\nGraphs shatter this regularity. Consider a social network. Some people have 5 friends, others have 500. Some molecules have 3 bonds, others have dozens. The number of neighbors varies wildly across nodes.\nThis irregularity raises fundamental questions. How do we define a “kernel” when neighborhoods have different sizes? How do we share parameters across nodes with vastly different connectivity patterns? How do we preserve the inductive biases that make CNNs so powerful?",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 1: From Images to Graphs"
    ]
  },
  {
    "objectID": "m06-graph/01-from-images-to-graphs.html#two-perspectives-spectral-and-spatial",
    "href": "m06-graph/01-from-images-to-graphs.html#two-perspectives-spectral-and-spatial",
    "title": "From Images to Graphs",
    "section": "Two Perspectives: Spectral and Spatial",
    "text": "Two Perspectives: Spectral and Spatial\nThe research community developed two complementary approaches to graph convolution, each offering unique insights.\n\nThe Spectral Perspective\nRemember the convolution theorem from signal processing? Convolution in the spatial domain corresponds to multiplication in the frequency domain. For images, we use the Fourier transform to decompose signals into sinusoidal basis functions.\nGraphs have their own notion of “frequency.” The graph Laplacian’s eigenvectors serve as basis functions, and eigenvalues indicate how much node features vary across edges. Small eigenvalues correspond to smooth signals (low frequency), where connected nodes have similar values. Large eigenvalues correspond to rapidly varying signals (high frequency), where neighbors differ significantly.\nThe spectral perspective defines graph convolution by designing filters in this frequency domain. We learn which frequencies to amplify or suppress, just as Fourier analysis filters images. This approach is mathematically elegant and connects to decades of spectral graph theory.\n\n\nThe Spatial Perspective\nThe spatial perspective takes a more direct route. Instead of transforming to a frequency domain, it defines convolution as aggregating features from local neighborhoods. Think of it as a message-passing scheme: each node collects information from its neighbors, combines it with its own features, and produces an updated representation.\nThis perspective leads to intuitive architectures. GraphSAGE samples fixed-size neighborhoods to control memory. Graph Attention Networks learn which neighbors to pay attention to. Graph Isomorphism Networks carefully design aggregation functions to maximize discriminative power.\nSpatial methods are often more efficient and easier to scale to large graphs. They also generalize naturally to new nodes not seen during training.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 1: From Images to Graphs"
    ]
  },
  {
    "objectID": "m06-graph/01-from-images-to-graphs.html#looking-ahead",
    "href": "m06-graph/01-from-images-to-graphs.html#looking-ahead",
    "title": "From Images to Graphs",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nWe have already learned what images are and how CNNs process them. Now we face a new frontier: irregular structures where the rules of regular grids no longer apply.\nIn Part 2, we dive into the spectral perspective, exploring how graph Laplacians define frequency domains and how spectral filters enable learnable convolutions.\nIn Part 3, we examine spatial graph networks, from the elegant simplicity of GCN to the sophisticated attention mechanisms of GAT.\nIn Part 4, we explore graph embeddings, learning low-dimensional representations that capture both local and global structure.\nThe journey from pixels to nodes reveals a deeper truth. Convolution is not about grids. It is about locality, parameter sharing, and hierarchical feature extraction. These principles transcend regular structure and apply wherever relationships exist.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 1: From Images to Graphs"
    ]
  },
  {
    "objectID": "m05-images/04-cnn-innovations.html",
    "href": "m05-images/04-cnn-innovations.html",
    "title": "Part 4: The Innovation Timeline",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module traces CNN evolution through successive innovations that solved specific architectural challenges.\nYou’ll learn:\n\nHow VGG demonstrated that depth matters through stacked 3×3 convolutions.\nHow Inception achieved efficiency through multi-scale features and 1×1 convolutions.\nHow ResNet enabled training very deep networks (152 layers) using skip connections.\nHow Vision Transformers replaced convolution with self-attention for global context.\nThe trade-offs between different architectures and how to choose the right one for your project.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 4: CNN Innovations"
    ]
  },
  {
    "objectID": "m05-images/04-cnn-innovations.html#the-quest-for-depth-and-efficiency",
    "href": "m05-images/04-cnn-innovations.html#the-quest-for-depth-and-efficiency",
    "title": "Part 4: The Innovation Timeline",
    "section": "The Quest for Depth and Efficiency",
    "text": "The Quest for Depth and Efficiency\nAlexNet proved that deep learning works at scale in 2012. This breakthrough sparked a race to improve CNN architectures. But simply adding more layers didn’t work. Networks deeper than 20 layers degraded during training. Computational costs exploded. Memory constraints limited model size.\nThe innovations we’ll explore emerged as solutions to these challenges. Each architecture addressed specific problems while introducing ideas that influenced everything that followed. This is not a random collection of models, but a coherent story of progress through clever problem-solving.\n\n\n\n\n\n\nImageNet competition winners from 2012 to 2017. Each year brought architectural innovations that pushed accuracy higher.\n\n\n\n\nFigure 1",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 4: CNN Innovations"
    ]
  },
  {
    "objectID": "m05-images/04-cnn-innovations.html#challenge-1-going-deeper-vgg-2014",
    "href": "m05-images/04-cnn-innovations.html#challenge-1-going-deeper-vgg-2014",
    "title": "Part 4: The Innovation Timeline",
    "section": "Challenge 1: Going Deeper (VGG, 2014)",
    "text": "Challenge 1: Going Deeper (VGG, 2014)\nAlexNet demonstrated the power of depth with 8 layers. But could we go deeper? By 2014, researchers at Oxford’s Visual Geometry Group posed this question directly.\n\nThe Depth Hypothesis\nThe intuition was compelling. Deeper networks should learn more complex representations. Early layers detect simple edges and colors. Middle layers combine these into textures and parts. Deep layers recognize complete objects and scenes. More layers mean more abstraction.\nBut training deep networks in 2014 remained difficult. Gradients vanished. Training took weeks. Most researchers stuck with networks under 20 layers.\n\n\nVGG’s Answer: Stacked 3×3 Convolutions\nVGGNet demonstrated that systematic depth works {footcite}simonyan2014very. The key insight was using small 3×3 convolutions exclusively, stacked together to build deep networks.\n\n\n\n\n\n\nVGG16 architecture showing progressive downsampling while doubling channels. The network uses only 3×3 convolutions throughout.\n\n\n\n\nFigure 2\n\n\n\nWhy stack 3×3 filters instead of using larger filters? Consider the receptive field. Two 3×3 convolutions have the same receptive field as one 5×5 convolution (both see a 5×5 region of the input). But the stacked version has fewer parameters.\nFor a single 5×5 convolution:\n\n\\text{parameters} = 5 \\times 5 = 25\n\nFor two stacked 3×3 convolutions:\n\n\\text{parameters} = 2 \\times (3 \\times 3) = 18\n\nThis yields a 28% parameter reduction while adding an extra ReLU nonlinearity between the layers, allowing the network to learn more complex functions.\n\n\n\n\n\n\nTwo stacked 3×3 convolutions achieve the same receptive field as one 5×5 convolution but with fewer parameters and added nonlinearity.\n\n\n\n\nFigure 3\n\n\n\n\n\nThe Architecture Pattern\nVGG introduced a clean, systematic pattern that influenced all subsequent architectures:\nAfter each pooling layer, double the channels:\n\n\\text{channels} = \\{64 \\to 128 \\to 256 \\to 512 \\to 512\\}\n\nSpatial dimensions halve:\n\n\\text{spatial dimensions} = \\{224 \\to 112 \\to 56 \\to 28 \\to 14 \\to 7\\}\n\nThis creates a pyramid structure where computational cost per layer stays roughly constant. As spatial dimensions decrease, increasing channel depth compensates by expanding representational capacity.\nVGG16 (16 layers) and VGG19 (19 layers) achieved strong results on ImageNet, validating that systematic depth improves accuracy. The architecture’s simplicity made it easy to understand and implement, contributing to its widespread adoption.\n\n\nThe Limitation\nVGG16 contains approximately 140 million parameters. The majority (102 million) concentrate in the first fully connected layer. This massive parameter count means:\n\nTraining requires significant computational resources\nInference is memory-intensive\nThe model is prone to overfitting without strong regularization\n\nThe question became: can we achieve similar accuracy with fewer parameters?",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 4: CNN Innovations"
    ]
  },
  {
    "objectID": "m05-images/04-cnn-innovations.html#challenge-2-computing-efficiently-inceptiongooglenet-2014",
    "href": "m05-images/04-cnn-innovations.html#challenge-2-computing-efficiently-inceptiongooglenet-2014",
    "title": "Part 4: The Innovation Timeline",
    "section": "Challenge 2: Computing Efficiently (Inception/GoogLeNet, 2014)",
    "text": "Challenge 2: Computing Efficiently (Inception/GoogLeNet, 2014)\nWhile VGG pushed depth systematically, researchers at Google asked a different question: how do we capture multi-scale features efficiently?\n\nMulti-Scale Feature Extraction\nLook at a photograph. Some objects are large and occupy significant image area. Others are small details. To recognize both, the network needs to examine features at multiple scales simultaneously.\nTraditional CNN layers use a single kernel size (like 3×3). But the optimal kernel size varies by context. Large kernels capture broad patterns. Small kernels detect fine details.\nInception’s answer: use multiple kernel sizes in parallel within the same layer {footcite}szegedy2015going.\n\n\nThe 1×1 Convolution Trick\nRunning multiple large convolutions in parallel is computationally expensive. Inception solves this through 1×1 convolutions for channel dimensionality reduction.\nAt first, 1×1 convolutions seem strange. They don’t look at neighboring pixels, only at different channels at the same location. But this is precisely their power. They compress information across channels before applying larger, more expensive filters.\nConsider a 3×3 convolution on a 256-channel feature map producing 256 output channels:\n\n\\text{parameters (without reduction)} = 3 \\times 3 \\times 256 \\times 256 = 589{,}824\n\nWith a 1×1 convolution reducing to 64 channels first:\n\n\\text{parameters (with reduction)} = (1 \\times 1 \\times 256 \\times 64) + (3 \\times 3 \\times 64 \\times 256) = 163{,}840\n\nThis achieves a 72% parameter reduction while maintaining similar expressive power.\nThe theoretical motivation behind 1×1 convolutions is elegant. Inception approximates sparse connectivity. Not every pixel needs to connect to every pixel in the next layer. The 1×1 convolutions sparsify connections efficiently by operating primarily across channels rather than spatial dimensions {footcite}paperswithcode-inception.\n\n\nThe Inception Module\nEach Inception module contains four parallel branches:\n\n1×1 convolution: Captures point-wise patterns\n1×1 → 3×3 convolution: Captures medium-scale patterns (with reduction)\n1×1 → 5×5 convolution: Captures large-scale patterns (with reduction)\n3×3 max pooling → 1×1 convolution: Preserves spatial structure differently\n\nThese branches process the same input simultaneously. Their outputs concatenate along the channel dimension, creating a multi-scale representation.\nMathematically, for input X:\n\nY_{\\text{inception}} = \\text{Concat}\\big(Y_{1\\times1}, \\,Y_{3\\times3}, \\,Y_{5\\times5}, \\,Y_{\\text{pool}}\\big)\n\nwhere each Y represents the output of its respective branch.\n\n\nGlobal Average Pooling\nVGG’s fully connected layers contain 102 million parameters. Inception eliminates this bottleneck through global average pooling {footcite}lin2013network.\nInstead of flattening feature maps and passing through dense layers, take the average value of each channel across all spatial positions. For a feature map with 1000 channels, this produces a 1000-dimensional vector directly, regardless of spatial size. This:\n\nDrastically reduces parameters (no heavy fully connected layers)\nCreates translation invariance (averaging eliminates spatial dependence)\nReduces overfitting risk\n\n\n\nAuxiliary Classifiers\nGoogLeNet introduced auxiliary classifiers at intermediate layers to combat vanishing gradients in deep networks. These classifiers attach to middle layers, computing losses that provide additional gradient signals during backpropagation.\n\n\n\n\n\n\nGoogLeNet architecture with auxiliary classifiers attached to intermediate layers to improve gradient flow.\n\n\n\n\nFigure 4\n\n\n\nDuring training, the total loss combines the main classifier loss with auxiliary losses (typically weighted at 0.3). At inference, only the main classifier is used.\n\n\nThe Impact\nGoogLeNet achieved accuracy comparable to VGG with 12× fewer parameters. This demonstrated that architecture efficiency matters as much as depth. The Inception ideas influenced countless subsequent designs.\nLater versions pushed these ideas further. Inception v2/v3 added batch normalization and factorized larger filters (5×5 became two 3×3 convolutions). Inception v4 integrated with residual connections. Xception used depthwise separable convolutions, pushing channel-spatial separation further.\nBatch Normalization, introduced around this time {footcite}ioffe2015batch, normalizes layer activations to zero mean and unit variance. This stabilizes training and allows higher learning rates. It became standard in nearly all subsequent architectures.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 4: CNN Innovations"
    ]
  },
  {
    "objectID": "m05-images/04-cnn-innovations.html#challenge-3-training-very-deep-networks-resnet-2015",
    "href": "m05-images/04-cnn-innovations.html#challenge-3-training-very-deep-networks-resnet-2015",
    "title": "Part 4: The Innovation Timeline",
    "section": "Challenge 3: Training Very Deep Networks (ResNet, 2015)",
    "text": "Challenge 3: Training Very Deep Networks (ResNet, 2015)\nBy 2015, researchers wanted networks with 50, 100, or even 150 layers. But a puzzling phenomenon blocked progress: networks deeper than about 20 layers exhibited degradation.\n\nThe Degradation Problem\nHere’s what was strange. Add more layers to a working network and training error increases. Not test error (which would indicate overfitting). Training error itself gets worse.\nThis shouldn’t happen. A deeper network could theoretically learn the identity function for extra layers, matching the shallower network’s performance. But in practice, optimization failed. The deeper network couldn’t even learn to copy what the shallower network already achieved.\nThis wasn’t vanishing gradients alone (batch normalization addressed that). This was a fundamental optimization difficulty.\n\n\nThe Residual Learning Solution\nMicrosoft Research proposed an elegant solution: skip connections {footcite}he2016deep.\nInstead of learning a direct mapping H(\\mathbf{x}) from input \\mathbf{x} to output, learn the residual F(\\mathbf{x}) = H(\\mathbf{x}) - \\mathbf{x}. Then add the input back:\n\nH(\\mathbf{x}) = F(\\mathbf{x}) + \\mathbf{x}\n\n\n\n\n\n\n\nResidual block. The skip connection carries the input directly to the output, while convolutional layers learn the residual.\n\n\n\n\nFigure 5\n\n\n\nWhy does this help? If the optimal mapping is close to identity (the layer isn’t very useful), the network can easily learn F(\\mathbf{x}) \\approx 0 by pushing weights toward zero. The skip connection ensures input information flows through unchanged.\nIf a more complex transformation is needed, F(\\mathbf{x}) can still learn it. The skip connection doesn’t constrain what the block can represent. It just makes optimization easier by providing a gradient highway.\n\n\nEnsemble-Like Gradient Flow\nSkip connections create multiple paths for gradients to flow backward. Some paths go through all convolutions. Others skip multiple blocks via cascaded skip connections. This ensemble of paths accelerates training and prevents vanishing gradients {footcite}veit2016residual.\n\n\n\n\n\n\nMultiple gradient paths in ResNet. Gradients can skip layers via identity connections, providing stable training for very deep networks.\n\n\n\n\nFigure 6\n\n\n\n\n\nBottleneck Blocks for Deeper Networks\nResNet-50, -101, and -152 use bottleneck blocks to maintain efficiency:\n\n1×1 convolution: Reduces channel dimension (e.g., 256 → 64)\n3×3 convolution: Operates on reduced dimension\n1×1 convolution: Restores dimension (e.g., 64 → 256)\n\n\n\n\n\n\n\nBottleneck block (left) vs. basic block (right). The bottleneck design reduces computational cost in very deep networks.\n\n\n\n\nFigure 7\n\n\n\nThis shrinks the intermediate feature map, dramatically reducing computational cost while maintaining representational capacity. The design was inspired by Inception’s bottleneck idea.\n\n\nThe Results\nResNet achieved:\n\n152 layers trained successfully without degradation\nTop-5 error of 3.57% on ImageNet (better than human performance on the test set)\nWidespread adoption across computer vision tasks\n\nThe impact extended beyond CNNs. Skip connections appeared in:\n\nU-Net for medical image segmentation\nDenseNet which connects every layer to every other layer\nTransformers for natural language processing\nNearly all modern deep architectures\n\nResNet showed that with the right architecture, depth isn’t a limitation. It’s a resource.\n\n\nResNeXt: Width Through Cardinality\nResNeXt {footcite}xie2017aggregated extended ResNet by increasing network width through grouped convolutions rather than just adding depth or channels.\nThe idea: split the bottleneck convolution path into multiple parallel groups (typically 32), each processing independently. Aggregate their outputs through concatenation or addition.\n\n\n\n\n\n\nResNeXt block with multiple grouped convolution paths. Increasing cardinality (number of groups) often improves accuracy more than increasing depth or channel count.\n\n\n\n\nFigure 8\n\n\n\nThis “cardinality” dimension provides another axis for scaling networks. ResNeXt achieves better accuracy than ResNet at similar computational cost by increasing cardinality instead of just going deeper.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 4: CNN Innovations"
    ]
  },
  {
    "objectID": "m05-images/04-cnn-innovations.html#challenge-4-global-context-vision-transformer-2020",
    "href": "m05-images/04-cnn-innovations.html#challenge-4-global-context-vision-transformer-2020",
    "title": "Part 4: The Innovation Timeline",
    "section": "Challenge 4: Global Context (Vision Transformer, 2020)",
    "text": "Challenge 4: Global Context (Vision Transformer, 2020)\nCNNs build global understanding slowly through stacked local operations. Early layers see only small patches (3×3 or 5×5 regions). Deeper layers expand the receptive field, but even in deep networks, truly global connections require many layers.\nWhat if we could capture global relationships immediately?\n\nThe Self-Attention Mechanism\nVision Transformers (ViT) replace convolution with self-attention {footcite}dosovitskiy2020image.\nSelf-attention computes relationships between all positions simultaneously. For each patch of the image, it determines which other patches are relevant, regardless of distance. This provides immediate global context.\nThe mechanism works as follows. Given input features X, compute three matrices through learned linear projections:\n\nQ = XW_Q, \\quad K = XW_K, \\quad V = XW_V\n\nwhere Q (queries), K (keys), and V (values) represent different views of the input.\nAttention scores measure similarity between queries and keys:\n\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\nThis computes a weighted average of values, where weights depend on query-key similarity. Intuitively, each position “asks” (via its query) what information to gather from all other positions (via their keys), then aggregates their values accordingly.\n\n\nPatches as Tokens\nViT treats images like text. Divide the image into fixed-size patches (typically 16×16). Flatten each patch into a vector. Treat these vectors as “tokens” (analogous to words in NLP).\nAdd positional encodings to preserve spatial information (since self-attention is permutation-invariant). Pass through a standard Transformer encoder with multiple self-attention layers.\nA special CLS token prepended to the sequence gathers global information. After all Transformer layers, the CLS token’s representation feeds into the classification head.\n\n\nTrade-offs: Data and Compute\nVision Transformers achieve state-of-the-art accuracy on large datasets like ImageNet-21k (14 million images). But they have important trade-offs. Vision Transformers provide a global receptive field from the first layer, better scaling properties with dataset size, and a unified architecture for vision and language. However, they require more training data than CNNs due to less inductive bias, impose higher computational cost since self-attention is O(n^2) in sequence length, and prove less effective on small datasets without strong augmentation.\nFor most practical applications, CNNs remain competitive. Use ViT when you have large datasets (millions of images), substantial computational resources, and tasks benefiting from global context like scene understanding or fine-grained classification. Use CNNs when you have limited data (thousands of images), constrained compute (edge devices, mobile), or tasks benefiting from spatial locality (object detection, segmentation).\n\n\nHybrid Approaches\nRecent research combines CNN and Transformer strengths. Swin Transformer {footcite}liu2021swin uses local attention windows (more like convolution) with hierarchical structure. CoAtNet {footcite}dai2021coatnet combines convolution in early layers with attention in later layers. ConvNeXt {footcite}liu2022convnet shows that modernized CNNs can match Transformer performance.\nThe field continues evolving, blending ideas from both paradigms.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 4: CNN Innovations"
    ]
  },
  {
    "objectID": "m05-images/04-cnn-innovations.html#the-narrative-of-progress",
    "href": "m05-images/04-cnn-innovations.html#the-narrative-of-progress",
    "title": "Part 4: The Innovation Timeline",
    "section": "The Narrative of Progress",
    "text": "The Narrative of Progress\nLet’s trace the thread connecting these innovations:\n2012 (AlexNet): Depth works, but only to 8 layers ↓ 2014 (VGG): Stack small convolutions to go deeper (16-19 layers) ↓ 2014 (Inception): Use multi-scale features and 1×1 convolutions for efficiency ↓ 2015 (ResNet): Skip connections enable training very deep networks (152 layers) ↓ 2017 (ResNeXt): Increase width through cardinality, not just depth ↓ 2020 (ViT): Replace convolution with self-attention for global context\nEach innovation addressed limitations of its predecessors while preserving their insights. Modern architectures mix and match these ideas: residual connections for depth, multi-scale features for efficiency, attention for global context.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 4: CNN Innovations"
    ]
  },
  {
    "objectID": "m05-images/04-cnn-innovations.html#choosing-the-right-architecture",
    "href": "m05-images/04-cnn-innovations.html#choosing-the-right-architecture",
    "title": "Part 4: The Innovation Timeline",
    "section": "Choosing the Right Architecture",
    "text": "Choosing the Right Architecture\nFor your next computer vision project, which architecture should you choose? ResNet-50 is the default choice, offering an excellent accuracy-computational cost trade-off with widely available pre-trained weights that work well across diverse tasks. EfficientNet matters when deployment efficiency is critical, carefully balancing depth, width, and resolution for optimal accuracy per parameter. MobileNet and EfficientNet-Lite serve mobile and edge devices, sacrificing some accuracy for fast inference and small model size. Vision Transformer (ViT) excels when you have large datasets (millions of images) and substantial compute, delivering state-of-the-art accuracy on challenging benchmarks. Swin Transformer provides Transformer benefits with more reasonable compute requirements, proving especially good for dense prediction tasks.\nStart with ResNet-50. It provides strong performance across almost all applications. Optimize later if specific constraints (speed, memory, accuracy) demand it.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 4: CNN Innovations"
    ]
  },
  {
    "objectID": "m05-images/04-cnn-innovations.html#summary",
    "href": "m05-images/04-cnn-innovations.html#summary",
    "title": "Part 4: The Innovation Timeline",
    "section": "Summary",
    "text": "Summary\nWe traced CNN evolution through successive innovations solving specific challenges. VGG demonstrated that depth matters through stacked 3×3 convolutions. Inception showed how to capture multi-scale features efficiently using 1×1 convolutions and parallel branches. ResNet enabled training very deep networks (152 layers) through skip connections that ease optimization and improve gradient flow. Vision Transformers replaced convolution with self-attention, trading inductive bias for global context at the cost of requiring more data and compute.\nEach architecture built on its predecessors’ insights. Modern networks combine ideas from all of them: residual connections for depth, multi-scale features for efficiency, attention for global understanding. Architecture design is problem-solving. Understanding why these innovations emerged helps you make informed choices for your own applications.\n:style: unsrt",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 4: CNN Innovations"
    ]
  },
  {
    "objectID": "m05-images/02-the-deep-learning-revolution.html",
    "href": "m05-images/02-the-deep-learning-revolution.html",
    "title": "Part 2: The Deep Learning Revolution",
    "section": "",
    "text": "What you’ll learn\n\n\n\nThis section traces the paradigm shift in computer vision from hand-crafted features to learned representations. We explore how researchers designed edge detectors and frequency transforms, examine LeNet’s pioneering approach to automated feature learning, and understand AlexNet’s breakthrough that demonstrated deep learning works at scale.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 2: The Deep Learning Revolution"
    ]
  },
  {
    "objectID": "m05-images/02-the-deep-learning-revolution.html#the-old-way-engineering-features-by-hand",
    "href": "m05-images/02-the-deep-learning-revolution.html#the-old-way-engineering-features-by-hand",
    "title": "Part 2: The Deep Learning Revolution",
    "section": "The Old Way: Engineering Features by Hand",
    "text": "The Old Way: Engineering Features by Hand\nBefore 2012, computer vision meant one thing: carefully designing features by hand. Experts would analyze problems and craft mathematical operations to extract useful information. Edge detection, texture analysis, object boundaries. Every feature required human insight and engineering effort.\nLet’s explore how this worked by examining edge detection, one of the fundamental problems in image processing.\n\nDetecting Edges Through Brightness Changes\nWhat makes an edge visible to human eyes? The answer is sudden changes in brightness. An edge appears when neighboring pixels have significantly different intensity values.\nRecall from Part 1 the small 6×6 grayscale image with a bright vertical line in the third column (values of 80) surrounded by dark pixels (values of 10). How do we detect that vertical edge?\nWe can approximate the horizontal derivative by subtracting the right neighbor from the left neighbor at each position. For the central pixel, this looks like:\n\n\\nabla Z_{22} = Z_{2,1} - Z_{2,3}\n\nApplied to the entire image, we get large values where brightness changes suddenly (the edge) and near-zero values elsewhere. This simple operation reveals structure.\n\n\nConvolution: A General Pattern Matching Operation\nThe derivative calculation we just performed is a special case of a more general operation called convolution. The idea is elegant: define a small matrix of weights called a kernel or filter, then slide it across the image, computing weighted sums at each position.\nFor a 3×3 kernel K applied to a local patch Z:\n\n\\text{output}_{i,j} = \\sum_{m=-1}^{1} \\sum_{n=-1}^{1} K_{m,n} \\cdot Z_{i+m, j+n}\n\nThe Prewitt operator provides kernels specifically designed for edge detection:\n\nK_h = \\begin{bmatrix}\n-1 & 0 & 1 \\\\\n-1 & 0 & 1 \\\\\n-1 & 0 & 1\n\\end{bmatrix}\n\\quad\\text{and}\\quad\nK_v = \\begin{bmatrix}\n-1 & -1 & -1 \\\\\n0 & 0 & 0 \\\\\n1 & 1 & 1\n\\end{bmatrix}\n\nThe horizontal kernel K_h detects vertical edges, while the vertical kernel K_v detects horizontal edges. Each kernel responds strongly when the image patch matches its pattern.\n\n# Define the vertical edge detection kernel\nK_v = np.array([[-1, -1, -1],\n                [0, 0, 0],\n                [1, 1, 1]])\n\n# Apply convolution to detect edges\nedges = convolve2d(img_gray, K_v, mode='same', boundary='symm')\n\n\n\nShow visualization code\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\naxes[0].imshow(img_gray, cmap='gray')\naxes[0].set_title(\"Original Grayscale Image\")\naxes[0].axis(\"off\")\n\naxes[1].imshow(edges, cmap='gray')\naxes[1].set_title(\"Vertical Edge Detection (Prewitt Filter)\")\naxes[1].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNotice how the filter highlights horizontal boundaries where brightness changes rapidly in the vertical direction. An excellent interactive demo of various image kernels can be found at Setosa Image Kernels.\n\n\nThinking in Frequencies: The Fourier Transform\nShift your attention from the spatial domain to the frequency domain. The Fourier transform offers an alternative view of images, representing them as combinations of sinusoidal patterns at different frequencies.\nFor a discrete signal x[n] of length N, the Discrete Fourier Transform is:\n\n\\mathcal{F}(x)[k] = \\sum_{n=0}^{N-1} x[n] \\cdot e^{-2\\pi i \\frac{nk}{N}}\n\nUsing Euler’s formula e^{ix} = \\cos(x) + i\\sin(x), we can rewrite this as:\n\n\\mathcal{F}(x)[k] = \\sum_{n=0}^{N-1} x[n]\\Big[\\cos(2\\pi \\tfrac{nk}{N}) - i\\sin(2\\pi \\tfrac{nk}{N})\\Big]\n\nThe Fourier transform decomposes a signal into its frequency components. Low frequencies correspond to smooth, slowly varying regions. High frequencies correspond to sharp edges and fine details.\nThe convolution theorem reveals a beautiful connection: convolution in the spatial domain is equivalent to multiplication in the frequency domain:\n\nX * K \\quad\\longleftrightarrow\\quad \\mathcal{F}(X) \\cdot \\mathcal{F}(K)\n\nThis means we can perform convolution by: 1. Taking the Fourier transform of both the image and the kernel 2. Multiplying them element-wise in the frequency domain 3. Taking the inverse Fourier transform to get back to the spatial domain\nFor large images, this approach can be computationally faster than direct convolution. For a beautiful visual explanation of the Fourier Transform, see 3Blue1Brown’s video.\n\n\nThe Fundamental Limitation\nHere’s the problem with hand-crafted features: experts had to design every single one. Want to detect corners? Design a corner detector. Need to recognize textures? Craft texture descriptors. Each feature required mathematical sophistication and domain expertise.\nThis approach worked for simple, well-defined tasks. But it scaled poorly to complex problems like recognizing thousands of object categories. The feature engineering bottleneck limited what computer vision could achieve.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 2: The Deep Learning Revolution"
    ]
  },
  {
    "objectID": "m05-images/02-the-deep-learning-revolution.html#the-first-breakthrough-lenet",
    "href": "m05-images/02-the-deep-learning-revolution.html#the-first-breakthrough-lenet",
    "title": "Part 2: The Deep Learning Revolution",
    "section": "The First Breakthrough: LeNet",
    "text": "The First Breakthrough: LeNet\nYann LeCun posed a radical question in the late 1980s: what if networks could learn features automatically from raw pixels? Instead of hand-designing edge detectors, let the network discover useful patterns through training.\nThis vision led to LeNet, a pioneering convolutional architecture that demonstrated automated feature learning on handwritten digit recognition {footcite}lecun1989backpropagation,lecun1998gradient.\n\n\n\n\n\n\nLeNet-1 architecture. The network learns to extract features through layers of convolution and pooling.\n\n\n\n\nFigure 1\n\n\n\n\nArchitecture: Hierarchical Feature Learning\nLeNet introduced a pattern that remains fundamental to modern CNNs:\n\nConvolutional layers apply learnable filters (not hand-designed) to extract local patterns\nPooling layers downsample feature maps, creating spatial invariance\nStacking multiple layers builds increasingly abstract representations\nFully connected layers at the end combine features for classification\n\nThe key innovation was making the convolutional filters learnable parameters. During training, backpropagation adjusts filter weights to extract features useful for the task. The network discovers edge detectors, corner detectors, and more complex patterns automatically.\nLeNet-5, the most influential version, processed 32×32 grayscale images through this architecture:\n\n\n\n\n\n\nLeNet-5 architecture with input normalization, sparse connectivity, and multiple convolution-pooling pairs.\n\n\n\n\nFigure 2\n\n\n\nLet’s understand each component:\nC1: First Convolutional Layer Takes the input image and applies learnable 5×5 filters. These filters start random but evolve during training to detect basic patterns like edges at various orientations.\nS2: Subsampling (Pooling) Reduces spatial dimensions through average pooling with 2×2 windows. This creates local translation invariance—small shifts in feature positions don’t change the output significantly.\nC3: Second Convolutional Layer Combines features from the previous layer to build more complex patterns. LeNet-5 used sparse connectivity here (not every feature map connects to every previous map), reducing parameters while encouraging diverse features.\nS4: Second Subsampling Further reduces spatial dimensions, allowing the network to focus on increasingly abstract representations.\nFully Connected Layers Flatten the spatial feature maps into a vector and make the final classification decision across 10 digit classes.\nYann LeCun’s work on applying backpropagation to convolutional architectures in the 1980s was met with skepticism. But LeNet’s success on real-world tasks like automated check reading at banks helped spark wider interest in neural networks.\n\n\nLeNet Architecture in Code\nHere’s a simplified LeNet-1 implementation showing the core architecture:\n\n\nShow LeNet-1 implementation\nimport torch\nimport torch.nn as nn\n\nclass LeNet1(nn.Module):\n    \"\"\"Simplified LeNet-1 architecture.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 4, kernel_size=5)\n        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(4, 12, kernel_size=5)\n        self.fc = nn.Linear(12 * 4 * 4, 10)\n\n    def forward(self, x):\n        x = torch.tanh(self.conv1(x))\n        x = self.pool(x)\n        x = torch.tanh(self.conv2(x))\n        x = self.pool(x)\n        x = x.view(-1, 12 * 4 * 4)\n        x = self.fc(x)\n        return x\n\nmodel = LeNet1()\nprint(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n\nTotal parameters: 3,246\n\n\nThis simple architecture achieves high accuracy on MNIST, demonstrating the power of learned features. The convolutional filters automatically discover edge detectors and pattern recognizers through training. We’ll explore how to train models like this in detail in Part 3.\n\n\nWhy LeNet Mattered\nLeNet proved a crucial concept: networks can learn better features than human experts can design. This automated feature learning was revolutionary, but LeNet’s impact remained limited. It worked well on simple tasks like digit recognition but struggled with complex, large-scale problems.\nThe computational constraints of the 1990s prevented training deeper, more powerful networks. GPU acceleration didn’t exist. Datasets were small. Training techniques were primitive compared to modern methods.\nFor nearly two decades, hand-crafted features remained dominant in computer vision. Techniques like SIFT (Scale-Invariant Feature Transform) and HOG (Histogram of Oriented Gradients) powered most practical systems. Neural networks were interesting research curiosities, not mainstream tools.\nThen came 2012.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 2: The Deep Learning Revolution"
    ]
  },
  {
    "objectID": "m05-images/02-the-deep-learning-revolution.html#the-revolution-alexnet",
    "href": "m05-images/02-the-deep-learning-revolution.html#the-revolution-alexnet",
    "title": "Part 2: The Deep Learning Revolution",
    "section": "The Revolution: AlexNet",
    "text": "The Revolution: AlexNet\nThe ImageNet Large Scale Visual Recognition Challenge (ILSVRC) posed a formidable test: classify images into 1000 categories using a training set of 1.2 million images. This scale dwarfed anything LeNet had tackled. The best systems in 2011 achieved around 25% top-5 error, using carefully engineered features and traditional machine learning methods.\n\n\n\n\n\n\nThe ImageNet Large Scale Visual Recognition Challenge dataset contains over 1.2 million training images across 1000 categories.\n\n\n\n\nFigure 3\n\n\n\nIn 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton submitted a deep convolutional network that reduced top-5 error to 16.4%. This more than 10 percentage point improvement shocked the community {footcite}krizhevsky2012alexnet.\n\n\n\n\n\n\nTop-5 error rates on ImageNet from 2010 to 2017. AlexNet’s breakthrough in 2012 sparked the deep learning revolution.\n\n\n\n\nFigure 4\n\n\n\nAlexNet didn’t just win. It demonstrated that deep learning could work at scale, igniting the revolution that transformed computer vision, speech recognition, natural language processing, and countless other domains.\n\nKey Innovation 1: ReLU Activation\nDeep networks suffer from the vanishing gradient problem. During backpropagation, gradients shrink as they flow backward through layers. Traditional activations like sigmoid:\n\n\\sigma(x) = \\frac{1}{1 + e^{-x}}\n\nsaturate for large positive or negative inputs, driving gradients toward zero. This makes early layers nearly impossible to train.\nAlexNet popularized the Rectified Linear Unit (ReLU) {footcite}nair2010rectified:\n\n\\text{ReLU}(x) = \\max(0, x)\n\n\n\n\n\n\n\nSigmoid saturates for large inputs (gradient approaches zero), while ReLU maintains constant gradient for positive inputs.\n\n\n\n\nFigure 5\n\n\n\nReLU offers critical advantages. First, there is no vanishing gradient for positive inputs, where the gradient is exactly 1. Second, the operation is computationally cheap, requiring only a comparison to zero. Third, ReLU creates sparse activation, where many neurons output zero, leading to efficient representations.\nThe drawback is that neurons can “die” if they always receive negative inputs, never activating again. Variants like Leaky ReLU introduce a small slope for negative inputs to mitigate this:\n\n\\text{Leaky ReLU}(x) = \\begin{cases}\nx & \\text{if } x &gt; 0 \\\\\n\\alpha x & \\text{if } x \\leq 0\n\\end{cases}\n\nwhere \\alpha is typically 0.01.\n\n\nKey Innovation 2: Dropout Regularization\nDeep networks with millions of parameters easily overfit training data. AlexNet introduced Dropout as a powerful regularization technique {footcite}srivastava2014dropout.\n\n\n\n\n\n\nDropout randomly disables neurons during training, forcing the network to learn robust features.\n\n\n\n\nFigure 6\n\n\n\nDuring training, Dropout randomly sets neuron outputs to zero with probability p (typically 0.5). This prevents the network from relying too heavily on any single neuron. The effect is like training an ensemble of networks that share weights.\nAt inference time, all neurons are active, but their outputs are scaled by (1-p) to maintain expected values. Modern implementations often use inverse dropout, scaling during training instead to avoid scaling at inference.\n\n\nKey Innovation 3: GPU Acceleration\nAlexNet demonstrated that deep learning needed massive computational power. The network was trained on two GPUs with 3GB memory each, splitting the computation to handle the large parameter count.\nThis wasn’t just an implementation detail. It showed that deep learning required specialized hardware. The success of AlexNet helped catalyze the GPU computing revolution that continues today, with modern networks training on dozens or hundreds of GPUs.\n\n\nKey Innovation 4: Data Augmentation\nTo combat overfitting with limited training data, AlexNet applied aggressive data augmentation. The team used random crops of 224×224 patches from 256×256 images, horizontal flips, and color and lighting perturbations through PCA-based color jittering. These transformations artificially expanded the training set, teaching the network to recognize objects regardless of position, orientation, or lighting conditions.\n\n\nThe Architecture\nAlexNet consists of five convolutional layers followed by three fully connected layers:\n\n\n\n\n\n\nAlexNet architecture with 5 convolutional layers and 3 fully connected layers. The network was split across two GPUs.\n\n\n\n\nFigure 7\n\n\n\nThe architecture processes a 224×224 RGB image through the following layers. Conv1 applies 96 filters of 11×11 with stride 4, followed by ReLU and max pooling (3×3, stride 2). Conv2 uses 256 filters of 5×5 with ReLU and max pooling. Conv3, Conv4, and Conv5 apply 384, 384, and 256 filters respectively using 3×3 kernels, with the final convolutional layer followed by max pooling. Three fully connected layers complete the network: FC6 and FC7 each contain 4096 neurons with ReLU and Dropout, while FC8 outputs 1000 class scores through Softmax.\nThe network has approximately 60 million parameters. The first convolutional layer uses large 11×11 filters with stride 4 to rapidly reduce spatial dimensions. Later layers use smaller 3×3 filters to refine features.\nAlexNet also used Local Response Normalization (LRN) to normalize activations across adjacent channels. This technique is less common in modern architectures, which typically use batch normalization instead.\n\n\nUsing Pre-Trained AlexNet\nRather than implementing AlexNet from scratch, we can leverage pre-trained models. PyTorch provides AlexNet trained on ImageNet, ready to use:\n\nimport torch\nimport torchvision.models as models\n\n# Load pre-trained AlexNet\nalexnet = models.alexnet(weights='IMAGENET1K_V1')\nalexnet.eval()\n\nprint(f\"Total parameters: {sum(p.numel() for p in alexnet.parameters()):,}\")\n\nTotal parameters: 61,100,840\n\n\nThis pre-trained model has learned rich visual representations from ImageNet’s 1.2 million images. In Part 3, we’ll explore how to use and adapt these pre-trained models for practical applications.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 2: The Deep Learning Revolution"
    ]
  },
  {
    "objectID": "m05-images/02-the-deep-learning-revolution.html#why-alexnet-was-a-paradigm-shift",
    "href": "m05-images/02-the-deep-learning-revolution.html#why-alexnet-was-a-paradigm-shift",
    "title": "Part 2: The Deep Learning Revolution",
    "section": "Why AlexNet Was a Paradigm Shift",
    "text": "Why AlexNet Was a Paradigm Shift\nAlexNet proved several critical points. Depth matters: deeper networks learn more powerful representations. Data scale matters: large datasets like ImageNet’s 1.2M images enable better learning. Compute matters: GPUs make training deep networks practical. Learned features win: automated feature learning beats hand-crafted features.\nBefore AlexNet, these points were debated. After AlexNet, they became accepted wisdom. The deep learning revolution had begun.\nWithin months, researchers worldwide abandoned hand-crafted features. Every computer vision competition became a deep learning competition. Companies invested billions in GPU infrastructure. The entire field transformed.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 2: The Deep Learning Revolution"
    ]
  },
  {
    "objectID": "m05-images/02-the-deep-learning-revolution.html#from-revolution-to-practice",
    "href": "m05-images/02-the-deep-learning-revolution.html#from-revolution-to-practice",
    "title": "Part 2: The Deep Learning Revolution",
    "section": "From Revolution to Practice",
    "text": "From Revolution to Practice\nAlexNet demonstrated that deep learning works at scale. But how do we actually use these powerful models in practice? How do we understand what’s happening inside these black boxes? And how did researchers push even further, building networks with hundreds of layers?\nThese questions lead us to the practical skills and advanced architectures we’ll explore in the remaining sections. You now understand the paradigm shift. Next, you’ll learn to harness it.\n:style: unsrt",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 2: The Deep Learning Revolution"
    ]
  },
  {
    "objectID": "m05-images/02-the-deep-learning-revolution.html#summary",
    "href": "m05-images/02-the-deep-learning-revolution.html#summary",
    "title": "Part 2: The Deep Learning Revolution",
    "section": "Summary",
    "text": "Summary\nWe traced computer vision’s evolution from hand-crafted features to learned representations. Traditional approaches required experts to design edge detectors, Fourier transforms, and pattern recognizers for each task. LeNet pioneered automated feature learning in the 1990s, showing that networks could discover useful patterns through training. But computational limits constrained its impact.\nAlexNet’s 2012 breakthrough on ImageNet demonstrated that deep learning works at scale. Key innovations included ReLU activation (solving vanishing gradients), Dropout (preventing overfitting), and GPU acceleration (enabling large-scale training). The 10+ percentage point improvement shocked the computer vision community and sparked the deep learning revolution.\nThis paradigm shift transformed how we approach machine perception. Networks now learn features automatically from data, outperforming carefully engineered alternatives. The question is no longer whether deep learning works, but how to apply it effectively.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 2: The Deep Learning Revolution"
    ]
  },
  {
    "objectID": "m04-text/word-embeddings.html",
    "href": "m04-text/word-embeddings.html",
    "title": "Word Embeddings",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nMeaning isn’t stored in words as containers. It emerges from geometric relationships in high-dimensional space. We’ll explore how Word2Vec learns these relationships through contrast, turning linguistic philosophy into runnable algorithms.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Embeddings"
    ]
  },
  {
    "objectID": "m04-text/word-embeddings.html#words-as-relationships-not-containers",
    "href": "m04-text/word-embeddings.html#words-as-relationships-not-containers",
    "title": "Word Embeddings",
    "section": "Words as Relationships, Not Containers",
    "text": "Words as Relationships, Not Containers\nWe intuitively assume words are containers for meaning. “Dog” holds the concept of a canine. This is incorrect. Structural linguistics reveals that a sign is defined solely by its relationships. “Dog” means “dog” only because it is not “cat”, “wolf”, or “log”. Meaning is differential, not intrinsic.\n\n\n\n\n\n\nFigure 1: Green is the color that is not non-green (not red, not blue, not yellow, etc.).\n\n\n\nWord2Vec, the foundational model grounding modern NLP, learns to map the statistical topology of language. Think of it like mapping a city based purely on traffic data. You don’t know what a “school” is, but you see that “buses” and “children” congregate there at 8 AM. By placing these entities close together on a map, you reconstruct the city’s functional structure. Word2Vec does this for language, turning semantic proximity into geometric distance.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Embeddings"
    ]
  },
  {
    "objectID": "m04-text/word-embeddings.html#exploring-word2vec",
    "href": "m04-text/word-embeddings.html#exploring-word2vec",
    "title": "Word Embeddings",
    "section": "Exploring Word2Vec",
    "text": "Exploring Word2Vec\nLet’s first experience the power of Word2Vec, then understand how it works. We’ll use a pre-trained model trained on 100 billion words of Google News. We aren’t teaching it anything; we’re simply inspecting the map it created.\n\nimport gensim.downloader as api\nimport numpy as np\n\n# Load pre-trained Word2vec embeddings\nprint(\"Loading Word2vec model...\")\nmodel = api.load(\"word2vec-google-news-300\")\nprint(f\"Loaded embeddings for {len(model):,} words.\")\n\nLoading Word2vec model...\nLoaded embeddings for 3,000,000 words.\n\n\nIf the map is accurate, “dog” should be surrounded by its semantic kin. We query the nearest neighbors in the vector space.\n\nsimilar_to_dog = model.most_similar(\"dog\", topn=10)\n\nprint(\"Words most similar to 'dog':\")\nfor word, similarity in similar_to_dog:\n    print(f\"  {word:20s} {similarity:.3f}\")\n\nWords most similar to 'dog':\n  dogs                 0.868\n  puppy                0.811\n  pit_bull             0.780\n  pooch                0.763\n  cat                  0.761\n  golden_retriever     0.750\n  German_shepherd      0.747\n  Rottweiler           0.744\n  beagle               0.742\n  pup                  0.741\n\n\nThe model groups “dog” with “dogs”, “puppy”, and “pooch” not because it knows biology, but because they are statistically interchangeable in sentences. Since words are vectors, we can perform arithmetic on meaning. The relationship between “King” and “Man” is a vector. If we add that vector to “Woman”, we should arrive at “Queen”.\n \\vec{\\text{King}} - \\vec{\\text{Man}} + \\vec{\\text{Woman}} \\approx \\vec{\\text{Queen}} \n\nresult = model.most_similar(\n  positive=['king', 'woman'],\n   negative=['man'], topn=5\n)\n\nprint(\"king - man + woman =\")\nfor word, similarity in result:\n    print(f\"  {word:15s} {similarity:.3f}\")\n\nking - man + woman =\n  queen           0.712\n  monarch         0.619\n  princess        0.590\n  crown_prince    0.550\n  prince          0.538\n\n\nWe cannot see in 300 dimensions, but we can project the space down to 2D using PCA. This reveals consistent structures like the “capital city” relationship that the model has learned.\n\n\nCode\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ncountries = [\"Germany\", \"France\", \"Italy\", \"Spain\", \"Portugal\", \"Greece\"]\ncapitals = [\"Berlin\", \"Paris\", \"Rome\", \"Madrid\", \"Lisbon\", \"Athens\"]\n\n# Get embeddings\ncountry_embeddings = np.array([model[country] for country in countries])\ncapital_embeddings = np.array([model[capital] for capital in capitals])\n\n# PCA to 2D\npca = PCA(n_components=2)\nembeddings = np.vstack([country_embeddings, capital_embeddings])\nembeddings_pca = pca.fit_transform(embeddings)\n\n# Create DataFrame\ndf = pd.DataFrame(embeddings_pca, columns=[\"PC1\", \"PC2\"])\ndf[\"Label\"] = countries + capitals\ndf[\"Type\"] = [\"Country\"] * len(countries) + [\"Capital\"] * len(capitals)\n\n# Plot\nfig, ax = plt.subplots(figsize=(12, 10))\n\nfor idx, row in df.iterrows():\n    color = \"#e74c3c\" if row[\"Type\"] == \"Country\" else \"#3498db\"\n    marker = \"o\" if row[\"Type\"] == \"Country\" else \"s\"\n    ax.scatter(\n        row[\"PC1\"],\n        row[\"PC2\"],\n        c=color,\n        marker=marker,\n        s=200,\n        edgecolors=\"black\",\n        linewidth=1.5,\n        alpha=0.7,\n        zorder=3,\n    )\n    ax.text(\n        row[\"PC1\"],\n        row[\"PC2\"] + 0.15,\n        row[\"Label\"],\n        fontsize=12,\n        ha=\"center\",\n        va=\"bottom\",\n        fontweight=\"bold\",\n        bbox=dict(facecolor=\"white\", edgecolor=\"none\", alpha=0.8),\n    )\n\n# Draw arrows\nfor i in range(len(countries)):\n    country_pos = df.iloc[i][[\"PC1\", \"PC2\"]].values\n    capital_pos = df.iloc[i + len(countries)][[\"PC1\", \"PC2\"]].values\n    ax.arrow(\n        country_pos[0],\n        country_pos[1],\n        capital_pos[0] - country_pos[0],\n        capital_pos[1] - country_pos[1],\n        color=\"gray\",\n        alpha=0.6,\n        linewidth=2,\n        head_width=0.15,\n        head_length=0.1,\n        zorder=2,\n    )\n\nax.set_title(\n    'The \"Capital Of\" Relationship as Parallel Transport',\n    fontsize=16,\n    fontweight=\"bold\",\n    pad=20,\n)\nax.grid(alpha=0.3, linestyle=\"--\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nThe ‘Capital Of’ relationship appears as a consistent direction in vector space.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Embeddings"
    ]
  },
  {
    "objectID": "m04-text/word-embeddings.html#how-word2vec-learns-meaning",
    "href": "m04-text/word-embeddings.html#how-word2vec-learns-meaning",
    "title": "Word Embeddings",
    "section": "How Word2Vec Learns Meaning",
    "text": "How Word2Vec Learns Meaning\nWe intuitively treat words as containers that hold meaning. “Green” contains the visual concept of a specific color. This is incorrect. Nature presents us with a messy, continuous spectrum without hard borders. Language is simply the set of arbitrary cuts we make in that continuum to create order.\nWord2Vec operationalizes this by treating meaning as a game of contrast. It functions as a pair of linguistic scissors. It does not learn what a word is by looking up a definition. It learns what a word is like by pulling it close to neighbors, and more importantly, it learns what a word is not by pushing it away from random noise. The meaning of “Green” is simply the geometric region that remains after we have pushed away “Red”, “Purple”, and “Banana”.\n\n\n\n\n\n\nFigure 2: Starting from initially random vectors, word2vec learns iteratively to push away the words that are not related and pull words that are related. The resulting vector space is a map of the relationships between words.\n\n\n\nThis process relies on a technique called contrastive learning. We cannot teach the model the exact meaning of each word, but we can let it learn the relationship between words through a binary classification problem: are these two words neighbors, or are they strangers?\nThe training loop provides a positive pair from the text, instructing the model to maximize the similarity between their vectors. Simultaneously, it grabs random negative samples (imposters from the vocabulary) and demands the model minimize their similarity. This push-and-pull mechanic creates the vector space. The “Green” cluster forms not because the model understands color, but because those words are statistically interchangeable when opposed to “Red”.\nTo generate these pairs without human labeling, we employ a sliding window technique. This moves over the raw text corpus, converting a sequence of words into a system of geometric queries.\n\n\n\n\n\n\nFigure 3: Without human labeling, word2vec assumes that words in the same context are related. Context is defined as the words within a window of predefined size. For example, in “The quick brown fox jumps over the lazy dog”, the context of “fox” includes “brown”, “jumps”, “over”, and “lazy”.\n\n\n\nWord2Vec is a simple neural network with one hidden layer. The input is a one-hot encoded vector of a word, which triggers neurons in the hidden layer to fire. The neural connection strength from the neuron representing the word to the neurons in the hidden layer (marked by red arrows) represents the query vector, u. The hidden layer neurons then trigger the firing of output layer neurons, which represents the probability of word w appearing in the context of word w_i. The connection strength from an output word neuron to the hidden layer neurons represents the key vector, v.\n\nThe word in the center of the window acts as the Query vector (u), broadcasting its position to the surrounding Context words, which act as Keys (v). The neural network adjusts its weights to maximize the dot product u \\cdot v for these specific context pairs while suppressing the dot product for the negative samples. The probability of a word appearing in context is thus a function of their vector alignment.\n\nP(j \\vert i) = \\frac{P(j) \\exp(u_i \\cdot v_j)}{\\sum_{k=1}^{V} P(k) \\exp(u_i \\cdot v_k)}\n\nwhere P(j) is the probability of word j appearing in the vocabulary.\nThe original Word2Vec paper uses a different formulation that omits P(j). This original formulation is correct conceptually but not practically. In practice, word2vec is trained with an efficient but biased training algorithm (negative sampling). The term P(j) enters the P(j \\vert i) when we account for bias, which is why we include it here.\nThis closes the loop between high-level linguistic philosophy and low-level matrix operations. The machine proves the structuralist hypothesis: that meaning is relational. By mechanically slicing the continuum of language and applying the pressure of negative sampling, the model reconstructs a functional map of human concepts. We have successfully turned a philosophy of meaning into a runnable algorithm.\n\n\n\n\n\n\nFigure 4",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Embeddings"
    ]
  },
  {
    "objectID": "m04-text/word-embeddings.html#key-takeaway",
    "href": "m04-text/word-embeddings.html#key-takeaway",
    "title": "Word Embeddings",
    "section": "Key Takeaway",
    "text": "Key Takeaway\nYou don’t need to know what a thing is to understand it. You only need to know where it stands relative to everything it isn’t. There’s a nice blog post by Chris McCormick that walks through the inner workings of Word2Vec. See here.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Embeddings"
    ]
  },
  {
    "objectID": "m04-text/transformers.html",
    "href": "m04-text/transformers.html",
    "title": "Transformers",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThe transformer revolution boils down to one insight: static embeddings assign one vector per word, ignoring that “bank” near “river” is mathematically different from “bank” near “money”. Transformers solve this with context-aware representations through weighted mixing, where weights emerge from learned comparisons (Query times Key) between words. The result: machines finally understand that meaning lives in distribution, not in the word itself.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Transformers"
    ]
  },
  {
    "objectID": "m04-text/transformers.html#the-problem-with-one-vector-per-word",
    "href": "m04-text/transformers.html#the-problem-with-one-vector-per-word",
    "title": "Transformers",
    "section": "The Problem with One Vector Per Word",
    "text": "The Problem with One Vector Per Word\n\n\n\n\n\nFor many years, natural language processing treated words as having fixed meanings. Each word (like “bank”) received a single vector of numbers, called a static embedding. But there’s a hidden catch in this “one meaning per word” mindset. With just a single fixed entry in the dictionary, “bank” means exactly the same thing in “I deposited money at the bank” as in “We had a picnic by the bank”. Every possible meaning gets mashed into a one-size-fits-all average, like describing the population by its average height and pretending nobody’s shorter or taller. The interesting details, the outliers, the context clues, all vanish in the mix.\nWhat if we simply mixed the target word with its neighbors? For “I deposited money at the bank,” we could compute a contextualized representation as:\n\n\\vec{v}_{\\text{bank (new)}} = w_1 \\cdot \\vec{v}_{\\text{bank}} + w_2 \\cdot \\vec{v}_{\\text{deposited}} + w_3 \\cdot \\vec{v}_{\\text{money}} + \\cdots\n\nwhere w_i are weights and \\vec{v}_i are word embeddings. The key question: how do we determine these weights? Consider that “bank” sits neutrally between financial terms (money) and geographical terms (river). Try manually adjusting the weights to contextualize “bank”:\n\nd3 = require(\"d3@7\", \"d3-simple-slider@1\")\n\n\n\n\n\n\n\nfunction sliderWithLabel(min, max, step, width, defaultValue, label) {\n  const slider = d3.sliderBottom()\n    .min(min).max(max).step(step).width(width).default(defaultValue);\n  const svg = d3.create(\"svg\").attr(\"width\", width + 50).attr(\"height\", 60);\n  svg.append(\"g\").attr(\"transform\", \"translate(25,20)\").call(slider);\n  svg.append(\"text\").attr(\"x\", (width + 50) / 2).attr(\"y\", 10).attr(\"text-anchor\", \"middle\").style(\"font-size\", \"5px\").text(label);\n  return svg.node();\n}\n\n\n\n\n\n\n\n{\n  function createWeightSlider(min, max, step, width, defaultValue, label) {\n    const slider = d3.sliderBottom()\n      .min(min).max(max).step(step).width(width).default(defaultValue);\n    const svg = d3.create(\"svg\").attr(\"width\", width + 50).attr(\"height\", 60);\n    const g = svg.append(\"g\").attr(\"transform\", \"translate(25,20)\");\n    g.call(slider);\n    svg.append(\"text\").attr(\"x\", (width + 50) / 2).attr(\"y\", 10)\n       .attr(\"text-anchor\", \"middle\").style(\"font-size\", \"12px\").text(label);\n    return { node: svg.node(), slider: slider };\n  }\n\n  const bankSliderObj = createWeightSlider(0, 1, 0.01, 120, 1.0, \"Bank weight\");\n  const moneySliderObj = createWeightSlider(0, 1, 0.01, 120, 0.0, \"Money weight\");\n  const riverSliderObj = createWeightSlider(0, 1, 0.01, 120, 0.0, \"River weight\");\n\n  const contextWords = [\"bank\", \"money\", \"river\"];\n  const contextEmbeddings = [\n    [0.0, 0.0],\n    [-1.6, -0.6],\n    [1.4, -1.0]\n  ];\n\n  const plotContainer = document.createElement(\"div\");\n\n  function update() {\n    const bankWeight = bankSliderObj.slider.value();\n    const moneyWeight = moneySliderObj.slider.value();\n    const riverWeight = riverSliderObj.slider.value();\n\n    const weights = [bankWeight, moneyWeight, riverWeight];\n    const total = weights.reduce((a, b) =&gt; a + b, 0);\n    const normalizedWeights = total &gt; 0 ? weights.map(w =&gt; w / total) : [0, 0, 0];\n\n    const newVec = [\n      normalizedWeights[0] * contextEmbeddings[0][0] +\n      normalizedWeights[1] * contextEmbeddings[1][0] +\n      normalizedWeights[2] * contextEmbeddings[2][0],\n      normalizedWeights[0] * contextEmbeddings[0][1] +\n      normalizedWeights[1] * contextEmbeddings[1][1] +\n      normalizedWeights[2] * contextEmbeddings[2][1]\n    ];\n\n    const originalData = contextWords.map((word, i) =&gt; ({\n      word: word,\n      x: contextEmbeddings[i][0],\n      y: contextEmbeddings[i][1],\n      type: \"Original\"\n    }));\n\n    const contextualizedData = [{\n      word: \"bank (new)\",\n      x: newVec[0],\n      y: newVec[1],\n      type: \"Contextualized\"\n    }];\n\n    const data = [...originalData, ...contextualizedData];\n\n    d3.select(plotContainer).selectAll(\"*\").remove();\n\n    const plot = Plot.plot({\n      width: 300,\n      height: 300,\n      marginTop: 60,\n      marginRight: 20,\n      marginBottom: 50,\n      marginLeft: 60,\n      style: {\n        background: \"white\",\n        color: \"black\"\n      },\n      x: {\n        domain: [-2, 2],\n        label: \"Dimension 1\",\n        grid: true,\n        ticks: 10\n      },\n      y: {\n        domain: [-2, 2],\n        label: \"Dimension 2\",\n        grid: true,\n        ticks: 10\n      },\n      color: {\n        domain: [\"Original\", \"Contextualized\"],\n        range: [\"#dadada\", \"#ff7f0e\"]\n      },\n      marks: [\n        Plot.dot(data, {\n          x: \"x\",\n          y: \"y\",\n          fill: \"type\",\n          r: 8,\n          tip: true\n        }),\n        Plot.text(data, {\n          x: \"x\",\n          y: \"y\",\n          text: \"word\",\n          dy: -15,\n          fontSize: 8,\n          fontWeight: \"bold\",\n          fill: \"black\"\n        }),\n        Plot.text([{x: 0, y: 2.3}], {\n          x: \"x\",\n          y: \"y\",\n          text: () =&gt; `Weights: Bank=${normalizedWeights[0].toFixed(2)}, Money=${normalizedWeights[1].toFixed(2)}, River=${normalizedWeights[2].toFixed(2)}`,\n          fontSize: 11,\n          fill: \"black\"\n        }),\n        Plot.dot([{x: -0.8, y: 2.7, color: \"#dadada\"}, {x: 0.8, y: 2.7, color: \"#ff7f0e\"}], {\n          x: \"x\",\n          y: \"y\",\n          fill: \"color\",\n          r: 6\n        }),\n        Plot.text([{x: -0.5, y: 2.7, label: \"Original\"}, {x: 1.1, y: 2.7, label: \"Contextualized\"}], {\n          x: \"x\",\n          y: \"y\",\n          text: \"label\",\n          fontSize: 10,\n          fill: \"black\",\n          textAnchor: \"start\"\n        })\n      ]\n    });\n\n    d3.select(plotContainer).node().appendChild(plot);\n  }\n\n  bankSliderObj.slider.on(\"onchange\", update);\n  moneySliderObj.slider.on(\"onchange\", update);\n  riverSliderObj.slider.on(\"onchange\", update);\n\n  update();\n\n  return html`&lt;div style=\"display: flex; align-items: center; gap: 40px; justify-content: center;\"&gt;\n    &lt;div style=\"display: flex; flex-direction: column; gap: 10px;\"&gt;\n      ${bankSliderObj.node}\n      ${moneySliderObj.node}\n      ${riverSliderObj.node}\n    &lt;/div&gt;\n    &lt;div&gt;\n      ${plotContainer}\n    &lt;/div&gt;\n  &lt;/div&gt;`;\n}\n\n\n\n\n\n\nBy changing the weights, we see that the vector for “bank” can lean more towards financial terms or geographical terms. So how do we determine the weights? The simplest idea gives each word equal weight: w_i = 1/N. This creates a basic bag-of-words average. But sentences aren’t this fair. Some words are much more important than others. In “I deposited money at the bank,” the words “deposited” and “money” are key, while “I”, “at”, and “the” add little meaning. If we treat all words the same, we lose the details that matter. We need a way to highlight important words and downplay the rest.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Transformers"
    ]
  },
  {
    "objectID": "m04-text/transformers.html#attention-mechanism",
    "href": "m04-text/transformers.html#attention-mechanism",
    "title": "Transformers",
    "section": "Attention Mechanism",
    "text": "Attention Mechanism\n\n\n\n\n\nLet’s walk through how transformers identify the surrounding words that are relevant to a focal word to be contextualized. This process is called the attention mechanism. Before diving in, let’s prepare some terminology.\nSuppose we have the sentence “I deposited money at the bank”. Given the word “bank”, we want to determine the weights w_i for the surrounding words “I”, “deposited”, “money”, and “at”. We call “bank” the query word, and the surrounding words the key words. At a high level, we compute the weights w_i for each query and key pair, then average them.\n\n\\vec{v}_{\\text{query}}^{\\text{c}} = \\sum_{i=1}^N w_i \\cdot \\vec{v}_{i}\n\nwith weights w_i being determined by the query and key vectors w_{i}:=f(\\vec{v}_{\\text{query}}, \\vec{v}_{i}). This function, f, is called the attention score function.\nIn transformers, the attention score function f is implemented as follows. Given the original vector for a word (whether it’s the query word or the key word), we linearly transform it into three vectors: Query, Key, and Value.\n\n\\begin{align}\n\\vec{q}_i &= W_Q \\vec{x}_i\\\\\n\\vec{k}_i &= W_K \\vec{x}_i\\\\\n\\vec{v}_i &= W_V \\vec{x}_i\n\\end{align}\n\nWhy do we need three different vectors? Imagine you’re at a dinner party. You want to identify people talking about a topic you care about. You listen to surrounding people (playing as a listener), broadcast your own interests (playing as a speaker), and engage with conversation content. The query vector represents you as a listener, the key vector represents the people as speakers, and the value vector represents the conversation content.\nOnce we have the query, key, and value vectors, we compute the attention scores between the query and key vector:\n\nw_{ij} = \\frac{\\exp(\\vec{q}_i \\cdot \\vec{k}_j / \\sqrt{d})}{\\sum_{\\ell} \\exp(\\vec{q}_i \\cdot \\vec{k}_\\ell / \\sqrt{d})},\n\nwhere \\vec{q}_i \\cdot \\vec{k}_j is the dot product between the query and key vectors, which is larger when the query and key vectors are similar (pointing to a similar direction). The division by \\sqrt{d} (where d is the embedding dimension) is a scaling factor that prevents vanishing gradients during training. Finally, compute the contextualized representation as a weighted sum: \\text{contextualized}_i = \\sum_j w_{ij} \\vec{v}_j.\nWhat is the vanishing gradient problem? It’s when gradients of the loss function with respect to weights become too small to be effective during training.\nExplore how different Query and Key transformations produce different attention patterns. First, let us create the key, query, and value vectors. In 2d, the linear transformation is just a scaling and a rotation.\n\nfunction createQKVSlider(min, max, step, width, defaultValue, label, valueSetter) {\n  const slider = d3.sliderBottom()\n    .min(min).max(max).step(step).width(width).default(defaultValue)\n    .on('onchange', val =&gt; valueSetter(val));\n  const svg = d3.create(\"svg\").attr(\"width\", width + 40).attr(\"height\", 50);\n  const g = svg.append(\"g\").attr(\"transform\", \"translate(20,15)\");\n  g.call(slider);\n  svg.append(\"text\").attr(\"x\", (width + 40) / 2).attr(\"y\", 10)\n     .attr(\"text-anchor\", \"middle\").style(\"font-size\", \"11px\").text(label);\n  return { node: svg.node(), slider: slider };\n}\n\n\n\n\n\n\n\nmutable qScaleXValue = 1.0\n\n\n\n\n\n\n\nmutable qScaleYValue = 1.0\n\n\n\n\n\n\n\nmutable qRotateValue = 0\n\n\n\n\n\n\n\nmutable kScaleXValue = 1.0\n\n\n\n\n\n\n\nmutable kScaleYValue = 1.0\n\n\n\n\n\n\n\nmutable kRotateValue = 0\n\n\n\n\n\n\n\nqScaleXSlider = createQKVSlider(0.1, 2, 0.1, 150, 1.0, \"Q Scale X\", val =&gt; mutable qScaleXValue = val)\n\n\n\n\n\n\n\nqScaleYSlider = createQKVSlider(0.1, 2, 0.1, 150, 1.0, \"Q Scale Y\", val =&gt; mutable qScaleYValue = val)\n\n\n\n\n\n\n\nqRotateSlider = createQKVSlider(-180, 180, 5, 150, 0, \"Q Rotate (deg)\", val =&gt; mutable qRotateValue = val)\n\n\n\n\n\n\n\nkScaleXSlider = createQKVSlider(0.1, 2, 0.1, 150, 1.0, \"K Scale X\", val =&gt; mutable kScaleXValue = val)\n\n\n\n\n\n\n\nkScaleYSlider = createQKVSlider(0.1, 2, 0.1, 150, 1.0, \"K Scale Y\", val =&gt; mutable kScaleYValue = val)\n\n\n\n\n\n\n\nkRotateSlider = createQKVSlider(-180, 180, 5, 150, 0, \"K Rotate (deg)\", val =&gt; mutable kRotateValue = val)\n\n\n\n\n\n\n\nqkvVisualization = {\n  const originalVectors = [\n    { name: \"bank\", vector: [1.5, 0.5] },\n    { name: \"money\", vector: [1.8, 0.8] },\n    { name: \"river\", vector: [0.5, 1.5] }\n  ];\n\n  const qPlotContainer = document.createElement(\"div\");\n  const kPlotContainer = document.createElement(\"div\");\n\n  function transformVector(vec, scaleX, scaleY, rotateDeg) {\n    const theta = (rotateDeg * Math.PI) / 180;\n    const cos = Math.cos(theta);\n    const sin = Math.sin(theta);\n    const scaledX = vec[0] * scaleX;\n    const scaledY = vec[1] * scaleY;\n    return [\n      scaledX * cos - scaledY * sin,\n      scaledX * sin + scaledY * cos\n    ];\n  }\n\n  const qScaleX = qScaleXValue;\n  const qScaleY = qScaleYValue;\n  const qRotate = qRotateValue;\n  const kScaleX = kScaleXValue;\n  const kScaleY = kScaleYValue;\n  const kRotate = kRotateValue;\n\n  const originalData = originalVectors.map(item =&gt; ({\n    name: item.name,\n    x: item.vector[0],\n    y: item.vector[1],\n    type: \"Original\"\n  }));\n\n  const qData = originalVectors.map(item =&gt; {\n    const qVec = transformVector(item.vector, qScaleX, qScaleY, qRotate);\n    return {\n      name: `q_${item.name}`,\n      x: qVec[0],\n      y: qVec[1],\n      type: \"Query\"\n    };\n  });\n\n  const kData = originalVectors.map(item =&gt; {\n    const kVec = transformVector(item.vector, kScaleX, kScaleY, kRotate);\n    return {\n      name: `k_${item.name}`,\n      x: kVec[0],\n      y: kVec[1],\n      type: \"Key\"\n    };\n  });\n\n  const qPlot = Plot.plot({\n    width: 250,\n    height: 250,\n    marginTop: 40,\n    marginRight: 20,\n    marginBottom: 50,\n    marginLeft: 60,\n    style: {\n      background: \"white\",\n      color: \"black\"\n    },\n    x: {\n      domain: [-3, 3],\n      label: \"Dimension 1\",\n      grid: true,\n      ticks: 10\n    },\n    y: {\n      domain: [-3, 3],\n      label: \"Dimension 2\",\n      grid: true,\n      ticks: 10\n    },\n    marks: [\n      Plot.dot([{x: 0, y: 0}], {\n        x: \"x\",\n        y: \"y\",\n        r: 3,\n        fill: \"black\"\n      }),\n      Plot.arrow([...originalData, ...qData], {\n        x1: 0,\n        y1: 0,\n        x2: \"x\",\n        y2: \"y\",\n        stroke: \"type\",\n        strokeWidth: 2,\n        headLength: 8\n      }),\n      Plot.dot([...originalData, ...qData], {\n        x: \"x\",\n        y: \"y\",\n        fill: \"type\",\n        r: 5,\n        tip: true\n      }),\n      Plot.text([...originalData, ...qData], {\n        x: \"x\",\n        y: \"y\",\n        text: \"name\",\n        dy: -12,\n        fontSize: 9,\n        fontWeight: \"bold\",\n        fill: \"black\"\n      }),\n      Plot.text([{ x: 0, y: 3.4 }], {\n        x: \"x\",\n        y: \"y\",\n        text: () =&gt; \"Query Space\",\n        fontSize: 12,\n        fontWeight: \"bold\",\n        fill: \"black\"\n      })\n    ],\n    color: {\n      domain: [\"Original\", \"Query\"],\n      range: [\"#666666\", \"#4682b4\"]\n    }\n  });\n\n  const kPlot = Plot.plot({\n    width: 250,\n    height: 250,\n    marginTop: 40,\n    marginRight: 20,\n    marginBottom: 50,\n    marginLeft: 60,\n    style: {\n      background: \"white\",\n      color: \"black\"\n    },\n    x: {\n      domain: [-3, 3],\n      label: \"Dimension 1\",\n      grid: true,\n      ticks: 10\n    },\n    y: {\n      domain: [-3, 3],\n      label: \"Dimension 2\",\n      grid: true,\n      ticks: 10\n    },\n    marks: [\n      Plot.dot([{x: 0, y: 0}], {\n        x: \"x\",\n        y: \"y\",\n        r: 3,\n        fill: \"black\"\n      }),\n      Plot.arrow([...originalData, ...kData], {\n        x1: 0,\n        y1: 0,\n        x2: \"x\",\n        y2: \"y\",\n        stroke: \"type\",\n        strokeWidth: 2,\n        headLength: 8\n      }),\n      Plot.dot([...originalData, ...kData], {\n        x: \"x\",\n        y: \"y\",\n        fill: \"type\",\n        r: 5,\n        tip: true\n      }),\n      Plot.text([...originalData, ...kData], {\n        x: \"x\",\n        y: \"y\",\n        text: \"name\",\n        dy: -12,\n        fontSize: 9,\n        fontWeight: \"bold\",\n        fill: \"black\"\n      }),\n      Plot.text([{ x: 0, y: 3.4 }], {\n        x: \"x\",\n        y: \"y\",\n        text: () =&gt; \"Key Space\",\n        fontSize: 12,\n        fontWeight: \"bold\",\n        fill: \"black\"\n      })\n    ],\n    color: {\n      domain: [\"Original\", \"Key\"],\n      range: [\"#666666\", \"#2e8b57\"]\n    }\n  });\n\n  d3.select(qPlotContainer).node().appendChild(qPlot);\n  d3.select(kPlotContainer).node().appendChild(kPlot);\n\n  return html`&lt;div style=\"display: flex; justify-content: center; gap: 40px;\"&gt;\n    &lt;div style=\"display: flex; flex-direction: column; gap: 20px;\"&gt;\n      &lt;div style=\"display: flex; align-items: center; gap: 20px;\"&gt;\n        &lt;div style=\"display: flex; flex-direction: column; gap: 8px;\"&gt;\n          &lt;div style=\"font-weight: bold; margin-bottom: 3px;\"&gt;Query (W_Q)&lt;/div&gt;\n          ${qScaleXSlider.node}\n          ${qScaleYSlider.node}\n          ${qRotateSlider.node}\n        &lt;/div&gt;\n        ${qPlotContainer}\n      &lt;/div&gt;\n      &lt;div style=\"display: flex; align-items: center; gap: 20px;\"&gt;\n        &lt;div style=\"display: flex; flex-direction: column; gap: 8px;\"&gt;\n          &lt;div style=\"font-weight: bold; margin-bottom: 3px;\"&gt;Key (W_K)&lt;/div&gt;\n          ${kScaleXSlider.node}\n          ${kScaleYSlider.node}\n          ${kRotateSlider.node}\n        &lt;/div&gt;\n        ${kPlotContainer}\n      &lt;/div&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;`;\n}\n\n\n\n\n\n\nUsing the transformations above, we can compute the attention weights showing how each word attends to every other word:\n\nattentionHeatmap = {\n  const attentionWords = [\"bank\", \"money\", \"river\"];\n  const attentionEmbeddings = [\n    [1.5, 0.5],\n    [1.8, 0.8],\n    [0.5, 1.5]\n  ];\n\n  function transformVector(vec, scaleX, scaleY, rotateDeg) {\n    const theta = (rotateDeg * Math.PI) / 180;\n    const cos = Math.cos(theta);\n    const sin = Math.sin(theta);\n    const scaledX = vec[0] * scaleX;\n    const scaledY = vec[1] * scaleY;\n    return [\n      scaledX * cos - scaledY * sin,\n      scaledX * sin + scaledY * cos\n    ];\n  }\n\n  const qScaleX = qScaleXValue;\n  const qScaleY = qScaleYValue;\n  const qRotate = qRotateValue;\n  const kScaleX = kScaleXValue;\n  const kScaleY = kScaleYValue;\n  const kRotate = kRotateValue;\n\n  const Q = attentionEmbeddings.map(vec =&gt;\n    transformVector(vec, qScaleX, qScaleY, qRotate)\n  );\n  const K = attentionEmbeddings.map(vec =&gt;\n    transformVector(vec, kScaleX, kScaleY, kRotate)\n  );\n\n  const scores = Q.map(q =&gt; K.map(k =&gt; q[0] * k[0] + q[1] * k[1]));\n\n  const attentionWeights = scores.map(row =&gt; {\n    const maxScore = Math.max(...row);\n    const expScores = row.map(s =&gt; Math.exp(s - maxScore));\n    const sumExp = expScores.reduce((a, b) =&gt; a + b, 0);\n    return expScores.map(e =&gt; e / sumExp);\n  });\n\n  const heatmapData = (() =&gt; {\n    const data = [];\n    for (let i = 0; i &lt; attentionWords.length; i++) {\n      for (let j = 0; j &lt; attentionWords.length; j++) {\n        data.push({\n          Query: attentionWords[i],\n          Key: attentionWords[j],\n          Weight: attentionWeights[i][j]\n        });\n      }\n    }\n    return data;\n  })();\n\n  const heatmapPlot = Plot.plot({\n    width: 320,\n    height: 320,\n    marginTop: 50,\n    marginBottom: 50,\n    marginLeft: 70,\n    marginRight: 80,\n    style: {\n      background: \"white\",\n      color: \"black\"\n    },\n    x: {\n      label: \"Key Word\",\n      domain: attentionWords\n    },\n    y: {\n      label: \"Query Word\",\n      domain: attentionWords\n    },\n    color: {\n      scheme: \"Blues\",\n      label: \"Attention\",\n      legend: true\n    },\n    marks: [\n      Plot.cell(heatmapData, {\n        x: \"Key\",\n        y: \"Query\",\n        fill: \"Weight\",\n        tip: true\n      }),\n      Plot.text(heatmapData, {\n        x: \"Key\",\n        y: \"Query\",\n        text: d =&gt; d.Weight.toFixed(2),\n        fill: d =&gt; d.Weight &gt; 0.35 ? \"white\" : \"black\",\n        fontSize: 11\n      }),\n      Plot.text([{ x: 0, y: 0 }], {\n        x: () =&gt; attentionWords.length / 2 - 0.5,\n        y: () =&gt; -0.8,\n        text: () =&gt; \"Attention Weights (Softmax)\",\n        fontSize: 12,\n        fontWeight: \"bold\",\n        frameAnchor: \"top\",\n        fill: \"black\"\n      })\n    ]\n  });\n\n  return html`&lt;div style=\"display: flex; justify-content: center;\"&gt;\n    ${heatmapPlot}\n  &lt;/div&gt;`;\n}\n\n\n\n\n\n\nRows represent words asking for context (Queries). Columns represent words providing context (Keys). Each cell (i,j) indicates how much word i attends to word j. Each row sums to 1, forming a probability distribution over context words.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Transformers"
    ]
  },
  {
    "objectID": "m04-text/transformers.html#multi-head-attention",
    "href": "m04-text/transformers.html#multi-head-attention",
    "title": "Transformers",
    "section": "Multi-head Attention",
    "text": "Multi-head Attention\n\n\n\n\n\nPutting it all together (query-key-value transformation, attention matrix, and softmax normalization), this forms one attention head of the transformer. We can have multiple attention heads in parallel, each with its own query-key-value transformation, attention matrix, and softmax normalization. The output of the attention heads are concatenated and then passed through a linear transformation to produce the final output.\n\n\\text{Output} = \\text{Linear}(\\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h))\n\nThis is one attention block of the transformer. Having parallel attention heads is a powerful technique to capture different aspects of the input data. The model can learn multiple relationships between the words in the input data.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Transformers"
    ]
  },
  {
    "objectID": "m04-text/transformers.html#transformer-architecture",
    "href": "m04-text/transformers.html#transformer-architecture",
    "title": "Transformers",
    "section": "Transformer Architecture",
    "text": "Transformer Architecture\nLet’s step back and look at the transformer architecture at a high level. We base our discussion on the original Transformer paper, “Attention Is All You Need”. Note that the transformer architecture has evolved since then, with many variants.\n\nEncoder Module\n\n\n\n\n\nThe encoder module consists of position embedding, multi-head attention, residual connection, and layer normalization, along with feed-forward networks. Let us go through each component in detail.\n\nPosition Embedding\n\n\n\n\n\nIn the encoder module, we start from the positional encoding, which fixes a key issue: the attention modules are permutation invariant. That is, attention produces the same output even if we shuffle the words in the sentence. But position matters in language understanding and generation. Position encoding fixes this issue.\nLet’s approach position encoding from a naive perspective. Suppose we have a sequence of T token embeddings, denoted by x_1, x_2, ..., x_T, each a d-dimensional vector. A simple way to encode position is to add a position index to each token embedding:\n\nx_t := x_t + \\beta t,\n\nwhere t = 1, 2, ..., T is the position index of the token in the sequence, and \\beta is the step size. This appears simple but has critical problems. First, the position index can be arbitrarily large. When models see sequences longer than those in training data, they may suffer because they’ll be exposed to position indices they’ve never seen before. Second, the position index is discrete, meaning the model cannot capture position information smoothly.\nBecause this naive approach has problems, consider another approach. Let’s represent position using a binary vector of length d. For example, with d=4:\n\n\\begin{align*}\n  0: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} & &\n  8: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n  1: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} & &\n  9: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} \\\\\n  2: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} & &\n  10: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n  3: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} & &\n  11: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} \\\\\n  4: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} & &\n  12: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n  5: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} & &\n  13: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} \\\\\n  6: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} & &\n  14: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n  7: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} & &\n  15: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} \\\\\n\\end{align*}\n\nThen, use the binary vector as the position embedding:\n\nx_{t,i} := x_{t,i} + \\text{Pos}(t, i),\n\nwhere \\text{Pos}(t, i) is the position embedding vector of position index t and dimension index i. This representation is bounded between 0 and 1, yet still discrete.\nAn elegant position embedding, used in transformers, is sinusoidal position embedding. It appears complicated but stay with me.\n\n\\text{Pos}(t, i) =\n\\begin{cases}\n\\sin\\left(\\dfrac{t}{10000^{2i/d}}\\right), & \\text{if } i \\text{ is even} \\\\\n\\cos\\left(\\dfrac{t}{10000^{2i/d}}\\right), & \\text{if } i \\text{ is odd}\n\\end{cases},\n\nwhere i is the dimension index of the position embedding vector. This position embedding is added to the input token embedding as:\n\nx_{t,i} := x_{t,i} + \\text{Pos}(t, i),\n\nIt appears complicated, but it’s a continuous version of the binary position embedding above. To see this, let’s plot the position embedding for the first 100 positions.\n\n\n\n\n\nThe position embedding exhibits the alternating pattern (vertically) with frequency increasing as the dimension index increases (horizontal axis). Additionally, sinusoidal position embedding is continuous, allowing the model to capture position information smoothly.\nAnother key property: the dot similarity between two position embedding vectors represents the similarity between the two positions, regardless of the position index.\n\n\n\n\n\nThe dot similarity between position embedding vectors represents the distance between positions, regardless of the position index.\nWhy additive position embedding? Sinusoidal position embedding is additive, altering the token embedding. Alternatively, one might concatenate the position embedding to the token embedding: x_{t,i} := [x_{t,i}; \\text{Pos}(t, i)]. This makes it easier for a model to distinguish position from token information. So why not use concatenation? One reason: concatenation requires a larger embedding dimension, increasing the number of parameters. Instead, adding the position embedding creates an interesting effect in the attention mechanism. Interested readers can check out this Reddit post.\nAbsolute position embedding is what we discussed above, where each position is represented by a unique vector. Relative position embedding, on the other hand, represents the position difference between two positions rather than the absolute position. Relative position embedding can be implemented by adding a learnable scalar to the unnormalized attention scores before softmax:\n\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T + B}{\\sqrt{d_k}}\\right)V\n\nwhere B is a learnable offset matrix added to the unnormalized attention scores. The matrix B is a function of the position difference between query and key: B = f(i-j), where i and j are the position indices of query and key. Such formulation is useful when the model needs to capture relative position between tokens.\n\n\nResidual Connection\n\n\n\n\n\nAnother important component is the residual connection. The input is first passed through multi-head attention, followed by layer normalization. Notice the parallel path from input to the output of the attention module. This is called a residual connection, or skip connection. It’s a technique used to stabilize the training of deep neural networks by mitigating the problem of too large or too small input values, which can cause network instability.\nLet’s denote by f the neural network we want to train (the multi-head attention or feed-forward networks in the transformer block). The residual connection is defined as:\n\n\\underbrace{x_{\\text{out}}}_{\\text{output}} = \\underbrace{x_{\\text{in}}}_{\\text{input}} + \\underbrace{f(x_{\\text{in}})}_{\\text{component}}.\n\nRather than learning the complete mapping from input to output, the network f learns to model the residual (difference) between them. This is particularly advantageous when the desired transformation approximates an identity mapping, as the network can simply learn to output values near zero.\nResidual connections help prevent the vanishing gradient problem. Deep learning models like LLMs consist of many layers, trained to minimize the loss function {\\cal L}_{\\text{loss}} with respect to parameters \\theta. The gradient of the loss is computed using the chain rule:\n\n\\frac{\\partial {\\cal L}_{\\text{loss}}}{\\partial \\theta} = \\frac{\\partial {\\cal L}_{\\text{loss}}}{\\partial f_L} \\cdot \\frac{\\partial f_L}{\\partial f_{L-1}} \\cdot \\frac{\\partial f_{L-1}}{\\partial f_{L-2}} \\cdot ... \\cdot \\frac{\\partial f_{l+1}}{\\partial f_l} \\cdot \\frac{\\partial f_l}{\\partial \\theta}\n\nwhere f_i is the output of the i-th layer. The gradient vanishing problem occurs when the individual terms \\frac{\\partial f_{i+1}}{\\partial f_i} are less than 1. As a result, the gradient becomes smaller and smaller as it flows backward through earlier layers. By adding the residual connection, the gradient for the individual term becomes:\n\n\\frac{\\partial x_{i+1}}{\\partial x_i} = 1 + \\frac{\\partial f_i(x_i)}{\\partial x_i}\n\nNotice the “+1” term, which is the direct path from input to output. The chain rule is thus modified to include this term. When we expand the product, we can group terms by their order (how many \\partial f_i terms are multiplied together):\n1 + O_1 + O_2 + O_3 + ...\nwhere the O_n terms represent various combinations of gradients at different orders. Without the residual connection, we only have the highest-order terms, which are subject to the gradient vanishing problem. With the residual connection, we have lower-order terms like O_1, O_2, O_3, ..., which are less susceptible to gradient vanishing.\nResidual connections are an architectural innovation that allows neural networks to be much deeper without degrading performance. They were proposed by He et al. for image processing from Microsoft Research.\nResidual connections also help prevent gradient explosion by providing alternative paths for gradients to flow. By distributing gradients between the residual path and the learning component path, the gradient is less likely to explode.\n\n\nLayer Normalization\n\n\n\n\n\nIn transformer models, you find multiple layer normalization steps. Layer normalization is a technique used to stabilize the training of deep neural networks. It mitigates the problem of too large or too small input values, which can cause network instability. More specifically, layer normalization is computed as:\n\n\\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sigma} + \\beta,\n\nwhere \\mu and \\sigma are the mean and standard deviation of the input, \\gamma is the scaling factor, and \\beta is the shifting factor. The variables \\gamma and \\beta are learnable parameters initialized to 1 and 0, and updated during training.\nNote that layer normalization is applied to individual tokens. The normalization is token-wise, rather than feature-wise. The mean and standard deviation are calculated for each token across all feature dimensions. This differs from feature-wise normalization, where mean and standard deviation are calculated for each feature across all tokens.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Transformers"
    ]
  },
  {
    "objectID": "m04-text/transformers.html#decoder-module",
    "href": "m04-text/transformers.html#decoder-module",
    "title": "Transformers",
    "section": "Decoder Module",
    "text": "Decoder Module\n\n\n\n\n\n\nCausal Attention\n\n\n\n\n\nOne key advantage of transformers is their ability to generate contextualized vectors in parallel. Recurrent neural networks (RNNs) read the input sequence sequentially, limiting parallelism. Transformer models, on the other hand, can compute attention scores and weighted averages of value vectors in parallel, generating contextualized vectors at once. This speeds up training.\nIn the decoder module, a vector is contextualized by attending to the previous vectors in the sequence. Importantly, it should not see the future token vectors, as that’s what the model is tasked to predict. We prevent this by setting the attention scores to zero for future tokens.\nAnother benefit of causal attention: the model doesn’t suffer from the error accumulation problem, where prediction error from one step carries over to the next.\nTo implement the masking, we set the attention scores to negative infinity for future tokens before the softmax operation, effectively zeroing out their contribution:\n\n\\text{Mask}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T + M}{\\sqrt{d_k}}\\right)V\n\nwhere M is a matrix with -\\infty for positions corresponding to future tokens. The result is attention scores where tokens attend only to previous tokens.\n\n\nCross-Attention\n\n\n\n\n\nCross-attention occurs when the Query comes from one sequence (like a sentence being generated) and the Keys and Values come from another sequence (like the input sentence you want to translate). Here, the model learns to align information from input to output, a sort of bilingual dictionary lookup, but learned and fuzzy.\nThe mechanism works by using queries (Q) from the decoder’s previous layer and keys (K) and values (V) from the encoder’s output. This enables each position in the decoder to attend to the full encoder sequence without any masking, since encoding is already complete.\nFor instance, in translating “I love you” to “Je t’aime”, cross-attention helps each French word focus on relevant English words. “Je” attends to “I”, and “t’aime” to “love”. This maintains semantic relationships between input and output.\nThe cross-attention formula is:\n\n\\text{CrossAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\nwhere Q comes from the decoder and K, V come from the encoder. This effectively bridges the encoding and decoding processes.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Transformers"
    ]
  },
  {
    "objectID": "m04-text/transformers.html#putting-it-all-together",
    "href": "m04-text/transformers.html#putting-it-all-together",
    "title": "Transformers",
    "section": "Putting It All Together",
    "text": "Putting It All Together\nLet’s overview the transformer architecture and see how the components fit into the overall architecture.\n\n\n\n\n\nWe hope that you now have a better understanding of the transformer architecture and how the components fit together into the overall architecture.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Transformers"
    ]
  },
  {
    "objectID": "m04-text/transformers.html#the-key-insight",
    "href": "m04-text/transformers.html#the-key-insight",
    "title": "Transformers",
    "section": "The Key Insight",
    "text": "The Key Insight\nEvery time you use GPT (ChatGPT, Claude, Gemini, etc.), you’re seeing transformers in action. Transformers don’t “think”. They perform statistical pattern matching at scale.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Transformers"
    ]
  },
  {
    "objectID": "m04-text/sentence-transformers.html",
    "href": "m04-text/sentence-transformers.html",
    "title": "Sentence Transformers",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nBERT produces a matrix of token vectors. Sentence Transformers collapse that matrix into a single coordinate, turning semantic similarity into geometric distance. This enables fast semantic search, clustering, and similarity comparisons across large document collections.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Sentence Transformers"
    ]
  },
  {
    "objectID": "m04-text/sentence-transformers.html#the-challenge-from-matrices-to-coordinates",
    "href": "m04-text/sentence-transformers.html#the-challenge-from-matrices-to-coordinates",
    "title": "Sentence Transformers",
    "section": "The Challenge: From Matrices to Coordinates",
    "text": "The Challenge: From Matrices to Coordinates\nBERT gives you a vector for every token in a sentence. If you want to compare two sentences, you’re stuck comparing two messy matrices of varying sizes. The naive approach (averaging all token vectors) throws away positional information and treats every word equally, which is wrong. The word “not” in “not good” should drastically change the sentence embedding, but simple averaging dilutes its impact.\nSentence-BERT (SBERT) solves this by training a Siamese Network. The same BERT model processes two sentences independently, producing their respective token matrices. We then apply pooling (mean, max, or CLS-token extraction) to collapse each matrix into a single vector. The training objective is contrastive: if the sentences are semantically similar (paraphrases), their vectors should be close in Euclidean or cosine space. If they’re unrelated, their vectors should be distant.\nThink of it like creating a library catalog. Instead of storing every word on every page, you compress each book into a single Dewey Decimal number. Books on similar topics get similar numbers, enabling efficient retrieval. The compression loses fine-grained detail, but gains search speed.\nThe mathematical trick is the Siamese architecture. Weight sharing ensures both sentences are embedded into the same vector space using identical transformations. This makes the distance between vectors meaningful: similar sentences cluster together, dissimilar ones push apart.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Sentence Transformers"
    ]
  },
  {
    "objectID": "m04-text/sentence-transformers.html#how-to-use-sentence-transformers",
    "href": "m04-text/sentence-transformers.html#how-to-use-sentence-transformers",
    "title": "Sentence Transformers",
    "section": "How to Use Sentence Transformers",
    "text": "How to Use Sentence Transformers\nSentence Transformers enable semantic search, clustering, and similarity comparisons. Let’s see how to use them in practice.\n\nBasic Semantic Search\nHere’s how to encode sentences and find the most similar matches:\n\nfrom sentence_transformers import SentenceTransformer, util\n\n# Load a pre-trained model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\ncorpus = [\n    \"A man is eating food.\",\n    \"A man is eating a piece of bread.\",\n    \"The girl is carrying a baby.\",\n    \"A man is riding a horse.\",\n    \"A woman is playing violin.\",\n    \"Two men pushed carts through the woods.\",\n    \"A man is riding a white horse on an enclosed ground.\",\n    \"A monkey is playing drums.\",\n    \"Someone in a gorilla costume is playing a set of drums.\"\n]\n\n# Encode all sentences into 384-dimensional vectors\ncorpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n\nquery = \"A man is eating pasta.\"\nquery_embedding = model.encode(query, convert_to_tensor=True)\n\n# Compute cosine similarities\nhits = util.semantic_search(query_embedding, corpus_embeddings, top_k=3)\n\nprint(f\"Query: {query}\")\nprint(\"\\nTop 3 most similar sentences:\")\nfor hit in hits[0]:\n    print(f\"{corpus[hit['corpus_id']]} (Score: {hit['score']:.4f})\")\n\nExpected output shows that the model correctly identifies “eating pasta” is semantically closest to “eating food” and “eating bread”, even though the exact words don’t match. This is semantic search: matching by meaning, not keywords.\n\n\nClustering Documents\nYou can also cluster documents by their semantic content:\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\nsentences = [\n    \"Python is a programming language\",\n    \"Java is used for software development\",\n    \"The cat sat on the mat\",\n    \"Dogs are loyal animals\",\n    \"Machine learning is a subset of AI\",\n    \"Neural networks mimic the brain\",\n]\n\nembeddings = model.encode(sentences)\n\n# Cluster into 2 groups\nnum_clusters = 2\nclustering_model = KMeans(n_clusters=num_clusters, random_state=42)\nclustering_model.fit(embeddings)\ncluster_assignment = clustering_model.labels_\n\nclustered_sentences = {}\nfor sentence_id, cluster_id in enumerate(cluster_assignment):\n    if cluster_id not in clustered_sentences:\n        clustered_sentences[cluster_id] = []\n    clustered_sentences[cluster_id].append(sentences[sentence_id])\n\nfor cluster_id, cluster_sentences in clustered_sentences.items():\n    print(f\"\\nCluster {cluster_id + 1}:\")\n    for sentence in cluster_sentences:\n        print(f\"  - {sentence}\")\n\nExpected clustering shows the model separates technical/programming sentences from animal-related sentences without any labeled data.\n\n\nChoosing the Right Model\nDifferent Sentence Transformer models optimize for different trade-offs. The all-MiniLM-L6-v2 model is fast and lightweight (384 dimensions), good for most applications. The all-mpnet-base-v2 model offers higher quality (768 dimensions), slower but more accurate. The multi-qa-mpnet-base-dot-v1 model is optimized for question-answering and retrieval tasks. The paraphrase-multilingual-mpnet-base-v2 model supports 50+ languages.\nChoose based on your constraints: speed vs. accuracy, monolingual vs. multilingual, general-purpose vs. domain-specific.\n\n\nArchitecture: The Siamese Network\nThe key innovation is the Siamese Network architecture:\n\n\n\nSiamese Network\n\n\nBoth sentences pass through the same BERT model (shared weights). This ensures they’re embedded into a common vector space. The pooling layer then collapses each token matrix into a single vector. During training, the loss function pushes similar sentence pairs together and dissimilar pairs apart.\nCommon pooling strategies include mean pooling (average all token vectors, most common), max pooling (take element-wise maximum across tokens), and CLS-token (use the [CLS] token’s final hidden state, BERT’s built-in sentence representation).\nMean pooling generally works best because it captures information from all tokens while being robust to varying sentence lengths.\n\n\nWhere This Breaks\nStatic compression is a limitation: a sentence gets exactly one vector, regardless of context. “The bank” in “the river bank” and “the financial bank” might get similar embeddings if they share enough surrounding words. The model compresses meaning into a fixed point, losing nuance.\nWord order sensitivity is another concern: “The dog bit the man” and “The man bit the dog” share the same words. If the model relies too heavily on lexical overlap (bag-of-words similarity), they’ll end up dangerously close in vector space. Good models learn syntax, but they’re not perfect.\nComputational cost matters too. Although retrieval is fast (dot products), encoding large corpora is expensive. Encoding 1 million sentences with a large model can take hours. Pre-compute and cache embeddings whenever possible.\nDomain shift is a practical issue: models trained on general text (Wikipedia, news) may perform poorly on specialized domains (medical, legal). Fine-tuning on domain-specific data helps, but requires labeled sentence pairs.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Sentence Transformers"
    ]
  },
  {
    "objectID": "m04-text/sentence-transformers.html#the-key-takeaway",
    "href": "m04-text/sentence-transformers.html#the-key-takeaway",
    "title": "Sentence Transformers",
    "section": "The Key Takeaway",
    "text": "The Key Takeaway\nSentence Transformers collapse BERT’s token matrix into a single vector using Siamese Networks and contrastive learning. The result is fast semantic search: encode once, compare with dot products. Choose your pooling strategy and model size based on speed-accuracy trade-offs, and remember that compression always loses information.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Sentence Transformers"
    ]
  },
  {
    "objectID": "m04-text/overview.html",
    "href": "m04-text/overview.html",
    "title": "Overview",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module opens the hood of Large Language Models to understand the revolution in Natural Language Processing. We will explore how LLMs work, master the mechanics of tokenization and transformers, and uncover the mathematical foundations of vector space models where meaning emerges as geometry.\nAt the core of agentic systems lies the Large Language Model (LLM). They act as a kernel of the operating system, and unlike actual computer systems, they speak in natural language. But how do LLMs understand natural language in the first place?\nThis module guides you through the foundational concepts of word embeddings to the state-of-the-art LLMs reshaping the world. By the end, you’ll understand both how to use these powerful tools and the mechanisms driving them, enabling you to build intelligent systems that truly work with text.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Overview"
    ]
  },
  {
    "objectID": "m04-text/overview.html#large-language-models-in-practice",
    "href": "m04-text/overview.html#large-language-models-in-practice",
    "title": "Overview",
    "section": "Large Language Models in Practice",
    "text": "Large Language Models in Practice\nWe start by interacting with the giants. You’ll explore what LLMs are, how they work at a high level, and how to control them effectively. See Large Language Models in Practice.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Overview"
    ]
  },
  {
    "objectID": "m04-text/overview.html#the-mechanics-of-meaning",
    "href": "m04-text/overview.html#the-mechanics-of-meaning",
    "title": "Overview",
    "section": "The Mechanics of Meaning",
    "text": "The Mechanics of Meaning\nHow do computers read? We’ll dive into the tokenization process and the architecture that makes it all possible: the Transformer. The key insight is that meaning emerges through context, not from isolated words. Explore Tokenization: Unboxing How LLMs Read Text, Transformers, and BERT, GPT, and Sentence Transformers.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Overview"
    ]
  },
  {
    "objectID": "m04-text/overview.html#vector-space-models",
    "href": "m04-text/overview.html#vector-space-models",
    "title": "Overview",
    "section": "Vector Space Models",
    "text": "Vector Space Models\nWe’ll uncover the mathematical foundation of modern NLP by representing words as vectors in high-dimensional space where meaning is geometric. Discover how Word Embeddings capture semantic relationships, learn Meaning as Direction with SemAxis, and examine Word Bias in learned representations.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Overview"
    ]
  },
  {
    "objectID": "m04-text/llm-intro.html",
    "href": "m04-text/llm-intro.html",
    "title": "Large Language Models in Practice",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nLarge language models don’t understand language—they compress statistical regularities from billions of text samples into probability distributions. We’ll explore what LLMs are, how they work at a high level, and how to control them effectively as tools for practical tasks.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Large Language Models"
    ]
  },
  {
    "objectID": "m04-text/llm-intro.html#do-llms-understand-language",
    "href": "m04-text/llm-intro.html#do-llms-understand-language",
    "title": "Large Language Models in Practice",
    "section": "Do LLMs Understand Language?",
    "text": "Do LLMs Understand Language?\nWhat do you think about this question: Can LLMs understand the world and reason about it?\nOne might argue that fluency demonstrates understanding. This is the intuition behind Turing’s 1950 test: if you can’t tell it’s a machine, treat it as intelligent. Fluency implies comprehension. But let’s examine counter-arguments starting with ELIZA.\nELIZA, developed by Joseph Weizenbaum in the mid-1960s, is widely considered one of the first chatbots. It simulated a Rogerian psychotherapist by using simple pattern matching and keyword substitution to generate responses. Despite its lack of true understanding, ELIZA famously convinced many users that they were conversing with an intelligent entity, highlighting the human tendency to anthropomorphize technology and the limitations of the Turing Test.\n\n\nAnother argument against fluency is the Chinese Room argument, proposed by philosopher John Searle. Imagine a person in a room who receives Chinese characters and, using an English rulebook, manipulates these symbols to produce new Chinese characters. To an outside observer, it appears the room understands Chinese. However, the person inside merely follows instructions to manipulate symbols without understanding their meaning. Searle argues that this is analogous to how computers, including LLMs, operate: they process symbols based on rules without genuine comprehension, raising questions about whether they can truly “understand” language or the world.\n\n\nSo do LLMs understand the world? Probably not in the same way we do. LLMs are lossy compression algorithms, compressing data into their parameters to generate fluent outputs. To predict “The capital of France is __,” the model must compress not just the fact (Paris) but the statistical regularities governing how facts appear in text. It learns that capitals follow “The capital of,” that France is a country, that countries have capitals. This compression is probabilistic, not factual. The model stores P(word{n+1} | word_1, …, word_n), which words tend to follow which other words in which contexts. Just as a lottery memorizer stores patterns of number sequences, the LLM stores patterns of word sequences.\n\n\n\n\n\nTraining feeds the model billions of sentences. For each sentence, the model predicts the next word, compares its prediction to the actual next word, and adjusts its parameters to increase the probability of the correct word. Repeat trillions of times. The result: a compressed representation of how language behaves statistically. The model doesn’t learn “Paris is the capital of France” as a fact; it learns that in contexts matching the pattern [The capital of France is], the token “Paris” appears with high probability. The lottery memorizer doesn’t understand what draws mean; it just knows what patterns appear most often. This is why LLMs create hallucinations—fluent but false outputs. Truth and fluency correlate in the training data, so the model is mostly truthful. But in the tails—obscure topics, recent events, precise recall—fluency diverges from truth, and the model follows fluency.\nKeep this limitation in mind and use LLMs as a tool to scale pattern recognition, not judgment. Let’s learn how to utilize them.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Large Language Models"
    ]
  },
  {
    "objectID": "m04-text/llm-intro.html#setting-up-ollama",
    "href": "m04-text/llm-intro.html#setting-up-ollama",
    "title": "Large Language Models in Practice",
    "section": "Setting Up Ollama",
    "text": "Setting Up Ollama\nFor this course, we use Ollama, a tool for running LLMs locally, with Gemma 3N, a 4-billion parameter open-source model. It’s free, private, and capable enough for research tasks. Visit ollama.ai, download the installer, and verify installation.\nollama --version\nollama pull gemma3n:latest\nollama run gemma3n:latest \"What is a complex system?\"\nIf you receive a coherent response, install the Python client and send your first prompt.\npip install ollama\n\nimport ollama\n\nparams_llm = {\"model\": \"gemma3n:latest\", \"options\": {\"temperature\": 0.3}}\n\nresponse = ollama.generate(\n    prompt=\"Explain emergence in two sentences.\",\n    **params_llm\n)\n\nprint(response.response)\n\nEmergence is when complex patterns and behaviors arise from simple interactions between individual components in a system. These emergent properties are not predictable from the properties of the individual parts alone, representing a novel level of organization. \n\n\n\nRun this code twice. You’ll get different outputs. Why? Because LLMs sample from probability distributions. The temperature parameter controls this randomness. Lower values (0.1) make outputs more deterministic; higher values (1.0) increase diversity. You’re controlling how far into the tail of the probability distribution the model samples. Low temperature: the model picks the most likely next word. High temperature: it ventures into less probable territory. Sometimes that produces creativity. Sometimes it produces nonsense.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Large Language Models"
    ]
  },
  {
    "objectID": "m04-text/llm-intro.html#research-applications",
    "href": "m04-text/llm-intro.html#research-applications",
    "title": "Large Language Models in Practice",
    "section": "Research Applications",
    "text": "Research Applications\nThe strategy is simple: use LLMs for tasks where speed trumps precision, then verify the outputs that matter. Three workflows demonstrate this pattern.\n\nAbstract Summarization\nYou collected 50 papers on network science. Which deserve detailed reading? You don’t have time to read all 50 abstracts carefully. An LLM scans them in seconds.\n\nabstract = \"\"\"\nCommunity detection in networks is a fundamental problem in complex systems.\nWhile many algorithms exist, most assume static networks. We propose a dynamic\ncommunity detection algorithm that tracks evolving communities over time using\na temporal smoothness constraint. We evaluate our method on synthetic and real\ntemporal networks, showing it outperforms static methods applied to temporal\nsnapshots. Our approach reveals how communities merge, split, and persist in\nsocial networks, biological systems, and transportation networks.\n\"\"\"\n\nprompt = f\"Summarize this abstract in one sentence:\\n\\n{abstract}\"\nresponse = ollama.generate(prompt=prompt, **params_llm)\nprint(response.response)\n\nThis paper introduces a novel dynamic community detection algorithm that effectively tracks evolving communities in networks over time, outperforming static methods and revealing community dynamics in various real-world systems.\n\n\n\n\n\n\nThe model captures the pattern: propose method, evaluate, outperform baselines. It doesn’t understand the paper; it has seen enough academic abstracts to recognize the structure. For multiple abstracts, loop through them.\n\nfor i, abstract in enumerate([\"Abstract 1...\", \"Abstract 2...\"], 1):\n    response = ollama.generate(prompt=f\"Summarize:\\n\\n{abstract}\", **params_llm)\n    print(f\"{i}. {response.response}\")\n\n1. Please provide me with \"Abstract 1\"! I need the text of the abstract to be able to summarize it for you. \n\nJust paste the abstract here, and I'll give you a concise summary. 😊 \n\nI'm ready when you are!\n2. Please provide me with the content of \"Abstract 2\"! I need the text of the abstract to be able to summarize it for you. \n\nJust paste the abstract here, and I'll give you a concise summary. 😊 \n\n\n\n\nLocal models are slow (2–5 seconds per abstract). For thousands of papers, switch to cloud APIs. But the workflow scales: delegate skimming to the model, retain judgment for yourself. I ran this on 200 abstracts about power-law distributions. Gemma flagged the 15 that used preferential attachment models. Saved me 4 hours. I still read all 15 myself.\n\n\nStructured Extraction\nTurn unstructured text into structured data automatically.\n\nabstract = \"\"\"\nWe analyze scientific collaboration networks using 5 million papers from\n2000-2020. Using graph neural networks and community detection, we identify\ndisciplinary boundaries and interdisciplinary bridges. Interdisciplinarity\nincreased 25%, with physics and CS showing strongest cross-connections.\n\"\"\"\n\nprompt = f\"\"\"Extract: Domain, Methods, Key Finding\\n\\n{abstract}\\n\\nFormat:\\nDomain:...\\nMethods:...\\nKey Finding:...\"\"\"\nresponse = ollama.generate(prompt=prompt, **params_llm)\nprint(response.response)\n\nHere's the extraction in the requested format:\n\nDomain: Scientific Collaboration Networks\nMethods: Graph Neural Networks, Community Detection, Analysis of 5 million papers (2000-2020)\nKey Finding: Interdisciplinarity increased by 25% between 2000-2020, with the strongest cross-connections observed between Physics and Computer Science.\n\n\n\n\n\n\nScale this to hundreds of papers for meta-analysis. Always verify. LLMs misinterpret obscure terminology and fabricate plausible-sounding technical details when uncertain. Remember: the model is pattern-matching against academic writing it’s seen, not reasoning about your domain.\n\n\nHypothesis Generation\nLLMs pattern-match against research questions they’ve encountered in training data.\n\ncontext = \"\"\"I study concept spread in citation networks. Highly cited papers\ncombine existing concepts novelty. What should I study next?\"\"\"\n\nprompt = f\"\"\"Suggest three follow-up research questions:\\n\\n{context}\"\"\"\nresponse = ollama.generate(prompt=prompt, **params_llm)\nprint(response.response)\n\nOkay, here are three follow-up research questions, building on your work on concept spread in citation networks, focusing on highly cited papers and the interplay of existing concepts and novelty.  I've tried to offer a mix of methodological and theoretical directions:\n\n**1.  How does the *type* of novelty (e.g., incremental, radical, convergent) in highly cited papers influence the rate and direction of concept spread?**\n\n*   **Rationale:** You've identified that highly cited papers combine existing concepts with novelty.  However, the *nature* of that novelty likely matters.  Is it a small tweak to an existing idea (incremental), a completely new paradigm (radical), or a synthesis of multiple existing ideas (convergent)?  Different types of novelty might spread differently through the citation network.\n*   **Methodology:**  This could involve:\n    *   **Concept Extraction & Categorization:**  Develop a method (potentially using NLP techniques like topic modeling or knowledge graph extraction) to identify and categorize the types of novelty present in highly cited papers.\n    *   **Network Analysis:**  Analyze the citation network to see if papers with different types of novelty have different citation patterns (e.g., different citation paths, different communities of citing papers).\n    *   **Temporal Analysis:** Track the spread of concepts over time, looking for differences in the spread dynamics based on the type of novelty.\n*   **Potential Insights:**  This could reveal whether incremental novelty spreads quickly within a well-established field, while radical novelty requires more time and a different set of initial citations to gain traction.\n\n**2.  To what extent does the *citation context* (i.e., how a highly cited paper is cited) mediate the relationship between novelty and concept spread?**\n\n*   **Rationale:**  It's not just *that* a paper is highly cited, but *how* it's cited that matters.  Is it cited as a foundational work, a contrasting viewpoint, a building block for further research, or something else? The citation context could significantly influence how the novelty is perceived and incorporated by subsequent researchers.\n*   **Methodology:**\n    *   **Citation Context Analysis:**  Develop a method to classify the citation context of highly cited papers (e.g., using NLP to analyze the surrounding text in citations).\n    *   **Network Analysis:**  Analyze the citation network to see if the citation context of a paper is correlated with the subsequent spread of concepts.\n    *   **Sentiment Analysis:**  Use sentiment analysis on the citation text to gauge the attitude towards the novelty being presented.\n*   **Potential Insights:**  This could reveal whether a paper's novelty is more likely to spread if it's cited as a key foundational work, or if it's more likely to be incorporated if it's cited as a contrasting viewpoint that sparks debate.\n\n**3.  Can we identify \"concept amplifiers\" – papers that, due to their specific combination of existing concepts and novelty, act as particularly effective catalysts for concept spread?**\n\n*   **Rationale:**  Not all highly cited papers are created equal in terms of their ability to spread concepts. Some papers might be inherently more influential due to their specific combination of existing knowledge and new ideas.\n*   **Methodology:**\n    *   **Feature Engineering:**  Develop a set of features that capture the combination of existing concepts and novelty in a paper (e.g., the number of distinct concepts introduced, the degree of overlap with existing concepts, the \"surprise\" or unexpectedness of the novelty).\n    *   **Machine Learning:**  Use machine learning techniques (e.g., regression, classification) to identify papers that are strong predictors of concept spread, based on their feature values.\n    *   **Network Analysis:**  Analyze the citation network to see if papers identified as \"concept amplifiers\" have distinct network properties (e.g., high betweenness centrality, strong connections to diverse communities).\n*   **Potential Insights:**  This could lead to a better understanding of the factors that contribute to the influence of scientific papers and potentially inform strategies for promoting impactful research.\n\n\n\nThese questions are designed to be relatively focused and address different aspects of your initial research.  They also offer opportunities to combine quantitative network analysis with qualitative analysis of the content and context of citations.  I hope this helps! Let me know if you'd like me to elaborate on any of these or suggest alternative directions.\n\n\n\n\n\n\nTreat the model as a thought partner, not an oracle. It helps structure thinking but doesn’t possess domain expertise. The suggestions reflect patterns in how research questions are framed, not deep knowledge of your field.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Large Language Models"
    ]
  },
  {
    "objectID": "m04-text/llm-intro.html#failure-modes-and-boundaries",
    "href": "m04-text/llm-intro.html#failure-modes-and-boundaries",
    "title": "Large Language Models in Practice",
    "section": "Failure Modes and Boundaries",
    "text": "Failure Modes and Boundaries\nThe failure modes follow directly from the mechanism. LLMs fabricate plausibly because they optimize for fluency, not truth. Ask about a non-existent “Smith et al. quantum paper” and receive fluent academic prose describing results that never happened. Always verify citations. The model has seen thousands of papers cited in the format “Smith et al. (2023) demonstrated that…” and generates outputs matching that pattern even when the citation is fictional.\nContext limits are architectural. Models see only 2,000–8,000 tokens at once. Paste 100 abstracts and early ones are mathematically evicted from working memory. The model doesn’t “remember” them; they’re gone. Knowledge cutoffs are temporal. Gemma 3N’s training ended early 2024. Ask about recent events and receive outdated information or plausible fabrications constructed from pre-cutoff patterns.\nReasoning is absent. LLMs pattern-match, they don’t reason. Ask “How many r’s in ‘Strawberry’?” and the model might answer correctly via pattern matching against similar questions in training data, not by counting letters. Sometimes right. Often wrong. The model has no internal representation of what counting means.\nThese aren’t bugs to be fixed. They’re intrinsic to the architecture. Use LLMs to accelerate work, not replace judgment. They excel at summarizing text, extracting structure, reformulating concepts, brainstorming, generating synthetic examples, and translation. They fail at literature reviews without verification, factual claims without sources, statistical analysis, and ethical decisions. Harvest the center of the distribution where fluency and truth correlate. Defend against the tails where they diverge.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Large Language Models"
    ]
  },
  {
    "objectID": "m04-text/llm-intro.html#what-comes-next",
    "href": "m04-text/llm-intro.html#what-comes-next",
    "title": "Large Language Models in Practice",
    "section": "What Comes Next",
    "text": "What Comes Next\nYou’ve seen LLMs in practice: setup, summarization, extraction, limitations. But how do they actually work? What happens inside when you send a prompt? The rest of this module unboxes the technology: prompt engineering (communicating with LLMs), embeddings (representing meaning as numbers), transformers (the architecture enabling modern NLP), fundamentals (from word counts to neural representations). First, let’s master talking to machines.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Large Language Models"
    ]
  },
  {
    "objectID": "m04-text/semaxis.html",
    "href": "m04-text/semaxis.html",
    "title": "SemAxis: Meaning as Direction",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nMeaning in embeddings emerges entirely from contrast, not from inherent word properties. SemAxis provides a framework for defining semantic dimensions by subtracting antonym vectors, isolating specific axes that reveal how words align on dimensions like sentiment, intensity, or any conceptual opposition.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Semaxis"
    ]
  },
  {
    "objectID": "m04-text/semaxis.html#embedding-space-as-contrast",
    "href": "m04-text/semaxis.html#embedding-space-as-contrast",
    "title": "SemAxis: Meaning as Direction",
    "section": "Embedding Space as Contrast",
    "text": "Embedding Space as Contrast\nWe intuitively treat word embeddings as static maps where “king” is simply near “queen”. We assume the meaning is inherent to the coordinate itself, much like a city has a fixed latitude and longitude. This is a convenient fiction. In embedding space, meaning emerges entirely from contrast, which is the key concept of SemAxis.\nSemAxis is a way to define a semantic axis by subtracting the vector of an antonym from a word (e.g., v_{good} - v_{bad}). This isolates a semantic dimension, an “axis”, that ignores all other information.\nFormally, given two pole words w_+ and w_-, the axis is defined as:\n\nv_{\\text{axis}} = \\frac{v_{w_+} - v_{w_-}}{||v_{w_+} - v_{w_-}\\||_2}\n\nwhere the denominator is the L_2 norm of the difference vector that ensures the axis vector v_{\\text{axis}} is a unit vector. Using this “ruler”, we project words into this axis. Operationally, the position of a word w is given by the cosine similarity between v_{w} and v_{\\text{axis}}.\n\n\\text{Position of w on axis } v_{\\text{axis}} = \\cos(v_{\\text{axis}},v_{w})\n\nLet’s build a “Sentiment Compass” to measure the emotional charge of words that aren’t explicitly emotional.\nFirst, we load the standard GloVe embeddings.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gensim.downloader as api\n\n# Download and load pre-trained GloVe embeddings\nmodel = api.load(\"glove-wiki-gigaword-100\")",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Semaxis"
    ]
  },
  {
    "objectID": "m04-text/semaxis.html#defining-the-axis",
    "href": "m04-text/semaxis.html#defining-the-axis",
    "title": "SemAxis: Meaning as Direction",
    "section": "Defining the Axis",
    "text": "Defining the Axis\nWe define the axis not as a point, but as the difference vector between two poles. This vector points from “bad” to “good”.\n\ndef create_axis(pos_word, neg_word, model):\n    return model[pos_word] - model[neg_word]\n\n\n# The \"Sentiment\" Axis\nsentiment_axis = create_axis(\"good\", \"bad\", model)",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Semaxis"
    ]
  },
  {
    "objectID": "m04-text/semaxis.html#measuring-alignment",
    "href": "m04-text/semaxis.html#measuring-alignment",
    "title": "SemAxis: Meaning as Direction",
    "section": "Measuring Alignment",
    "text": "Measuring Alignment\nTo see where a word falls on this axis, we project it. Mathematically, this is the dot product (normalized). If the vector points in the same direction, the score is positive. If it points away, it is negative.\n\ndef get_score(word, axis, model):\n    v_word = model[word]\n    # Cosine similarity is just a normalized dot product\n    return np.dot(v_word, axis) / (np.linalg.norm(v_word) * np.linalg.norm(axis))\n\n\nwords = [\"excellent\", \"terrible\", \"mediocre\", \"stone\", \"flower\"]\nfor w in words:\n    print(f\"{w}: {get_score(w, sentiment_axis, model):.3f}\")\n\nexcellent: 0.523\nterrible: -0.208\nmediocre: -0.001\nstone: 0.181\nflower: 0.204",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Semaxis"
    ]
  },
  {
    "objectID": "m04-text/semaxis.html#robustness-via-centroids",
    "href": "m04-text/semaxis.html#robustness-via-centroids",
    "title": "SemAxis: Meaning as Direction",
    "section": "Robustness via Centroids",
    "text": "Robustness via Centroids\nSingle words are noisy. “Bad” might carry connotations of “naughty” or “poor quality”. To fix this, we don’t use single words. We use the centroid of a cluster of synonyms. This averages out the noise and leaves only the pure semantic signal.\n\ndef create_robust_axis(pos_word, neg_word, model, k=5):\n    # Get k nearest neighbors for both poles\n    pos_group = [pos_word]\n    pos_words = model.most_similar(pos_word, topn=k)\n    for word, _ in pos_words:\n        pos_group.append(word)\n\n    neg_group = [neg_word]\n    neg_words = model.most_similar(neg_word, topn=k)\n    for word, _ in neg_words:\n        neg_group.append(word)\n\n    # Average them to find the centroid\n    pos_vec = np.mean([model[w] for w in pos_group], axis=0)\n    neg_vec = np.mean([model[w] for w in neg_group], axis=0)\n\n    return pos_vec - neg_vec\n\n\nrobust_axis = create_robust_axis(\"good\", \"bad\", model)",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Semaxis"
    ]
  },
  {
    "objectID": "m04-text/semaxis.html#the-2d-semantic-space",
    "href": "m04-text/semaxis.html#the-2d-semantic-space",
    "title": "SemAxis: Meaning as Direction",
    "section": "The 2D Semantic Space",
    "text": "The 2D Semantic Space\nThe real power comes when we cross two axes. By plotting words against “Sentiment” and “Intensity” (Strong vs. Weak), we reveal relationships that a single list hides.\n\n\nCode\ndef plot_2d(words, axis_x, axis_y, model):\n    x_scores = [get_score(w, axis_x, model) for w in words]\n    y_scores = [get_score(w, axis_y, model) for w in words]\n\n    plt.figure(figsize=(5, 5))\n    plt.scatter(x_scores, y_scores)\n\n    for i, w in enumerate(words):\n        plt.annotate(\n            w,\n            (x_scores[i], y_scores[i]),\n            xytext=(5, 5),\n            textcoords=\"offset points\",\n            fontsize=16,\n        )\n\n    plt.axhline(0, color=\"k\", alpha=0.3)\n    plt.axvline(0, color=\"k\", alpha=0.3)\n    plt.xlabel(\"Sentiment (Bad -&gt; Good)\")\n    plt.ylabel(\"Intensity (Weak -&gt; Strong)\")\n    plt.show()\n\n\nintensity_axis = create_axis(\"strong\", \"weak\", model)\ntest_words = [\n    \"excellent\",\n    \"terrible\",\n    \"mediocre\",\n    \"mild\",\n    \"extreme\",\n    \"murder\",\n    \"charity\",\n]\n\nplot_2d(test_words, sentiment_axis, intensity_axis, model)\n\n\n\n\n\n2D Semantic Space",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Semaxis"
    ]
  },
  {
    "objectID": "m04-text/semaxis.html#the-key-insight",
    "href": "m04-text/semaxis.html#the-key-insight",
    "title": "SemAxis: Meaning as Direction",
    "section": "The Key Insight",
    "text": "The Key Insight\nTo define a concept, you must first define its opposite. Meaning isn’t stored in the word itself. It lives in the contrast space, the relationship between poles that defines an axis. SemAxis operationalizes this principle: by defining opposition, we isolate the dimension that matters.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Semaxis"
    ]
  },
  {
    "objectID": "m04-text/tokenization.html",
    "href": "m04-text/tokenization.html",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nLLMs don’t read words as you do. They read compressed fragments called tokens, optimized for probability engines. This section explores why subword tokenization matters, how it works, and what it means for model behavior.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m04-text/tokenization.html#why-not-just-words",
    "href": "m04-text/tokenization.html#why-not-just-words",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "Why Not Just Words?",
    "text": "Why Not Just Words?\nYou might assume an LLM reads text the way you do, word by word, treating each word as an atomic unit. This assumption is wrong. The model operates on tokens, which are subword chunks. These could be full words like “the”, word parts like “ingham”, or single characters like “B”. This choice isn’t arbitrary. It’s a geometric compression strategy.\nIf we used whole words, the vocabulary would balloon to millions of entries. Each entry requires a row in the embedding matrix, so memory scales linearly with vocabulary size. A 1-million-word vocabulary with 2048-dimensional embeddings would require over 8GB just for the lookup table. Subword tokenization collapses this problem by focusing on frequently occurring fragments. With roughly 50,000 subwords, the model reconstructs both common words (stored as single tokens) and rare words (assembled from parts). The system trades a small increase in sequence length for massive reductions in memory and computational overhead.\nThis compression also explains a quirk: why LLMs sometimes fail at seemingly trivial tasks like counting letters. The word “strawberry” might tokenize as [“straw”, “berry”], meaning the model never sees individual “r” characters as separate units. It’s not stupidity, it’s compression artifacts.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m04-text/tokenization.html#how-tokenization-works-in-practice",
    "href": "m04-text/tokenization.html#how-tokenization-works-in-practice",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "How Tokenization Works in Practice",
    "text": "How Tokenization Works in Practice\nLet’s unbox an actual tokenizer from Hugging Face and trace the pipeline from raw text to embeddings. We’ll use Phi-1.5, a compact model from Microsoft. For tokenization experiments, we only need the tokenizer itself, not the full multi-gigabyte model.\n\nfrom transformers import AutoTokenizer\nimport os\n\nmodel_name = \"microsoft/phi-1.5\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nLet’s inspect the tokenizer’s constraints.\n\n\nCode\nprint(f\"Vocabulary size: {tokenizer.vocab_size:,} tokens\")\nprint(f\"Max sequence length: {tokenizer.model_max_length} tokens\")\n\n\nVocabulary size: 50,257 tokens\nMax sequence length: 2048 tokens\n\n\nThis tokenizer knows 50,257 unique tokens and enforces a maximum sequence length of 2048 tokens. If your input exceeds this limit, the model will truncate or reject it. This is a hard boundary imposed by the positional encoding system, not a soft guideline.\n\nFrom Text to Tokens\nTokenization splits text into the subword fragments the model actually processes. Watch what happens when we tokenize a university name.\n\ntext = \"Binghamton University.\"\n\ntokens = tokenizer.tokenize(text)\n\n\n\nCode\nprint(f\"Tokens: {tokens}\")\n\n\nTokens: ['B', 'ingham', 'ton', 'ĠUniversity', '.']\n\n\nThe rare word “Binghamton” fractures into [‘B’, ‘ingham’, ‘ton’]. The common word “University” survives intact (with a leading space marker). The tokenizer learned these splits from frequency statistics during training. High-frequency words get dedicated tokens; rare words get decomposed into reusable parts.\nThe Ġ character (U+0120) is a GPT-style tokenizer convention for encoding spaces. When you see ĠUniversity, it means “University” preceded by a space. This preserves word boundaries while allowing subword splits.\nLet’s test a few more examples to see the pattern.\n\n\nCode\ntexts = [\n    \"Bearcats\",\n    \"New York\",\n]\n\nprint(\"Word tokenization examples:\\n\")\nfor text in texts:\n    tokens = tokenizer.tokenize(text)\n    print(f\"{text:10s} → {tokens}\")\n\n\nWord tokenization examples:\n\nBearcats   → ['Bear', 'cats']\nNew York   → ['New', 'ĠYork']\n\n\n“Bearcats” splits because it’s domain-specific jargon. “New York” remains whole because it’s common. The tokenizer’s behavior directly reflects its training corpus.\nCheck out OpenAI’s tokenizer to see how different models slice the same text differently.\n\n\nFrom Tokens to Token IDs\nTokens are still strings. The model needs integers. Each token maps to a unique ID in the vocabulary dictionary.\n\n\nCode\ntext = \"Binghamton University\"\n\n# Get token IDs\ntoken_ids = tokenizer.encode(text, add_special_tokens=False)\ntokens = tokenizer.tokenize(text)\n\nprint(\"Token → Token ID mapping:\\n\")\nfor token, token_id in zip(tokens, token_ids):\n    print(f\"{token:10s} → {token_id:6d}\")\n\n\nToken → Token ID mapping:\n\nB          →     33\ningham     →  25875\nton        →   1122\nĠUniversity →   2059\n\n\nEach token receives a unique integer ID. The vocabulary is a dictionary mapping token strings to integer IDs. Let’s peek inside.\n\n# Get the full vocabulary\nvocab = tokenizer.get_vocab()\n\n# Sample some tokens\nsample_tokens = list(vocab.items())[:5]\nfor token, id in sample_tokens:\n    print(f\"  {id:6d}: '{token}'\")\n\n   31818: 'Ġinconvenience'\n   39472: 'Ġunknow'\n   31083: 'Ġconcluding'\n   35540: 'Ġ;)'\n    5506: 'ĠAnn'\n\n\nMost LLMs reserve special tokens for sequence boundaries or control signals. Phi-1.5 uses &lt;|endoftext|&gt; as a separator during training. Let’s verify.\n\ntoken_id = [50256]\ntoken = tokenizer.convert_ids_to_tokens(token_id)[0]\nprint(f\"Token ID: {token_id} → Token: {token}\")\n\nToken ID: [50256] → Token: &lt;|endoftext|&gt;\n\n\nToken ID 50256 is Phi-specific. Other models use different conventions (BERT uses [SEP] and [CLS]). Always check your tokenizer’s special tokens before preprocessing data.\n\n\nFrom Token IDs to Embeddings\n\nNow we need the full model to access the embedding layer, the matrix that converts token IDs into dense vectors.\n\nfrom transformers import AutoModelForCausalLM\nimport torch\n\n# Load the model\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Retrieve the embedding layer\nembedding_layer = model.model.embed_tokens\n\nThe embedding layer is a simple lookup table: a 51,200 × 2,048 matrix where each row is the embedding for a token in the vocabulary. Let’s examine the first few entries.\n\n\nCode\nprint(embedding_layer.weight[:5, :10])\n\n\ntensor([[ 0.0097, -0.0155,  0.0603,  0.0326, -0.0108, -0.0271, -0.0273,  0.0178,\n         -0.0242,  0.0100],\n        [ 0.0243,  0.0543,  0.0178, -0.0679, -0.0130,  0.0265, -0.0423, -0.0287,\n         -0.0051, -0.0179],\n        [-0.0416,  0.0370, -0.0160, -0.0254, -0.0371, -0.0208, -0.0023,  0.0647,\n          0.0468,  0.0013],\n        [-0.0051, -0.0044,  0.0248, -0.0489,  0.0399,  0.0005, -0.0070,  0.0148,\n          0.0030,  0.0070],\n        [ 0.0289,  0.0362, -0.0027, -0.0775, -0.0136, -0.0203, -0.0566, -0.0558,\n          0.0269, -0.0067]], grad_fn=&lt;SliceBackward0&gt;)\n\n\nThese numbers are learned parameters, optimized during training to capture semantic relationships. Token IDs are discrete symbols; embeddings are continuous coordinates in a 2048-dimensional space. This is what the transformer layers operate on.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m04-text/tokenization.html#the-full-pipeline",
    "href": "m04-text/tokenization.html#the-full-pipeline",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "The Full Pipeline",
    "text": "The Full Pipeline\nYou’ve now traced the complete pipeline: raw text fractures into subword tokens, tokens map to integer IDs, and IDs retrieve vector embeddings from a learned matrix. This tokenization step is foundational. Without it, the model cannot begin processing language. The transformer layers come next, using attention mechanisms to extract patterns from these embeddings.\nRemember three key constraints. First, LLMs operate on subwords, not words, because vocabulary size is a memory bottleneck. Second, tokenization is learned from data, not hand-designed, so different models split text differently. Third, compression has side effects. Tasks like character counting fail because the model never sees individual characters as atomic units.\nWith this machinery exposed, we’re ready to examine the transformer itself. It’s the architecture that processes these embeddings and enables LLMs to predict the next token.\n\nNext: Transformers: The Architecture Behind the Magic",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m04-text/word-bias.html",
    "href": "m04-text/word-bias.html",
    "title": "Word Bias",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nWord embeddings capture and reinforce societal biases from their training data through geometric relationships between vectors. This section explores how semantic axes reveal gender bias in occupations and concepts, demonstrating both the benefits and risks of embeddings encoding real-world associations.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Bias"
    ]
  },
  {
    "objectID": "m04-text/word-bias.html#understanding-bias-in-word-embeddings",
    "href": "m04-text/word-bias.html#understanding-bias-in-word-embeddings",
    "title": "Word Bias",
    "section": "Understanding Bias in Word Embeddings",
    "text": "Understanding Bias in Word Embeddings\nWord embeddings can capture and reinforce societal biases from their training data through geometric relationships between word vectors. These relationships often reflect stereotypes about gender, race, age, and other social factors. By using semantic axes, we can analyze gender bias in job-related terms, showing both the benefits and risks of word embeddings capturing these real-world associations.\nSemAxis is a powerful tool to analyze gender bias in word embeddings by measuring word alignments along semantic axes. Using antonym pairs like “she-he” as poles, it quantifies gender associations in words on a scale from -1 to 1, where positive values indicate feminine associations and negative values indicate masculine associations.\nLet’s start with a simple example of analyzing gender bias in occupations.\n\nimport gensim.downloader as api\nimport numpy as np\n\n# Load pre-trained Word2vec embeddings\nmodel = api.load(\"word2vec-google-news-300\")\n\n\ndef compute_bias(word, microframe, model):\n    word_vector = model[word]\n    numerator = np.dot(word_vector, microframe)\n    denominator = np.linalg.norm(word_vector) * np.linalg.norm(microframe)\n    return numerator / denominator\n\n\ndef analyze(word, pos_word, neg_word, model):\n    if word not in model:\n        return 0.0\n    microframe = model[pos_word] - model[neg_word]\n    bias = compute_bias(word, microframe, model)\n    return bias\n\nThe compute_bias function calculates the cosine similarity between a word vector and a semantic axis (microframe). The numerator computes the dot product, which projects the word onto the axis. The denominator normalizes by vector lengths to get a score between -1 and 1.\nWe will use the following occupations:\n\n# Occupations from the paper\nshe_occupations = [\n    \"homemaker\",\n    \"nurse\",\n    \"receptionist\",\n    \"librarian\",\n    \"socialite\",\n    \"hairdresser\",\n    \"nanny\",\n    \"bookkeeper\",\n    \"stylist\",\n    \"housekeeper\",\n]\n\nhe_occupations = [\n    \"maestro\",\n    \"skipper\",\n    \"protege\",\n    \"philosopher\",\n    \"captain\",\n    \"architect\",\n    \"financier\",\n    \"warrior\",\n    \"broadcaster\",\n    \"magician\",\n    \"boss\",\n]\n\nWe measure the gender bias in these occupations by measuring how they align with the “she-he” axis.\n\n\nCode\nprint(\"Gender Bias in Occupations (she-he axis):\")\nprint(\"\\nShe-associated occupations:\")\nfor occupation in she_occupations:\n    bias = analyze(occupation, \"she\", \"he\", model)\n    print(f\"{occupation}: {bias:.3f}\")\n\nprint(\"\\nHe-associated occupations:\")\nfor occupation in he_occupations:\n    bias = analyze(occupation, \"she\", \"he\", model)\n    print(f\"{occupation}: {bias:.3f}\")\n\n\nGender Bias in Occupations (she-he axis):\n\nShe-associated occupations:\nhomemaker: 0.360\nnurse: 0.333\nreceptionist: 0.329\nlibrarian: 0.300\nsocialite: 0.310\nhairdresser: 0.307\nnanny: 0.287\nbookkeeper: 0.264\nstylist: 0.252\nhousekeeper: 0.260\n\nHe-associated occupations:\nmaestro: -0.203\nskipper: -0.177\nprotege: -0.148\nphilosopher: -0.155\ncaptain: -0.130\narchitect: -0.151\nfinancier: -0.145\nwarrior: -0.120\nbroadcaster: -0.124\nmagician: -0.110\nboss: -0.090\n\n\nInterpreting the scores: Positive scores (greater than 0) indicate closer association to “she” (e.g., nurse, librarian). Negative scores (less than 0) indicate closer association to “he” (e.g., architect, captain). Magnitude indicates the strength of the gender association. A larger absolute value represents a stronger gender association.\nNotice how occupations historically associated with women (like nurse and librarian) have strong positive scores, while those associated with men (like captain and architect) have negative scores. This confirms that the model has learned these gender stereotypes from the text data.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Bias"
    ]
  },
  {
    "objectID": "m04-text/word-bias.html#stereotype-analogies",
    "href": "m04-text/word-bias.html#stereotype-analogies",
    "title": "Word Bias",
    "section": "Stereotype Analogies",
    "text": "Stereotype Analogies\nSince word embeddings capture semantic relationships learned from large text corpora, they inevitably encode societal biases and stereotypes present in that training data. We can leverage this property to identify pairs of words that exhibit stereotypical gender associations. By measuring how different words align with the gender axis (she-he), we find pairs where one word shows strong feminine bias while its counterpart shows masculine bias, revealing ingrained stereotypes in language use.\n\n# Stereotype analogies from the paper\nstereotype_pairs = [\n    (\"sewing\", \"carpentry\"),\n    (\"nurse\", \"surgeon\"),\n    (\"softball\", \"baseball\"),\n    (\"cosmetics\", \"pharmaceuticals\"),\n    (\"vocalist\", \"guitarist\"),\n]\n\n\n\nCode\nprint(\"\\nAnalyzing Gender Stereotype Pairs:\")\nfor word1, word2 in stereotype_pairs:\n    bias1 = analyze(word1, \"she\", \"he\", model)\n    bias2 = analyze(word2, \"she\", \"he\", model)\n    print(f\"\\n{word1} vs {word2}\")\n    print(f\"{word1}: {bias1:.3f}\")\n    print(f\"{word2}: {bias2:.3f}\")\n\n\n\nAnalyzing Gender Stereotype Pairs:\n\nsewing vs carpentry\nsewing: 0.302\ncarpentry: -0.028\n\nnurse vs surgeon\nnurse: 0.333\nsurgeon: -0.048\n\nsoftball vs baseball\nsoftball: 0.260\nbaseball: -0.066\n\ncosmetics vs pharmaceuticals\ncosmetics: 0.331\npharmaceuticals: -0.011\n\nvocalist vs guitarist\nvocalist: 0.140\nguitarist: -0.041\n\n\nThe results show clear stereotypical alignments. Sewing and nurse align with “she”, while carpentry and surgeon align with “he”. This mirrors the “man is to computer programmer as woman is to homemaker” analogy found in early word embedding research.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Bias"
    ]
  },
  {
    "objectID": "m04-text/word-bias.html#indirect-bias-when-neutral-words-become-gendered",
    "href": "m04-text/word-bias.html#indirect-bias-when-neutral-words-become-gendered",
    "title": "Word Bias",
    "section": "Indirect Bias: When Neutral Words Become Gendered",
    "text": "Indirect Bias: When Neutral Words Become Gendered\nIndirect bias occurs when seemingly neutral words or concepts become associated with gender through their relationships with other words. For example, while “softball” and “football” are not inherently gendered terms, they may show gender associations in word embeddings due to how they’re used in language and society.\nWe can detect indirect bias by identifying word pairs that form a semantic axis (like softball-football), measuring how other words align with this axis, and examining if alignment correlates with gender bias. This reveals how gender stereotypes can be encoded indirectly through word associations, even when the words themselves don’t explicitly reference gender.\nLet’s see how this works in practice. We first measure the gender bias of the following words:\n\n# Words associated with softball-football axis\nsoftball_associations = [\n    \"pitcher\",\n    \"bookkeeper\",\n    \"receptionist\",\n    \"nurse\",\n    \"waitress\"\n]\n\nfootball_associations = [\n    \"footballer\",\n    \"businessman\",\n    \"pundit\",\n    \"maestro\",\n    \"cleric\"\n]\n\n# Calculate biases for all words\ngender_biases = []\nsports_biases = []\nwords = softball_associations + football_associations\n\nfor word in words:\n    gender_bias = analyze(word, \"she\", \"he\", model)\n    sports_bias = analyze(word, \"softball\", \"football\", model)\n    gender_biases.append(gender_bias)\n    sports_biases.append(sports_bias)\n\nLet’s plot the results:\n\n\nCode\n# Analyze bias along both gender and sports axes\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom adjustText import adjust_text\n\n# Create scatter plot\nfig, ax = plt.subplots(figsize=(6, 6))\nsns.scatterplot(x=gender_biases, y=sports_biases, ax=ax)\nax.set_xlabel(\"Gender Bias (she-he)\")\nax.set_ylabel(\"Sports Bias (softball-football)\")\nax.set_title(\"Indirect Bias Analysis: Gender vs Sports\")\n\n# Add labels for each point\ntexts = []\nfor i, word in enumerate(words):\n    texts.append(ax.text(gender_biases[i], sports_biases[i], word, fontsize=12))\n\nadjust_text(texts, arrowprops=dict(arrowstyle='-', color='gray', lw=0.5))\n\nax.grid(True, alpha=0.3)\nplt.show()\nsns.despine()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nThe plot reveals a correlation: words associated with “softball” (y-axis greater than 0) also tend to be associated with “she” (x-axis greater than 0). Conversely, “football” terms align with “he”. This suggests that even if we remove explicit gender words, the structure of the space still encodes gender through these proxy dimensions.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Bias"
    ]
  },
  {
    "objectID": "m04-text/word-bias.html#the-impact-and-path-forward",
    "href": "m04-text/word-bias.html#the-impact-and-path-forward",
    "title": "Word Bias",
    "section": "The Impact and Path Forward",
    "text": "The Impact and Path Forward\nWord embeddings, while powerful, inevitably capture and reflect societal biases present in the large text corpora they are trained on. We observed both direct bias, where occupations or attributes align strongly with specific gender pronouns, and indirect bias, where seemingly neutral concepts become gendered through their associations with other words. This analysis highlights the importance of understanding and mitigating these biases to prevent the perpetuation of stereotypes in AI systems and ensure fairness in applications like search, recommendation, and hiring.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Bias"
    ]
  },
  {
    "objectID": "m05-images/01-what-is-an-image.html",
    "href": "m05-images/01-what-is-an-image.html",
    "title": "Part 1: What is an Image?",
    "section": "",
    "text": "What you’ll learn\n\n\n\nThis section introduces images as structured data. We explore how computers represent visual information as numbers, examine the role of pixels and channels, and understand why spatial relationships matter for machine perception.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 1: What is an Image?"
    ]
  },
  {
    "objectID": "m05-images/01-what-is-an-image.html#a-photograph-is-just-numbers",
    "href": "m05-images/01-what-is-an-image.html#a-photograph-is-just-numbers",
    "title": "Part 1: What is an Image?",
    "section": "A Photograph is Just Numbers",
    "text": "A Photograph is Just Numbers\nLet’s talk about what an image really is from a computer’s perspective. When you look at a photograph, you see faces, objects, and scenes. But to a machine, that same photograph is simply a grid of numbers. Each number represents the brightness or color at a specific location.\nThis representation might seem strange at first. How can numbers capture the richness of visual information? The answer lies in spatial structure. Unlike a spreadsheet where row order doesn’t matter, the arrangement of numbers in an image is everything. Neighboring pixels relate to each other, forming edges, textures, and patterns.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 1: What is an Image?"
    ]
  },
  {
    "objectID": "m05-images/01-what-is-an-image.html#grayscale-images-the-simplest-case",
    "href": "m05-images/01-what-is-an-image.html#grayscale-images-the-simplest-case",
    "title": "Part 1: What is an Image?",
    "section": "Grayscale Images: The Simplest Case",
    "text": "Grayscale Images: The Simplest Case\nThe very first step in understanding images is to examine the grayscale case. A grayscale image contains only brightness information, with no color. We can think of it as a 2D matrix where each entry is a pixel intensity value.\nConsider a tiny 6×6 grayscale image:\n\nX = \\begin{bmatrix}\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10\n\\end{bmatrix}\n\nHere, the value 10 represents dark pixels, while 80 represents bright pixels. The third column forms a bright vertical line. This simple example shows how spatial patterns emerge from the arrangement of numbers.\n\n\n\n\n\n\nA grayscale image represented as a matrix of pixel intensity values. Each number encodes brightness at that location.\n\n\n\n\nFigure 1",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 1: What is an Image?"
    ]
  },
  {
    "objectID": "m05-images/01-what-is-an-image.html#loading-and-inspecting-real-images",
    "href": "m05-images/01-what-is-an-image.html#loading-and-inspecting-real-images",
    "title": "Part 1: What is an Image?",
    "section": "Loading and Inspecting Real Images",
    "text": "Loading and Inspecting Real Images\nLet’s make this concrete by loading an actual image and examining its structure. We’ll use Python with standard libraries to see what an image really looks like under the hood.\n\n\nShow image display code\n# Display the image\nplt.figure(figsize=(8, 6))\nplt.imshow(img_array)\nplt.title(\"Original Image\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n# Examine the image properties\nprint(f\"Image shape: {img_array.shape}\")\nprint(f\"Data type: {img_array.dtype}\")\nprint(f\"Value range: [{img_array.min()}, {img_array.max()}]\")\n\nImage shape: (300, 600, 3)\nData type: uint8\nValue range: [0, 255]\n\n\nWhat does the shape tell us? The output (height, width, 3) reveals three dimensions. The first two dimensions specify spatial location, while the third dimension holds three color channels.\nLet’s zoom into a small patch to see the actual numbers:\n\n# Extract a tiny 5x5 patch from the center\ncenter_y, center_x = img_array.shape[0] // 2, img_array.shape[1] // 2\npatch = img_array[center_y:center_y+5, center_x:center_x+5, 0]  # Red channel only\n\nprint(\"A 5x5 patch of pixel values (Red channel):\")\nprint(patch)\n\nA 5x5 patch of pixel values (Red channel):\n[[ 45  49  40  93 141]\n [ 55  52  55 113 102]\n [ 35  46  63 121 104]\n [136  52  48  84 124]\n [225  90  41  73 136]]\n\n\nThese are the actual numbers the computer sees. Each value between 0 and 255 represents brightness in the red channel for that pixel location.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 1: What is an Image?"
    ]
  },
  {
    "objectID": "m05-images/01-what-is-an-image.html#color-images-three-stacked-layers",
    "href": "m05-images/01-what-is-an-image.html#color-images-three-stacked-layers",
    "title": "Part 1: What is an Image?",
    "section": "Color Images: Three Stacked Layers",
    "text": "Color Images: Three Stacked Layers\nColor images extend the grayscale concept by using three separate matrices, one for each color channel: Red, Green, and Blue (RGB). Think of these as three grayscale images stacked on top of each other.\nWhen you combine the values from all three channels at a given location, you get the color for that pixel. For example, [255, 0, 0] is pure red, [0, 255, 0] is pure green, and [255, 255, 255] is white.\nLet’s visualize the three channels separately:\n\n\nShow channel visualization code\nfig, axes = plt.subplots(1, 4, figsize=(16, 4))\n\n# Original image\naxes[0].imshow(img_array)\naxes[0].set_title(\"Original Image\")\naxes[0].axis(\"off\")\n\n# Individual channels\nchannel_names = ['Red', 'Green', 'Blue']\ncolors = ['Reds', 'Greens', 'Blues']\n\nfor i, (name, cmap) in enumerate(zip(channel_names, colors)):\n    axes[i+1].imshow(img_array[:, :, i], cmap=cmap)\n    axes[i+1].set_title(f\"{name} Channel\")\n    axes[i+1].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNotice how each channel emphasizes different aspects of the scene. The red channel might be bright where red objects appear, while the blue channel highlights sky and water.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 1: What is an Image?"
    ]
  },
  {
    "objectID": "m05-images/01-what-is-an-image.html#why-spatial-structure-matters",
    "href": "m05-images/01-what-is-an-image.html#why-spatial-structure-matters",
    "title": "Part 1: What is an Image?",
    "section": "Why Spatial Structure Matters",
    "text": "Why Spatial Structure Matters\nShift your attention from individual pixel values to relationships between pixels. This is what makes images fundamentally different from tabular data.\nIn a spreadsheet, you can shuffle the rows without losing information. But in an image, shuffling pixels destroys everything. The spatial arrangement is the information. An edge appears when neighboring pixels have very different values. A texture emerges from repeating patterns across nearby locations. An object is a coherent region of similar pixels.\nThis spatial structure is why neural networks for images need special architectures. Fully connected networks treat every input independently, ignoring spatial relationships. Convolutional networks, which we’ll explore soon, are designed specifically to exploit this structure.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 1: What is an Image?"
    ]
  },
  {
    "objectID": "m05-images/01-what-is-an-image.html#converting-between-representations",
    "href": "m05-images/01-what-is-an-image.html#converting-between-representations",
    "title": "Part 1: What is an Image?",
    "section": "Converting Between Representations",
    "text": "Converting Between Representations\nLet’s practice manipulating image representations to build intuition:\n\n# Convert to grayscale by averaging channels\ngrayscale = np.mean(img_array, axis=2).astype(np.uint8)\n\nprint(f\"RGB shape: {img_array.shape}\")\nprint(f\"Grayscale shape: {grayscale.shape}\")\n\nRGB shape: (300, 600, 3)\nGrayscale shape: (300, 600)\n\n\n\n\nShow RGB vs grayscale comparison\n# Create a side-by-side comparison\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\naxes[0].imshow(img_array)\naxes[0].set_title(\"RGB Image (3 channels)\")\naxes[0].axis(\"off\")\n\naxes[1].imshow(grayscale, cmap='gray')\naxes[1].set_title(\"Grayscale Image (1 channel)\")\naxes[1].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe grayscale version loses color information but preserves spatial structure. For many computer vision tasks, this simplified representation is sufficient.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 1: What is an Image?"
    ]
  },
  {
    "objectID": "m05-images/01-what-is-an-image.html#image-dimensions-in-deep-learning",
    "href": "m05-images/01-what-is-an-image.html#image-dimensions-in-deep-learning",
    "title": "Part 1: What is an Image?",
    "section": "Image Dimensions in Deep Learning",
    "text": "Image Dimensions in Deep Learning\nWhen we feed images into neural networks, we need to be precise about dimensions. Different frameworks use different conventions, so let’s clarify the PyTorch standard.\nPyTorch expects images in (batch_size, channels, height, width) format, often abbreviated as NCHW:\n\nN: Batch size (number of images processed together)\nC: Channels (3 for RGB, 1 for grayscale)\nH: Height in pixels\nW: Width in pixels\n\nLet’s convert our image to PyTorch format:\n\nimport torch\n\n# Original numpy array is (H, W, C)\nprint(f\"NumPy format (H, W, C): {img_array.shape}\")\n\n# Convert to PyTorch format (C, H, W) for a single image\nimg_tensor = torch.from_numpy(img_array).permute(2, 0, 1).float() / 255.0\nprint(f\"PyTorch format (C, H, W): {img_tensor.shape}\")\n\n# Add batch dimension to get (N, C, H, W)\nimg_batch = img_tensor.unsqueeze(0)\nprint(f\"Batch format (N, C, H, W): {img_batch.shape}\")\n\nNumPy format (H, W, C): (300, 600, 3)\nPyTorch format (C, H, W): torch.Size([3, 300, 600])\nBatch format (N, C, H, W): torch.Size([1, 3, 300, 600])\n\n\nWe also normalized pixel values from [0, 255] to [0, 1] by dividing by 255. This normalization helps neural networks train more stably.\n\n\n\n\n\n\nTry it yourself\n\n\n\nLoad your own image and explore its properties. Extract and visualize a small patch of pixels as numbers to see the underlying data structure. Modify some pixel values directly and observe how the image changes. Swap the red and blue channels to see the dramatic color shift this creates. Convert between NumPy and PyTorch formats to build fluency with both representations.\nUnderstanding these representations at a hands-on level will make everything that follows more intuitive.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 1: What is an Image?"
    ]
  },
  {
    "objectID": "m05-images/01-what-is-an-image.html#from-pixels-to-patterns",
    "href": "m05-images/01-what-is-an-image.html#from-pixels-to-patterns",
    "title": "Part 1: What is an Image?",
    "section": "From Pixels to Patterns",
    "text": "From Pixels to Patterns\nNow that we understand what images are as data structures, the next question becomes: how do we detect patterns in them? Human vision effortlessly recognizes edges, textures, and objects. But what computational operations allow machines to do the same?\nThis question leads us to convolution, feature extraction, and ultimately to the deep learning revolution. In the next section, we’ll explore how computer vision evolved from hand-crafted feature detectors to learned representations that can match or exceed human performance.\nThe key insight to carry forward is this: images are spatial data where relationships between neighboring pixels encode visual information. Any successful vision system must respect and exploit this spatial structure.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 1: What is an Image?"
    ]
  },
  {
    "objectID": "m05-images/01-what-is-an-image.html#summary",
    "href": "m05-images/01-what-is-an-image.html#summary",
    "title": "Part 1: What is an Image?",
    "section": "Summary",
    "text": "Summary\nWe explored images as structured numerical data. A grayscale image is a 2D matrix of brightness values, while a color image adds two more matrices for the other color channels. Spatial relationships between pixels encode edges, textures, and objects. Unlike tabular data, the arrangement of values matters fundamentally.\nWe saw how to load, inspect, and manipulate images in Python, converting between NumPy arrays and PyTorch tensors. This hands-on understanding prepares us to work with the deep learning models that process these image representations.\nImages are not just collections of numbers. They are spatially organized data where local patterns combine into global structure. This insight motivates everything that follows in computer vision.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 1: What is an Image?"
    ]
  },
  {
    "objectID": "m05-images/03-using-cnn-models.html",
    "href": "m05-images/03-using-cnn-models.html",
    "title": "Part 3: Using CNN Models",
    "section": "",
    "text": "What you’ll learn\n\n\n\nThis section transforms you into a CNN practitioner. We explore the fundamental building blocks (convolution, pooling, stride, padding), understand key properties like translation equivariance, learn to use pre-trained models from torchvision, and master transfer learning techniques for adapting models to new tasks.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 3: Using CNN Models"
    ]
  },
  {
    "objectID": "m05-images/03-using-cnn-models.html#understanding-cnn-building-blocks",
    "href": "m05-images/03-using-cnn-models.html#understanding-cnn-building-blocks",
    "title": "Part 3: Using CNN Models",
    "section": "Understanding CNN Building Blocks",
    "text": "Understanding CNN Building Blocks\nAlexNet proved that deep learning works at scale. But how do these networks actually process images? Let’s break down the fundamental operations that make CNNs powerful.\n\nConvolutional Layers: Learnable Pattern Detectors\nAt the heart of CNNs lies a remarkably elegant operation called convolution. Imagine sliding a small window (a kernel or filter) across an image. At each position, we multiply the kernel values by the overlapping image pixels and sum the results. This produces a single output value. Repeat across all positions to create an output feature map.\n\n\n\n\n\n\nConvolution operation. The kernel slides across the input, computing weighted sums at each position to produce a feature map.\n\n\n\n\nFigure 1\n\n\n\nMathematically, for a single-channel input (grayscale image), 2D convolution is:\n\n(I * K)_{i,j} = \\sum_{m=0}^{L-1}\\sum_{n=0}^{L-1} I_{i+m,j+n} \\cdot K_{m,n}\n\nwhere I is the input image, K is the kernel of size L \\times L, and (i,j) specifies the output position.\nWhat makes CNNs powerful is that these kernels are learnable parameters. During training, each kernel evolves to detect specific visual patterns. Some kernels might become edge detectors, highlighting vertical or horizontal edges. Others might respond to textures, colors, or more complex patterns. The network discovers useful features automatically.\nReal-world images have multiple channels (RGB). Convolution extends naturally to 3D inputs using 3D kernels:\n\n(I * K)_{i,j} = \\sum_{c=1}^{C}\\sum_{m=0}^{L-1}\\sum_{n=0}^{L-1} I_{c,i+m,j+n} \\cdot K_{c,m,n}\n\nwhere C is the number of input channels. Each kernel processes all channels simultaneously, combining color information into a single output value.\n\n\n\n\n\n\nMulti-channel convolution. Each kernel processes all input channels, producing one output feature map.\n\n\n\n\nFigure 2\n\n\n\n\n\n\n\n\n\nInteractive visualizations\n\n\n\nExplore CNN operations interactively. The CNN Explainer shows how convolution, activation, and pooling work step-by-step. The Interactive Node-Link Visualization lets you see activations flow through a trained network.\n\n\n\n\nTranslation Equivariance: A Key Property\nOne crucial feature of convolutional layers is translation equivariance. This means that if you shift the input, the output shifts by the same amount.\nConsider detecting a vertical edge. If the edge moves one pixel to the right in the input image, the detected edge feature also moves one pixel to the right in the output. The detection operation doesn’t care about absolute position, only relative patterns.\n\n\n\n\n\n\nTranslation equivariance. The same kernel detects the same feature regardless of position in the input.\n\n\n\n\nFigure 3\n\n\n\nThis property allows CNNs to recognize objects anywhere in an image. A cat detector learned on centered cats will also detect cats in image corners. The network doesn’t need to learn separate detectors for every possible position.\n\n\nParameter Sharing: Efficient Learning\nUnlike fully connected networks where each weight is used once, convolutional layers reuse kernel weights across all spatial positions. A 3×3 kernel applied to a 224×224 RGB image uses just 27 parameters (3×3×3), not the millions required by a fully connected layer.\nThis weight-sharing dramatically reduces parameter count while preserving spatial relationships in the data. It’s a key reason CNNs can process high-resolution images efficiently.\n\n\nReceptive Field: Seeing More with Depth\nThe receptive field is the region of input pixels that influence each output pixel. In the first convolutional layer, a 3×3 kernel has a receptive field of 3×3 pixels. But as we stack layers, the receptive field grows.\nConsider two 3×3 convolutional layers. Each output pixel in the second layer depends on a 3×3 region in the first layer’s output. But each of those positions depends on a 3×3 region in the input. So the second layer’s receptive field is 5×5 in the original input.\n\n\n\n\n\n\nReceptive field grows with network depth. Deeper layers see increasingly large regions of the input image.\n\n\n\n\nFigure 4\n\n\n\nThis hierarchical structure allows CNNs to detect increasingly complex, abstract features. Early layers detect edges and simple patterns. Middle layers combine these into textures and parts. Deep layers recognize complete objects and scenes.\n\n\nStride and Padding: Controlling Dimensions\nStride determines how many pixels we skip when sliding the kernel. With stride 1, we move one pixel at a time, creating dense feature maps. With stride 2, we skip every other position, effectively downsampling the output.\nFor a 1D example with input [a,b,c,d,e,f] and kernel [1,2]:\nStride 1: \n[1a + 2b, 1b + 2c, 1c + 2d, 1d + 2e, 1e + 2f]\n\nStride 2: \n[1a + 2b, 1c + 2d, 1e + 2f]\n\nLarger strides reduce computational cost and increase the receptive field, but might miss fine details.\n\n\n\n\n\n\nStride controls how far the kernel moves at each step. Stride 2 produces half the spatial dimensions of stride 1.\n\n\n\n\nFigure 5\n\n\n\nPadding addresses information loss at borders. Without padding (called “valid” padding), the output shrinks after each convolution because the kernel can’t fully overlap with border pixels. Zero padding adds a border of zeros around the input, allowing the kernel to process edge pixels and control output dimensions.\n\n\n\n\n\n\nZero padding extends the input with zeros, preserving spatial dimensions and processing border pixels.\n\n\n\n\nFigure 6\n\n\n\nFor a square input of size W with kernel size K, stride S, and padding P, the output dimension is:\n\nO = \\left\\lfloor\\frac{W - K + 2P}{S}\\right\\rfloor + 1\n\nExample: 224×224 input, 3×3 kernel, stride 2, padding 1:\n\nO = \\left\\lfloor\\frac{224 - 3 + 2(1)}{2}\\right\\rfloor + 1 = 112\n\nThe interplay between stride and padding lets network designers control spatial dimensions and computational efficiency. Try the Convolution Visualizer to experiment with different stride and padding settings interactively.\n\n\nPooling Layers: Downsampling with Invariance\nPooling layers downsample feature maps, reducing spatial dimensions while preserving important information. Max pooling selects the maximum value in each local window:\n\nP_{i,j} = \\max_{m,n} F_{si+m,sj+n}\n\nwhere F is the feature map, s is the stride (typically equal to the window size), and (m,n) range over the pooling window.\nAverage pooling computes the mean instead:\n\nP_{i,j} = \\frac{1}{w^2}\\sum_{m=0}^{w-1}\\sum_{n=0}^{w-1} F_{si+m,sj+n}\n\nMax pooling creates local translation invariance. If an edge moves slightly within a pooling window, the maximum value (and thus the output) remains unchanged. This helps the network focus on whether a feature is present, not its exact position.\nPooling also reduces computational cost in subsequent layers by decreasing spatial dimensions. A common pattern is to double the number of channels while halving spatial dimensions, maintaining roughly constant computational load across layers. Some recent architectures replace pooling with strided convolutions, arguing that learnable downsampling might be more effective {footcite}springenberg2015striving. The choice involves trade-offs between parameter efficiency and flexibility.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 3: Using CNN Models"
    ]
  },
  {
    "objectID": "m05-images/03-using-cnn-models.html#transfer-learning-adapting-pre-trained-models",
    "href": "m05-images/03-using-cnn-models.html#transfer-learning-adapting-pre-trained-models",
    "title": "Part 3: Using CNN Models",
    "section": "Transfer Learning: Adapting Pre-Trained Models",
    "text": "Transfer Learning: Adapting Pre-Trained Models\nPre-trained models learn general visual features from ImageNet’s 1000 categories. But what if you want to classify different objects? Transfer learning adapts these models to new tasks.\n\nWhy Transfer Learning Works\nImageNet-trained models learn a hierarchy of features. Early layers detect edges, colors, and simple textures that are universal across tasks. Middle layers detect patterns, parts, and compositions that are somewhat task-specific. Late layers detect complete objects specific to ImageNet categories. The early and middle layers learn representations useful for many vision tasks. We can reuse these features and only retrain the final layers for our specific problem.\n\n\nTwo Approaches: Feature Extraction vs. Fine-Tuning\nFeature Extraction: Freeze all convolutional layers, only train a new classifier head. Fast and works well with small datasets.\nFine-Tuning: Initialize with pre-trained weights, then train the entire network (or parts of it) on your data. Better accuracy but requires more data and computation.\n\n\nExample: Fine-Tuning for Custom Classification\nLet’s adapt ResNet-50 to classify 10 animal species:\n\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Load pre-trained ResNet-50\nmodel = models.resnet50(weights='IMAGENET1K_V1')\n\n# Replace the final fully connected layer\n# Original: 2048 -&gt; 1000 (ImageNet classes)\n# New: 2048 -&gt; 10 (our custom classes)\nnum_classes = 10\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nprint(f\"Modified final layer: {model.fc}\")\n\nModified final layer: Linear(in_features=2048, out_features=10, bias=True)\n\n\nFor feature extraction, freeze early layers:\n\n# Freeze all layers except the final classifier\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Unfreeze the final layer\nfor param in model.fc.parameters():\n    param.requires_grad = True\n\n# Count trainable parameters\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(f\"Trainable parameters: {trainable_params:,} / {total_params:,}\")\n\nTrainable parameters: 20,490 / 23,528,522\n\n\nOnly 20,490 parameters (the final layer) are trainable. This makes training fast and prevents overfitting on small datasets.\n\n\nTraining Loop\n\n\nShow training loop implementation\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n\n# Training loop (pseudo-code, requires actual data)\ndef train_epoch(model, dataloader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for inputs, labels in dataloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    epoch_loss = running_loss / len(dataloader)\n    epoch_acc = correct / total\n    return epoch_loss, epoch_acc\n\n# For fine-tuning instead of feature extraction:\n# 1. Unfreeze all or some layers\n# 2. Use a smaller learning rate (e.g., 1e-4 or 1e-5)\n# 3. Train for more epochs\n\n\n\n\nData Augmentation: Essential for Small Datasets\nWhen training on limited data, augmentation is crucial. Transform each image differently each epoch to artificially expand the training set:\n\n\nShow training augmentation pipeline\n# Training transforms with aggressive augmentation\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(224),      # Random crop and resize\n    transforms.RandomHorizontalFlip(),       # Flip with 50% probability\n    transforms.ColorJitter(                  # Random brightness, contrast\n        brightness=0.2,\n        contrast=0.2,\n        saturation=0.2\n    ),\n    transforms.RandomRotation(15),           # Rotate up to 15 degrees\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    ),\n])\n\n\n\n\nShow validation transform pipeline\n# Validation transforms (deterministic, no randomness)\nval_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    ),\n])\n\n\nNote that validation uses deterministic transforms (no randomness) for reproducible evaluation.\n\n\nBest Practices for Transfer Learning\nStart with feature extraction by training only the final layer first. This is fast and often achieves good results. Then try fine-tuning if accuracy is insufficient by unfreezing earlier layers and training with a small learning rate (10× smaller than initial training). Use learning rate schedules to reduce the learning rate when validation loss plateaus, helping the model converge to better solutions. Monitor for overfitting by using validation data to detect when the model stops generalizing, then apply more augmentation or stronger regularization (dropout, weight decay) if needed. Always match preprocessing to the pre-training dataset, using ImageNet statistics for most models.\n\n\n\n\n\n\nTry it yourself\n\n\n\nPractice transfer learning on your own image dataset. Collect 100-500 images per class (even phone camera photos work), then split into train/val/test sets (70/15/15). Start with ResNet-50 feature extraction and train for 10-20 epochs before evaluating on the test set.\nYou’ll likely achieve 80-90%+ accuracy with just a few hundred images per class, demonstrating the power of pre-trained features.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 3: Using CNN Models"
    ]
  },
  {
    "objectID": "m05-images/03-using-cnn-models.html#visualizing-what-networks-learn",
    "href": "m05-images/03-using-cnn-models.html#visualizing-what-networks-learn",
    "title": "Part 3: Using CNN Models",
    "section": "Visualizing What Networks Learn",
    "text": "Visualizing What Networks Learn\nLet’s peek inside a trained network to see what features it detects:\n\n\nShow activation visualization code\n# Extract intermediate feature maps\ndef get_activation(name, activations):\n    def hook(model, input, output):\n        activations[name] = output.detach()\n    return hook\n\n# Register hooks to capture activations\nactivations = {}\nmodel.layer1[0].conv1.register_forward_hook(get_activation('layer1', activations))\nmodel.layer2[0].conv1.register_forward_hook(get_activation('layer2', activations))\nmodel.layer3[0].conv1.register_forward_hook(get_activation('layer3', activations))\n\n# Run inference\nwith torch.no_grad():\n    _ = model(input_batch)\n\n# Visualize first layer activations\nlayer1_act = activations['layer1'][0]  # [C, H, W]\n\nfig, axes = plt.subplots(4, 8, figsize=(16, 8))\nfor i, ax in enumerate(axes.flat):\n    if i &lt; layer1_act.shape[0]:\n        ax.imshow(layer1_act[i].cpu(), cmap='viridis')\n    ax.axis('off')\nplt.suptitle(\"Layer 1 Feature Maps\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nEarly layers show edge detection and simple patterns. Deeper layers show increasingly abstract features that are harder to interpret but encode high-level semantic information.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 3: Using CNN Models"
    ]
  },
  {
    "objectID": "m05-images/03-using-cnn-models.html#summary",
    "href": "m05-images/03-using-cnn-models.html#summary",
    "title": "Part 3: Using CNN Models",
    "section": "Summary",
    "text": "Summary\nWe explored the building blocks that make CNNs powerful: convolution operations with learnable kernels, translation equivariance that enables position-invariant recognition, parameter sharing for efficiency, growing receptive fields through depth, stride and padding for dimension control, and pooling for downsampling with invariance.\nWe learned to use pre-trained models from torchvision for immediate deployment. Transfer learning lets us adapt these models to custom tasks through feature extraction (training only the classifier) or fine-tuning (training the entire network with small learning rates). Data augmentation artificially expands small datasets, preventing overfitting.\nThese practical skills transform you from understanding CNNs conceptually to deploying them on real problems. You can now load state-of-the-art models, adapt them to your data, and achieve strong results with limited computational resources.\n:style: unsrt",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 3: Using CNN Models"
    ]
  },
  {
    "objectID": "m05-images/overview.html",
    "href": "m05-images/overview.html",
    "title": "Module 5: Deep Learning for Images",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module takes you from pixels to state-of-the-art vision models.\nYou’ll learn:\n\nWhat images really are as data structures and why spatial structure matters.\nHow the deep learning revolution shifted computer vision from hand-crafted to learned features.\nPractical skills for using CNNs: building blocks, pre-trained models, and transfer learning.\nThe innovation timeline from VGG to Vision Transformers and the architectural insights that made them possible.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Overview"
    ]
  },
  {
    "objectID": "m05-images/overview.html#the-journey",
    "href": "m05-images/overview.html#the-journey",
    "title": "Module 5: Deep Learning for Images",
    "section": "The Journey",
    "text": "The Journey\nLet’s talk about where this module takes you. We begin with the fundamentals and build up to cutting-edge architectures.\nPart 1: Understanding Images\nBefore we can process images with neural networks, we need to understand what images are. How do computers represent visual information? Why does spatial structure matter? We answer these questions by examining images as multidimensional arrays.\nPart 2: The Deep Learning Revolution\nComputer vision didn’t always work this way. Shift your attention to the historical moment when neural networks transformed the field. We contrast the old paradigm of hand-crafted features with learned representations, following the path from LeNet to AlexNet’s breakthrough.\nPart 3: Becoming a Practitioner\nNow you’ll learn the skills to actually use these models. We cover CNN building blocks like convolution and pooling. You’ll work with pre-trained models and master transfer learning. By the end, you’ll have hands-on implementation experience.\nPart 4: The Innovation Timeline\nThe very best way to understand modern architectures is to see them as solutions to specific problems. Why did networks need to get deeper? How did researchers overcome training difficulties? We trace the quest for better networks through VGG, Inception, ResNet, and Vision Transformers.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Overview"
    ]
  },
  {
    "objectID": "m05-images/overview.html#why-this-matters",
    "href": "m05-images/overview.html#why-this-matters",
    "title": "Module 5: Deep Learning for Images",
    "section": "Why This Matters",
    "text": "Why This Matters\nHere’s something remarkable. Computer vision is no longer about manually designing features. Modern systems learn representations automatically from data. This shift changed everything about how we build vision applications.\nThis module gives you both conceptual understanding and practical skills. You’ll know why architectures evolved the way they did. You’ll also be able to use state-of-the-art vision models in your own projects.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Overview"
    ]
  },
  {
    "objectID": "m05-images/overview.html#prerequisites",
    "href": "m05-images/overview.html#prerequisites",
    "title": "Module 5: Deep Learning for Images",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou should be comfortable with basic Python programming and NumPy arrays. Neural network fundamentals matter here: forward propagation, backpropagation, and gradient descent. You’ll also need PyTorch basics like tensors, autograd, and simple model training.\nIf you need to refresh these topics, review the earlier modules in this course.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Overview"
    ]
  },
  {
    "objectID": "m05-images/overview.html#what-youll-build",
    "href": "m05-images/overview.html#what-youll-build",
    "title": "Module 5: Deep Learning for Images",
    "section": "What You’ll Build",
    "text": "What You’ll Build\nBy the end of this module, you will understand how images are represented as tensors. You’ll implement classic CNN architectures from scratch. You’ll use pre-trained models for transfer learning. You’ll make informed decisions about architecture selection. Most importantly, you’ll gain practical hands-on experience with real vision models.\nLet’s begin by understanding what an image really is.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Overview"
    ]
  },
  {
    "objectID": "m06-graph/02-spectral-perspective.html",
    "href": "m06-graph/02-spectral-perspective.html",
    "title": "The Spectral Perspective",
    "section": "",
    "text": "What you’ll learn in this part\n\n\n\nThis part introduces the spectral approach to graph convolution. We define a frequency domain for graphs using the Laplacian matrix, design spectral filters that control which frequencies pass through, and build learnable spectral graph convolutional networks.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 2: The Spectral Perspective"
    ]
  },
  {
    "objectID": "m06-graph/02-spectral-perspective.html#finding-frequency-in-graphs",
    "href": "m06-graph/02-spectral-perspective.html#finding-frequency-in-graphs",
    "title": "The Spectral Perspective",
    "section": "Finding Frequency in Graphs",
    "text": "Finding Frequency in Graphs\nImages live in a spatial domain, but the Fourier transform reveals their frequency content. Low frequencies capture smooth gradients and overall structure. High frequencies capture sharp edges and fine details.\nGraphs need their own notion of frequency. Consider a network of nodes where each node has a feature value. What makes a signal “smooth” on a graph? Intuitively, smoothness means connected nodes have similar values. Roughness means neighbors differ significantly.\nLet’s formalize this intuition.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 2: The Spectral Perspective"
    ]
  },
  {
    "objectID": "m06-graph/02-spectral-perspective.html#total-variation-measuring-roughness",
    "href": "m06-graph/02-spectral-perspective.html#total-variation-measuring-roughness",
    "title": "The Spectral Perspective",
    "section": "Total Variation: Measuring Roughness",
    "text": "Total Variation: Measuring Roughness\nConsider a network of N nodes where each node i has a scalar feature x_i \\in \\mathbb{R}. Define the total variation as:\n\nJ = \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N A_{ij}(x_i - x_j)^2\n\nwhere A_{ij} is the adjacency matrix. This quantity sums the squared differences between connected nodes. Small J means smooth variation (low frequency). Large J means rough variation (high frequency).\nWe can rewrite J in matrix form. Through algebraic manipulation, we obtain:\n\nJ = {\\bf x}^\\top {\\bf L} {\\bf x}\n\nwhere {\\bf x} = [x_1, x_2, \\ldots, x_N]^\\top and {\\bf L} is the graph Laplacian:\n\nL_{ij} = \\begin{cases}\nk_i & \\text{if } i = j \\\\\n-1 & \\text{if } i \\text{ and } j \\text{ are connected} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\nwith k_i being the degree of node i.\n\n\n\n\n\n\nDetailed derivation\n\n\n\nStarting from the definition of total variation:\n\n\\begin{aligned}\nJ &= \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N A_{ij}(x_i - x_j)^2 \\\\\n&= \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N A_{ij}(x_i^2 + x_j^2 - 2x_ix_j) \\\\\n&= \\sum_{i=1}^N x_i^2 \\sum_{j=1}^N A_{ij} - \\sum_{i=1}^N\\sum_{j=1}^N A_{ij}x_ix_j \\\\\n&= \\sum_{i=1}^N x_i^2 k_i - \\sum_{i=1}^N\\sum_{j=1}^N A_{ij}x_ix_j \\\\\n&= {\\bf x}^\\top {\\bf D} {\\bf x} - {\\bf x}^\\top {\\bf A} {\\bf x} \\\\\n&= {\\bf x}^\\top ({\\bf D} - {\\bf A}) {\\bf x} \\\\\n&= {\\bf x}^\\top {\\bf L} {\\bf x}\n\\end{aligned}\n\nwhere {\\bf D} is the diagonal degree matrix with D_{ii} = k_i.\n\n\nThe Laplacian {\\bf L} plays the same role for graphs that the derivative operator plays for continuous signals. It measures how much a signal varies across edges.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 2: The Spectral Perspective"
    ]
  },
  {
    "objectID": "m06-graph/02-spectral-perspective.html#eigenvectors-as-basis-functions",
    "href": "m06-graph/02-spectral-perspective.html#eigenvectors-as-basis-functions",
    "title": "The Spectral Perspective",
    "section": "Eigenvectors as Basis Functions",
    "text": "Eigenvectors as Basis Functions\nIn Fourier analysis, we decompose signals into sinusoidal basis functions. For graphs, eigenvectors of the Laplacian serve as basis functions.\nConsider the eigendecomposition of {\\bf L}:\n\n{\\bf L} = \\sum_{i=1}^N \\lambda_i {\\bf u}_i {\\bf u}_i^\\top\n\nwhere \\lambda_i are eigenvalues and {\\bf u}_i are eigenvectors. We can decompose the total variation J as:\n\nJ = {\\bf x}^\\top {\\bf L} {\\bf x} = \\sum_{i=1}^N \\lambda_i ({\\bf x}^\\top {\\bf u}_i)^2\n\nEach term ({\\bf x}^\\top {\\bf u}_i)^2 measures how much signal {\\bf x} aligns with eigenvector {\\bf u}_i. The eigenvalue \\lambda_i weights this contribution.\nWhat do eigenvalues tell us? Compute the total variation of eigenvector {\\bf u}_i itself:\n\nJ_i = {\\bf u}_i^\\top {\\bf L} {\\bf u}_i = \\lambda_i\n\nThis reveals the key insight. Eigenvalues measure the total variation of their corresponding eigenvectors. Small eigenvalues correspond to smooth eigenvectors (low frequency). Large eigenvalues correspond to rough eigenvectors (high frequency).\nIf signal {\\bf x} aligns strongly with a low-eigenvalue eigenvector, {\\bf x} is smooth. If {\\bf x} aligns with a high-eigenvalue eigenvector, {\\bf x} is rough.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 2: The Spectral Perspective"
    ]
  },
  {
    "objectID": "m06-graph/02-spectral-perspective.html#spectral-filtering",
    "href": "m06-graph/02-spectral-perspective.html#spectral-filtering",
    "title": "The Spectral Perspective",
    "section": "Spectral Filtering",
    "text": "Spectral Filtering\nEigenvalues act as natural filters. We can design custom filters h(\\lambda_i) to control which frequencies pass through.\nLow-pass Filter (smoothing): \nh_{\\text{low}}(\\lambda) = \\frac{1}{1 + \\alpha\\lambda}\n\nThis suppresses high frequencies (large \\lambda) and preserves low frequencies (small \\lambda), resulting in smoother signals.\nHigh-pass Filter (edge detection): \nh_{\\text{high}}(\\lambda) = \\frac{\\alpha\\lambda}{1 + \\alpha\\lambda}\n\nThis suppresses low frequencies and emphasizes high frequencies, highlighting rapid variations like boundaries between communities.\n\n\n\n\n\n\n\n\n\nThe parameter \\alpha controls the filter’s strength. Larger \\alpha makes the transition between frequencies sharper.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 2: The Spectral Perspective"
    ]
  },
  {
    "objectID": "m06-graph/02-spectral-perspective.html#learnable-spectral-filters",
    "href": "m06-graph/02-spectral-perspective.html#learnable-spectral-filters",
    "title": "The Spectral Perspective",
    "section": "Learnable Spectral Filters",
    "text": "Learnable Spectral Filters\nHand-designed filters work for specific tasks, but what if we want to learn filters from data? This leads to spectral graph convolutional networks.\nThe simplest learnable filter uses a linear combination of eigenvectors:\n\n{\\bf L}_{\\text{learn}} = \\sum_{k=1}^K \\theta_k {\\bf u}_k {\\bf u}_k^\\top\n\nwhere \\theta_k are learnable parameters and K is the filter rank. Instead of fixed weights \\lambda_k, we learn weights \\theta_k that maximize performance on downstream tasks.\nBuilding on this idea, Bruna et al. (2014) proposed spectral convolutional neural networks by adding nonlinearity:\n\n{\\bf x}^{(\\ell+1)} = h\\left( {\\bf L}_{\\text{learn}} {\\bf x}^{(\\ell)}\\right)\n\nwhere h is an activation function (e.g., ReLU) and {\\bf x}^{(\\ell)} is the feature vector at layer \\ell.\nFor multidimensional features {\\bf X} \\in \\mathbb{R}^{N \\times f_{\\text{in}}}, we extend this to produce outputs {\\bf X}' \\in \\mathbb{R}^{N \\times f_{\\text{out}}}:\n\n{\\bf X}^{(\\ell+1)}_j = h\\left( \\sum_{i=1}^{f_{\\text{in}}} {\\bf L}_{\\text{learn}}^{(i,j)} {\\bf X}^{(\\ell)}_i\\right)\n\nwhere {\\bf L}_{\\text{learn}}^{(i,j)} = \\sum_{k=1}^K \\theta_{k, (i,j)} {\\bf u}_k {\\bf u}_k^\\top is a separate learnable filter for each input-output dimension pair.\nThis architecture stacks multiple layers, each learning which spectral components matter for the task. Deeper layers capture increasingly complex patterns.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 2: The Spectral Perspective"
    ]
  },
  {
    "objectID": "m06-graph/02-spectral-perspective.html#the-computational-challenge",
    "href": "m06-graph/02-spectral-perspective.html#the-computational-challenge",
    "title": "The Spectral Perspective",
    "section": "The Computational Challenge",
    "text": "The Computational Challenge\nSpectral methods face two critical limitations:\n\nComputational cost: Computing eigendecomposition requires \\mathcal{O}(N^3) operations, prohibitive for large graphs with millions of nodes.\nLack of spatial locality: Learned filters operate on all eigenvectors, meaning every node can influence every other node. This loses the inductive bias of locality that makes CNNs powerful.\n\nThese limitations motivated the development of spatial methods, which define convolution directly in the graph domain without requiring eigendecomposition. We explore spatial approaches in the next part.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 2: The Spectral Perspective"
    ]
  },
  {
    "objectID": "m06-graph/02-spectral-perspective.html#key-takeaways",
    "href": "m06-graph/02-spectral-perspective.html#key-takeaways",
    "title": "The Spectral Perspective",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nThe spectral perspective reveals that graphs have frequency domains. The Laplacian’s eigenvectors serve as basis functions, and eigenvalues indicate how rapidly signals vary. Spectral filters control which frequencies pass through, and we can learn these filters for specific tasks.\nThis mathematical elegance comes at a cost. Eigendecomposition is expensive, and spectral filters lack spatial locality. But the insights gained inform spatial methods, creating a bridge between frequency and spatial domains.\nIn Part 3, we see how spatial methods address these limitations while preserving the power of learned graph convolution.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 2: The Spectral Perspective"
    ]
  },
  {
    "objectID": "m06-graph/04-graph-embeddings.html",
    "href": "m06-graph/04-graph-embeddings.html",
    "title": "Graph Embeddings",
    "section": "",
    "text": "What you’ll learn in this part\n\n\n\nThis part explores graph embeddings that map nodes to continuous vector spaces. We examine both spectral methods rooted in linear algebra and neural methods inspired by language modeling. These embeddings enable clustering, visualization, and downstream machine learning tasks.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 4: Graph Embeddings"
    ]
  },
  {
    "objectID": "m06-graph/04-graph-embeddings.html#what-is-network-embedding",
    "href": "m06-graph/04-graph-embeddings.html#what-is-network-embedding",
    "title": "Graph Embeddings",
    "section": "What is Network Embedding?",
    "text": "What is Network Embedding?\n\n\n\n\n\n\nFigure 1: This figure is taken from DeepWalk: Online Learning of Social Representations.\n\n\n\nNetworks are high-dimensional discrete structures that resist traditional machine learning algorithms designed for continuous data. Network embedding transforms graphs into low-dimensional continuous spaces where each node becomes a point in \\mathbb{R}^d (typically d \\ll N) while preserving important structural properties.\nThe goal is simple but powerful. Map nodes to vectors such that similar nodes have similar embeddings. “Similar” can mean many things: connected by edges, sharing neighbors, playing similar structural roles, or belonging to the same community.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 4: Graph Embeddings"
    ]
  },
  {
    "objectID": "m06-graph/04-graph-embeddings.html#spectral-embeddings",
    "href": "m06-graph/04-graph-embeddings.html#spectral-embeddings",
    "title": "Graph Embeddings",
    "section": "Spectral Embeddings",
    "text": "Spectral Embeddings\nSpectral methods use eigendecomposition to find low-dimensional representations. We explore three approaches: adjacency-based, modularity-based, and Laplacian-based.\n\nCompressing Networks\nSuppose we have an adjacency matrix {\\bf A} of size N \\times N. We want to compress it into a matrix {\\bf U} of size N \\times d where d \\ll N. Good embeddings should reconstruct the original network well:\n\n\\min_{{\\bf U}} J({\\bf U}), \\quad J({\\bf U}) = \\| {\\bf A} - {\\bf U}{\\bf U}^\\top \\|_F^2\n\nwhere \\|\\cdot\\|_F is the Frobenius norm (sum of squared elements).\nThe outer product {\\bf U}{\\bf U}^\\top reconstructs the adjacency matrix from embeddings. Minimizing the difference between {\\bf A} and this reconstruction yields the best low-dimensional representation.\n\n\nSpectral Decomposition Solution\nConsider the eigendecomposition of {\\bf A}:\n\n{\\bf A} = \\sum_{i=1}^N \\lambda_i {\\bf u}_i {\\bf u}_i^\\top\n\nEach term \\lambda_i {\\bf u}_i {\\bf u}_i^\\top is a rank-one matrix capturing part of the network structure. Eigenvalues \\lambda_i indicate importance.\n\nTo compress the network, select the d eigenvectors with largest eigenvalues and form embedding matrix {\\bf U} = [{\\bf u}_1, {\\bf u}_2, \\ldots, {\\bf u}_d]. This is provably optimal for minimizing reconstruction error.\n\n\nModularity Embedding\nInstead of the adjacency matrix, we can embed the modularity matrix:\n\nQ_{ij} = \\frac{1}{2m}A_{ij} - \\frac{k_i k_j}{4m^2}\n\nwhere k_i is node degree and m is the number of edges.\nThe modularity matrix emphasizes connections beyond what random chance predicts. Its eigenvectors naturally separate communities. A simple community detection algorithm: group nodes by the sign of the second eigenvector (Newman 2006).\n\n\nLaplacian Eigenmap\nLaplacian Eigenmap (Belkin and Niyogi 2003) positions connected nodes close together. The optimization problem is:\n\n\\min_{{\\bf U}} J_{LE}({\\bf U}), \\quad J_{LE}({\\bf U}) = \\frac{1}{2}\\sum_{i,j} A_{ij} \\| {\\bf u}_i - {\\bf u}_j \\|^2\n\nThrough algebraic manipulation, this becomes:\n\nJ_{LE}({\\bf U}) = \\text{Tr}({\\bf U}^\\top {\\bf L} {\\bf U})\n\nwhere {\\bf L} = {\\bf D} - {\\bf A} is the graph Laplacian and \\text{Tr} denotes the trace.\n\n\nDerivation:\n\n\\begin{aligned}\nJ_{LE} &= \\frac{1}{2}\\sum_{i,j} A_{ij} (\\| {\\bf u}_i \\|^2 - 2 {\\bf u}_i^\\top {\\bf u}_j + \\| {\\bf u}_j \\|^2) \\\\\n&= \\sum_i k_i \\| {\\bf u}_i \\|^2 - \\sum_{i,j} A_{ij} {\\bf u}_i^\\top {\\bf u}_j \\\\\n&= \\text{Tr}({\\bf U}^\\top {\\bf D} {\\bf U}) - \\text{Tr}({\\bf U}^\\top {\\bf A} {\\bf U}) \\\\\n&= \\text{Tr}({\\bf U}^\\top {\\bf L} {\\bf U})\n\\end{aligned}\n\nThe solution is the d eigenvectors corresponding to the smallest eigenvalues of {\\bf L} (excluding the trivial zero eigenvalue).\n\n\nThe smallest eigenvalue is always zero with eigenvector of all ones. In practice, compute d+1 smallest eigenvectors and discard the first.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 4: Graph Embeddings"
    ]
  },
  {
    "objectID": "m06-graph/04-graph-embeddings.html#neural-embeddings-with-word2vec",
    "href": "m06-graph/04-graph-embeddings.html#neural-embeddings-with-word2vec",
    "title": "Graph Embeddings",
    "section": "Neural Embeddings with word2vec",
    "text": "Neural Embeddings with word2vec\nNeural methods learn embeddings using neural networks trained on data. Before applying these to graphs, we introduce word2vec, which forms the foundation for many graph embedding techniques.\n\nHow word2vec Works\nword2vec (Mikolov et al. 2013) learns word meanings from context, following the principle: “You shall know a word by the company it keeps” (church1988word?).\n\n\nThis phrase comes from Aesop’s fable The Ass and his Purchaser. A man brings an ass to his farm on trial. The ass immediately joins the laziest, greediest ass in the herd. The man returns it, knowing its character by the company it chose.\n\n\nThe Core Idea: Given a target word, predict surrounding context words within a fixed window. For example, in “The quick brown fox jumps over a lazy dog,” the context of fox (window size 2) includes quick, brown, jumps, over.\nWords appearing in similar contexts get similar embeddings. Both fox and dog appear with “quick,” “brown,” and action verbs, so they have similar embeddings. But student appears with “studies” and “library,” giving it a distant embedding.\nNetwork Architecture:\n\n\n\n\n\n\nInput layer: One-hot encoding of target word\nHidden layer: The learned word embedding (low-dimensional)\nOutput layer: Probability distribution over context words (softmax)\n\nThe hidden layer activations become dense, low-dimensional vectors capturing semantic relationships.\n\n\nFor a visual walkthrough, see The Illustrated Word2vec by Jay Alammar.\n\n\nEfficient Training\nComputing the full softmax over vocabulary is expensive:\n\nP(w_c | w_t) = \\frac{\\exp({\\bf v}_{w_c} \\cdot {\\bf v}_{w_t})}{\\sum_{w \\in V} \\exp({\\bf v}_w \\cdot {\\bf v}_{w_t})}\n\nThe denominator sums over all words (100,000+ terms), making training slow.\nTwo solutions:\n\nHierarchical Softmax: Organizes vocabulary as a binary tree. Computing probability becomes traversing root-to-leaf paths, reducing complexity from \\mathcal{O}(|V|) to \\mathcal{O}(\\log |V|).\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\nNegative Sampling: Instead of normalizing over all words, sample a few “negative” (non-context) words and contrast them with true context words. This approximates the full softmax efficiently with 5-20 negative samples.\n\n\n\nword2vec’s Power\nword2vec embeddings capture semantic relationships through simple linear algebra:\n\n\n\n\n\n\nFigure 3\n\n\n\nFamous examples include analogies: man is to woman as king is to queen. Relationships like countries and capitals form parallel vectors in the embedding space.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 4: Graph Embeddings"
    ]
  },
  {
    "objectID": "m06-graph/04-graph-embeddings.html#graph-embedding-with-word2vec",
    "href": "m06-graph/04-graph-embeddings.html#graph-embedding-with-word2vec",
    "title": "Graph Embeddings",
    "section": "Graph Embedding with word2vec",
    "text": "Graph Embedding with word2vec\nHow can we apply word2vec to graphs? The challenge is that word2vec expects sequences, while graphs are unordered. The solution: random walks transform graphs into sequences of nodes. Treat walks as sentences and nodes as words.\n\n\nRandom walks create “sentences” from graphs: each walk is a sequence of nodes, just like a sentence is a sequence of words.\n\nDeepWalk\n\nDeepWalk (Perozzi, Al-Rfou, and Skiena 2014) pioneered applying word2vec to graphs:\n\nSample multiple random walks from the graph\nTreat walks as sentences and feed them to word2vec\n\nDeepWalk uses skip-gram with hierarchical softmax for efficient training. Nodes appearing in similar walk contexts get similar embeddings.\n\n\nnode2vec\nnode2vec (Grover and Leskovec 2016) extends DeepWalk with biased random walks controlled by parameters p and q:\n\nP(v_{t+1} = x | v_t = v, v_{t-1} = t) \\propto\n\\begin{cases}\n\\frac{1}{p} & \\text{if } d(t,x) = 0 \\text{ (return to previous)} \\\\\n1 & \\text{if } d(t,x) = 1 \\text{ (close neighbor)} \\\\\n\\frac{1}{q} & \\text{if } d(t,x) = 2 \\text{ (explore further)}\n\\end{cases}\n\nwhere d(t,x) is the shortest path from previous node t to candidate x.\nControlling Exploration:\n\nLow p → return bias (local revisiting)\nLow q → outward bias (exploration, DFS-like)\nHigh q → inward bias (stay local, BFS-like)\n\n\nTwo Exploration Strategies:\n\nBFS-like (low q): Explore immediate neighborhoods → captures community structure\nDFS-like (high q): Explore deep paths → captures structural roles\n\n\n\n\nTechnical Note: node2vec uses negative sampling instead of hierarchical softmax, affecting embedding characteristics (Kojaku et al. 2021; Dyer 2014).\n\n\nLINE\nLINE (Tang et al. 2015) is equivalent to node2vec with p=1, q=1, and window size 1. It directly optimizes graph structure preservation.\n\n\nNeural methods seem less transparent than spectral methods, but recent work establishes equivalences under specific conditions (Qiu et al. 2018; kojaku2024network?). Surprisingly, DeepWalk, node2vec, and LINE are provably optimal for stochastic block models (kojaku2024network?).",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 4: Graph Embeddings"
    ]
  },
  {
    "objectID": "m06-graph/04-graph-embeddings.html#hands-on-implementing-embeddings",
    "href": "m06-graph/04-graph-embeddings.html#hands-on-implementing-embeddings",
    "title": "Graph Embeddings",
    "section": "Hands-On: Implementing Embeddings",
    "text": "Hands-On: Implementing Embeddings\nLet’s implement these methods on the Karate Club network.\n\nData Preparation\n\nimport numpy as np\nimport igraph\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the karate club network\ng = igraph.Graph.Famous(\"Zachary\")\nA = g.get_adjacency_sparse()\n\n# Get community labels (Mr. Hi = 0, Officer = 1)\nlabels = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\ng.vs[\"label\"] = labels\n\n# Visualize the network\npalette = sns.color_palette().as_hex()\nigraph.plot(g, vertex_color=[palette[label] for label in labels], bbox=(300, 300))\n\n\n\n\n\n\n\n\n\n\nSpectral Embedding Example\n\n# Convert to dense array for eigendecomposition\nA_dense = A.toarray()\n\n# Compute the spectral decomposition\neigvals, eigvecs = np.linalg.eig(A_dense)\n\n# Find the top d eigenvectors\nd = 2\nsorted_indices = np.argsort(eigvals)[::-1][:d]\neigvals_top = eigvals[sorted_indices]\neigvecs_top = eigvecs[:, sorted_indices]\n\n# Plot the results\nfig, ax = plt.subplots(figsize=(7, 5))\nsns.scatterplot(x=eigvecs_top[:, 0], y=eigvecs_top[:, 1], hue=labels, ax=ax)\nax.set_title('Spectral Embedding')\nax.set_xlabel('Eigenvector 1')\nax.set_ylabel('Eigenvector 2')\nplt.show()\n\n/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/matplotlib/cbook.py:1709: ComplexWarning: Casting complex values to real discards the imaginary part\n  return math.isfinite(val)\n/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/matplotlib/cbook.py:1709: ComplexWarning: Casting complex values to real discards the imaginary part\n  return math.isfinite(val)\n/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:133: ComplexWarning: Casting complex values to real discards the imaginary part\n  return arr.astype(dtype, copy=True)\n/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:133: ComplexWarning: Casting complex values to real discards the imaginary part\n  return arr.astype(dtype, copy=True)\n\n\n\n\n\n\n\n\n\nThe first eigenvector corresponds to eigencentrality. The second eigenvector captures community structure, clearly separating the two groups.\n\n\nLaplacian Eigenmap Example\n\nD = np.diag(np.sum(A_dense, axis=1))\nL = D - A_dense\n\neigvals_L, eigvecs_L = np.linalg.eig(L)\n\n# Sort and select eigenvalues (exclude first)\nsorted_indices_L = np.argsort(eigvals_L)[1:d+1]\neigvals_L_top = eigvals_L[sorted_indices_L]\neigvecs_L_top = eigvecs_L[:, sorted_indices_L]\n\n# Plot the results\nfig, ax = plt.subplots(figsize=(7, 5))\nsns.scatterplot(x=eigvecs_L_top[:, 0], y=eigvecs_L_top[:, 1], hue=labels, ax=ax)\nax.set_title('Laplacian Eigenmap')\nax.set_xlabel('Eigenvector 2')\nax.set_ylabel('Eigenvector 3')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDeepWalk Example\nFirst, implement random walk sampling:\n\ndef random_walk(net, start_node, walk_length):\n    \"\"\"Generate a random walk starting from start_node.\"\"\"\n    walk = [start_node]\n\n    while len(walk) &lt; walk_length:\n        cur = walk[-1]\n        cur_nbrs = list(net[cur].indices)\n\n        if len(cur_nbrs) &gt; 0:\n            walk.append(np.random.choice(cur_nbrs))\n        else:\n            break  # Dead end\n\n    return walk\n\n# Generate random walks\nn_nodes = g.vcount()\nn_walkers_per_node = 10\nwalk_length = 50\n\nwalks = []\nfor i in range(n_nodes):\n    for _ in range(n_walkers_per_node):\n        walks.append(random_walk(A, i, walk_length))\n\nprint(f\"Generated {len(walks)} random walks\")\nprint(f\"Example walk: {walks[0][:10]}...\")\n\nGenerated 340 random walks\nExample walk: [0, 31, 33, 20, 32, 20, 33, 26, 33, 22]...\n\n\nTrain word2vec on the walks:\n\nfrom gensim.models import Word2Vec\n\nmodel = Word2Vec(\n    walks,\n    vector_size=32,\n    window=3,\n    min_count=1,\n    sg=1,  # Skip-gram\n    hs=1,  # Hierarchical softmax\n    workers=1,\n)\n\n# Extract embeddings\nembedding = np.array([model.wv[i] for i in range(n_nodes)])\nprint(f\"Embedding matrix shape: {embedding.shape}\")\n\nEmbedding matrix shape: (34, 32)\n\n\nVisualize using UMAP:\n\n\n/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n  warn(\nOMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n\n\n    \n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\n\n  \n\n\n\n\n\nNodes from the same community cluster together, showing that DeepWalk captures community structure.\n\n\nClustering with K-means\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\ndef find_optimal_clusters(embedding, n_clusters_range=(2, 10)):\n    \"\"\"Find optimal number of clusters using silhouette score.\"\"\"\n    silhouette_scores = []\n\n    for n_clusters in range(*n_clusters_range):\n        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n        cluster_labels = kmeans.fit_predict(embedding)\n        score = silhouette_score(embedding, cluster_labels)\n        silhouette_scores.append((n_clusters, score))\n        print(f\"k={n_clusters}: silhouette score = {score:.3f}\")\n\n    optimal_k = max(silhouette_scores, key=lambda x: x[1])[0]\n    print(f\"\\nOptimal number of clusters: {optimal_k}\")\n\n    kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n    return kmeans.fit_predict(embedding)\n\n# Find clusters\ncluster_labels = find_optimal_clusters(embedding)\n\n# Visualize clustering\ncmap = sns.color_palette().as_hex()\nigraph.plot(\n    g,\n    vertex_color=[cmap[label] for label in cluster_labels],\n    bbox=(500, 500),\n    vertex_size=20\n)\n\nk=2: silhouette score = 0.475\nk=3: silhouette score = 0.516\nk=4: silhouette score = 0.527\nk=5: silhouette score = 0.414\nk=6: silhouette score = 0.421\nk=7: silhouette score = 0.387\nk=8: silhouette score = 0.352\nk=9: silhouette score = 0.359\n\nOptimal number of clusters: 4\n\n\n\n\n\n\n\n\n\nK-means successfully identifies communities using only learned embeddings, demonstrating that DeepWalk captures meaningful structural properties.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 4: Graph Embeddings"
    ]
  },
  {
    "objectID": "m06-graph/04-graph-embeddings.html#comparing-approaches",
    "href": "m06-graph/04-graph-embeddings.html#comparing-approaches",
    "title": "Graph Embeddings",
    "section": "Comparing Approaches",
    "text": "Comparing Approaches\nWe have explored multiple embedding methods, each with distinct trade-offs:\n\n\n\n\n\n\n\n\n\nMethod\nApproach\nStrengths\nLimitations\n\n\n\n\nSpectral (Adjacency)\nEigendecomposition\nPrincipled, captures centrality\nExpensive, requires full graph\n\n\nLaplacian Eigenmap\nMinimize edge distances\nPreserves local structure\nSensitive to disconnected components\n\n\nDeepWalk\nRandom walks + word2vec\nScalable, flexible\nRandom initialization sensitive\n\n\nnode2vec\nBiased random walks\nControls exploration\nMore hyperparameters\n\n\n\nSpectral methods offer mathematical guarantees but scale poorly. Neural methods scale to large graphs and generalize to unseen nodes but require careful hyperparameter tuning.\nRecent work shows these methods are more connected than they appear. Under specific conditions, neural embeddings provably approximate spectral embeddings (Qiu et al. 2018; kojaku2024network?). This bridges the gap between elegant theory and practical scalability.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 4: Graph Embeddings"
    ]
  },
  {
    "objectID": "m06-graph/04-graph-embeddings.html#key-takeaways",
    "href": "m06-graph/04-graph-embeddings.html#key-takeaways",
    "title": "Graph Embeddings",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nGraph embeddings transform discrete networks into continuous vector spaces, enabling standard machine learning algorithms. Spectral methods use eigendecomposition to find optimal low-dimensional representations. Neural methods learn embeddings by treating random walks as training data for word2vec.\nThese embeddings power downstream tasks: node classification, link prediction, community detection, and visualization. They bridge the gap between graph theory and deep learning, showing that geometric intuitions about similarity and distance extend naturally to network data.\nIn this module, we journeyed from pixels to nodes, extending convolution beyond regular grids. We explored spectral and spatial perspectives on graph neural networks. We learned how embeddings compress networks into vectors while preserving structure.\nThe principles transcend specific architectures. Locality matters. Parameter sharing generalizes. Hierarchical features extract increasingly abstract patterns. These insights apply wherever relationships exist, from molecules to social networks to knowledge graphs.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 4: Graph Embeddings"
    ]
  },
  {
    "objectID": "m06-graph/graph-convolutional-network.html",
    "href": "m06-graph/graph-convolutional-network.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "title: “Spatial Graph Convolutional Networks”\n\n\njupyter: advnetsci\n\n\nexecute:\n\n\nenabled: true"
  },
  {
    "objectID": "m06-graph/graph-convolutional-network.html#graphsage-sample-and-aggregate",
    "href": "m06-graph/graph-convolutional-network.html#graphsage-sample-and-aggregate",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "GraphSAGE: Sample and Aggregate",
    "text": "GraphSAGE: Sample and Aggregate\nGraphSAGE (Hamilton, Ying, and Leskovec 2017) introduced a different GCN that can be generalized to unseen nodes (they called it “inductive”). While previous approaches like ChebNet and GCN operate on the entire graph, GraphSAGE proposes an inductive framework that generates embeddings by sampling and aggregating features from a node’s neighborhood.\n\n\nKey Ideas\nGraphSAGE involves two key ideas: (1) sampling and (2) aggregation.\n\n\nNeighborhood Sampling\nThe key idea is the neighborhood sampling. Instead of using all neighbors, GraphSAGE samples a fixed-size set of neighbors for each node. This controls memory complexity, a key limitation of the previous GNNs.\nAnother key advantage of neighborhood sampling is that it enables GraphSAGE to handle dynamic, growing networks. Consider a citation network where new papers (nodes) are continuously added. Traditional GCNs would need to recompute filters for the entire network with each new addition. In contrast, GraphSAGE can immediately generate embeddings for new nodes by simply sampling their neighbors, without any retraining or recomputation.\n\n\nAggregation\nAnother key idea is the aggregation. GraphSAGE makes a distinction between self-information and neighborhood information. While previous GNNs treat them equally and aggregate them, GraphSAGE treats them differently. Specifically, GraphSAGE introduces an additional step: it concatenates the self-information and the neighborhood information as the input of the convolution.\n\nZ_v = \\text{CONCAT}(X_v, X_{\\mathcal{N}(v)})\n\nwhere X_v is the feature of the node itself and X_{\\mathcal{N}(v)} is the aggregation of the features of its neighbors. GraphSAGE introduces different ways to aggregate information from neighbors:\nX_{\\mathcal{N}(v)} = \\text{AGGREGATE}_k(\\{X_u, \\forall u \\in \\mathcal{N}(v)\\})\nCommon aggregation functions include: - Mean aggregator: \\text{AGGREGATE} = \\text{mean}(\\{h_u, \\forall u \\in \\mathcal{N}(v)\\}) - Max-pooling: \\text{AGGREGATE} = \\max(\\{\\sigma(W_{\\text{pool}}h_u + b), \\forall u \\in \\mathcal{N}(v)\\}) - LSTM aggregator: Apply LSTM to randomly permuted neighbors\nThe concatenated feature Z_v is normalized by the L2 norm.\n\n\\hat{Z}_v = \\frac{Z_v}{\\|Z_v\\|_2}\n\nand then fed into the convolution.\n\nX_v^k = \\sigma(W^k \\hat{Z}_v + b^k)"
  },
  {
    "objectID": "m06-graph/graph-convolutional-network.html#graph-attention-networks-gat-differentiate-individual-neighbors",
    "href": "m06-graph/graph-convolutional-network.html#graph-attention-networks-gat-differentiate-individual-neighbors",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "Graph Attention Networks (GAT): Differentiate Individual Neighbors",
    "text": "Graph Attention Networks (GAT): Differentiate Individual Neighbors\nA key innovation of GraphSAGE is to treat the self and neighborhood information differently. But should all neighbors be treated equally? Graph Attention Networks (GAT) address this by letting the model learn which neighbors to pay attention to.\n\nAttention Mechanism\n\nThe core idea is beautifully simple: instead of using fixed weights like GCN, let’s learn attention weights \\alpha_{ij} that determine how much node i should attend to node j. These weights are computed dynamically based on node features:\n\n\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}(i)} \\exp(e_{ik})}\n\nwhere e_{ij} represents the importance of the edge between node i and node j. Variable e_{ij} is a learnable parameter and can be negative, and the exponential function is applied to transform it to a non-negative value, with the normalization term \\sum_{k \\in \\mathcal{N}(i)} \\exp(e_{ik}) to ensure the weights sum to 1.\nHow to compute e_{ij}? One simple choice is to use a neural network with a shared weight matrix W and a LeakyReLU activation function. Specifically:\n\nLet’s focus on computing e_{ij} for node i and its neighbor j.\nWe use a shared weight matrix W to transform the features of node i and j. \n\\mathbf{\\tilde h}_i  = \\mathbf{h}_i, \\quad \\mathbf{\\tilde h}_j  = W\\mathbf{h}_j\n\nWe concatenate the transformed features and apply a LeakyReLU activation function.\n\n\ne_{ij} = \\text{LeakyReLU}(\\mathbf{a}^T[\\mathbf{\\tilde h}_i, \\mathbf{\\tilde h}_j])\n\nwhere \\mathbf{a} is a trainable parameter vector that sums the two transformed features.\nOnce we have these attention weights, the node update is straightforward - just a weighted sum of neighbor features:\n\\mathbf{h}'_i = \\sigma\\left(\\sum_{j \\in \\mathcal{N}(i) \\cup \\{i\\}} \\alpha_{ij}{\\bf W}_{\\text{feature}}\\mathbf{h}_j\\right)\nwhere {\\bf W}_{\\text{feature}} is a trainable weight matrix. To stabilize training, GAT uses multiple attention heads and concatenates their outputs:\n\\mathbf{h}'_i = \\parallel_{k=1}^K \\sigma\\left(\\sum_{j \\in \\mathcal{N}(i) \\cup \\{i\\}} \\alpha_{ij}^k{\\bf W}^k_{\\text{feature}}\\mathbf{h}_j\\right)"
  },
  {
    "objectID": "m06-graph/graph-convolutional-network.html#graph-isomorphism-network-gin-differentiate-the-aggregation",
    "href": "m06-graph/graph-convolutional-network.html#graph-isomorphism-network-gin-differentiate-the-aggregation",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "Graph Isomorphism Network (GIN): Differentiate the Aggregation",
    "text": "Graph Isomorphism Network (GIN): Differentiate the Aggregation\nGraph Isomorphism Networks (GIN) is another popular GNN that born out of a question: what is the maximum discriminative power achievable by Graph Neural Networks? The answer lies in its theoretical connection to the Weisfeiler-Lehman (WL) test, a powerful algorithm for graph isomorphism testing.\n\nWeisfeiler-Lehman Test\nAre two graphs structurally identical? Graph isomorphism testing determines if two graphs are structurally identical, with applications in graph classification, clustering, and other tasks.\n\nWhile the general problem has no known polynomial-time solution, the WL test is an efficient heuristic that works well in practice. The WL test iteratively refines node labels by hashing the multiset of neighboring labels\n\nThe WL test works as follows:\n\nAssign all nodes the same initial label.\nFor each node, collect the labels of all its neighbors and aggregate them into a hash (e.g., new label). For example, the top node gets {0} from its neighbors, resulting in a collection {0,0}. A new label is created via a hash function h that maps {0, {0, 0}} to a new label 1.\nRepeat the process for a fixed number of iterations or until convergence. å After these iterations:\n\n\nNodes with the same label are structurally identical, meaning that they are indistinguishable unless we label them differently.\nTwo graphs are structurally identical if and only if they have the same node labels after the WL test.\n\nThe WL test is a heuristic and can fail on some graphs. For example, it cannot distinguish regular graphs with the same number of nodes and edges.\n\n\nThe WL test above is called the 1-WL test. There are higher-order WL tests that can distinguish more graphs, which are the basis of advanced GNNs. Check out this note\n\n\nGIN\nGIN (Xu et al. 2019) is a GNN that is based on the WL test. The key idea is to focus on the parallel between the WL test and the GNN update rule. - In the WL test, we iteratively collect the labels of neighbors and aggregate them through a hash function. - In the GraphSAGE and GAT, the labels are the nodes’ features, and the aggregation is some arithmetic operations such as mean or max.\nThe key difference is that the hash function in the WL test always distinguishes different sets of neighbors’ labels, while the aggregation in GraphSAGE and GAT does not always do so. For example, if all nodes have the same feature (e.g., all 1), the aggregation by the mean or max will result in the same value for all nodes, whereas the hash function in the WL test can still distinguish different sets of neighbors’ labels by the count of each label.\nThe resulting convolution update rule is:\n\nh_v^{(k+1)} = \\text{MLP}^{(k)}\\left((1 + \\epsilon^{(k)}) \\cdot h_v^{(k)} + \\sum_{u \\in \\mathcal{N}(v)} h_u^{(k)}\\right)\n\nwhere \\text{MLP}^{(k)} is a multi-layer perceptron (MLP) with k layers, and \\epsilon^{(k)} is a fixed or trainable parameter."
  },
  {
    "objectID": "course/about.html",
    "href": "course/about.html",
    "title": "About Us",
    "section": "",
    "text": "Complex systems are everywhere—from social networks shaping our daily interactions to neural networks powering AI systems. This course explores computational intelligence from the ground up, combining hands-on deep learning with complexity science to unlock the secrets of emergent behaviors in large-scale systems.",
    "crumbs": [
      "Home",
      "Course Information",
      "About Us"
    ]
  },
  {
    "objectID": "course/about.html#about-us",
    "href": "course/about.html#about-us",
    "title": "About Us",
    "section": "About Us",
    "text": "About Us\n\nInstructor\n\n\n\nWelcome! My name is Sadamori Kojaku, the instructor of this course. I started my career as a computer scientist but couldn’t resist falling in love with Complex Systems and Network Science right after I got my Ph.D. Complex systems appear in many different forms in our daily lives—social networks, biological systems, the internet, power grids, and you name it. But when we represent them using modern deep learning techniques, we can understand them in unified ways. We can study them using the same toolkit no matter what the domain is, and can find universal patterns and principles that govern seemingly different systems. Even more fascinating, large-scale deep learning models themselves exhibit emergent behaviors as complex systems! Sounds fun, right 😉?\nThis course will guide you through the fascinating intersection of deep learning and complex systems, from foundational theory to hands-on coding and real-world applications. I hope you will enjoy and find the course useful in your future endeavors.\n\n\n\n\n\n\n\n\nTA\nTeaching Assistant is not yet assigned.\n\n\nAI Tutor (Minidora)\n\n\n\nMinidora is an AI tutor robot conceived in the 22nd century and deployed in the present era to support students. (The original character is designed by Fujiko Fujio for a famous Japanese manga called Doraemon). Minidora supports students in this course by providing dialogic explanations, quiz questions, and coding guidance on the course Discord.",
    "crumbs": [
      "Home",
      "Course Information",
      "About Us"
    ]
  },
  {
    "objectID": "course/discord.html",
    "href": "course/discord.html",
    "title": "Discord",
    "section": "",
    "text": "We use a dedicated Discord server for this course to facilitate communication, Q&A, and collaboration outside of class. The Discord server is a space where you can:\n\nAsk questions about lectures, assignments, and projects\nDiscuss concepts and share resources with your peers\nGet support from the instructor, TA, and Minidora (the AI tutor)\nJoin study groups and participate in informal discussions\n\nInvitation links to the Discord server will be distributed via Brightspace. Please check the Brightspace announcements or course materials for the latest invite link.\nOnce you join, you’ll find channels for different topics (e.g., #random, #questions, #study-groups) and can interact with both classmates, AI tutor, and course staff. If you’re new to Discord, it’s a free platform available on web, desktop, and mobile.\n\n\n\nScreenshot of the course Discord server\n\n\nExample screenshot of the course Discord server interface.\nIf you have any trouble joining, please contact the instructor for assistance.",
    "crumbs": [
      "Home",
      "Course Information",
      "Discord"
    ]
  },
  {
    "objectID": "course/welcome.html",
    "href": "course/welcome.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome to the course! In this class, we will explore how deep learning serves as a powerful tool to operationalize high-dimensional data from social and physical systems. You’ll learn to model complex system dynamics using modern computational intelligence, and in turn, use perspectives from complexity science to understand the emergent behaviors of large-scale models.\nThis course is designed to provide you with both a theoretical foundation and hands-on experience in applied soft computing. You will learn how to apply representation learning, sequence modeling, and graph analytics to model real-world complex systems using Python and modern deep learning frameworks.\n\n\nThis course is divided into three chapters: Foundation, Deep Learning and Advanced Topics.\nFoundation chapter covers the foundational concepts of data visualization, data science, and reproducibility. This will prepare you for building your own data science projects with modern deep learning tools.\nThe Deep Learning chapter covers the fundamental concepts of deep learning for text, images, and graphs. Through hands-on coding, you will learn how to build your own deep learning models for different data types.\nThe Advanced Topics chapter elevates you from a user to a creator of advanced soft computing models. You will learn how to build your own large language models and self-supervised learning models.\n\n\n\n\nEngaging Lectures: Each week, we’ll dive into key concepts in deep learning and complex systems, supported by interactive discussions and in-class activities.\nHands-on Coding: You’ll work with real data from text, images, and networks using Python and modern deep learning tools.\nCollaborative Learning: Participate in group activities, discussions, and our dedicated Discord server for Q&A and support.\nProjects and Assignments: Apply your knowledge through coding assignments, quizzes, and a final project.\nSupport: Get help from the instructor, TA, and our AI tutor.\n\n\n\n\n\nWhy applied soft computing? Read the Overview page to understand the importance of applied soft computing.\nRead the About Us page to meet your instructor, TA, and AI tutor.\nJoin the Discord server for announcements, help, and community.\nSet up your Python environment by following the Setup Guide.\nLearn how to submit assignments using GitHub Classroom.",
    "crumbs": [
      "Home",
      "Course Information",
      "Welcome"
    ]
  },
  {
    "objectID": "course/welcome.html#welcome-to-applied-soft-computing",
    "href": "course/welcome.html#welcome-to-applied-soft-computing",
    "title": "Welcome",
    "section": "",
    "text": "Welcome to the course! In this class, we will explore how deep learning serves as a powerful tool to operationalize high-dimensional data from social and physical systems. You’ll learn to model complex system dynamics using modern computational intelligence, and in turn, use perspectives from complexity science to understand the emergent behaviors of large-scale models.\nThis course is designed to provide you with both a theoretical foundation and hands-on experience in applied soft computing. You will learn how to apply representation learning, sequence modeling, and graph analytics to model real-world complex systems using Python and modern deep learning frameworks.\n\n\nThis course is divided into three chapters: Foundation, Deep Learning and Advanced Topics.\nFoundation chapter covers the foundational concepts of data visualization, data science, and reproducibility. This will prepare you for building your own data science projects with modern deep learning tools.\nThe Deep Learning chapter covers the fundamental concepts of deep learning for text, images, and graphs. Through hands-on coding, you will learn how to build your own deep learning models for different data types.\nThe Advanced Topics chapter elevates you from a user to a creator of advanced soft computing models. You will learn how to build your own large language models and self-supervised learning models.\n\n\n\n\nEngaging Lectures: Each week, we’ll dive into key concepts in deep learning and complex systems, supported by interactive discussions and in-class activities.\nHands-on Coding: You’ll work with real data from text, images, and networks using Python and modern deep learning tools.\nCollaborative Learning: Participate in group activities, discussions, and our dedicated Discord server for Q&A and support.\nProjects and Assignments: Apply your knowledge through coding assignments, quizzes, and a final project.\nSupport: Get help from the instructor, TA, and our AI tutor.\n\n\n\n\n\nWhy applied soft computing? Read the Overview page to understand the importance of applied soft computing.\nRead the About Us page to meet your instructor, TA, and AI tutor.\nJoin the Discord server for announcements, help, and community.\nSet up your Python environment by following the Setup Guide.\nLearn how to submit assignments using GitHub Classroom.",
    "crumbs": [
      "Home",
      "Course Information",
      "Welcome"
    ]
  },
  {
    "objectID": "m04-text/what-to-learn.html",
    "href": "m04-text/what-to-learn.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "In this module, we will learn about recurrent neural networks (RNNs) that can process sequential text data. We will learn: - Recurrent Neural Networks (RNNs) for processing sequential text data - Long-Short Term Memory (LSTM) networks for capturing long-range dependencies - Sequence-to-sequence models for machine translation - The Attention mechanism for focusing on relevant parts of text"
  },
  {
    "objectID": "m04-text/what-to-learn.html#what-to-learn-in-this-module",
    "href": "m04-text/what-to-learn.html#what-to-learn-in-this-module",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "In this module, we will learn about recurrent neural networks (RNNs) that can process sequential text data. We will learn: - Recurrent Neural Networks (RNNs) for processing sequential text data - Long-Short Term Memory (LSTM) networks for capturing long-range dependencies - Sequence-to-sequence models for machine translation - The Attention mechanism for focusing on relevant parts of text"
  },
  {
    "objectID": "m05-images/pen-and-paper.html",
    "href": "m05-images/pen-and-paper.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Pen and paper exercises\n\n✍️ Pen and paper exercises"
  },
  {
    "objectID": "course/minidora-usage.html#getting-started-with-minidora",
    "href": "course/minidora-usage.html#getting-started-with-minidora",
    "title": "Using Minidora",
    "section": "Getting Started with Minidora",
    "text": "Getting Started with Minidora\nMinidora is your personal AI tutor available 24/7 through Discord to help you master deep learning and complex systems concepts. She’s designed to provide personalized learning support, answer questions about course materials, and guide you through challenging topics with patience and clarity. To interact with Minidora, simply use Discord slash commands or mention her directly in any channel or thread where she’s present.\nCheck out the instruction here on how to use Minidora: Minidora Usage. Minidora is available on Discord, and you can find the invitation link on the email sent in the first week of the semester. Or you can find the invitation link on the Brightspace.\n\n\n\n\n\n\nNote\n\n\n\nSome students could not find Minidora on Discord. The easiest way to get around this is:\n\nGo to the course discord server\nOpen the “applied-soft-computing” channel.\nClick the Minidora icon and send a direct message\nType “/” and see if the available commands are shown.",
    "crumbs": [
      "Home",
      "Course Information",
      "Using Minidora"
    ]
  },
  {
    "objectID": "course/minidora-usage.html#asking-questions",
    "href": "course/minidora-usage.html#asking-questions",
    "title": "Using Minidora",
    "section": "Asking Questions",
    "text": "Asking Questions\nThe most straightforward way to get help is using the /ask command followed by your question. For example, suppose that you want to ask about a subject (Word embeddings) in module 3.\n\nType /ask then type space.\nType your question (e.g., What are word embeddings and how does Word2Vec work?)\nType space\nYou will be prompted to specify the module id. The id consists of “m”. For example, if it is module 3, you should type m03. Type the module id.\nThen type enter.\n\nMinidora will then read the lecture content and provide an explanation.",
    "crumbs": [
      "Home",
      "Course Information",
      "Using Minidora"
    ]
  },
  {
    "objectID": "course/minidora-usage.html#natural-conversations-and-interactive-learning",
    "href": "course/minidora-usage.html#natural-conversations-and-interactive-learning",
    "title": "Using Minidora",
    "section": "Natural Conversations and Interactive Learning",
    "text": "Natural Conversations and Interactive Learning\nFor a more conversational experience, use the /chat command which allows you to interact with Minidora in a natural, free-flowing manner. You can say things like /chat I'm confused about transformers, can you explain the attention mechanism step by step? or /chat Can you help me debug this Python code for training a CNN? Minidora will engage in back-and-forth dialogue, ask clarifying questions, and adapt her explanations based on your responses.\nNote that /chat does not contextualize the Minidora to the course materials. That means that it does not read the lecture content and interact with the students with its built-in knowledge.",
    "crumbs": [
      "Home",
      "Course Information",
      "Using Minidora"
    ]
  },
  {
    "objectID": "course/minidora-usage.html#quizzes-and-assessment",
    "href": "course/minidora-usage.html#quizzes-and-assessment",
    "title": "Using Minidora",
    "section": "Quizzes and Assessment",
    "text": "Quizzes and Assessment\nTo test your understanding and reinforce learning, Minidora offers intelligent quiz features through the /quiz command. She can generate concept-based questions using /concept-quiz m03 multiple-choice for theoretical understanding, or coding challenges with /code-quiz m03 to practice implementation skills. Minidora tracks your progress and adapts quiz difficulty based on your performance, focusing on areas where you need more practice. You can also request quizzes on specific topics by adding subject keywords, such as /quiz m04 convolutional neural networks.",
    "crumbs": [
      "Home",
      "Course Information",
      "Using Minidora"
    ]
  },
  {
    "objectID": "course/minidora-usage.html#tracking-your-progress",
    "href": "course/minidora-usage.html#tracking-your-progress",
    "title": "Using Minidora",
    "section": "Tracking Your Progress",
    "text": "Tracking Your Progress\nUse the /status command to monitor your learning journey and see detailed insights about your progress. Minidora provides different status views: /status summary gives you a quick overview of questions asked and concepts mastered, while /status concepts shows which topics you’ve learned and what to study next. The /status profile command reveals your personalized learning profile, including your preferred difficulty level, learning style, and areas where you excel or need additional support. This helps Minidora provide increasingly personalized assistance as you continue learning.",
    "crumbs": [
      "Home",
      "Course Information",
      "Using Minidora"
    ]
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Home",
    "section": "Course Overview",
    "text": "Course Overview\nThis course explores how deep learning serves as a powerful tool to operationalize high-dimensional data from social and physical systems. You’ll learn to model complex system dynamics using modern computational intelligence, and in turn, use perspectives from complexity science to understand the emergent behaviors of large-scale models.\nThe course combines:\n\nHands-on coding with real data from text, images, and networks\nTheoretical foundations of deep learning and complex systems\nReproducible data science practices with modern tools\nEthical considerations in AI and computational modeling",
    "crumbs": [
      "Home",
      "Course Information",
      "Home"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Home",
    "section": "Getting Started",
    "text": "Getting Started\n\nRead the Welcome page\nLearn About Us\nJoin our Discord server\nFollow the Setup Guide\nLearn How to Submit Assignments",
    "crumbs": [
      "Home",
      "Course Information",
      "Home"
    ]
  },
  {
    "objectID": "m01-toolkit/overview.html",
    "href": "m01-toolkit/overview.html",
    "title": "Overview",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module introduces the tools and principles of reproducible data science. We will explore version control with Git and GitHub to track changes and collaborate effectively. We will examine data provenance and tidy data principles to ensure your data history is clear and your structure is sound. Finally, we will learn how to build reproducible environments so others can replicate your work exactly.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Overview"
    ]
  },
  {
    "objectID": "m01-toolkit/overview.html#the-reproducibility-crisis",
    "href": "m01-toolkit/overview.html#the-reproducibility-crisis",
    "title": "Overview",
    "section": "The Reproducibility Crisis",
    "text": "The Reproducibility Crisis\nImagine spending months on a groundbreaking data analysis, only to find that you can’t reproduce your own results. Or imagine a colleague asks for your code and data from a project you did last year, and you suddenly realize you can’t remember where you saved the files or which version produced the final answer. These scenarios are all too common in data science.\nWhat ties all these stories together is the need for something called provenance. This is simply a complete lineage of the data and code from its origin to its final form. It’s the backbone of good science, allowing others to verify your findings and build upon your work.\nThis module will teach you the tools and principles that make reproducible data science pipelines possible. A little bit of organization upfront can save you hours of pain down the road.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Overview"
    ]
  },
  {
    "objectID": "m01-toolkit/overview.html#version-control",
    "href": "m01-toolkit/overview.html#version-control",
    "title": "Overview",
    "section": "Version Control",
    "text": "Version Control\nLet’s start with version control. Without proper tracking, accidents happen: you lose days of work to an overwrite, or worse, a security breach because no one knows which version of code is running. Version control transforms chaos into clarity.\nGit and GitHub let you track changes, collaborate without stepping on each other’s toes, and recover from mistakes. Ready to dive deeper? Learn more about Version Control with Git & GitHub.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Overview"
    ]
  },
  {
    "objectID": "m01-toolkit/overview.html#data-provenance-and-tidy-data",
    "href": "m01-toolkit/overview.html#data-provenance-and-tidy-data",
    "title": "Overview",
    "section": "Data Provenance and Tidy Data",
    "text": "Data Provenance and Tidy Data\nNow shift your attention from the tools to the data itself. You need two things: knowing the history of your data (called data provenance) and structuring it in a way that makes analysis straightforward (called tidy data).\nKnowing where your data came from, how it was collected, and what transformations were applied is critical for trust and reproducibility. When you structure data tidily, analysis becomes faster, clearer, and less error-prone. Ready to explore? Check out Data Provenance and Tidy Data.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Overview"
    ]
  },
  {
    "objectID": "m01-toolkit/overview.html#reproducible-environments",
    "href": "m01-toolkit/overview.html#reproducible-environments",
    "title": "Overview",
    "section": "Reproducible Environments",
    "text": "Reproducible Environments\nNow let’s think about the environment where your code runs. Your code might work perfectly on your machine today, but will it run on your colleague’s machine tomorrow? Will it work on your machine six months from now after library updates?\nReproducible environments ensure your work replicates exactly, no matter where or when it runs. This is the final piece of the reproducibility puzzle. Ready to complete the picture? Discover Reproducible Environments & Projects.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Overview"
    ]
  },
  {
    "objectID": "m01-toolkit/overview.html#what-youll-gain",
    "href": "m01-toolkit/overview.html#what-youll-gain",
    "title": "Overview",
    "section": "What You’ll Gain",
    "text": "What You’ll Gain\nBy the end of this module, you’ll have a solid foundation in reproducible data science. You’ll track changes with version control, structure data clearly, and build replicable environments. These practices make you a more effective and trustworthy collaborator.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Overview"
    ]
  },
  {
    "objectID": "m05-images/03-using-cnn-models.html#using-pre-trained-models",
    "href": "m05-images/03-using-cnn-models.html#using-pre-trained-models",
    "title": "Part 3: Using CNN Models",
    "section": "Using Pre-Trained Models",
    "text": "Using Pre-Trained Models\nNow that we understand CNN building blocks, let’s use them in practice. Training a CNN from scratch on ImageNet requires weeks of GPU time. But we can leverage pre-trained models trained by research labs with vast computational resources.\n\nLoading Models from torchvision\nPyTorch’s torchvision.models provides pre-trained implementations of major architectures:\n\nimport torch\nimport torchvision.models as models\nfrom torchvision import transforms\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\n# Load a pre-trained ResNet-50 model\nresnet50 = models.resnet50(weights='IMAGENET1K_V1')\nresnet50.eval()  # Set to evaluation mode\n\nprint(f\"Model type: {type(resnet50)}\")\nprint(f\"Number of parameters: {sum(p.numel() for p in resnet50.parameters()):,}\")\n\nModel type: &lt;class 'torchvision.models.resnet.ResNet'&gt;\nNumber of parameters: 25,557,032\n\n\nThe model is trained on ImageNet with 1000 classes. Let’s use it to classify an image.\n\n\nImage Classification Example\nTo use a pre-trained model, we must preprocess images the same way they were during training. ImageNet models expect images resized to 224×224 (or 299×299 for some models) with pixel values normalized to mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225].\n\n# Define the preprocessing transform pipeline\npreprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    ),\n])\n\n\n\nShow image display code\n# Display the original image\nplt.figure(figsize=(6, 6))\nplt.imshow(img)\nplt.title(\"Input Image (from CIFAR-10)\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n# Apply preprocessing and prepare for model input\ninput_tensor = preprocess(img)\ninput_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n\nprint(f\"Input shape: {input_batch.shape}\")  # [1, 3, 224, 224]\n\nInput shape: torch.Size([1, 3, 224, 224])\n\n\nNow classify the image:\n\n# Perform inference\nwith torch.no_grad():\n    output = resnet50(input_batch)\n\n# Output is logits for 1000 classes\nprint(f\"Output shape: {output.shape}\")  # [1, 1000]\n\n# Convert to probabilities and get top 5 predictions\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\n\n# Display predictions\nprint(\"\\nTop 5 predictions:\")\nfor i in range(5):\n    print(f\"{i+1}. {labels[top5_catid[i]]}: {top5_prob[i].item()*100:.2f}%\")\n\nOutput shape: torch.Size([1, 1000])\n\nTop 5 predictions:\n1. macaque: 25.89%\n2. frilled-necked lizard: 24.47%\n3. consomme: 12.40%\n4. patas monkey: 12.35%\n5. hot pot: 11.11%\n\n\nThe model correctly identifies the object with high confidence. This demonstrates the power of pre-trained networks: they’ve learned rich visual representations from ImageNet’s diverse images.\n\n\nWhen to Use Which Architecture\nDifferent architectures offer trade-offs between accuracy, speed, and memory:\nResNet-50: Excellent general-purpose model. Good accuracy, reasonable speed. Default choice for most applications.\nEfficientNet: Optimized for mobile and edge devices. Best accuracy-per-parameter ratio.\nVGG-16: Simple architecture, easy to understand. Larger and slower than modern alternatives.\nMobileNet: Designed for mobile deployment. Fast inference, lower accuracy.\nVision Transformer (ViT): State-of-the-art accuracy on large datasets. Requires more data and compute.\nFor most applications, start with ResNet-50. It provides strong performance across diverse tasks. Optimize for speed or accuracy later if needed.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 3: Using CNN Models"
    ]
  }
]
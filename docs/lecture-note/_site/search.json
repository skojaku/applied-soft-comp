[
  {
    "objectID": "toc.html",
    "href": "toc.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Home\nWelcome\nAbout\nWhy Applied Soft Computing?\nDiscord\nSetup\nMinidora Usage\nHow to Submit Assignment\nDeliverables\n\n\n\n\n\nOverview\nVersion Control with Git & GitHub\nThe Tidy Data Philosophy\nData Provenance\nReproducibility\n\n\n\n\n\nOverview\nPrinciples of Effective Visualization\nVisualizing 1D Data\nVisualizing 2D Data\nVisualizing High-Dimensional Data\nVisualizing Networks\nVisualizing Time-Series\n\n\n\n\n\nOverview\nHands-on\nPrompt Tuning\nAgentic AI\nContext Engineering\n\n\n\n\n\nOverview\nLarge Language Models\nGPT Inference: Sampling Strategies\nTokenization: Unboxing How LLMs Read Text\nTransformers\nBERT & GPT\nSentence Transformers\nWord Embeddings\nSemaxis\nWord Bias\n\n\n\n\n\nOverview\nImage Processing Fundamentals\nConvolutional Neural Networks\nLeNet Architecture\nAlexNet: Deep CNN Revolution\nVGG Networks\nInception & Multi-Scale Features\nBatch Normalization\nResNet & Skip Connections\n\n\n\n\n\nOverview\nSpectral Graph Embedding\nGraph Embeddings with Word2Vec\nSpectral vs.¬†Neural Embeddings\nFrom Images to Graphs\nGraph Convolutional Networks\nPopular GNN Architectures\nGNN Software & Tools"
  },
  {
    "objectID": "toc.html#course-information",
    "href": "toc.html#course-information",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Home\nWelcome\nAbout\nWhy Applied Soft Computing?\nDiscord\nSetup\nMinidora Usage\nHow to Submit Assignment\nDeliverables"
  },
  {
    "objectID": "toc.html#module-1-the-data-scientists-toolkit",
    "href": "toc.html#module-1-the-data-scientists-toolkit",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Overview\nVersion Control with Git & GitHub\nThe Tidy Data Philosophy\nData Provenance\nReproducibility"
  },
  {
    "objectID": "toc.html#module-2-visualizing-complexity",
    "href": "toc.html#module-2-visualizing-complexity",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Overview\nPrinciples of Effective Visualization\nVisualizing 1D Data\nVisualizing 2D Data\nVisualizing High-Dimensional Data\nVisualizing Networks\nVisualizing Time-Series"
  },
  {
    "objectID": "toc.html#module-3-agentic-coding",
    "href": "toc.html#module-3-agentic-coding",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Overview\nHands-on\nPrompt Tuning\nAgentic AI\nContext Engineering"
  },
  {
    "objectID": "toc.html#module-4-deep-learning-for-text",
    "href": "toc.html#module-4-deep-learning-for-text",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Overview\nLarge Language Models\nGPT Inference: Sampling Strategies\nTokenization: Unboxing How LLMs Read Text\nTransformers\nBERT & GPT\nSentence Transformers\nWord Embeddings\nSemaxis\nWord Bias"
  },
  {
    "objectID": "toc.html#module-5-deep-learning-for-images",
    "href": "toc.html#module-5-deep-learning-for-images",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Overview\nImage Processing Fundamentals\nConvolutional Neural Networks\nLeNet Architecture\nAlexNet: Deep CNN Revolution\nVGG Networks\nInception & Multi-Scale Features\nBatch Normalization\nResNet & Skip Connections"
  },
  {
    "objectID": "toc.html#module-6-deep-learning-for-graphs",
    "href": "toc.html#module-6-deep-learning-for-graphs",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Overview\nSpectral Graph Embedding\nGraph Embeddings with Word2Vec\nSpectral vs.¬†Neural Embeddings\nFrom Images to Graphs\nGraph Convolutional Networks\nPopular GNN Architectures\nGNN Software & Tools"
  },
  {
    "objectID": "m06-llms/summary.html",
    "href": "m06-llms/summary.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Summary\nWe began by exploring the fundamental architecture of transformers, learning about their key components including multi-head attention mechanisms, layer normalization, and position embeddings. We then studied BERT‚Äôs bidirectional encoder architecture, which revolutionized NLP by introducing masked language modeling and next sentence prediction for pre-training, demonstrating strong performance across various tasks.\nWe then examined GPT‚Äôs decoder-only architecture, which employs causal attention for powerful text generation, and explored various inference strategies including greedy search, beam search, and sophisticated sampling techniques like nucleus sampling. Finally, we studied T5, which unified various NLP tasks into a text-to-text framework, demonstrating the effectiveness of different pre-training objectives and the importance of dataset quality. Throughout our exploration, we saw how each model builds upon its predecessors, introducing innovations that advance the field of natural language processing, ultimately laying the groundwork for modern large language models."
  },
  {
    "objectID": "m06-llms/prompt-tuning.html",
    "href": "m06-llms/prompt-tuning.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Prompt Tuning\nThe notebook on prompt tuning is available at Prompt Tuning.",
    "crumbs": [
      "Home",
      "Module 6: Large Language Models & Emergent Behavior",
      "Prompt Engineering & In-Context Learning"
    ]
  },
  {
    "objectID": "m06-llms/from-language-model-to-instruction-following.html",
    "href": "m06-llms/from-language-model-to-instruction-following.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Autoregressive models such as GPT-2 generate text sequentially, predicting each new token based entirely on previous context. While GPT-2 can produce impressively coherent text, it fundamentally lacks the capability to interpret prompts as explicit instructions. Instead, it treats input prompts merely as initial conditions for text generation, limiting precise user control over the generated content.\nFor instance, prompting GPT-2 with the phrase:\n\n‚ÄúTranslate the following sentence into French: ‚ÄòHow are you?‚Äô‚Äù\n\nmight lead to GPT-2 continuing the prompt with a conversational response or explanation rather than providing a direct translation.\n\n\n\nRecognizing the inherent limitations of GPT-2 and purely autoregressive models, researchers moved toward frameworks that explicitly interpret prompts as structured instructions rather than mere context. Google‚Äôs Text-to-Text Transfer Transformer (T5) marked a pivotal step in this evolution.\nUnlike GPT-2, which generates text solely by predicting the next token based on preceding context, T5 was designed using a unified text-to-text approach. Every NLP task‚Äîwhether translation, summarization, sentiment analysis, or question-answering‚Äîwas transformed into text-to-text tasks. And the model is trained on multiple tasks specified in the prompt. This is crucial. Training on a single task does not require model to understand the instruction. Training on multiple tasks means that ‚Äúunderstanding a given instruction‚Äù is a part of the training objective.\n```hvhdbrdlogrp T5 paper: :class: tip\nThe paper on T5 is an important milestone in the development of language models. It succinctly summarizes the key ideas developed in the past. By comparing the effectiveness of different ideas, the authors rationalized the design choices of T5 they have made. I highly recommend to read the paper since it is very rewarding, and you can grasp the big picture of the development of language models.\n[1910.10683] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\nYou can also find my summary of the paper in the Appendix.\n\n\n## Instruction Fine-Tuning: The Emergence of FLAN-T5\n\n\n![](https://www.shaynelongpre.com/publication/scaling-flan-2022/featured.png)\n\nWhile meta-learning made a big step forward, challenge still remains. T5 is like an old version of Siri that can only follow the rigid task specifications in predefined formats (e.g., \"translate:\", \"summarize:\"), limiting user interaction flexibility. This is attributed to:\n- **Limited instruction diversity:** T5 was trained on a relatively narrow set of task formats and instruction patterns, making it struggle with novel phrasings or complex instructions.\n- **Poor generalization to unseen tasks:** The model couldn't effectively transfer its instruction-following capabilities to tasks outside its training distribution.\n- **Rigid interpretation:** T5 often interpreted instructions literally rather than understanding the underlying intent, leading to brittle performance when instructions varied slightly.\n- **Lack of nuance in responses:** Outputs tended to follow templated patterns rather than adapting to the specific nuances requested in instructions.\n\n\n**FLAN-T5 (Fine-tuned Language Net - T5)** represents a pivotal step towards more flexible and nuanced instruction following. The critical difference between T5 and FLAN-T5 is how they handle instructions: T5 understood basic task types but had limited flexibility in instruction formats, whereas FLAN-T5 was explicitly fine-tuned to comprehend and follow diverse, detailed natural language instructions.\n\nFor instance, instead of simply recognizing \"summarize:\" as a task identifier, FLAN-T5 can interpret and execute nuanced prompts like: \"Provide a two-sentence summary highlighting the main causes of climate change mentioned in this paragraph.\" This fine-tuning enables FLAN-T5 to understand instructions with significantly more depth and variation than its predecessor.\n\n## Reinforcement Learning with Human Feedback (RLHF)\n\nBuilding on instruction fine-tuning, Reinforcement Learning with Human Feedback (RLHF) emerged as a powerful technique designed to align model outputs more closely with human expectations and preferences. RLHF integrates human evaluations into a reinforcement learning framework, creating a feedback loop where model outputs are continually refined based on direct human judgment.\n\nIn RLHF, human evaluators review model-generated outputs, rating or ranking them according to accuracy, coherence, appropriateness, or other desired characteristics. These human-generated scores guide the model's reinforcement learning algorithm to adjust its responses iteratively, allowing it to increasingly reflect nuanced human values and preferences.\n\n**Concrete Example:**\n\nConsider a language model tasked with answering the question: ‚ÄúWhat are some healthy snacks?‚Äù\n\n- **Initial Output:**\n‚ÄúCandy bars and potato chips.‚Äù\n- **Human Feedback:** Low rating (unhealthy options)\n\n- **Subsequent Output (After RLHF):**\n‚ÄúFresh fruit, nuts, yogurt, or vegetables with hummus.‚Äù ``` - Human Feedback: High rating (appropriate and accurate)\nThe model gradually learns to produce outputs aligned with human preferences through repeated cycles of evaluation and feedback.\nHere is a good blog about RLHF: Reinforcement Learning from Human Feedback (RLHF) - a simplified explanation ¬∑ GitHub",
    "crumbs": [
      "Home",
      "Module 6: Large Language Models & Emergent Behavior",
      "From Language Models to Instruction Following"
    ]
  },
  {
    "objectID": "m06-llms/from-language-model-to-instruction-following.html#transition-to-meta-learning-googles-t5-model",
    "href": "m06-llms/from-language-model-to-instruction-following.html#transition-to-meta-learning-googles-t5-model",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Recognizing the inherent limitations of GPT-2 and purely autoregressive models, researchers moved toward frameworks that explicitly interpret prompts as structured instructions rather than mere context. Google‚Äôs Text-to-Text Transfer Transformer (T5) marked a pivotal step in this evolution.\nUnlike GPT-2, which generates text solely by predicting the next token based on preceding context, T5 was designed using a unified text-to-text approach. Every NLP task‚Äîwhether translation, summarization, sentiment analysis, or question-answering‚Äîwas transformed into text-to-text tasks. And the model is trained on multiple tasks specified in the prompt. This is crucial. Training on a single task does not require model to understand the instruction. Training on multiple tasks means that ‚Äúunderstanding a given instruction‚Äù is a part of the training objective.\n```hvhdbrdlogrp T5 paper: :class: tip\nThe paper on T5 is an important milestone in the development of language models. It succinctly summarizes the key ideas developed in the past. By comparing the effectiveness of different ideas, the authors rationalized the design choices of T5 they have made. I highly recommend to read the paper since it is very rewarding, and you can grasp the big picture of the development of language models.\n[1910.10683] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\nYou can also find my summary of the paper in the Appendix.\n\n\n## Instruction Fine-Tuning: The Emergence of FLAN-T5\n\n\n![](https://www.shaynelongpre.com/publication/scaling-flan-2022/featured.png)\n\nWhile meta-learning made a big step forward, challenge still remains. T5 is like an old version of Siri that can only follow the rigid task specifications in predefined formats (e.g., \"translate:\", \"summarize:\"), limiting user interaction flexibility. This is attributed to:\n- **Limited instruction diversity:** T5 was trained on a relatively narrow set of task formats and instruction patterns, making it struggle with novel phrasings or complex instructions.\n- **Poor generalization to unseen tasks:** The model couldn't effectively transfer its instruction-following capabilities to tasks outside its training distribution.\n- **Rigid interpretation:** T5 often interpreted instructions literally rather than understanding the underlying intent, leading to brittle performance when instructions varied slightly.\n- **Lack of nuance in responses:** Outputs tended to follow templated patterns rather than adapting to the specific nuances requested in instructions.\n\n\n**FLAN-T5 (Fine-tuned Language Net - T5)** represents a pivotal step towards more flexible and nuanced instruction following. The critical difference between T5 and FLAN-T5 is how they handle instructions: T5 understood basic task types but had limited flexibility in instruction formats, whereas FLAN-T5 was explicitly fine-tuned to comprehend and follow diverse, detailed natural language instructions.\n\nFor instance, instead of simply recognizing \"summarize:\" as a task identifier, FLAN-T5 can interpret and execute nuanced prompts like: \"Provide a two-sentence summary highlighting the main causes of climate change mentioned in this paragraph.\" This fine-tuning enables FLAN-T5 to understand instructions with significantly more depth and variation than its predecessor.\n\n## Reinforcement Learning with Human Feedback (RLHF)\n\nBuilding on instruction fine-tuning, Reinforcement Learning with Human Feedback (RLHF) emerged as a powerful technique designed to align model outputs more closely with human expectations and preferences. RLHF integrates human evaluations into a reinforcement learning framework, creating a feedback loop where model outputs are continually refined based on direct human judgment.\n\nIn RLHF, human evaluators review model-generated outputs, rating or ranking them according to accuracy, coherence, appropriateness, or other desired characteristics. These human-generated scores guide the model's reinforcement learning algorithm to adjust its responses iteratively, allowing it to increasingly reflect nuanced human values and preferences.\n\n**Concrete Example:**\n\nConsider a language model tasked with answering the question: ‚ÄúWhat are some healthy snacks?‚Äù\n\n- **Initial Output:**\n‚ÄúCandy bars and potato chips.‚Äù\n- **Human Feedback:** Low rating (unhealthy options)\n\n- **Subsequent Output (After RLHF):**\n‚ÄúFresh fruit, nuts, yogurt, or vegetables with hummus.‚Äù ``` - Human Feedback: High rating (appropriate and accurate)\nThe model gradually learns to produce outputs aligned with human preferences through repeated cycles of evaluation and feedback.\nHere is a good blog about RLHF: Reinforcement Learning from Human Feedback (RLHF) - a simplified explanation ¬∑ GitHub",
    "crumbs": [
      "Home",
      "Module 6: Large Language Models & Emergent Behavior",
      "From Language Models to Instruction Following"
    ]
  },
  {
    "objectID": "m05-images/pen-and-paper.html",
    "href": "m05-images/pen-and-paper.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Pen and paper exercises\n\n‚úçÔ∏è Pen and paper exercises"
  },
  {
    "objectID": "m05-images/archive/what-to-learn.html",
    "href": "m05-images/archive/what-to-learn.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "In this module, we will learn how to use neural networks to learn representations of images. We will learn: - Fourier transform on images - Image processing using Fourier transform - Convolutional neural networks - LeNet - AlexNet - VGG - Inception - ResNet - VisTransformer (if time permits)"
  },
  {
    "objectID": "m05-images/archive/what-to-learn.html#what-to-learn-in-this-module",
    "href": "m05-images/archive/what-to-learn.html#what-to-learn-in-this-module",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "In this module, we will learn how to use neural networks to learn representations of images. We will learn: - Fourier transform on images - Image processing using Fourier transform - Convolutional neural networks - LeNet - AlexNet - VGG - Inception - ResNet - VisTransformer (if time permits)"
  },
  {
    "objectID": "m05-images/archive/batch-normalization.html",
    "href": "m05-images/archive/batch-normalization.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Batch Normalization (BN) is a technique used in deep neural networks to stabilize and accelerate training by normalizing the inputs to layers within the network.\n\n\nNormalize the activations coming out of a layer for each feature (channel) independently within the current mini-batch so they have zero mean and unit variance.\n\n\n\nFor a mini-batch B = \\{x_1, ..., x_m\\} and considering a single activation feature:\n\nCalculate Mini-Batch Mean: \\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i\nCalculate Mini-Batch Variance: \\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2\nNormalize: Using the mini-batch mean and variance (adding a small \\epsilon for numerical stability): \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\nScale and Shift: Introduce two learnable parameters, \\gamma (gamma) for scaling and \\beta (beta) for shifting: y_i = \\gamma \\hat{x}_i + \\beta\n\n\nThe parameters \\gamma and \\beta are learned during backpropagation alongside the network‚Äôs weights.\nThis process is applied independently to each feature/channel dimension.\n\n\n\n\nIf we just normalized to zero mean and unit variance, the network‚Äôs ability to represent information might be limited. Forcing activations into this specific distribution might not always be optimal for the subsequent layers or activation functions.\n\n\\gamma and \\beta give the network flexibility.\nThey allow the network to learn the optimal scale and shift for the normalized activations.\nIf needed, the network can even learn parameters (\\gamma = \\sqrt{\\sigma_B^2 + \\epsilon}, \\beta = \\mu_B) that effectively undo the normalization, restoring the original activation distribution if that proves beneficial.\n\n\n\n\nDuring inference (when making predictions), we often process samples one by one, so calculating mini-batch statistics isn‚Äôt feasible or representative.\n\nInstead, BN layers maintain running averages of the mean (\\mu_{pop}) and variance (\\sigma^2_{pop}) encountered across all mini-batches during training.\n\nThese are typically updated using a momentum term:\n\n\\mu_{pop} = \\alpha \\times \\mu_{pop} + (1 - \\alpha) \\times \\mu_B\n\\sigma_{pop}^2 = \\alpha \\times \\sigma_{pop}^2 + (1 - \\alpha) \\times \\sigma_B^2\nwhere \\alpha is a momentum parameter.\n\n\nAt inference time, these fixed population statistics are used for normalization: $ = $\nThe learned \\gamma and \\beta parameters from training are still applied:  y = \\gamma \\hat{x} + \\beta \n\n\n\n\n\nIt‚Äôs common practice to place the Batch Norm layer after the Convolutional or Fully Connected layer and before the non-linear activation function (like ReLU).\n\nConv / FC -&gt; Batch Norm -&gt; Activation (ReLU)\n\nHowever, variations in placement exist in different architectures.\n\n(Note: While originally thought to primarily combat ‚Äúinternal covariate shift‚Äù, recent research suggests BN‚Äôs effectiveness might be more related to smoothing the optimization landscape.)"
  },
  {
    "objectID": "m05-images/archive/batch-normalization.html#the-core-idea",
    "href": "m05-images/archive/batch-normalization.html#the-core-idea",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Normalize the activations coming out of a layer for each feature (channel) independently within the current mini-batch so they have zero mean and unit variance."
  },
  {
    "objectID": "m05-images/archive/batch-normalization.html#how-it-works-during-training",
    "href": "m05-images/archive/batch-normalization.html#how-it-works-during-training",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "For a mini-batch B = \\{x_1, ..., x_m\\} and considering a single activation feature:\n\nCalculate Mini-Batch Mean: \\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i\nCalculate Mini-Batch Variance: \\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2\nNormalize: Using the mini-batch mean and variance (adding a small \\epsilon for numerical stability): \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\nScale and Shift: Introduce two learnable parameters, \\gamma (gamma) for scaling and \\beta (beta) for shifting: y_i = \\gamma \\hat{x}_i + \\beta\n\n\nThe parameters \\gamma and \\beta are learned during backpropagation alongside the network‚Äôs weights.\nThis process is applied independently to each feature/channel dimension."
  },
  {
    "objectID": "m05-images/archive/batch-normalization.html#why-scale-and-shift-gamma-and-beta",
    "href": "m05-images/archive/batch-normalization.html#why-scale-and-shift-gamma-and-beta",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "If we just normalized to zero mean and unit variance, the network‚Äôs ability to represent information might be limited. Forcing activations into this specific distribution might not always be optimal for the subsequent layers or activation functions.\n\n\\gamma and \\beta give the network flexibility.\nThey allow the network to learn the optimal scale and shift for the normalized activations.\nIf needed, the network can even learn parameters (\\gamma = \\sqrt{\\sigma_B^2 + \\epsilon}, \\beta = \\mu_B) that effectively undo the normalization, restoring the original activation distribution if that proves beneficial."
  },
  {
    "objectID": "m05-images/archive/batch-normalization.html#batch-normalization-during-inference",
    "href": "m05-images/archive/batch-normalization.html#batch-normalization-during-inference",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "During inference (when making predictions), we often process samples one by one, so calculating mini-batch statistics isn‚Äôt feasible or representative.\n\nInstead, BN layers maintain running averages of the mean (\\mu_{pop}) and variance (\\sigma^2_{pop}) encountered across all mini-batches during training.\n\nThese are typically updated using a momentum term:\n\n\\mu_{pop} = \\alpha \\times \\mu_{pop} + (1 - \\alpha) \\times \\mu_B\n\\sigma_{pop}^2 = \\alpha \\times \\sigma_{pop}^2 + (1 - \\alpha) \\times \\sigma_B^2\nwhere \\alpha is a momentum parameter.\n\n\nAt inference time, these fixed population statistics are used for normalization: $ = $\nThe learned \\gamma and \\beta parameters from training are still applied:  y = \\gamma \\hat{x} + \\beta"
  },
  {
    "objectID": "m05-images/archive/batch-normalization.html#placement",
    "href": "m05-images/archive/batch-normalization.html#placement",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "It‚Äôs common practice to place the Batch Norm layer after the Convolutional or Fully Connected layer and before the non-linear activation function (like ReLU).\n\nConv / FC -&gt; Batch Norm -&gt; Activation (ReLU)\n\nHowever, variations in placement exist in different architectures.\n\n(Note: While originally thought to primarily combat ‚Äúinternal covariate shift‚Äù, recent research suggests BN‚Äôs effectiveness might be more related to smoothing the optimization landscape.)"
  },
  {
    "objectID": "m04-text/what-to-learn.html",
    "href": "m04-text/what-to-learn.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "In this module, we will learn about recurrent neural networks (RNNs) that can process sequential text data. We will learn: - Recurrent Neural Networks (RNNs) for processing sequential text data - Long-Short Term Memory (LSTM) networks for capturing long-range dependencies - Sequence-to-sequence models for machine translation - The Attention mechanism for focusing on relevant parts of text"
  },
  {
    "objectID": "m04-text/what-to-learn.html#what-to-learn-in-this-module",
    "href": "m04-text/what-to-learn.html#what-to-learn-in-this-module",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "In this module, we will learn about recurrent neural networks (RNNs) that can process sequential text data. We will learn: - Recurrent Neural Networks (RNNs) for processing sequential text data - Long-Short Term Memory (LSTM) networks for capturing long-range dependencies - Sequence-to-sequence models for machine translation - The Attention mechanism for focusing on relevant parts of text"
  },
  {
    "objectID": "m04-text/archive/summary.html",
    "href": "m04-text/archive/summary.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Summary\nWe began our exploration of sequential text processing with Recurrent Neural Networks (RNNs), the fundamental building blocks that handle sequences through a hidden state acting as working memory. When we encountered RNNs‚Äô limitations with long-term dependencies due to vanishing gradients, we studied Long Short-Term Memory (LSTM) networks, which introduced controlled memory cells with forget, input, and output gates to maintain information over longer sequences. We then examined Embeddings from Language Models (ELMo), which combines character-level CNNs with bidirectional LSTMs to generate context-aware word representations.\nWe continued with Sequence-to-Sequence (Seq2Seq) models, consisting of encoder and decoder components that transform input sequences into output sequences. We discovered a key innovation in Seq2Seq models, the attention mechanism, which enables the model to focus on relevant parts of the input sequence during decoding, rather than relying on a fixed-size context vector. The attention mechanism laid crucial groundwork for the transformer architecture, which we will explore in the next section."
  },
  {
    "objectID": "m04-text/archive/pen-and-paper.html",
    "href": "m04-text/archive/pen-and-paper.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Pen and Paper Exercise\npen-and-paper-exercise ‚úçÔ∏è"
  },
  {
    "objectID": "m04-text/archive/elmo.html",
    "href": "m04-text/archive/elmo.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Embedding from Language Models (ELMo)\nELMo is an embedding model that uses a deep, bidirectional LSTM architecture to generate word representations.\n```pzuwfbbg https://miro.medium.com/v2/resize:fit:1400/0*AfZigCsjl2nfggbl.png :alt: ELMo architecture :width: 100% :align: center\nELMo architecture\n\n## Overview\n\nELMo consists of two main components, i.e., a character-level CNN and a bidirectional LSTM.\n\n### Character-level CNN\n\nCharacter-level CNN is a type of neural network architecture that processes text at the character level rather than the word level. This approach is particularly useful for handling out-of-vocabulary words, as it can process any sequence of characters, regardless of whether they form a valid word in the training set.\n\nIt is easy to understand it by considering an example of embedding a word \"playing\". The word \"playing\" is generated from characters \"p\", \"l\", \"a\", \"y\", \"i\", \"n\", \"g\". Each character is mapped to a learned embedding vector.\nThe character embeddings are then convolved (i.e., weighted sum) with weights learned from data. The convolved output is then passed through a max-pooling layer that extracts the maximum value across the word length. This max-pooling operation creates a fixed-size word-level representation.\n\n```{figure} ../figs/character-level-cnn.jpg\n:alt: ELMo architecture\n:width: 100%\n:align: center\n\nCharacter-level CNN. Word \"playing\" is generated from characters \"p\", \"l\", \"a\", \"y\", \"i\", \"n\", \"g\". Each character is mapped to a learned embedding vector, which is then convolved and max-pooled to create a fixed-size word-level representation.\n\nBidirectional LSTM\nBidirectional LSTM is a type of recurrent neural network that processes text in both forward and backward directions. Given a sequence of words, the forward LSTM processes the words from the first to the last, while the backward LSTM processes the words from the last to the first. The two LSTM outputs are then concatenated to form a single vector representation for each word.\n```pzuwfbbg https://miro.medium.com/v2/resize:fit:1400/0*AfZigCsjl2nfggbl.png :alt: ELMo architecture :width: 100% :align: center\nELMo architecture ```"
  },
  {
    "objectID": "course/welcome.html",
    "href": "course/welcome.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome to the course! In this class, we will explore how deep learning serves as a powerful tool to operationalize high-dimensional data from social and physical systems. You‚Äôll learn to model complex system dynamics using modern computational intelligence, and in turn, use perspectives from complexity science to understand the emergent behaviors of large-scale models.\nThis course is designed to provide you with both a theoretical foundation and hands-on experience in applied soft computing. You will learn how to apply representation learning, sequence modeling, and graph analytics to model real-world complex systems using Python and modern deep learning frameworks.\n\n\nThis course is divided into three chapters: Foundation, Deep Learning and Advanced Topics.\nFoundation chapter covers the foundational concepts of data visualization, data science, and reproducibility. This will prepare you for building your own data science projects with modern deep learning tools.\nThe Deep Learning chapter covers the fundamental concepts of deep learning for text, images, and graphs. Through hands-on coding, you will learn how to build your own deep learning models for different data types.\nThe Advanced Topics chapter elevates you from a user to a creator of advanced soft computing models. You will learn how to build your own large language models and self-supervised learning models.\n\n\n\n\nEngaging Lectures: Each week, we‚Äôll dive into key concepts in deep learning and complex systems, supported by interactive discussions and in-class activities.\nHands-on Coding: You‚Äôll work with real data from text, images, and networks using Python and modern deep learning tools.\nCollaborative Learning: Participate in group activities, discussions, and our dedicated Discord server for Q&A and support.\nProjects and Assignments: Apply your knowledge through coding assignments, quizzes, and a final project.\nSupport: Get help from the instructor, TA, and our AI tutor.\n\n\n\n\n\nWhy applied soft computing? Read the Overview page to understand the importance of applied soft computing.\nRead the About Us page to meet your instructor, TA, and AI tutor.\nJoin the Discord server for announcements, help, and community.\nSet up your Python environment by following the Setup Guide.\nLearn how to submit assignments using GitHub Classroom.",
    "crumbs": [
      "Home",
      "Course Information",
      "Welcome"
    ]
  },
  {
    "objectID": "course/welcome.html#welcome-to-applied-soft-computing",
    "href": "course/welcome.html#welcome-to-applied-soft-computing",
    "title": "Welcome",
    "section": "",
    "text": "Welcome to the course! In this class, we will explore how deep learning serves as a powerful tool to operationalize high-dimensional data from social and physical systems. You‚Äôll learn to model complex system dynamics using modern computational intelligence, and in turn, use perspectives from complexity science to understand the emergent behaviors of large-scale models.\nThis course is designed to provide you with both a theoretical foundation and hands-on experience in applied soft computing. You will learn how to apply representation learning, sequence modeling, and graph analytics to model real-world complex systems using Python and modern deep learning frameworks.\n\n\nThis course is divided into three chapters: Foundation, Deep Learning and Advanced Topics.\nFoundation chapter covers the foundational concepts of data visualization, data science, and reproducibility. This will prepare you for building your own data science projects with modern deep learning tools.\nThe Deep Learning chapter covers the fundamental concepts of deep learning for text, images, and graphs. Through hands-on coding, you will learn how to build your own deep learning models for different data types.\nThe Advanced Topics chapter elevates you from a user to a creator of advanced soft computing models. You will learn how to build your own large language models and self-supervised learning models.\n\n\n\n\nEngaging Lectures: Each week, we‚Äôll dive into key concepts in deep learning and complex systems, supported by interactive discussions and in-class activities.\nHands-on Coding: You‚Äôll work with real data from text, images, and networks using Python and modern deep learning tools.\nCollaborative Learning: Participate in group activities, discussions, and our dedicated Discord server for Q&A and support.\nProjects and Assignments: Apply your knowledge through coding assignments, quizzes, and a final project.\nSupport: Get help from the instructor, TA, and our AI tutor.\n\n\n\n\n\nWhy applied soft computing? Read the Overview page to understand the importance of applied soft computing.\nRead the About Us page to meet your instructor, TA, and AI tutor.\nJoin the Discord server for announcements, help, and community.\nSet up your Python environment by following the Setup Guide.\nLearn how to submit assignments using GitHub Classroom.",
    "crumbs": [
      "Home",
      "Course Information",
      "Welcome"
    ]
  },
  {
    "objectID": "course/discord.html",
    "href": "course/discord.html",
    "title": "Discord",
    "section": "",
    "text": "We use a dedicated Discord server for this course to facilitate communication, Q&A, and collaboration outside of class. The Discord server is a space where you can:\n\nAsk questions about lectures, assignments, and projects\nDiscuss concepts and share resources with your peers\nGet support from the instructor, TA, and Minidora (the AI tutor)\nJoin study groups and participate in informal discussions\n\nInvitation links to the Discord server will be distributed via Brightspace. Please check the Brightspace announcements or course materials for the latest invite link.\nOnce you join, you‚Äôll find channels for different topics (e.g., #random, #questions, #study-groups) and can interact with both classmates, AI tutor, and course staff. If you‚Äôre new to Discord, it‚Äôs a free platform available on web, desktop, and mobile.\n\n\n\nScreenshot of the course Discord server\n\n\nExample screenshot of the course Discord server interface.\nIf you have any trouble joining, please contact the instructor for assistance.",
    "crumbs": [
      "Home",
      "Course Information",
      "Discord"
    ]
  },
  {
    "objectID": "course/about.html",
    "href": "course/about.html",
    "title": "About Us",
    "section": "",
    "text": "Complex systems are everywhere‚Äîfrom social networks shaping our daily interactions to neural networks powering AI systems. This course explores computational intelligence from the ground up, combining hands-on deep learning with complexity science to unlock the secrets of emergent behaviors in large-scale systems.",
    "crumbs": [
      "Home",
      "Course Information",
      "About Us"
    ]
  },
  {
    "objectID": "course/about.html#about-us",
    "href": "course/about.html#about-us",
    "title": "About Us",
    "section": "About Us",
    "text": "About Us\n\nInstructor\n\n\n\nWelcome! My name is Sadamori Kojaku, the instructor of this course. I started my career as a computer scientist but couldn‚Äôt resist falling in love with Complex Systems and Network Science right after I got my Ph.D. Complex systems appear in many different forms in our daily lives‚Äîsocial networks, biological systems, the internet, power grids, and you name it. But when we represent them using modern deep learning techniques, we can understand them in unified ways. We can study them using the same toolkit no matter what the domain is, and can find universal patterns and principles that govern seemingly different systems. Even more fascinating, large-scale deep learning models themselves exhibit emergent behaviors as complex systems! Sounds fun, right üòâ?\nThis course will guide you through the fascinating intersection of deep learning and complex systems, from foundational theory to hands-on coding and real-world applications. I hope you will enjoy and find the course useful in your future endeavors.\n\n\n\n\n\n\n\n\nTA\nTeaching Assistant is not yet assigned.\n\n\nAI Tutor (Minidora)\n\n\n\nMinidora is an AI tutor robot conceived in the 22nd century and deployed in the present era to support students. (The original character is designed by Fujiko Fujio for a famous Japanese manga called Doraemon). Minidora supports students in this course by providing dialogic explanations, quiz questions, and coding guidance on the course Discord.",
    "crumbs": [
      "Home",
      "Course Information",
      "About Us"
    ]
  },
  {
    "objectID": "m06-llms/transformers.html",
    "href": "m06-llms/transformers.html",
    "title": "Transformers",
    "section": "",
    "text": "::::{grid} 1 :class-container: spoiler-block\n:::{grid-item-card} Spoiler Transformers don‚Äôt process sequences; they process relationships between every position simultaneously. :::\n::::\n\n\nYou‚Äôve been taught to think of language models as sequential processors‚Äîreading left to right, one word triggering the next, like dominoes falling. This intuition comes from recurrent neural networks (RNNs), where information flows step by step, each word depending on the hidden state from the previous word. The transformer architecture throws this away entirely.\nInstead of sequential processing, transformers operate through parallel relationship mapping. When you read ‚ÄúThe cat sat on the mat because it was tired,‚Äù you don‚Äôt actually process word-by-word in isolation. Your brain simultaneously evaluates which words relate to which‚Äî‚Äúit‚Äù connects to ‚Äúcat,‚Äù ‚Äútired‚Äù explains ‚Äúsat,‚Äù ‚Äúmat‚Äù anchors ‚Äúon.‚Äù Transformers formalize this intuition mathematically. Every position in the input sequence simultaneously computes its relationship to every other position. The mechanism is attention, and the result is a system where context flows in all directions at once, not just forward through time.\nThis parallelism is why transformers scaled when RNNs didn‚Äôt. Recurrent architectures impose sequential computation‚Äîyou can‚Äôt process word 100 until you‚Äôve processed word 99. Transformers eliminate this bottleneck. Every position can be computed in parallel, which means training time scales with sequence complexity, not sequence length. This architectural shift is what enabled GPT-3, GPT-4, and Claude to exist.\n\n\n\nModern LLMs stack multiple transformer blocks‚Äîmodular units that take a sequence of token vectors as input and output a transformed sequence of the same length. GPT-3 uses 96 of these blocks; GPT-4 likely uses more. Each block refines the representation, adding layers of contextual understanding.\n```ligmtoun ../figs/transformer-overview.jpg :name: transformer-overview :alt: Transformer Overview :width: 50% :align: center\nThe basic architecture of the transformer-based LLMs.\n\nThese blocks come in two forms: **encoders** and **decoders**. The encoder processes the input sequence and builds a contextualized representation. The decoder generates the output sequence, attending to both its own previous outputs and the encoder's representation. For translation tasks (\"I love you\" ‚Üí \"Je t'aime\"), the encoder processes English, the decoder generates French. For language modeling (GPT-style systems), only the decoder is used‚Äîit generates text autoregressively, predicting the next token based on all previous tokens.\n\n```{figure} ../figs/transformer-encoder-decoder.jpg\n:name: transformer-encoder-decoder\n:alt: Transformer Encoder-Decoder\n:width: 80%\n:align: center\n\nThe encoder-decoder architecture. The encoder builds a representation of the input sequence; the decoder generates the output sequence while attending to the encoder's output.\nInside each block are three core components: multi-head attention (the relationship mapper), layer normalization (numerical stabilization), and feed-forward networks (nonlinear transformation). We‚Äôll build these components step by step.\n```ligmtoun ../figs/transformer-component.jpg :name: transformer-wired-components :alt: Transformer Wired Components :width: 80% :align: center\nInternal structure of encoder and decoder blocks.\n\n## Attention: The Relationship Engine\n\n**Self-attention**‚Äîthe core of the transformer‚Äîcomputes how much each position in a sequence should \"attend to\" every other position. Unlike earlier attention mechanisms in seq2seq models, which attended from one sentence to another, self-attention operates within a single sequence. It answers the question: \"Given this word, which other words matter most?\"\n\n```{figure} ../figs/transformer-attention.jpg\n:name: transformer-attention\n:alt: Attention Mechanism\n:width: 80%\n:align: center\n\nThe attention mechanism computes relationships between all positions simultaneously.\nFor each word, the attention mechanism creates three vectors: query (Q), key (K), and value (V). Think of these as a library search: the query is what you‚Äôre looking for, the keys are book titles, and the values are the actual content. When you search for ‚Äúmachine learning‚Äù (your query), you match it against book titles (keys) to find relevant content (values).\nMathematically, each of these vectors is created by a learned linear transformation of the input word embedding. Given an input embedding x, we compute:\n\nQ = x W_Q, \\quad K = x W_K, \\quad V = x W_V\n\nwhere W_Q, W_K, and W_V are learned weight matrices. The attention mechanism then computes which keys are most relevant to each query using the dot product, which measures vector similarity. The dot product QK^T produces a matrix of attention scores‚Äîlarge values indicate strong relationships, small values indicate weak ones.\nThese raw scores are scaled by \\sqrt{d_k} (the square root of the key dimension) to prevent extreme values, then normalized using softmax to produce a probability distribution. Finally, these normalized attention weights are used to compute a weighted sum of the value vectors. The complete operation is:\n\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\nwhere Q \\in \\mathbb{R}^{n \\times d_k}, K \\in \\mathbb{R}^{n \\times d_k}, and V \\in \\mathbb{R}^{n \\times d_v} represent matrices containing n query, key, and value vectors respectively.\nThe interactive visualization below demonstrates how learned Query and Key transformations produce different attention patterns. Adjust the transformation parameters to see how different W_Q and W_K matrices change which words attend to which:\n\n\npython {marimo} import marimo as mo import numpy as np import pandas as pd import altair as alt\n```python {marimo} attention_words = [‚Äúbank‚Äù, ‚Äúmoney‚Äù, ‚Äúloan‚Äù, ‚Äúriver‚Äù, ‚Äúshore‚Äù] attention_embeddings = np.array([ [0.0, 0.0], # bank (center) [-0.8, -0.3], # money [-0.7, -0.6], # loan [0.7, -0.5], # river [0.6, -0.7], # shore]) * 2\n\n\nq_scale_x = mo.ui.slider(-2, 2, 0.1, value=1.0, label=‚ÄúQ Scale X‚Äù) q_scale_y = mo.ui.slider(-2, 2, 0.1, value=1.0, label=‚ÄúQ Scale Y‚Äù) q_rotate = mo.ui.slider(-180, 180, 5, value=0, label=‚ÄúQ Rotate (deg)‚Äù)\n\n\n\nk_scale_x = mo.ui.slider(-2, 2, 0.1, value=1.0, label=‚ÄúK Scale X‚Äù) k_scale_y = mo.ui.slider(-2, 2, 0.1, value=1.0, label=‚ÄúK Scale Y‚Äù) k_rotate = mo.ui.slider(-180, 180, 5, value=0, label=‚ÄúK Rotate (deg)‚Äù)\nq_controls = mo.vstack([mo.md(‚ÄúQuery Transformation‚Äù), q_scale_x, q_scale_y, q_rotate]) k_controls = mo.vstack([mo.md(‚ÄúKey Transformation‚Äù), k_scale_x, k_scale_y, k_rotate])\n\n```python {marimo}\ndef _transform_embeddings(emb, scale_x, scale_y, rotate_deg):\n    theta = np.radians(rotate_deg)\n    rot_matrix = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n    scale_matrix = np.diag([scale_x, scale_y])\n    W = rot_matrix @ scale_matrix\n    return emb @ W.T\n\nQ = _transform_embeddings(attention_embeddings, q_scale_x.value, q_scale_y.value, q_rotate.value)\nK = _transform_embeddings(attention_embeddings, k_scale_x.value, k_scale_y.value, k_rotate.value)\n\n# Compute attention scores\n_scores = Q @ K.T\n_exp_scores = np.exp(_scores - np.max(_scores, axis=1, keepdims=True))\nattention_weights = _exp_scores / np.sum(_exp_scores, axis=1, keepdims=True)\n\n# Create visualizations\n_df_q = pd.DataFrame({\"word\": attention_words, \"x\": Q[:, 0], \"y\": Q[:, 1]})\n_df_k = pd.DataFrame({\"word\": attention_words, \"x\": K[:, 0], \"y\": K[:, 1]})\n\n_chart_q = alt.Chart(_df_q).mark_circle(size=100).encode(\n    x=alt.X('x:Q', scale=alt.Scale(domain=[-4, 4]), title='Q1'),\n    y=alt.Y('y:Q', scale=alt.Scale(domain=[-4, 4]), title='Q2'),\n    tooltip=['word:N']\n).properties(width=200, height=200, title=\"Query (Q)\")\n_text_q = _chart_q.mark_text(dy=-12, fontSize=10, fontWeight='bold').encode(text='word:N')\n\n_chart_k = alt.Chart(_df_k).mark_circle(size=100).encode(\n    x=alt.X('x:Q', scale=alt.Scale(domain=[-4, 4]), title='K1'),\n    y=alt.Y('y:Q', scale=alt.Scale(domain=[-4, 4]), title='K2'),\n    tooltip=['word:N']\n).properties(width=200, height=200, title=\"Key (K)\")\n_text_k = _chart_k.mark_text(dy=-12, fontSize=10, fontWeight='bold').encode(text='word:N')\n\n# Heatmap\n_heatmap_data = []\nfor i, word_i in enumerate(attention_words):\n    for j, word_j in enumerate(attention_words):\n        _heatmap_data.append({\"Query\": word_i, \"Key\": word_j, \"Weight\": attention_weights[i, j]})\n_df_heatmap = pd.DataFrame(_heatmap_data)\n\n_heatmap = alt.Chart(_df_heatmap).mark_rect().encode(\n    x=alt.X('Key:N', title='Key Word'),\n    y=alt.Y('Query:N', title='Query Word'),\n    color=alt.Color('Weight:Q', scale=alt.Scale(scheme='blues'), title='Attention'),\n    tooltip=['Query:N', 'Key:N', alt.Tooltip('Weight:Q', format='.3f')]\n).properties(width=250, height=250, title=\"Attention Weights (Softmax)\")\n\nmo.vstack([\n    mo.hstack([q_controls, k_controls], align=\"center\"),\n    mo.hstack([_chart_q + _text_q, _chart_k + _text_k, _heatmap], align=\"center\")\n])\n\n\n\nThe output is a contextualized vector for each word‚Äîa representation that changes based on surrounding context. The word ‚Äúbank‚Äù produces different vectors in ‚Äúriver bank‚Äù versus ‚Äúfinancial bank‚Äù because the attention mechanism incorporates information from neighboring words.\nTo see this in action, consider how we might contextualize the word ‚Äúbank‚Äù by mixing it with surrounding words. The visualization below shows static word embeddings‚Äînotice how ‚Äúbank‚Äù sits neutrally between financial terms (money, loan) and geographical terms (river, shore).\n\n\n```python {marimo} static_words = [‚Äúbank‚Äù, ‚Äúmoney‚Äù, ‚Äúloan‚Äù, ‚Äúriver‚Äù, ‚Äúshore‚Äù] static_embeddings = np.array([ [0.0, 0.0], # bank (center) [-0.8, -0.3], # money [-0.7, -0.6], # loan [0.7, -0.5], # river [0.6, -0.7], # shore]) * 2\n_df_static = pd.DataFrame({‚Äúword‚Äù: static_words, ‚Äúx‚Äù: static_embeddings[:, 0], ‚Äúy‚Äù: static_embeddings[:, 1]})\n_chart_static = alt.Chart(_df_static).mark_circle(size=200).encode( x=alt.X(‚Äòx:Q‚Äô, scale=alt.Scale(domain=[-2, 2]), title=‚ÄòDimension 1‚Äô), y=alt.Y(‚Äòy:Q‚Äô, scale=alt.Scale(domain=[-2, 2]), title=‚ÄòDimension 2‚Äô), text=‚Äòword:N‚Äô, tooltip=[‚Äòword:N‚Äô, ‚Äòx:Q‚Äô, ‚Äòy:Q‚Äô] ).properties(width=300, height=300, title=‚ÄúStatic Word Embeddings‚Äù)\n_text_static = _chart_static.mark_text(dy=-15, fontSize=14, fontWeight=‚Äòbold‚Äô).encode(text=‚Äòword:N‚Äô)\n_chart_static + _text_static\n\n&lt;/marimo-iframe&gt;\n&lt;/div&gt;\n\nNow, try adjusting the weights below to create a contextualized version of \"bank.\" If the sentence is \"Money in bank,\" adjust the weights to shift \"bank\" toward \"money.\" If the sentence is \"River bank,\" shift it toward \"river.\"\n\n&lt;div&gt;\n&lt;marimo-iframe data-height=\"500px\" data-show-code=\"false\"&gt;\n\n```python {marimo}\ncontext_words = [\"bank\", \"money\", \"loan\", \"river\", \"shore\"]\ncontext_embeddings = np.array([\n    [0.0, 0.0],  # bank (center)\n    [-0.8, -0.3],  # money\n    [-0.7, -0.6],  # loan\n    [0.7, -0.5],  # river\n    [0.6, -0.7],  # shore\n]) * 2\n\nslider_bank = mo.ui.slider(0, 1, 0.01, value=1.0, label=\"Bank Weight\")\nslider_money = mo.ui.slider(0, 1, 0.01, value=0, label=\"Money Weight\")\nslider_loan = mo.ui.slider(0, 1, 0.01, value=0, label=\"Loan Weight\")\nslider_river = mo.ui.slider(0, 1, 0.01, value=0, label=\"River Weight\")\nslider_shore = mo.ui.slider(0, 1, 0.01, value=0, label=\"Shore Weight\")\n\ncontext_sliders = mo.vstack([slider_bank, slider_money, slider_loan, slider_river, slider_shore])\n```python {marimo} _weights = np.array([slider_bank.value, slider_money.value, slider_loan.value, slider_river.value, slider_shore.value]) _total = _weights.sum() if _total &gt; 0: _weights = _weights / _total _new_vec = context_embeddings.T @ _weights else: _new_vec = np.zeros(2)\n_df_orig = pd.DataFrame({‚Äúword‚Äù: context_words, ‚Äúx‚Äù: context_embeddings[:, 0], ‚Äúy‚Äù: context_embeddings[:, 1], ‚Äútype‚Äù: [‚ÄúOriginal‚Äù] * 5}) _df_new = pd.DataFrame({‚Äúword‚Äù: [‚ÄúContextualized Bank‚Äù], ‚Äúx‚Äù: [_new_vec[0]], ‚Äúy‚Äù: [_new_vec[1]], ‚Äútype‚Äù: [‚ÄúContextualized‚Äù]}) _df_combined = pd.concat([_df_orig, _df_new])\n_chart_context = alt.Chart(_df_combined).mark_circle(size=200).encode( x=alt.X(‚Äòx:Q‚Äô, scale=alt.Scale(domain=[-2, 2]), title=‚ÄòDimension 1‚Äô), y=alt.Y(‚Äòy:Q‚Äô, scale=alt.Scale(domain=[-2, 2]), title=‚ÄòDimension 2‚Äô), color=alt.Color(‚Äòtype:N‚Äô, scale=alt.Scale(domain=[‚ÄòOriginal‚Äô, ‚ÄòContextualized‚Äô], range=[‚Äò#dadada‚Äô, ‚Äò#ff7f0e‚Äô])), tooltip=[‚Äòword:N‚Äô, ‚Äòx:Q‚Äô, ‚Äòy:Q‚Äô] ).properties(width=350, height=350, title=‚ÄúContextualized Bank‚Äù)\n_text_context = _chart_context.mark_text(dy=-15, fontSize=14, fontWeight=‚Äòbold‚Äô).encode(text=‚Äòword:N‚Äô, color=alt.value(‚Äòblack‚Äô))\nmo.hstack([context_sliders, _chart_context + _text_context], align=‚Äúcenter‚Äù)\n\n&lt;/marimo-iframe&gt;\n&lt;/div&gt;\n\nThis manual weighting captures the intuition, but how do we learn which words to attend to? This is where queries and keys come in.\n\n### Multi-Head Attention: Multiple Perspectives\n\nA single attention mechanism captures one type of relationship. **Multi-head attention** runs multiple attention operations in parallel, each with different learned parameters. Each head can specialize‚Äîone might focus on syntactic dependencies (subject-verb relationships), another on semantic similarity (synonyms and antonyms), another on positional proximity (nearby words).\n\n```{figure} ../figs/transformer-multihead-attention.jpg\n:name: transformer-multihead-attention\n:alt: Multi-Head Attention\n:width: 50%\n:align: center\n\nMulti-head attention runs multiple attention operations in parallel, each capturing different relationships.\nThe outputs from all heads are concatenated and passed through a final linear transformation to produce the multi-head attention output. In the original transformer paper {footcite:p}vaswani2017attention, the authors used h=8 attention heads, with each head using dimension d_k = d_v = d/h = 64, where d=512 is the model dimension.\n\n\nDeep networks suffer from numerical instability‚Äîactivations can grow explosively large or vanish to zero as they propagate through layers. Layer normalization stabilizes training by rescaling activations to have zero mean and unit variance.\n```ligmtoun https://miro.medium.com/v2/resize:fit:1400/0*Agdt1zYwfUxXMJGJ :name: transformer-layer-normalization :alt: Layer Normalization :width: 80% :align: center\nLayer normalization computes mean and standard deviation across all features for each sample, then normalizes.\n\nFor each input vector $x$, layer normalization computes:\n\n$$\n\\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sigma} + \\beta\n$$\n\nwhere $\\mu$ and $\\sigma$ are the mean and standard deviation of $x$, and $\\gamma$ and $\\beta$ are learned scaling and shifting parameters (initialized to 1 and 0 respectively). This ensures that no matter how the input distribution shifts during training, each layer receives inputs in a stable numerical range.\n\n## The Encoder Block\n\nNow we wire the components together. The **encoder block** processes the input sequence through four stages:\n\n1. **Multi-head self-attention** computes contextualized representations\n2. **Residual connection + normalization** stabilizes training\n3. **Feed-forward network** applies nonlinear transformation\n4. **Residual connection + normalization** again\n\n```{figure} ../figs/transformer-encoder.jpg\n:name: transformer-block\n:alt: Transformer Block\n:width: 50%\n:align: center\n\nInformation flows through multi-head attention, normalization, feed-forward networks, and final normalization.\nThe feed-forward network is a simple two-layer MLP applied independently to each position:\n\n\\text{FFN}(x) = \\text{ReLU}(x W_1 + b_1) W_2 + b_2\n\nThe residual connections (also called skip connections) are critical for training deep networks. Instead of learning a direct mapping f(x), we learn the residual:\n\nx_{\\text{out}} = x_{\\text{in}} + f(x_{\\text{in}})\n\nThis simple addition has profound consequences for gradient flow. During backpropagation, the gradient of the loss \\mathcal{L} with respect to layer l is:\n\n\\frac{\\partial \\mathcal{L}}{\\partial x_l} = \\frac{\\partial \\mathcal{L}}{\\partial x_{l+1}} \\left(1 + \\frac{\\partial f_l}{\\partial x_l}\\right)\n\nNotice the ‚Äú+1‚Äù term‚Äîthis provides a direct gradient path from the output back to the input. Without residual connections, gradients must pass through the chain:\n\n\\frac{\\partial f_L}{\\partial f_{L-1}} \\cdot \\frac{\\partial f_{L-1}}{\\partial f_{L-2}} \\cdot \\ldots \\cdot \\frac{\\partial f_1}{\\partial x}\n\nIf any term is less than 1, the gradient shrinks exponentially‚Äîthis is the vanishing gradient problem. With residual connections, the gradient expansion becomes:\n\n1 + O_1 + O_2 + O_3 + \\ldots\n\nwhere O_1 contains first-order terms, O_2 contains second-order products, etc. The constant ‚Äú1‚Äù ensures gradients can flow even when the learned components f_i produce small derivatives. This architectural innovation, originally developed for computer vision {footcite:p}he2015deep, is what allows transformers to scale to hundreds of layers.\n\n\n\nThe decoder block extends the encoder with two modifications: masked self-attention and cross-attention.\n```ligmtoun ../figs/transformer-decoder.jpg :name: transformer-decoder :alt: Transformer Decoder :width: 50% :align: center\nThe decoder adds masked self-attention (to prevent future peeking) and cross-attention (to access encoder outputs).\n\n### Masked Self-Attention: Preventing Future Leakage\n\nDuring training, we know the entire target sequence. For translation (\"I love you\" ‚Üí \"Je t'aime\"), we have both input and output. A naive decoder could \"cheat\" by looking at future words in the target sequence. Masked self-attention prevents this by zeroing out attention to future positions.\n\nThe mask is implemented by setting attention scores to $-\\infty$ before the softmax:\n\n$$\n\\text{MaskedAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T + M}{\\sqrt{d_k}}\\right)V\n$$\n\nwhere $M$ is a matrix with $-\\infty$ at positions $(i,j)$ where $j &gt; i$ (future positions) and 0 elsewhere. After softmax, these $-\\infty$ values become zero, eliminating information flow from future tokens.\n\n```{figure} https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png\n:name: transformer-masked-attention\n:alt: Masked Attention\n:width: 80%\n:align: center\n\nMasked attention zeros out future positions, allowing parallel training without information leakage.\nThis enables parallel training. Instead of generating ‚ÄúJe‚Äù, then ‚Äút‚Äôaime‚Äù, then the final token sequentially, we can train all positions simultaneously‚Äîeach with access only to its causal past. During inference, masking happens naturally because future tokens don‚Äôt exist yet.\n\n\nThe second attention layer in the decoder uses cross-attention to access the encoder‚Äôs output. The queries (Q) come from the decoder‚Äôs previous layer, while the keys (K) and values (V) come from the encoder‚Äôs output:\n\n\\text{CrossAttention}(Q_{\\text{decoder}}, K_{\\text{encoder}}, V_{\\text{encoder}}) = \\text{softmax}\\left(\\frac{Q_{\\text{decoder}}K_{\\text{encoder}}^T}{\\sqrt{d_k}}\\right)V_{\\text{encoder}}\n\n```ligmtoun ../figs/transformer-cross-attention.jpg :name: transformer-cross-attention :alt: Cross-Attention :width: 60% :align: center\nCross-attention allows the decoder to query the encoder‚Äôs representation.\n\nThis is how translation works: when generating \"Je\", the decoder attends to \"I\"; when generating \"t'aime\", it attends to \"love\". The attention mechanism learns these alignments automatically from data, without explicit supervision.\n\n## Position Embedding: Encoding Order\n\nAttention is **permutation invariant**‚Äîit produces the same output regardless of input order. \"The cat sat on the mat\" and \"mat the on sat cat the\" yield identical attention outputs because the dot product doesn't encode position. We need to inject positional information.\n\nThe naive approach is to add a position index: $x_t := x_t + \\beta t$. This fails for two reasons:\n\n1. **Unbounded**: Position indices grow arbitrarily large. Models trained on sequences of length 512 fail on sequences of length 1000 because they've never seen position 513.\n2. **Discrete**: Positions 10 and 11 are no more similar than positions 10 and 100.\n\nA better approach is **binary position encoding**. Represent position $t$ as a binary vector:\n\n$$\n\\begin{align*}\n  0: \\ \\ \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} & \\quad &\n  8: \\ \\ \\ \\ \\texttt{1} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\\\\n  1: \\ \\ \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{1} & &\n  9: \\ \\ \\ \\ \\texttt{1} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{1} \\\\\n  2: \\ \\ \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{1} \\ \\ \\texttt{0} & &\n  10: \\ \\ \\ \\ \\texttt{1} \\ \\ \\texttt{0} \\ \\ \\texttt{1} \\ \\ \\texttt{0}\n\\end{align*}\n$$\n\nThis is unbounded‚Äîyou can represent arbitrarily large positions by adding bits‚Äîbut still discrete. The transformer solution is **sinusoidal position embedding**, a continuous version of binary encoding:\n\n$$\n\\text{Pos}(t, i) =\n\\begin{cases}\n\\sin\\left(\\dfrac{t}{10000^{2i/d}}\\right), & \\text{if } i \\text{ is even} \\\\\n\\cos\\left(\\dfrac{t}{10000^{2i/d}}\\right), & \\text{if } i \\text{ is odd}\n\\end{cases}\n$$\n\nwhere $t$ is the position index and $i$ is the dimension index. This encoding has three critical properties:\n\n1. **Continuous**: Smooth interpolation between positions\n2. **Bounded**: All values lie in $[-1, 1]$\n3. **Relative distance preservation**: The dot product $\\text{Pos}(t) \\cdot \\text{Pos}(t+k)$ depends only on the offset $k$, not the absolute position $t$\n\n```{figure} https://kazemnejad.com/img/transformer_architecture_positional_encoding/positional_encoding.png\n:name: transformer-position-embedding\n:alt: Transformer Position Embedding\n:width: 80%\n:align: center\n\nSinusoidal position embeddings exhibit periodic patterns across dimensions. Image from [Amirhossein Kazemnejad](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/).\nNotice the alternating pattern‚Äîjust like binary encoding, but continuous. Low-frequency dimensions (right) flip slowly across positions; high-frequency dimensions (left) flip rapidly. This creates a unique fingerprint for each position while preserving distance relationships.\n```ligmtoun https://kazemnejad.com/img/transformer_architecture_positional_encoding/time-steps_dot_product.png :name: transformer-position-embedding-similarity :alt: Transformer Position Embedding Similarity :width: 80% :align: center\nDot product between position embeddings depends only on relative distance, not absolute position. Image from Amirhossein Kazemnejad.\n\nThe position embedding is added directly to the token embedding: $x_{t,i} := x_{t,i} + \\text{Pos}(t, i)$. Why addition instead of concatenation? Concatenation would increase the model dimension, adding parameters. Addition creates an interesting interaction in the attention mechanism‚Äîqueries and keys now encode both content and position, allowing the model to attend based on both \"what\" (semantic similarity) and \"where\" (positional proximity).\n\n## The Takeaway\n\nTransformers replaced sequential computation with parallel relationship mapping. Every position simultaneously computes its context from every other position. This architectural shift‚Äîfrom recurrent bottlenecks to parallel attention‚Äîis what allowed language models to scale from millions to hundreds of billions of parameters. The mechanism is simple: query, key, value. The result is GPT-4.\n\n```{footbibliography}\n:style: unsrt\n:filter: docname in docnames"
  },
  {
    "objectID": "m06-llms/transformers.html#the-mechanism",
    "href": "m06-llms/transformers.html#the-mechanism",
    "title": "Transformers",
    "section": "",
    "text": "You‚Äôve been taught to think of language models as sequential processors‚Äîreading left to right, one word triggering the next, like dominoes falling. This intuition comes from recurrent neural networks (RNNs), where information flows step by step, each word depending on the hidden state from the previous word. The transformer architecture throws this away entirely.\nInstead of sequential processing, transformers operate through parallel relationship mapping. When you read ‚ÄúThe cat sat on the mat because it was tired,‚Äù you don‚Äôt actually process word-by-word in isolation. Your brain simultaneously evaluates which words relate to which‚Äî‚Äúit‚Äù connects to ‚Äúcat,‚Äù ‚Äútired‚Äù explains ‚Äúsat,‚Äù ‚Äúmat‚Äù anchors ‚Äúon.‚Äù Transformers formalize this intuition mathematically. Every position in the input sequence simultaneously computes its relationship to every other position. The mechanism is attention, and the result is a system where context flows in all directions at once, not just forward through time.\nThis parallelism is why transformers scaled when RNNs didn‚Äôt. Recurrent architectures impose sequential computation‚Äîyou can‚Äôt process word 100 until you‚Äôve processed word 99. Transformers eliminate this bottleneck. Every position can be computed in parallel, which means training time scales with sequence complexity, not sequence length. This architectural shift is what enabled GPT-3, GPT-4, and Claude to exist."
  },
  {
    "objectID": "m06-llms/transformers.html#the-architecture",
    "href": "m06-llms/transformers.html#the-architecture",
    "title": "Transformers",
    "section": "",
    "text": "Modern LLMs stack multiple transformer blocks‚Äîmodular units that take a sequence of token vectors as input and output a transformed sequence of the same length. GPT-3 uses 96 of these blocks; GPT-4 likely uses more. Each block refines the representation, adding layers of contextual understanding.\n```ligmtoun ../figs/transformer-overview.jpg :name: transformer-overview :alt: Transformer Overview :width: 50% :align: center\nThe basic architecture of the transformer-based LLMs.\n\nThese blocks come in two forms: **encoders** and **decoders**. The encoder processes the input sequence and builds a contextualized representation. The decoder generates the output sequence, attending to both its own previous outputs and the encoder's representation. For translation tasks (\"I love you\" ‚Üí \"Je t'aime\"), the encoder processes English, the decoder generates French. For language modeling (GPT-style systems), only the decoder is used‚Äîit generates text autoregressively, predicting the next token based on all previous tokens.\n\n```{figure} ../figs/transformer-encoder-decoder.jpg\n:name: transformer-encoder-decoder\n:alt: Transformer Encoder-Decoder\n:width: 80%\n:align: center\n\nThe encoder-decoder architecture. The encoder builds a representation of the input sequence; the decoder generates the output sequence while attending to the encoder's output.\nInside each block are three core components: multi-head attention (the relationship mapper), layer normalization (numerical stabilization), and feed-forward networks (nonlinear transformation). We‚Äôll build these components step by step.\n```ligmtoun ../figs/transformer-component.jpg :name: transformer-wired-components :alt: Transformer Wired Components :width: 80% :align: center\nInternal structure of encoder and decoder blocks.\n\n## Attention: The Relationship Engine\n\n**Self-attention**‚Äîthe core of the transformer‚Äîcomputes how much each position in a sequence should \"attend to\" every other position. Unlike earlier attention mechanisms in seq2seq models, which attended from one sentence to another, self-attention operates within a single sequence. It answers the question: \"Given this word, which other words matter most?\"\n\n```{figure} ../figs/transformer-attention.jpg\n:name: transformer-attention\n:alt: Attention Mechanism\n:width: 80%\n:align: center\n\nThe attention mechanism computes relationships between all positions simultaneously.\nFor each word, the attention mechanism creates three vectors: query (Q), key (K), and value (V). Think of these as a library search: the query is what you‚Äôre looking for, the keys are book titles, and the values are the actual content. When you search for ‚Äúmachine learning‚Äù (your query), you match it against book titles (keys) to find relevant content (values).\nMathematically, each of these vectors is created by a learned linear transformation of the input word embedding. Given an input embedding x, we compute:\n\nQ = x W_Q, \\quad K = x W_K, \\quad V = x W_V\n\nwhere W_Q, W_K, and W_V are learned weight matrices. The attention mechanism then computes which keys are most relevant to each query using the dot product, which measures vector similarity. The dot product QK^T produces a matrix of attention scores‚Äîlarge values indicate strong relationships, small values indicate weak ones.\nThese raw scores are scaled by \\sqrt{d_k} (the square root of the key dimension) to prevent extreme values, then normalized using softmax to produce a probability distribution. Finally, these normalized attention weights are used to compute a weighted sum of the value vectors. The complete operation is:\n\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\nwhere Q \\in \\mathbb{R}^{n \\times d_k}, K \\in \\mathbb{R}^{n \\times d_k}, and V \\in \\mathbb{R}^{n \\times d_v} represent matrices containing n query, key, and value vectors respectively.\nThe interactive visualization below demonstrates how learned Query and Key transformations produce different attention patterns. Adjust the transformation parameters to see how different W_Q and W_K matrices change which words attend to which:\n\n\npython {marimo} import marimo as mo import numpy as np import pandas as pd import altair as alt\n```python {marimo} attention_words = [‚Äúbank‚Äù, ‚Äúmoney‚Äù, ‚Äúloan‚Äù, ‚Äúriver‚Äù, ‚Äúshore‚Äù] attention_embeddings = np.array([ [0.0, 0.0], # bank (center) [-0.8, -0.3], # money [-0.7, -0.6], # loan [0.7, -0.5], # river [0.6, -0.7], # shore]) * 2\n\n\nq_scale_x = mo.ui.slider(-2, 2, 0.1, value=1.0, label=‚ÄúQ Scale X‚Äù) q_scale_y = mo.ui.slider(-2, 2, 0.1, value=1.0, label=‚ÄúQ Scale Y‚Äù) q_rotate = mo.ui.slider(-180, 180, 5, value=0, label=‚ÄúQ Rotate (deg)‚Äù)\n\n\n\nk_scale_x = mo.ui.slider(-2, 2, 0.1, value=1.0, label=‚ÄúK Scale X‚Äù) k_scale_y = mo.ui.slider(-2, 2, 0.1, value=1.0, label=‚ÄúK Scale Y‚Äù) k_rotate = mo.ui.slider(-180, 180, 5, value=0, label=‚ÄúK Rotate (deg)‚Äù)\nq_controls = mo.vstack([mo.md(‚ÄúQuery Transformation‚Äù), q_scale_x, q_scale_y, q_rotate]) k_controls = mo.vstack([mo.md(‚ÄúKey Transformation‚Äù), k_scale_x, k_scale_y, k_rotate])\n\n```python {marimo}\ndef _transform_embeddings(emb, scale_x, scale_y, rotate_deg):\n    theta = np.radians(rotate_deg)\n    rot_matrix = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n    scale_matrix = np.diag([scale_x, scale_y])\n    W = rot_matrix @ scale_matrix\n    return emb @ W.T\n\nQ = _transform_embeddings(attention_embeddings, q_scale_x.value, q_scale_y.value, q_rotate.value)\nK = _transform_embeddings(attention_embeddings, k_scale_x.value, k_scale_y.value, k_rotate.value)\n\n# Compute attention scores\n_scores = Q @ K.T\n_exp_scores = np.exp(_scores - np.max(_scores, axis=1, keepdims=True))\nattention_weights = _exp_scores / np.sum(_exp_scores, axis=1, keepdims=True)\n\n# Create visualizations\n_df_q = pd.DataFrame({\"word\": attention_words, \"x\": Q[:, 0], \"y\": Q[:, 1]})\n_df_k = pd.DataFrame({\"word\": attention_words, \"x\": K[:, 0], \"y\": K[:, 1]})\n\n_chart_q = alt.Chart(_df_q).mark_circle(size=100).encode(\n    x=alt.X('x:Q', scale=alt.Scale(domain=[-4, 4]), title='Q1'),\n    y=alt.Y('y:Q', scale=alt.Scale(domain=[-4, 4]), title='Q2'),\n    tooltip=['word:N']\n).properties(width=200, height=200, title=\"Query (Q)\")\n_text_q = _chart_q.mark_text(dy=-12, fontSize=10, fontWeight='bold').encode(text='word:N')\n\n_chart_k = alt.Chart(_df_k).mark_circle(size=100).encode(\n    x=alt.X('x:Q', scale=alt.Scale(domain=[-4, 4]), title='K1'),\n    y=alt.Y('y:Q', scale=alt.Scale(domain=[-4, 4]), title='K2'),\n    tooltip=['word:N']\n).properties(width=200, height=200, title=\"Key (K)\")\n_text_k = _chart_k.mark_text(dy=-12, fontSize=10, fontWeight='bold').encode(text='word:N')\n\n# Heatmap\n_heatmap_data = []\nfor i, word_i in enumerate(attention_words):\n    for j, word_j in enumerate(attention_words):\n        _heatmap_data.append({\"Query\": word_i, \"Key\": word_j, \"Weight\": attention_weights[i, j]})\n_df_heatmap = pd.DataFrame(_heatmap_data)\n\n_heatmap = alt.Chart(_df_heatmap).mark_rect().encode(\n    x=alt.X('Key:N', title='Key Word'),\n    y=alt.Y('Query:N', title='Query Word'),\n    color=alt.Color('Weight:Q', scale=alt.Scale(scheme='blues'), title='Attention'),\n    tooltip=['Query:N', 'Key:N', alt.Tooltip('Weight:Q', format='.3f')]\n).properties(width=250, height=250, title=\"Attention Weights (Softmax)\")\n\nmo.vstack([\n    mo.hstack([q_controls, k_controls], align=\"center\"),\n    mo.hstack([_chart_q + _text_q, _chart_k + _text_k, _heatmap], align=\"center\")\n])\n\n\n\nThe output is a contextualized vector for each word‚Äîa representation that changes based on surrounding context. The word ‚Äúbank‚Äù produces different vectors in ‚Äúriver bank‚Äù versus ‚Äúfinancial bank‚Äù because the attention mechanism incorporates information from neighboring words.\nTo see this in action, consider how we might contextualize the word ‚Äúbank‚Äù by mixing it with surrounding words. The visualization below shows static word embeddings‚Äînotice how ‚Äúbank‚Äù sits neutrally between financial terms (money, loan) and geographical terms (river, shore).\n\n\n```python {marimo} static_words = [‚Äúbank‚Äù, ‚Äúmoney‚Äù, ‚Äúloan‚Äù, ‚Äúriver‚Äù, ‚Äúshore‚Äù] static_embeddings = np.array([ [0.0, 0.0], # bank (center) [-0.8, -0.3], # money [-0.7, -0.6], # loan [0.7, -0.5], # river [0.6, -0.7], # shore]) * 2\n_df_static = pd.DataFrame({‚Äúword‚Äù: static_words, ‚Äúx‚Äù: static_embeddings[:, 0], ‚Äúy‚Äù: static_embeddings[:, 1]})\n_chart_static = alt.Chart(_df_static).mark_circle(size=200).encode( x=alt.X(‚Äòx:Q‚Äô, scale=alt.Scale(domain=[-2, 2]), title=‚ÄòDimension 1‚Äô), y=alt.Y(‚Äòy:Q‚Äô, scale=alt.Scale(domain=[-2, 2]), title=‚ÄòDimension 2‚Äô), text=‚Äòword:N‚Äô, tooltip=[‚Äòword:N‚Äô, ‚Äòx:Q‚Äô, ‚Äòy:Q‚Äô] ).properties(width=300, height=300, title=‚ÄúStatic Word Embeddings‚Äù)\n_text_static = _chart_static.mark_text(dy=-15, fontSize=14, fontWeight=‚Äòbold‚Äô).encode(text=‚Äòword:N‚Äô)\n_chart_static + _text_static\n\n&lt;/marimo-iframe&gt;\n&lt;/div&gt;\n\nNow, try adjusting the weights below to create a contextualized version of \"bank.\" If the sentence is \"Money in bank,\" adjust the weights to shift \"bank\" toward \"money.\" If the sentence is \"River bank,\" shift it toward \"river.\"\n\n&lt;div&gt;\n&lt;marimo-iframe data-height=\"500px\" data-show-code=\"false\"&gt;\n\n```python {marimo}\ncontext_words = [\"bank\", \"money\", \"loan\", \"river\", \"shore\"]\ncontext_embeddings = np.array([\n    [0.0, 0.0],  # bank (center)\n    [-0.8, -0.3],  # money\n    [-0.7, -0.6],  # loan\n    [0.7, -0.5],  # river\n    [0.6, -0.7],  # shore\n]) * 2\n\nslider_bank = mo.ui.slider(0, 1, 0.01, value=1.0, label=\"Bank Weight\")\nslider_money = mo.ui.slider(0, 1, 0.01, value=0, label=\"Money Weight\")\nslider_loan = mo.ui.slider(0, 1, 0.01, value=0, label=\"Loan Weight\")\nslider_river = mo.ui.slider(0, 1, 0.01, value=0, label=\"River Weight\")\nslider_shore = mo.ui.slider(0, 1, 0.01, value=0, label=\"Shore Weight\")\n\ncontext_sliders = mo.vstack([slider_bank, slider_money, slider_loan, slider_river, slider_shore])\n```python {marimo} _weights = np.array([slider_bank.value, slider_money.value, slider_loan.value, slider_river.value, slider_shore.value]) _total = _weights.sum() if _total &gt; 0: _weights = _weights / _total _new_vec = context_embeddings.T @ _weights else: _new_vec = np.zeros(2)\n_df_orig = pd.DataFrame({‚Äúword‚Äù: context_words, ‚Äúx‚Äù: context_embeddings[:, 0], ‚Äúy‚Äù: context_embeddings[:, 1], ‚Äútype‚Äù: [‚ÄúOriginal‚Äù] * 5}) _df_new = pd.DataFrame({‚Äúword‚Äù: [‚ÄúContextualized Bank‚Äù], ‚Äúx‚Äù: [_new_vec[0]], ‚Äúy‚Äù: [_new_vec[1]], ‚Äútype‚Äù: [‚ÄúContextualized‚Äù]}) _df_combined = pd.concat([_df_orig, _df_new])\n_chart_context = alt.Chart(_df_combined).mark_circle(size=200).encode( x=alt.X(‚Äòx:Q‚Äô, scale=alt.Scale(domain=[-2, 2]), title=‚ÄòDimension 1‚Äô), y=alt.Y(‚Äòy:Q‚Äô, scale=alt.Scale(domain=[-2, 2]), title=‚ÄòDimension 2‚Äô), color=alt.Color(‚Äòtype:N‚Äô, scale=alt.Scale(domain=[‚ÄòOriginal‚Äô, ‚ÄòContextualized‚Äô], range=[‚Äò#dadada‚Äô, ‚Äò#ff7f0e‚Äô])), tooltip=[‚Äòword:N‚Äô, ‚Äòx:Q‚Äô, ‚Äòy:Q‚Äô] ).properties(width=350, height=350, title=‚ÄúContextualized Bank‚Äù)\n_text_context = _chart_context.mark_text(dy=-15, fontSize=14, fontWeight=‚Äòbold‚Äô).encode(text=‚Äòword:N‚Äô, color=alt.value(‚Äòblack‚Äô))\nmo.hstack([context_sliders, _chart_context + _text_context], align=‚Äúcenter‚Äù)\n\n&lt;/marimo-iframe&gt;\n&lt;/div&gt;\n\nThis manual weighting captures the intuition, but how do we learn which words to attend to? This is where queries and keys come in.\n\n### Multi-Head Attention: Multiple Perspectives\n\nA single attention mechanism captures one type of relationship. **Multi-head attention** runs multiple attention operations in parallel, each with different learned parameters. Each head can specialize‚Äîone might focus on syntactic dependencies (subject-verb relationships), another on semantic similarity (synonyms and antonyms), another on positional proximity (nearby words).\n\n```{figure} ../figs/transformer-multihead-attention.jpg\n:name: transformer-multihead-attention\n:alt: Multi-Head Attention\n:width: 50%\n:align: center\n\nMulti-head attention runs multiple attention operations in parallel, each capturing different relationships.\nThe outputs from all heads are concatenated and passed through a final linear transformation to produce the multi-head attention output. In the original transformer paper {footcite:p}vaswani2017attention, the authors used h=8 attention heads, with each head using dimension d_k = d_v = d/h = 64, where d=512 is the model dimension.\n\n\nDeep networks suffer from numerical instability‚Äîactivations can grow explosively large or vanish to zero as they propagate through layers. Layer normalization stabilizes training by rescaling activations to have zero mean and unit variance.\n```ligmtoun https://miro.medium.com/v2/resize:fit:1400/0*Agdt1zYwfUxXMJGJ :name: transformer-layer-normalization :alt: Layer Normalization :width: 80% :align: center\nLayer normalization computes mean and standard deviation across all features for each sample, then normalizes.\n\nFor each input vector $x$, layer normalization computes:\n\n$$\n\\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sigma} + \\beta\n$$\n\nwhere $\\mu$ and $\\sigma$ are the mean and standard deviation of $x$, and $\\gamma$ and $\\beta$ are learned scaling and shifting parameters (initialized to 1 and 0 respectively). This ensures that no matter how the input distribution shifts during training, each layer receives inputs in a stable numerical range.\n\n## The Encoder Block\n\nNow we wire the components together. The **encoder block** processes the input sequence through four stages:\n\n1. **Multi-head self-attention** computes contextualized representations\n2. **Residual connection + normalization** stabilizes training\n3. **Feed-forward network** applies nonlinear transformation\n4. **Residual connection + normalization** again\n\n```{figure} ../figs/transformer-encoder.jpg\n:name: transformer-block\n:alt: Transformer Block\n:width: 50%\n:align: center\n\nInformation flows through multi-head attention, normalization, feed-forward networks, and final normalization.\nThe feed-forward network is a simple two-layer MLP applied independently to each position:\n\n\\text{FFN}(x) = \\text{ReLU}(x W_1 + b_1) W_2 + b_2\n\nThe residual connections (also called skip connections) are critical for training deep networks. Instead of learning a direct mapping f(x), we learn the residual:\n\nx_{\\text{out}} = x_{\\text{in}} + f(x_{\\text{in}})\n\nThis simple addition has profound consequences for gradient flow. During backpropagation, the gradient of the loss \\mathcal{L} with respect to layer l is:\n\n\\frac{\\partial \\mathcal{L}}{\\partial x_l} = \\frac{\\partial \\mathcal{L}}{\\partial x_{l+1}} \\left(1 + \\frac{\\partial f_l}{\\partial x_l}\\right)\n\nNotice the ‚Äú+1‚Äù term‚Äîthis provides a direct gradient path from the output back to the input. Without residual connections, gradients must pass through the chain:\n\n\\frac{\\partial f_L}{\\partial f_{L-1}} \\cdot \\frac{\\partial f_{L-1}}{\\partial f_{L-2}} \\cdot \\ldots \\cdot \\frac{\\partial f_1}{\\partial x}\n\nIf any term is less than 1, the gradient shrinks exponentially‚Äîthis is the vanishing gradient problem. With residual connections, the gradient expansion becomes:\n\n1 + O_1 + O_2 + O_3 + \\ldots\n\nwhere O_1 contains first-order terms, O_2 contains second-order products, etc. The constant ‚Äú1‚Äù ensures gradients can flow even when the learned components f_i produce small derivatives. This architectural innovation, originally developed for computer vision {footcite:p}he2015deep, is what allows transformers to scale to hundreds of layers.\n\n\n\nThe decoder block extends the encoder with two modifications: masked self-attention and cross-attention.\n```ligmtoun ../figs/transformer-decoder.jpg :name: transformer-decoder :alt: Transformer Decoder :width: 50% :align: center\nThe decoder adds masked self-attention (to prevent future peeking) and cross-attention (to access encoder outputs).\n\n### Masked Self-Attention: Preventing Future Leakage\n\nDuring training, we know the entire target sequence. For translation (\"I love you\" ‚Üí \"Je t'aime\"), we have both input and output. A naive decoder could \"cheat\" by looking at future words in the target sequence. Masked self-attention prevents this by zeroing out attention to future positions.\n\nThe mask is implemented by setting attention scores to $-\\infty$ before the softmax:\n\n$$\n\\text{MaskedAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T + M}{\\sqrt{d_k}}\\right)V\n$$\n\nwhere $M$ is a matrix with $-\\infty$ at positions $(i,j)$ where $j &gt; i$ (future positions) and 0 elsewhere. After softmax, these $-\\infty$ values become zero, eliminating information flow from future tokens.\n\n```{figure} https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png\n:name: transformer-masked-attention\n:alt: Masked Attention\n:width: 80%\n:align: center\n\nMasked attention zeros out future positions, allowing parallel training without information leakage.\nThis enables parallel training. Instead of generating ‚ÄúJe‚Äù, then ‚Äút‚Äôaime‚Äù, then the final token sequentially, we can train all positions simultaneously‚Äîeach with access only to its causal past. During inference, masking happens naturally because future tokens don‚Äôt exist yet.\n\n\nThe second attention layer in the decoder uses cross-attention to access the encoder‚Äôs output. The queries (Q) come from the decoder‚Äôs previous layer, while the keys (K) and values (V) come from the encoder‚Äôs output:\n\n\\text{CrossAttention}(Q_{\\text{decoder}}, K_{\\text{encoder}}, V_{\\text{encoder}}) = \\text{softmax}\\left(\\frac{Q_{\\text{decoder}}K_{\\text{encoder}}^T}{\\sqrt{d_k}}\\right)V_{\\text{encoder}}\n\n```ligmtoun ../figs/transformer-cross-attention.jpg :name: transformer-cross-attention :alt: Cross-Attention :width: 60% :align: center\nCross-attention allows the decoder to query the encoder‚Äôs representation.\n\nThis is how translation works: when generating \"Je\", the decoder attends to \"I\"; when generating \"t'aime\", it attends to \"love\". The attention mechanism learns these alignments automatically from data, without explicit supervision.\n\n## Position Embedding: Encoding Order\n\nAttention is **permutation invariant**‚Äîit produces the same output regardless of input order. \"The cat sat on the mat\" and \"mat the on sat cat the\" yield identical attention outputs because the dot product doesn't encode position. We need to inject positional information.\n\nThe naive approach is to add a position index: $x_t := x_t + \\beta t$. This fails for two reasons:\n\n1. **Unbounded**: Position indices grow arbitrarily large. Models trained on sequences of length 512 fail on sequences of length 1000 because they've never seen position 513.\n2. **Discrete**: Positions 10 and 11 are no more similar than positions 10 and 100.\n\nA better approach is **binary position encoding**. Represent position $t$ as a binary vector:\n\n$$\n\\begin{align*}\n  0: \\ \\ \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} & \\quad &\n  8: \\ \\ \\ \\ \\texttt{1} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\\\\n  1: \\ \\ \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{1} & &\n  9: \\ \\ \\ \\ \\texttt{1} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{1} \\\\\n  2: \\ \\ \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{1} \\ \\ \\texttt{0} & &\n  10: \\ \\ \\ \\ \\texttt{1} \\ \\ \\texttt{0} \\ \\ \\texttt{1} \\ \\ \\texttt{0}\n\\end{align*}\n$$\n\nThis is unbounded‚Äîyou can represent arbitrarily large positions by adding bits‚Äîbut still discrete. The transformer solution is **sinusoidal position embedding**, a continuous version of binary encoding:\n\n$$\n\\text{Pos}(t, i) =\n\\begin{cases}\n\\sin\\left(\\dfrac{t}{10000^{2i/d}}\\right), & \\text{if } i \\text{ is even} \\\\\n\\cos\\left(\\dfrac{t}{10000^{2i/d}}\\right), & \\text{if } i \\text{ is odd}\n\\end{cases}\n$$\n\nwhere $t$ is the position index and $i$ is the dimension index. This encoding has three critical properties:\n\n1. **Continuous**: Smooth interpolation between positions\n2. **Bounded**: All values lie in $[-1, 1]$\n3. **Relative distance preservation**: The dot product $\\text{Pos}(t) \\cdot \\text{Pos}(t+k)$ depends only on the offset $k$, not the absolute position $t$\n\n```{figure} https://kazemnejad.com/img/transformer_architecture_positional_encoding/positional_encoding.png\n:name: transformer-position-embedding\n:alt: Transformer Position Embedding\n:width: 80%\n:align: center\n\nSinusoidal position embeddings exhibit periodic patterns across dimensions. Image from [Amirhossein Kazemnejad](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/).\nNotice the alternating pattern‚Äîjust like binary encoding, but continuous. Low-frequency dimensions (right) flip slowly across positions; high-frequency dimensions (left) flip rapidly. This creates a unique fingerprint for each position while preserving distance relationships.\n```ligmtoun https://kazemnejad.com/img/transformer_architecture_positional_encoding/time-steps_dot_product.png :name: transformer-position-embedding-similarity :alt: Transformer Position Embedding Similarity :width: 80% :align: center\nDot product between position embeddings depends only on relative distance, not absolute position. Image from Amirhossein Kazemnejad.\n\nThe position embedding is added directly to the token embedding: $x_{t,i} := x_{t,i} + \\text{Pos}(t, i)$. Why addition instead of concatenation? Concatenation would increase the model dimension, adding parameters. Addition creates an interesting interaction in the attention mechanism‚Äîqueries and keys now encode both content and position, allowing the model to attend based on both \"what\" (semantic similarity) and \"where\" (positional proximity).\n\n## The Takeaway\n\nTransformers replaced sequential computation with parallel relationship mapping. Every position simultaneously computes its context from every other position. This architectural shift‚Äîfrom recurrent bottlenecks to parallel attention‚Äîis what allowed language models to scale from millions to hundreds of billions of parameters. The mechanism is simple: query, key, value. The result is GPT-4.\n\n```{footbibliography}\n:style: unsrt\n:filter: docname in docnames"
  },
  {
    "objectID": "m06-llms/transformers.html#layer-normalization-numerical-stability",
    "href": "m06-llms/transformers.html#layer-normalization-numerical-stability",
    "title": "Transformers",
    "section": "",
    "text": "Deep networks suffer from numerical instability‚Äîactivations can grow explosively large or vanish to zero as they propagate through layers. Layer normalization stabilizes training by rescaling activations to have zero mean and unit variance.\n```ligmtoun https://miro.medium.com/v2/resize:fit:1400/0*Agdt1zYwfUxXMJGJ :name: transformer-layer-normalization :alt: Layer Normalization :width: 80% :align: center\nLayer normalization computes mean and standard deviation across all features for each sample, then normalizes.\n\nFor each input vector $x$, layer normalization computes:\n\n$$\n\\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sigma} + \\beta\n$$\n\nwhere $\\mu$ and $\\sigma$ are the mean and standard deviation of $x$, and $\\gamma$ and $\\beta$ are learned scaling and shifting parameters (initialized to 1 and 0 respectively). This ensures that no matter how the input distribution shifts during training, each layer receives inputs in a stable numerical range.\n\n## The Encoder Block\n\nNow we wire the components together. The **encoder block** processes the input sequence through four stages:\n\n1. **Multi-head self-attention** computes contextualized representations\n2. **Residual connection + normalization** stabilizes training\n3. **Feed-forward network** applies nonlinear transformation\n4. **Residual connection + normalization** again\n\n```{figure} ../figs/transformer-encoder.jpg\n:name: transformer-block\n:alt: Transformer Block\n:width: 50%\n:align: center\n\nInformation flows through multi-head attention, normalization, feed-forward networks, and final normalization.\nThe feed-forward network is a simple two-layer MLP applied independently to each position:\n\n\\text{FFN}(x) = \\text{ReLU}(x W_1 + b_1) W_2 + b_2\n\nThe residual connections (also called skip connections) are critical for training deep networks. Instead of learning a direct mapping f(x), we learn the residual:\n\nx_{\\text{out}} = x_{\\text{in}} + f(x_{\\text{in}})\n\nThis simple addition has profound consequences for gradient flow. During backpropagation, the gradient of the loss \\mathcal{L} with respect to layer l is:\n\n\\frac{\\partial \\mathcal{L}}{\\partial x_l} = \\frac{\\partial \\mathcal{L}}{\\partial x_{l+1}} \\left(1 + \\frac{\\partial f_l}{\\partial x_l}\\right)\n\nNotice the ‚Äú+1‚Äù term‚Äîthis provides a direct gradient path from the output back to the input. Without residual connections, gradients must pass through the chain:\n\n\\frac{\\partial f_L}{\\partial f_{L-1}} \\cdot \\frac{\\partial f_{L-1}}{\\partial f_{L-2}} \\cdot \\ldots \\cdot \\frac{\\partial f_1}{\\partial x}\n\nIf any term is less than 1, the gradient shrinks exponentially‚Äîthis is the vanishing gradient problem. With residual connections, the gradient expansion becomes:\n\n1 + O_1 + O_2 + O_3 + \\ldots\n\nwhere O_1 contains first-order terms, O_2 contains second-order products, etc. The constant ‚Äú1‚Äù ensures gradients can flow even when the learned components f_i produce small derivatives. This architectural innovation, originally developed for computer vision {footcite:p}he2015deep, is what allows transformers to scale to hundreds of layers."
  },
  {
    "objectID": "m06-llms/transformers.html#the-decoder-block",
    "href": "m06-llms/transformers.html#the-decoder-block",
    "title": "Transformers",
    "section": "",
    "text": "The decoder block extends the encoder with two modifications: masked self-attention and cross-attention.\n```ligmtoun ../figs/transformer-decoder.jpg :name: transformer-decoder :alt: Transformer Decoder :width: 50% :align: center\nThe decoder adds masked self-attention (to prevent future peeking) and cross-attention (to access encoder outputs).\n\n### Masked Self-Attention: Preventing Future Leakage\n\nDuring training, we know the entire target sequence. For translation (\"I love you\" ‚Üí \"Je t'aime\"), we have both input and output. A naive decoder could \"cheat\" by looking at future words in the target sequence. Masked self-attention prevents this by zeroing out attention to future positions.\n\nThe mask is implemented by setting attention scores to $-\\infty$ before the softmax:\n\n$$\n\\text{MaskedAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T + M}{\\sqrt{d_k}}\\right)V\n$$\n\nwhere $M$ is a matrix with $-\\infty$ at positions $(i,j)$ where $j &gt; i$ (future positions) and 0 elsewhere. After softmax, these $-\\infty$ values become zero, eliminating information flow from future tokens.\n\n```{figure} https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png\n:name: transformer-masked-attention\n:alt: Masked Attention\n:width: 80%\n:align: center\n\nMasked attention zeros out future positions, allowing parallel training without information leakage.\nThis enables parallel training. Instead of generating ‚ÄúJe‚Äù, then ‚Äút‚Äôaime‚Äù, then the final token sequentially, we can train all positions simultaneously‚Äîeach with access only to its causal past. During inference, masking happens naturally because future tokens don‚Äôt exist yet.\n\n\nThe second attention layer in the decoder uses cross-attention to access the encoder‚Äôs output. The queries (Q) come from the decoder‚Äôs previous layer, while the keys (K) and values (V) come from the encoder‚Äôs output:\n\n\\text{CrossAttention}(Q_{\\text{decoder}}, K_{\\text{encoder}}, V_{\\text{encoder}}) = \\text{softmax}\\left(\\frac{Q_{\\text{decoder}}K_{\\text{encoder}}^T}{\\sqrt{d_k}}\\right)V_{\\text{encoder}}\n\n```ligmtoun ../figs/transformer-cross-attention.jpg :name: transformer-cross-attention :alt: Cross-Attention :width: 60% :align: center\nCross-attention allows the decoder to query the encoder‚Äôs representation.\n\nThis is how translation works: when generating \"Je\", the decoder attends to \"I\"; when generating \"t'aime\", it attends to \"love\". The attention mechanism learns these alignments automatically from data, without explicit supervision.\n\n## Position Embedding: Encoding Order\n\nAttention is **permutation invariant**‚Äîit produces the same output regardless of input order. \"The cat sat on the mat\" and \"mat the on sat cat the\" yield identical attention outputs because the dot product doesn't encode position. We need to inject positional information.\n\nThe naive approach is to add a position index: $x_t := x_t + \\beta t$. This fails for two reasons:\n\n1. **Unbounded**: Position indices grow arbitrarily large. Models trained on sequences of length 512 fail on sequences of length 1000 because they've never seen position 513.\n2. **Discrete**: Positions 10 and 11 are no more similar than positions 10 and 100.\n\nA better approach is **binary position encoding**. Represent position $t$ as a binary vector:\n\n$$\n\\begin{align*}\n  0: \\ \\ \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} & \\quad &\n  8: \\ \\ \\ \\ \\texttt{1} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\\\\n  1: \\ \\ \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{1} & &\n  9: \\ \\ \\ \\ \\texttt{1} \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{1} \\\\\n  2: \\ \\ \\ \\ \\texttt{0} \\ \\ \\texttt{0} \\ \\ \\texttt{1} \\ \\ \\texttt{0} & &\n  10: \\ \\ \\ \\ \\texttt{1} \\ \\ \\texttt{0} \\ \\ \\texttt{1} \\ \\ \\texttt{0}\n\\end{align*}\n$$\n\nThis is unbounded‚Äîyou can represent arbitrarily large positions by adding bits‚Äîbut still discrete. The transformer solution is **sinusoidal position embedding**, a continuous version of binary encoding:\n\n$$\n\\text{Pos}(t, i) =\n\\begin{cases}\n\\sin\\left(\\dfrac{t}{10000^{2i/d}}\\right), & \\text{if } i \\text{ is even} \\\\\n\\cos\\left(\\dfrac{t}{10000^{2i/d}}\\right), & \\text{if } i \\text{ is odd}\n\\end{cases}\n$$\n\nwhere $t$ is the position index and $i$ is the dimension index. This encoding has three critical properties:\n\n1. **Continuous**: Smooth interpolation between positions\n2. **Bounded**: All values lie in $[-1, 1]$\n3. **Relative distance preservation**: The dot product $\\text{Pos}(t) \\cdot \\text{Pos}(t+k)$ depends only on the offset $k$, not the absolute position $t$\n\n```{figure} https://kazemnejad.com/img/transformer_architecture_positional_encoding/positional_encoding.png\n:name: transformer-position-embedding\n:alt: Transformer Position Embedding\n:width: 80%\n:align: center\n\nSinusoidal position embeddings exhibit periodic patterns across dimensions. Image from [Amirhossein Kazemnejad](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/).\nNotice the alternating pattern‚Äîjust like binary encoding, but continuous. Low-frequency dimensions (right) flip slowly across positions; high-frequency dimensions (left) flip rapidly. This creates a unique fingerprint for each position while preserving distance relationships.\n```ligmtoun https://kazemnejad.com/img/transformer_architecture_positional_encoding/time-steps_dot_product.png :name: transformer-position-embedding-similarity :alt: Transformer Position Embedding Similarity :width: 80% :align: center\nDot product between position embeddings depends only on relative distance, not absolute position. Image from Amirhossein Kazemnejad.\n\nThe position embedding is added directly to the token embedding: $x_{t,i} := x_{t,i} + \\text{Pos}(t, i)$. Why addition instead of concatenation? Concatenation would increase the model dimension, adding parameters. Addition creates an interesting interaction in the attention mechanism‚Äîqueries and keys now encode both content and position, allowing the model to attend based on both \"what\" (semantic similarity) and \"where\" (positional proximity).\n\n## The Takeaway\n\nTransformers replaced sequential computation with parallel relationship mapping. Every position simultaneously computes its context from every other position. This architectural shift‚Äîfrom recurrent bottlenecks to parallel attention‚Äîis what allowed language models to scale from millions to hundreds of billions of parameters. The mechanism is simple: query, key, value. The result is GPT-4.\n\n```{footbibliography}\n:style: unsrt\n:filter: docname in docnames"
  },
  {
    "objectID": "m06-llms/prompt-tuning-exercise.html",
    "href": "m06-llms/prompt-tuning-exercise.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Spoiler\n\n\n\nLLMs are deterministic pattern matchers that cannot generate true randomness‚Äîbut they can be prompted to generate sequences that statistically approximate random distributions by activating patterns where similar constraints preceded statistically valid outputs.\n\n\n\n\nLLMs optimize for fluency and pattern matching, not mathematical computation. When asked to generate random numbers, they typically produce sequences that fail statistical tests for randomness‚Äînumbers cluster, patterns emerge, distributions skew. The model has seen random numbers in training data, but it has no internal representation of what randomness means. It cannot compute true randomness; it can only recall patterns that look random.\nYet with careful prompting, you can bias the model toward outputs that pass statistical validation. The task: craft a prompt that makes an LLM generate at least 100 normally distributed random numbers (comma-separated, like 0.5,-1.2,0.8,...) that pass a Kolmogorov-Smirnov test with p-value greater than 0.20. Use Gemma3 27B on Google AI Studio or OpenRouter. No external tools‚Äîthe model must generate the numbers directly from its internal patterns.\nThe constraint forces you to think about what patterns in the training data correlate with valid statistical outputs. Asking for ‚Äújust the numbers‚Äù eliminates extraneous tokens that disrupt parsing. Requesting the model to ‚Äúthink about how normal distributions work‚Äù before generating may activate patterns where reasoning preceded statistically valid sequences. You‚Äôre not teaching the model statistics; you‚Äôre activating pre-existing patterns where statistical reasoning preceded appropriate outputs.\n\n\nimport marimo as mo\nimport altair as alt\nimport pandas as pd\nimport scipy.stats as stats\nimport numpy as np\ntext_area = mo.ui.text_area(placeholder = \"Enter numbers separated by commas\", value = \"1,2,3,4,5,6,7,8,9,10\")\nbutton = mo.ui.button(\"Runt test\")\n\nmo.vstack([text_area, button])\ntry:\n    numbers = np.array([float(num.strip()) for num in text_area.value.split(\",\")])\n    if len(numbers) &gt;= 100:\n        # KS test\n        pval = stats.kstest(numbers, stats.norm(loc=0.0, scale=1.0).cdf)[1]\n        test_result = \"The numbers are normal distributed (p-value = {:.2f})\".format(pval) if pval &gt; 0.20 else \"The numbers are not normal distributed (p-value = {:.2f})\".format(pval)\n        message = mo.callout(test_result, kind = \"success\" if pval &gt; 0.20 else \"danger\")\n    else:\n        message = mo.callout(\"The number of samples is too small. Need at least 100 samples.\", kind = \"warn\")\n\n    # Convert the numbers to a DataFrame for Altair\n    df = pd.DataFrame({'value': numbers})\n\n    # Create an Altair histogram\n    fig = alt.Chart(df).mark_bar().encode(\n        x=alt.X('value:Q', bin=alt.Bin(maxbins=30)),\n        y='count()'\n    ).properties(\n        title='Histogram of Values'\n    )\n\n    mo.hstack([fig, message])\nexcept:\n    message = mo.callout(\"Parse failed. Please check if your input follows the specified format.\", kind = \"danger\")\n    fig = None\n\nmo.hstack([fig, message]) if fig is not None else message\n\n\n\n\n\nLLMs can generate syntactically valid code in languages they‚Äôve seen during training‚Äîincluding SVG, the XML-based vector graphics format. The challenge: prompt Gemma3 27B to generate an SVG diagram of a neural network with specific structural requirements. The network must have same-colored neurons within each layer, connections between all neurons across adjacent layers, and labels for ‚ÄúInput layer,‚Äù ‚ÄúHidden layer,‚Äù and ‚ÄúOutput layer.‚Äù\nTest your prompt on Google AI Studio or OpenRouter, then paste the generated SVG code into SVG Viewer to visualize the result. The model has seen countless SVG examples during training, but it has no internal representation of what a neural network diagram ‚Äúshould‚Äù look like. It can only pattern match against examples where similar prompts preceded valid SVG structures.\n\n\n\n\n\n\nThese exercises expose the boundary between pattern matching and computation. LLMs cannot perform true mathematical operations or generate genuine randomness‚Äîthey can only recall patterns that resemble these capabilities. Success requires understanding what patterns in training data correlate with desired outputs, then crafting prompts that activate those patterns. You‚Äôre not teaching the model to compute; you‚Äôre navigating its compressed representation of how computation appears in text. The constraint is the teacher: when the model fails, the failure reveals what patterns are missing or weak in its training data."
  },
  {
    "objectID": "m06-llms/prompt-tuning-exercise.html#the-challenge",
    "href": "m06-llms/prompt-tuning-exercise.html#the-challenge",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "LLMs optimize for fluency and pattern matching, not mathematical computation. When asked to generate random numbers, they typically produce sequences that fail statistical tests for randomness‚Äînumbers cluster, patterns emerge, distributions skew. The model has seen random numbers in training data, but it has no internal representation of what randomness means. It cannot compute true randomness; it can only recall patterns that look random.\nYet with careful prompting, you can bias the model toward outputs that pass statistical validation. The task: craft a prompt that makes an LLM generate at least 100 normally distributed random numbers (comma-separated, like 0.5,-1.2,0.8,...) that pass a Kolmogorov-Smirnov test with p-value greater than 0.20. Use Gemma3 27B on Google AI Studio or OpenRouter. No external tools‚Äîthe model must generate the numbers directly from its internal patterns.\nThe constraint forces you to think about what patterns in the training data correlate with valid statistical outputs. Asking for ‚Äújust the numbers‚Äù eliminates extraneous tokens that disrupt parsing. Requesting the model to ‚Äúthink about how normal distributions work‚Äù before generating may activate patterns where reasoning preceded statistically valid sequences. You‚Äôre not teaching the model statistics; you‚Äôre activating pre-existing patterns where statistical reasoning preceded appropriate outputs.\n\n\nimport marimo as mo\nimport altair as alt\nimport pandas as pd\nimport scipy.stats as stats\nimport numpy as np\ntext_area = mo.ui.text_area(placeholder = \"Enter numbers separated by commas\", value = \"1,2,3,4,5,6,7,8,9,10\")\nbutton = mo.ui.button(\"Runt test\")\n\nmo.vstack([text_area, button])\ntry:\n    numbers = np.array([float(num.strip()) for num in text_area.value.split(\",\")])\n    if len(numbers) &gt;= 100:\n        # KS test\n        pval = stats.kstest(numbers, stats.norm(loc=0.0, scale=1.0).cdf)[1]\n        test_result = \"The numbers are normal distributed (p-value = {:.2f})\".format(pval) if pval &gt; 0.20 else \"The numbers are not normal distributed (p-value = {:.2f})\".format(pval)\n        message = mo.callout(test_result, kind = \"success\" if pval &gt; 0.20 else \"danger\")\n    else:\n        message = mo.callout(\"The number of samples is too small. Need at least 100 samples.\", kind = \"warn\")\n\n    # Convert the numbers to a DataFrame for Altair\n    df = pd.DataFrame({'value': numbers})\n\n    # Create an Altair histogram\n    fig = alt.Chart(df).mark_bar().encode(\n        x=alt.X('value:Q', bin=alt.Bin(maxbins=30)),\n        y='count()'\n    ).properties(\n        title='Histogram of Values'\n    )\n\n    mo.hstack([fig, message])\nexcept:\n    message = mo.callout(\"Parse failed. Please check if your input follows the specified format.\", kind = \"danger\")\n    fig = None\n\nmo.hstack([fig, message]) if fig is not None else message"
  },
  {
    "objectID": "m06-llms/prompt-tuning-exercise.html#generating-structured-visual-code",
    "href": "m06-llms/prompt-tuning-exercise.html#generating-structured-visual-code",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "LLMs can generate syntactically valid code in languages they‚Äôve seen during training‚Äîincluding SVG, the XML-based vector graphics format. The challenge: prompt Gemma3 27B to generate an SVG diagram of a neural network with specific structural requirements. The network must have same-colored neurons within each layer, connections between all neurons across adjacent layers, and labels for ‚ÄúInput layer,‚Äù ‚ÄúHidden layer,‚Äù and ‚ÄúOutput layer.‚Äù\nTest your prompt on Google AI Studio or OpenRouter, then paste the generated SVG code into SVG Viewer to visualize the result. The model has seen countless SVG examples during training, but it has no internal representation of what a neural network diagram ‚Äúshould‚Äù look like. It can only pattern match against examples where similar prompts preceded valid SVG structures."
  },
  {
    "objectID": "m06-llms/prompt-tuning-exercise.html#the-takeaway",
    "href": "m06-llms/prompt-tuning-exercise.html#the-takeaway",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "These exercises expose the boundary between pattern matching and computation. LLMs cannot perform true mathematical operations or generate genuine randomness‚Äîthey can only recall patterns that resemble these capabilities. Success requires understanding what patterns in training data correlate with desired outputs, then crafting prompts that activate those patterns. You‚Äôre not teaching the model to compute; you‚Äôre navigating its compressed representation of how computation appears in text. The constraint is the teacher: when the model fails, the failure reveals what patterns are missing or weak in its training data."
  },
  {
    "objectID": "m06-llms/bert.html",
    "href": "m06-llms/bert.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "We learned about ELMo‚Äôs approach to the problem of polysemy using bidirectional LSTMs. BERT builds upon its bidirectional approach by using a self-attention mechanism in transformers. BERT has become the leading transformer model for natural language processing tasks like question answering and text classification. Its effectiveness led Google to incorporate it into their search engine to improve query understanding. In this section, we will explore BERT‚Äôs architecture and mechanisms.\n```cgwejstc https://cdn.botpenguin.com/assets/website/BERT_c35709b509.webp :name: bert_mlm :alt: BERT MLM :width: 50% :align: center\n\n```{admonition} BERT in interactive mode:\n:class: tip\n\n[Here is a demo notebook for BERT](https://static.marimo.app/static/bert-ux7g)\n\nTo run the notebook, download the notebook as a `.py` file and run it with:\n\n&gt; marimo edit --sandbox bert.py\n\nYou will need to install `marimo` and `uv` to run the notebook. But other packages will be installed automatically in uv's virtual environment.\n\n\nBERT consists of a stack of encoder transformer layers. Each layer is composed of a self-attention mechanism, a feed-forward neural network, and layer normalization, wired together with residual connections. The output of each layer is fed into the next layer, and as we go through the layers, the token embeddings get more and more contextualized, reflecting the context more and more, thanks to the self-attention mechanism.\n```cgwejstc https://www.researchgate.net/publication/372906672/figure/fig2/AS:11431281179224913@1691164535766/BERT-model-architecture.ppm :name: bert_architecture :alt: BERT architecture :width: 80% :align: center\nBERT consists of a stack of encoder transformer layers. The position embeddings are added to the token embeddings to provide the model with information about the position of the tokens in the sequence.\n\n```{admonition} Which layer of BERT should we use?\n:class: tip\n\nBERT internally generates multiple hierarchical representations of the input sentence. The higher layers of the model capture more abstract and context-sensitive information, while the lower layers capture more local and surface-level information. Which layer to use depends on the task. For example, if we want to do text classification, we should use the output of the last layer. If we are interested in word-level representations, we should use the output of the first layer.\n\n\n\nBERT uses several special tokens to represent the input sentence.\n\n[CLS] is used to represent the start of the sentence.\n[SEP] is used to represent the end of the sentence.\n[MASK] is used to represent the masked words.\n[UNK] is used to represent the unknown words.\n\nFor example, the sentence ‚ÄúThe cat sat on the mat. It then went to sleep.‚Äù is represented as ‚Äú[CLS] The cat sat on the mat [SEP] It then went to sleep [SEP]‚Äù.\nIn BERT, [CLS] token is used to classify the input sentences, as we will see later. As a result, the model learns to encode a summary of the input sentence into the [CLS] token, which is particularly useful when we want the embedding of the whole input text, instead of the token level embeddings. {footcite}reimers2019sentence\n\n\n\nBERT uses position and segment embeddings to provide the model with information about the position of the tokens in the sequence.\n\nPosition embeddings are used to provide the model with information about the position of the tokens in the sequence. Unlike the sinusoidal position embedding used in the original transformer paper {footcite}vaswani2017attention, BERT uses learnable position embeddings.\nThe segment embeddings are used to distinguish the sentences in the input. For example, for the sentence ‚ÄúThe cat sat on the mat. It then went to sleep.‚Äù, the tokens in the first sentence are added with segment embedding 0, and the tokens in the second sentence are added with segment embedding 1. These segmend embeddings are also learned during the pre-training process.\n\n```cgwejstc https://lh3.googleusercontent.com/stK9CWIWiSuF_aq75q7_6wUqyqfePKzeLxqVet9IVNqrcyJqqg9hXkhuFXBXXbIjaGY15gSF9Yr7kyjceVXs5HbDMpmkhet49fhbtLsm9-4E4iCYckzGTsYSxOqRaVGNTkkhWykg :name: bert_position_segment_embeddings :alt: BERT position and segment embeddings :width: 80% :align: center\nPosition and segment embeddings in BERT. Position embeddings, which are learnable, are added to the token embeddings. Segment embeddings indicate the sentence that the token belongs to (e.g., E_A and E_B).\n\n```{tip}\n:class: tip\n\nPosition embeddings can be either absolute or relative:\n\nAbsolute position embeddings (like in BERT) directly encode the position of each token as a fixed index (1st, 2nd, 3rd position etc). Each position gets its own unique embedding vector that is learned during training.\n\nRelative position embeddings (like sinusoidal embeddings in the original Transformer) encode the relative distance between tokens rather than their absolute positions. For example, they can encode that token A is 2 positions away from token B, regardless of their absolute positions in the sequence. This makes them more flexible for handling sequences of varying lengths.\n\nFor interested readers, you can read more about the difference between absolute and relative position embeddings in [The Use Case for Relative Position Embeddings ‚Äì Ofir Press](https://ofir.io/The-Use-Case-for-Relative-Position-Embeddings/) and [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409).\n\n\nA key aspect of BERT is its pre-training process, which involves two main objectives:\n\nMasked Language Modeling (MLM)\nNext Sentence Prediction (NSP)\n\nBoth objectives are designed to learn the language structure, such as the relationship between words and sentences.\n\n\nIn MLM, the model is trained to predict the original words that are masked in the input sentence. The masked words are replaced with a special token, [MASK], and the model is trained to predict the original words. For example, the sentence ‚ÄúThe cat [MASK] on the mat‚Äù is transformed into ‚ÄúThe cat [MASK] on the mat‚Äù. The model is trained to predict the original word ‚Äúsat‚Äù in the sentence.\n```cgwejstc https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/MLM.png :name: bert_mlm :alt: BERT MLM :width: 80% :align: center\nMasked Language Modeling (MLM). A token is randomly masked and the model is trained to predict the original word.\n\nTo generate training data for MLM, BERT randomly masks 15% of the tokens in each sequence. However, the masking process is not as straightforward as simply replacing words with [MASK] tokens. For the 15% of tokens chosen for masking:\n\n- 80% of the time, replace the word with the [MASK] token\n  - Example: \"the cat sat on the mat\" ‚Üí \"the cat [MASK] on the mat\"\n\n- 10% of the time, replace the word with a random word\n  - Example: \"the cat sat on the mat\" ‚Üí \"the cat dog on the mat\"\n\n- 10% of the time, keep the word unchanged\n  - Example: \"the cat sat on the mat\" ‚Üí \"the cat sat on the mat\"\n\nThe model must predict the original token for all selected positions, regardless of whether they were masked, replaced, or left unchanged. This helps prevent the model from simply learning to detect replaced tokens.\n\nDuring training, the model processes the modified input sequence through its transformer layers and predicts the original token at each masked position using the contextual representations.\n\n```{tip}\nWhile replacing words with random tokens or leaving them unchanged may seem counterintuitive, research has shown this approach is effective {footcite}`raffel2020exploring`. It has become an essential component of BERT's pre-training process.\n\n\n\n```cgwejstc https://amitness.com/posts/images/bert-nsp.png :name: bert_nsp :alt: BERT NSP :width: 80% :align: center\nNext Sentence Prediction (NSP). The model is trained to predict whether two sentences are consecutive or not.\n\nNext Sentence Prediction (NSP) trains BERT to understand relationships between sentences. The model learns to predict whether two sentences naturally follow each other in text. During training, half of the sentence pairs are consecutive sentences from documents (labeled as IsNext), while the other half are random sentence pairs (labeled as NotNext).\n\nThe input format uses special tokens to structure the sentence pairs: a [CLS] token at the start, the first sentence, a [SEP] token, the second sentence, and a final [SEP] token. For instance:\n\n$$\n\\text{``[CLS] }\\underbrace{\\text{I went to the store}}_{\\text{Sentence 1}}\\text{ [SEP] }\\underbrace{\\text{They were out of milk}}_{\\text{Sentence 2}}\\text{ [SEP]}\".\n$$\n\nBERT uses the final hidden state of the [CLS] token to classify whether the sentences are consecutive or not. This helps the model develop a broader understanding of language context and relationships between sentences.\n\nThese two objectives help BERT learn the structure of language, such as the relationship between words and sentences.\n\n\n## Fine-tuning\n\nA powerful aspect of BERT is its ability to be fine-tuned on a wide range of tasks with minimal changes to the model architecture. This is achieved through transfer learning, where the pre-trained BERT model is used as a starting point for specific tasks.\n\nConsider a hospital that wants to classify patient reviews. Due to privacy concerns, collecting enough data to train a deep learning model from scratch would be difficult. This is where BERT shines - since it's already pre-trained on vast amounts of text data and understands language structure, it can be fine-tuned effectively even with a small dataset of patient reviews. The pre-trained BERT model can be adapted to this specific classification task with only minor architectural changes.\n\n```{tip}\n:class: tip\n\nYou can find many fine-tuned and pre-trained models for various tasks by searching the [Hugging Face model hub](https://huggingface.co/models), with the keyword \"BERT\".\n\n\n\n\n\n**RoBERTa (Robustly Optimized BERT Approach)* {footcite}liu2019roberta* improved upon BERT through several optimizations: removing the Next Sentence Prediction task, using dynamic masking that changes the mask patterns across training epochs, training with larger batches, and using a larger dataset. These changes led to significant performance improvements while maintaining BERT‚Äôs core architecture.\nDistilBERT {footcite}sanh2019distilbert focused on making BERT more efficient through knowledge distillation, where a smaller student model learns from the larger BERT teacher model. It achieves 95% of BERT‚Äôs performance while being 40% smaller and 60% faster, making it more suitable for resource-constrained environments and real-world applications.\nALBERT {footcite}lan2019albert introduced parameter reduction techniques to address BERT‚Äôs memory limitations. It uses factorized embedding parameterization and cross-layer parameter sharing to dramatically reduce parameters while maintaining performance. ALBERT also replaced Next Sentence Prediction with Sentence Order Prediction, where the model must determine if two consecutive sentences are in the correct order.\nDomain-specific BERT models have been trained on specialized corpora to better handle specific fields. Examples include BioBERT {footcite}lee2020biobert for biomedical text, SciBERT {footcite}reimers2019sentence for scientific papers, and FinBERT {footcite}araci2019finbert for financial documents. These models demonstrate superior performance in their respective domains compared to the general-purpose BERT.\nMultilingual BERT (mBERT) {footcite}liu2019roberta was trained on Wikipedia data from 104 languages, using a shared vocabulary across all languages. Despite not having explicit cross-lingual objectives during training, mBERT shows remarkable zero-shot cross-lingual transfer abilities, allowing it to perform tasks in languages it wasn‚Äôt explicitly aligned on. This has made it a valuable resource for low-resource languages and cross-lingual applications.\n\n\n\nLet us load a pre-trained BERT model and see how it works using a sense disambiguation task. The sense disambiguation task is a task that involves identifying the correct sense of a word in a sentence. For example, given a sentence with word ‚Äúapple‚Äù, we need to identify whether it refers to the fruit or the technology company.\nLet us first load the necessary libraries.\nngfhipdlqzb python import pandas as pd import numpy as np import sys import torch import transformers import matplotlib.pyplot as plt from tqdm import tqdm from sklearn.decomposition import PCA from bokeh.plotting import figure, show from bokeh.io import output_notebook from bokeh.models import ColumnDataSource, HoverTool\nWe will use CoarseWSD-20. The dataset contains sentences with polysemous words and their sense labels. We will see how to use BERT to disambiguate the word senses. Read the README for more details.\n```ngfhipdlqzb python def load_data(focal_word, is_train, n_samples=100): data_type = ‚Äútrain‚Äù if is_train else ‚Äútest‚Äù data_file = f‚Äùhttps://raw.githubusercontent.com/danlou/bert-disambiguation/master/data/CoarseWSD-20/{focal_word}/{data_type}.data.txt‚Äù label_file = f‚Äùhttps://raw.githubusercontent.com/danlou/bert-disambiguation/master/data/CoarseWSD-20/{focal_word}/{data_type}.gold.txt‚Äù\ndata_table = pd.read_csv(\n    data_file,\n    sep=\"\\t\",\n    header=None,\n    dtype={\"word_pos\": int, \"sentence\": str},\n    names=[\"word_pos\", \"sentence\"],\n)\nlabel_table = pd.read_csv(\n    label_file,\n    sep=\"\\t\",\n    header=None,\n    dtype={\"label\": int},\n    names=[\"label\"],\n)\ncombined_table = pd.concat([data_table, label_table], axis=1)\nreturn combined_table.sample(n_samples)\nfocal_word = ‚Äúapple‚Äù\ntrain_data = load_data(focal_word, is_train=True)\ntrain_data.head(10)\n\nWe will use transformers library developed by Hugging Face to define the BERT model. To use the model, we will need:\n\n- BERT tokenizer that converts the text into tokens.\n- BERT model that computes the embeddings of the tokens.\n\nWe will use the bert-base-uncased model and tokenizer. Let's define the model and tokenizer.\n\n```{code-cell} python\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = transformers.AutoModel.from_pretrained(\"bert-base-uncased\")\nmodel.eval() # set the model to evaluation mode\nprint(model) # Print the model architecture\nThis prints the model architecture, which shows:\n\nBertEmbeddings layer that converts tokens into embeddings using:\n\nWord embeddings (30522 vocab size, 768 dimensions)\nPosition embeddings (512 positions, 768 dimensions)\nToken type embeddings (2 types, 768 dimensions)\nLayer normalization and dropout\n\nBertEncoder with 12 identical BertLayers, each containing:\n\nSelf-attention mechanism with query/key/value projections\nIntermediate layer with GELU activation\nOutput layer with layer normalization\n\nBertPooler that processes the [CLS] token embedding with:\n\nDense layer (768 dimensions)\nTanh activation\n\n\nAll layers maintain the 768-dimensional hidden size, except the intermediate layer which expands to 3072 dimensions.\nWith BERT, we need to prepare text in ways that BERT can understand. Specifically, we prepend it with [CLS] and append [SEP]. We will then convert the text to a tensor of token ids, which is ready to be fed into the model.\n```ngfhipdlqzb python def prepare_text(text): text = ‚Äú[CLS]‚Äù + text + ‚Äù [SEP]‚Äù tokenized_text = tokenizer.tokenize(text) indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\nsegments_ids = torch.ones((1, len(indexed_tokens)), dtype=torch.long)\ntokens_tensor = torch.tensor([indexed_tokens])\nsegments_tensor = segments_ids.clone()\nreturn tokenized_text, tokens_tensor, segments_tensor\n\nLet's get the BERT embeddings for the sentence \"Bank is located in the city of London\".\n\n```{code-cell} python\ntext = \"Bank is located in the city of London\"\ntokenized_text, tokens_tensor, segments_tensor = prepare_text(text)\nThis produces the following output. Tokenized text: ngfhipdlqzb python print(tokenized_text) Token IDs: ngfhipdlqzb python print(tokens_tensor) Segment IDs: ngfhipdlqzb python print(segments_tensor)\nThen, let‚Äôs get the BERT embeddings for each token.\n```ngfhipdlqzb python # Configure model to return hidden states model.config.output_hidden_states = True\noutputs = model(tokens_tensor, segments_tensor)\n\nThe output includes `loss`, `logits`, and `hidden_states`. We will use `hidden_states`, which contains the embeddings of the tokens.\n\n```{code-cell} python\nhidden_states = outputs.hidden_states\n\nprint(\"how many layers? \", len(hidden_states))\nprint(\"Shape? \", hidden_states[0].shape)\nThe hidden states are a list of 13 tensors, where each tensor is of shape (batch_size, sequence_length, hidden_size). The first tensor is the input embeddings, and the subsequent tensors are the hidden states of the BERT layers.\nSo, we have 13 choice of hidden states. Deep layers close to the output capture the context of the word from the previous layers.\nHere we will take the average over the last four hidden states for each token.\n```ngfhipdlqzb python last_four_layers = hidden_states[-4:] # Stack the layers and then calculate mean stacked_layers = torch.stack(last_four_layers) emb = torch.mean(stacked_layers, dim=0)\nprint(emb.shape)\nemb is of shape (sequence_length, hidden_size). Let us summarize the embeddings of the tokens into a function.\n\n```{code-cell} python\ndef get_bert_embeddings(text):\n    tokenized_text, tokens_tensor, segments_tensor = prepare_text(text)\n    outputs = model(tokens_tensor, segments_tensor)\n    hidden_states = outputs[2]  # Access hidden states from tuple output\n    # Stack the last 4 layers then take mean\n    stacked_layers = torch.stack(hidden_states[-4:])\n    emb = torch.mean(stacked_layers, dim=0)\n    return emb, tokenized_text\nNow, let us embed text and get the embeddings of the focal token.\n```ngfhipdlqzb python labels = [] # label emb = [] # embedding sentences = [] # sentence\ndef get_focal_token_embedding(text, focal_word_idx): emb, tokenized_text = get_bert_embeddings(text) return emb[0][focal_word_idx] # Access first batch dimension\nfor index, row in train_data.iterrows(): text = row[‚Äúsentence‚Äù] focal_word_idx = row[‚Äúword_pos‚Äù] _emb = get_focal_token_embedding(text, focal_word_idx) labels.append(row[‚Äúlabel‚Äù]) emb.append(_emb) sentences.append(text)\nFinally, let us visualize the embeddings using PCA.\n\n```{code-cell} python\n:tags: [hide-input]\n\n# Convert list of tensors to numpy array\nemb_numpy = torch.stack(emb).detach().numpy()\n\npca = PCA(n_components=2, random_state=42)\nxy = pca.fit_transform(emb_numpy)\n\noutput_notebook()\n\n# Create data source for Bokeh\nsource = ColumnDataSource(data=dict(\n    x=xy[:, 0],\n    y=xy[:, 1],\n    label=labels,\n    sentence=sentences\n))\n\n# Create Bokeh figure\np = figure(title=\"Word Embeddings Visualization\", x_axis_label=\"PCA 1\", y_axis_label=\"PCA 2\",\n           width=700, height=500)\n\n# Add hover tool\nhover = HoverTool(tooltips=[\n    ('Label', '@label'),\n    ('Sentence', '@sentence')\n])\np.add_tools(hover)\n\n# Create color map for labels\nimport seaborn as sns\n\nunique_labels = list(set(labels))\ncolor_map = sns.color_palette().as_hex()[0:len(unique_labels)]\nsource.data['color'] = [color_map[label] for label in labels]\n\n# Add scatter plot\np.scatter('x', 'y', size=12, line_color=\"DarkSlateGrey\", line_width=2,\n         fill_color='color', source=source)\n\nshow(p)\n\n\n\nWe have used the last 4 layers of BERT to generate the embeddings of the tokens. Now, let‚Äôs use the last k = 1, 2, 3 layers of BERT to generate the embeddings of the tokens. Then plot the embeddings using PCA.\n\n\n\n:style: unsrt"
  },
  {
    "objectID": "m06-llms/bert.html#architecture",
    "href": "m06-llms/bert.html#architecture",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "BERT consists of a stack of encoder transformer layers. Each layer is composed of a self-attention mechanism, a feed-forward neural network, and layer normalization, wired together with residual connections. The output of each layer is fed into the next layer, and as we go through the layers, the token embeddings get more and more contextualized, reflecting the context more and more, thanks to the self-attention mechanism.\n```cgwejstc https://www.researchgate.net/publication/372906672/figure/fig2/AS:11431281179224913@1691164535766/BERT-model-architecture.ppm :name: bert_architecture :alt: BERT architecture :width: 80% :align: center\nBERT consists of a stack of encoder transformer layers. The position embeddings are added to the token embeddings to provide the model with information about the position of the tokens in the sequence.\n\n```{admonition} Which layer of BERT should we use?\n:class: tip\n\nBERT internally generates multiple hierarchical representations of the input sentence. The higher layers of the model capture more abstract and context-sensitive information, while the lower layers capture more local and surface-level information. Which layer to use depends on the task. For example, if we want to do text classification, we should use the output of the last layer. If we are interested in word-level representations, we should use the output of the first layer."
  },
  {
    "objectID": "m06-llms/bert.html#special-tokens",
    "href": "m06-llms/bert.html#special-tokens",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "BERT uses several special tokens to represent the input sentence.\n\n[CLS] is used to represent the start of the sentence.\n[SEP] is used to represent the end of the sentence.\n[MASK] is used to represent the masked words.\n[UNK] is used to represent the unknown words.\n\nFor example, the sentence ‚ÄúThe cat sat on the mat. It then went to sleep.‚Äù is represented as ‚Äú[CLS] The cat sat on the mat [SEP] It then went to sleep [SEP]‚Äù.\nIn BERT, [CLS] token is used to classify the input sentences, as we will see later. As a result, the model learns to encode a summary of the input sentence into the [CLS] token, which is particularly useful when we want the embedding of the whole input text, instead of the token level embeddings. {footcite}reimers2019sentence"
  },
  {
    "objectID": "m06-llms/bert.html#position-and-segment-embeddings",
    "href": "m06-llms/bert.html#position-and-segment-embeddings",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "BERT uses position and segment embeddings to provide the model with information about the position of the tokens in the sequence.\n\nPosition embeddings are used to provide the model with information about the position of the tokens in the sequence. Unlike the sinusoidal position embedding used in the original transformer paper {footcite}vaswani2017attention, BERT uses learnable position embeddings.\nThe segment embeddings are used to distinguish the sentences in the input. For example, for the sentence ‚ÄúThe cat sat on the mat. It then went to sleep.‚Äù, the tokens in the first sentence are added with segment embedding 0, and the tokens in the second sentence are added with segment embedding 1. These segmend embeddings are also learned during the pre-training process.\n\n```cgwejstc https://lh3.googleusercontent.com/stK9CWIWiSuF_aq75q7_6wUqyqfePKzeLxqVet9IVNqrcyJqqg9hXkhuFXBXXbIjaGY15gSF9Yr7kyjceVXs5HbDMpmkhet49fhbtLsm9-4E4iCYckzGTsYSxOqRaVGNTkkhWykg :name: bert_position_segment_embeddings :alt: BERT position and segment embeddings :width: 80% :align: center\nPosition and segment embeddings in BERT. Position embeddings, which are learnable, are added to the token embeddings. Segment embeddings indicate the sentence that the token belongs to (e.g., E_A and E_B).\n\n```{tip}\n:class: tip\n\nPosition embeddings can be either absolute or relative:\n\nAbsolute position embeddings (like in BERT) directly encode the position of each token as a fixed index (1st, 2nd, 3rd position etc). Each position gets its own unique embedding vector that is learned during training.\n\nRelative position embeddings (like sinusoidal embeddings in the original Transformer) encode the relative distance between tokens rather than their absolute positions. For example, they can encode that token A is 2 positions away from token B, regardless of their absolute positions in the sequence. This makes them more flexible for handling sequences of varying lengths.\n\nFor interested readers, you can read more about the difference between absolute and relative position embeddings in [The Use Case for Relative Position Embeddings ‚Äì Ofir Press](https://ofir.io/The-Use-Case-for-Relative-Position-Embeddings/) and [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409).\n\n\nA key aspect of BERT is its pre-training process, which involves two main objectives:\n\nMasked Language Modeling (MLM)\nNext Sentence Prediction (NSP)\n\nBoth objectives are designed to learn the language structure, such as the relationship between words and sentences.\n\n\nIn MLM, the model is trained to predict the original words that are masked in the input sentence. The masked words are replaced with a special token, [MASK], and the model is trained to predict the original words. For example, the sentence ‚ÄúThe cat [MASK] on the mat‚Äù is transformed into ‚ÄúThe cat [MASK] on the mat‚Äù. The model is trained to predict the original word ‚Äúsat‚Äù in the sentence.\n```cgwejstc https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/MLM.png :name: bert_mlm :alt: BERT MLM :width: 80% :align: center\nMasked Language Modeling (MLM). A token is randomly masked and the model is trained to predict the original word.\n\nTo generate training data for MLM, BERT randomly masks 15% of the tokens in each sequence. However, the masking process is not as straightforward as simply replacing words with [MASK] tokens. For the 15% of tokens chosen for masking:\n\n- 80% of the time, replace the word with the [MASK] token\n  - Example: \"the cat sat on the mat\" ‚Üí \"the cat [MASK] on the mat\"\n\n- 10% of the time, replace the word with a random word\n  - Example: \"the cat sat on the mat\" ‚Üí \"the cat dog on the mat\"\n\n- 10% of the time, keep the word unchanged\n  - Example: \"the cat sat on the mat\" ‚Üí \"the cat sat on the mat\"\n\nThe model must predict the original token for all selected positions, regardless of whether they were masked, replaced, or left unchanged. This helps prevent the model from simply learning to detect replaced tokens.\n\nDuring training, the model processes the modified input sequence through its transformer layers and predicts the original token at each masked position using the contextual representations.\n\n```{tip}\nWhile replacing words with random tokens or leaving them unchanged may seem counterintuitive, research has shown this approach is effective {footcite}`raffel2020exploring`. It has become an essential component of BERT's pre-training process.\n\n\n\n```cgwejstc https://amitness.com/posts/images/bert-nsp.png :name: bert_nsp :alt: BERT NSP :width: 80% :align: center\nNext Sentence Prediction (NSP). The model is trained to predict whether two sentences are consecutive or not.\n\nNext Sentence Prediction (NSP) trains BERT to understand relationships between sentences. The model learns to predict whether two sentences naturally follow each other in text. During training, half of the sentence pairs are consecutive sentences from documents (labeled as IsNext), while the other half are random sentence pairs (labeled as NotNext).\n\nThe input format uses special tokens to structure the sentence pairs: a [CLS] token at the start, the first sentence, a [SEP] token, the second sentence, and a final [SEP] token. For instance:\n\n$$\n\\text{``[CLS] }\\underbrace{\\text{I went to the store}}_{\\text{Sentence 1}}\\text{ [SEP] }\\underbrace{\\text{They were out of milk}}_{\\text{Sentence 2}}\\text{ [SEP]}\".\n$$\n\nBERT uses the final hidden state of the [CLS] token to classify whether the sentences are consecutive or not. This helps the model develop a broader understanding of language context and relationships between sentences.\n\nThese two objectives help BERT learn the structure of language, such as the relationship between words and sentences.\n\n\n## Fine-tuning\n\nA powerful aspect of BERT is its ability to be fine-tuned on a wide range of tasks with minimal changes to the model architecture. This is achieved through transfer learning, where the pre-trained BERT model is used as a starting point for specific tasks.\n\nConsider a hospital that wants to classify patient reviews. Due to privacy concerns, collecting enough data to train a deep learning model from scratch would be difficult. This is where BERT shines - since it's already pre-trained on vast amounts of text data and understands language structure, it can be fine-tuned effectively even with a small dataset of patient reviews. The pre-trained BERT model can be adapted to this specific classification task with only minor architectural changes.\n\n```{tip}\n:class: tip\n\nYou can find many fine-tuned and pre-trained models for various tasks by searching the [Hugging Face model hub](https://huggingface.co/models), with the keyword \"BERT\"."
  },
  {
    "objectID": "m06-llms/bert.html#variants-and-improvements",
    "href": "m06-llms/bert.html#variants-and-improvements",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "**RoBERTa (Robustly Optimized BERT Approach)* {footcite}liu2019roberta* improved upon BERT through several optimizations: removing the Next Sentence Prediction task, using dynamic masking that changes the mask patterns across training epochs, training with larger batches, and using a larger dataset. These changes led to significant performance improvements while maintaining BERT‚Äôs core architecture.\nDistilBERT {footcite}sanh2019distilbert focused on making BERT more efficient through knowledge distillation, where a smaller student model learns from the larger BERT teacher model. It achieves 95% of BERT‚Äôs performance while being 40% smaller and 60% faster, making it more suitable for resource-constrained environments and real-world applications.\nALBERT {footcite}lan2019albert introduced parameter reduction techniques to address BERT‚Äôs memory limitations. It uses factorized embedding parameterization and cross-layer parameter sharing to dramatically reduce parameters while maintaining performance. ALBERT also replaced Next Sentence Prediction with Sentence Order Prediction, where the model must determine if two consecutive sentences are in the correct order.\nDomain-specific BERT models have been trained on specialized corpora to better handle specific fields. Examples include BioBERT {footcite}lee2020biobert for biomedical text, SciBERT {footcite}reimers2019sentence for scientific papers, and FinBERT {footcite}araci2019finbert for financial documents. These models demonstrate superior performance in their respective domains compared to the general-purpose BERT.\nMultilingual BERT (mBERT) {footcite}liu2019roberta was trained on Wikipedia data from 104 languages, using a shared vocabulary across all languages. Despite not having explicit cross-lingual objectives during training, mBERT shows remarkable zero-shot cross-lingual transfer abilities, allowing it to perform tasks in languages it wasn‚Äôt explicitly aligned on. This has made it a valuable resource for low-resource languages and cross-lingual applications."
  },
  {
    "objectID": "m06-llms/bert.html#hands-on",
    "href": "m06-llms/bert.html#hands-on",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Let us load a pre-trained BERT model and see how it works using a sense disambiguation task. The sense disambiguation task is a task that involves identifying the correct sense of a word in a sentence. For example, given a sentence with word ‚Äúapple‚Äù, we need to identify whether it refers to the fruit or the technology company.\nLet us first load the necessary libraries.\nngfhipdlqzb python import pandas as pd import numpy as np import sys import torch import transformers import matplotlib.pyplot as plt from tqdm import tqdm from sklearn.decomposition import PCA from bokeh.plotting import figure, show from bokeh.io import output_notebook from bokeh.models import ColumnDataSource, HoverTool\nWe will use CoarseWSD-20. The dataset contains sentences with polysemous words and their sense labels. We will see how to use BERT to disambiguate the word senses. Read the README for more details.\n```ngfhipdlqzb python def load_data(focal_word, is_train, n_samples=100): data_type = ‚Äútrain‚Äù if is_train else ‚Äútest‚Äù data_file = f‚Äùhttps://raw.githubusercontent.com/danlou/bert-disambiguation/master/data/CoarseWSD-20/{focal_word}/{data_type}.data.txt‚Äù label_file = f‚Äùhttps://raw.githubusercontent.com/danlou/bert-disambiguation/master/data/CoarseWSD-20/{focal_word}/{data_type}.gold.txt‚Äù\ndata_table = pd.read_csv(\n    data_file,\n    sep=\"\\t\",\n    header=None,\n    dtype={\"word_pos\": int, \"sentence\": str},\n    names=[\"word_pos\", \"sentence\"],\n)\nlabel_table = pd.read_csv(\n    label_file,\n    sep=\"\\t\",\n    header=None,\n    dtype={\"label\": int},\n    names=[\"label\"],\n)\ncombined_table = pd.concat([data_table, label_table], axis=1)\nreturn combined_table.sample(n_samples)\nfocal_word = ‚Äúapple‚Äù\ntrain_data = load_data(focal_word, is_train=True)\ntrain_data.head(10)\n\nWe will use transformers library developed by Hugging Face to define the BERT model. To use the model, we will need:\n\n- BERT tokenizer that converts the text into tokens.\n- BERT model that computes the embeddings of the tokens.\n\nWe will use the bert-base-uncased model and tokenizer. Let's define the model and tokenizer.\n\n```{code-cell} python\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = transformers.AutoModel.from_pretrained(\"bert-base-uncased\")\nmodel.eval() # set the model to evaluation mode\nprint(model) # Print the model architecture\nThis prints the model architecture, which shows:\n\nBertEmbeddings layer that converts tokens into embeddings using:\n\nWord embeddings (30522 vocab size, 768 dimensions)\nPosition embeddings (512 positions, 768 dimensions)\nToken type embeddings (2 types, 768 dimensions)\nLayer normalization and dropout\n\nBertEncoder with 12 identical BertLayers, each containing:\n\nSelf-attention mechanism with query/key/value projections\nIntermediate layer with GELU activation\nOutput layer with layer normalization\n\nBertPooler that processes the [CLS] token embedding with:\n\nDense layer (768 dimensions)\nTanh activation\n\n\nAll layers maintain the 768-dimensional hidden size, except the intermediate layer which expands to 3072 dimensions.\nWith BERT, we need to prepare text in ways that BERT can understand. Specifically, we prepend it with [CLS] and append [SEP]. We will then convert the text to a tensor of token ids, which is ready to be fed into the model.\n```ngfhipdlqzb python def prepare_text(text): text = ‚Äú[CLS]‚Äù + text + ‚Äù [SEP]‚Äù tokenized_text = tokenizer.tokenize(text) indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\nsegments_ids = torch.ones((1, len(indexed_tokens)), dtype=torch.long)\ntokens_tensor = torch.tensor([indexed_tokens])\nsegments_tensor = segments_ids.clone()\nreturn tokenized_text, tokens_tensor, segments_tensor\n\nLet's get the BERT embeddings for the sentence \"Bank is located in the city of London\".\n\n```{code-cell} python\ntext = \"Bank is located in the city of London\"\ntokenized_text, tokens_tensor, segments_tensor = prepare_text(text)\nThis produces the following output. Tokenized text: ngfhipdlqzb python print(tokenized_text) Token IDs: ngfhipdlqzb python print(tokens_tensor) Segment IDs: ngfhipdlqzb python print(segments_tensor)\nThen, let‚Äôs get the BERT embeddings for each token.\n```ngfhipdlqzb python # Configure model to return hidden states model.config.output_hidden_states = True\noutputs = model(tokens_tensor, segments_tensor)\n\nThe output includes `loss`, `logits`, and `hidden_states`. We will use `hidden_states`, which contains the embeddings of the tokens.\n\n```{code-cell} python\nhidden_states = outputs.hidden_states\n\nprint(\"how many layers? \", len(hidden_states))\nprint(\"Shape? \", hidden_states[0].shape)\nThe hidden states are a list of 13 tensors, where each tensor is of shape (batch_size, sequence_length, hidden_size). The first tensor is the input embeddings, and the subsequent tensors are the hidden states of the BERT layers.\nSo, we have 13 choice of hidden states. Deep layers close to the output capture the context of the word from the previous layers.\nHere we will take the average over the last four hidden states for each token.\n```ngfhipdlqzb python last_four_layers = hidden_states[-4:] # Stack the layers and then calculate mean stacked_layers = torch.stack(last_four_layers) emb = torch.mean(stacked_layers, dim=0)\nprint(emb.shape)\nemb is of shape (sequence_length, hidden_size). Let us summarize the embeddings of the tokens into a function.\n\n```{code-cell} python\ndef get_bert_embeddings(text):\n    tokenized_text, tokens_tensor, segments_tensor = prepare_text(text)\n    outputs = model(tokens_tensor, segments_tensor)\n    hidden_states = outputs[2]  # Access hidden states from tuple output\n    # Stack the last 4 layers then take mean\n    stacked_layers = torch.stack(hidden_states[-4:])\n    emb = torch.mean(stacked_layers, dim=0)\n    return emb, tokenized_text\nNow, let us embed text and get the embeddings of the focal token.\n```ngfhipdlqzb python labels = [] # label emb = [] # embedding sentences = [] # sentence\ndef get_focal_token_embedding(text, focal_word_idx): emb, tokenized_text = get_bert_embeddings(text) return emb[0][focal_word_idx] # Access first batch dimension\nfor index, row in train_data.iterrows(): text = row[‚Äúsentence‚Äù] focal_word_idx = row[‚Äúword_pos‚Äù] _emb = get_focal_token_embedding(text, focal_word_idx) labels.append(row[‚Äúlabel‚Äù]) emb.append(_emb) sentences.append(text)\nFinally, let us visualize the embeddings using PCA.\n\n```{code-cell} python\n:tags: [hide-input]\n\n# Convert list of tensors to numpy array\nemb_numpy = torch.stack(emb).detach().numpy()\n\npca = PCA(n_components=2, random_state=42)\nxy = pca.fit_transform(emb_numpy)\n\noutput_notebook()\n\n# Create data source for Bokeh\nsource = ColumnDataSource(data=dict(\n    x=xy[:, 0],\n    y=xy[:, 1],\n    label=labels,\n    sentence=sentences\n))\n\n# Create Bokeh figure\np = figure(title=\"Word Embeddings Visualization\", x_axis_label=\"PCA 1\", y_axis_label=\"PCA 2\",\n           width=700, height=500)\n\n# Add hover tool\nhover = HoverTool(tooltips=[\n    ('Label', '@label'),\n    ('Sentence', '@sentence')\n])\np.add_tools(hover)\n\n# Create color map for labels\nimport seaborn as sns\n\nunique_labels = list(set(labels))\ncolor_map = sns.color_palette().as_hex()[0:len(unique_labels)]\nsource.data['color'] = [color_map[label] for label in labels]\n\n# Add scatter plot\np.scatter('x', 'y', size=12, line_color=\"DarkSlateGrey\", line_width=2,\n         fill_color='color', source=source)\n\nshow(p)"
  },
  {
    "objectID": "m06-llms/bert.html#exercise",
    "href": "m06-llms/bert.html#exercise",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "We have used the last 4 layers of BERT to generate the embeddings of the tokens. Now, let‚Äôs use the last k = 1, 2, 3 layers of BERT to generate the embeddings of the tokens. Then plot the embeddings using PCA."
  },
  {
    "objectID": "m06-llms/bert.html#references",
    "href": "m06-llms/bert.html#references",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": ":style: unsrt"
  },
  {
    "objectID": "m05-images/archive/vgg.html",
    "href": "m05-images/archive/vgg.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "VGGNet {footcite}simonyan2014vg, introduced by Karen Simonyan and Andrew Zisserman from the Visual Geometry Group (VGG) at Oxford University, represents a significant milestone in the evolution of Convolutional Neural Networks (CNNs). At its core, VGGNet demonstrated that network depth is crucial for achieving superior performance in visual recognition tasks, a finding that would influence CNN design for years to come.\nHistorical Context: In 2014, when VGGNet secured victory at the ILSVRC challenge, the common belief was that deeper networks would be too difficult to train due to vanishing gradients and computational constraints. VGGNet's success challenged this assumption and paved the way for even deeper architectures like ResNet.\n\n\nVGGNet employs a systematic stack of convolutional layers using exclusively 3√ó3 filters with stride 1 and padding 1, interspersed with 2√ó2 max pooling layers with stride 2. This uniformity makes the architecture conceptually simple.\n```ofnzlmwp ../figs/vgg-architecture.jpg :width: 40% :align: center\nA schematic representation of VGG16 architecture showing the progression of spatial dimensions and feature channels through the network. The input is a 224√ó224√ó3 image, and the output is a 1000-dimensional vector for ImageNet classification. The image is taken from https://www.researchgate.net/profile/Max-Ferguson.\n\nVGG has multiple variants, and the most popular one is VGG16, which has 16 layers. The full architecture of VGG16 is as follows:\n\n* **Input**: [224 x 224] normalized, 3-channel color image (with color whitening, see section 3.2.1 in AlexNet article)\n* **Conv1_1**: Convolutional layer - [3 x 3] kernel x 64 channels + ReLU\n* **Conv1_2**: Convolutional layer - [3 x 3] kernel x 64 channels + ReLU\n* **P1**: Pooling layer - Max pooling, [2 x 2] kernel, stride = 2\n* **Conv2_1**: Convolutional layer - [3 x 3] kernel x 128 channels + ReLU\n* **Conv2_2**: Convolutional layer - [3 x 3] kernel x 128 channels + ReLU\n* **P2**: Pooling layer - Max pooling, [2 x 2] kernel, stride = 2\n* **Conv3_1**: Convolutional layer - [3 x 3] kernel x 256 channels + ReLU\n* **Conv3_2**: Convolutional layer - [3 x 3] kernel x 256 channels + ReLU\n* **Conv3_3**: Convolutional layer - [1 x 1] kernel x 256 channels + ReLU\n* **P3**: Pooling layer - Max pooling, [2 x 2] kernel, stride = 2\n* **Conv4_1**: Convolutional layer - [3 x 3] kernel x 512 channels + ReLU\n* **Conv4_2**: Convolutional layer - [3 x 3] kernel x 512 channels + ReLU\n* **Conv4_3**: Convolutional layer - [1 x 1] kernel x 512 channels + ReLU\n* **P4**: Pooling layer - Max pooling, [2 x 2] kernel, stride = 2\n* **Conv5_1**: Convolutional layer - [3 x 3] kernel x 512 channels + ReLU\n* **Conv5_2**: Convolutional layer - [3 x 3] kernel x 512 channels + ReLU\n* **Conv5_3**: Convolutional layer - [1 x 1] kernel x 512 channels + ReLU\n* **P5**: Pooling layer - Max pooling [7 x 7] kernel (aggressive downsampling at this stage)\n* (During training only: **Dropout**)\n* **FC14**: Fully connected layer - (7 x 7 x 512) ‚Üí 4096\n* (During training only: **Dropout**)\n* **FC15**: Fully connected layer - 4096 ‚Üí 4096\n* **FC16**: Fully connected layer - 4096 ‚Üí 1000\n* **Output**: 1000-dimensional vector probability distribution (output probability for each dimension) using softmax function\n\n\nThe network progressively increases the number of feature channels after each pooling operation, following a clear doubling pattern:\n\n$$\n\\text{channels} = \\{64 \\rightarrow 128 \\rightarrow 256 \\rightarrow 512 \\rightarrow 512\\}\n$$\n\nThe spatial dimensions of the feature maps decrease after each pooling layer, while the number of channels increases, creating a characteristic pyramid structure:\n\n$$\n\\text{spatial dimensions} = \\{224 \\rightarrow 112 \\rightarrow 56 \\rightarrow 28 \\rightarrow 14 \\rightarrow 7\\}\n$$\n\nDespite its apparent simplicity, VGG16 contains approximately 140 million parameters, with the majority concentrated in the first fully connected layer (approximately 102 million parameters). This large parameter count highlights an interesting trade-off in the architecture: while the convolutional layers follow a clean and efficient design, the fully connected layers remain computationally intensive. This issue is later resolved by global average pooling proposed by {footcite}`lin2013network`.\n\n## Key Design Principles\n\nThe success of VGGNet stems from several key design principles that work in tandem to create a powerful yet conceptually simple architecture. These principles represent new best practices in CNN design. Let us examine each of these design choices and understand their theoretical foundations.\n\n### Parameter Reduction using Stacked 3x3 Kernels\n\nOne of the most influential contributions is the demonstration that stacking multiple 3√ó3 convolution layers can effectively replace larger kernels while reducing the total number of parameters. This principle is based on a fundamental insight about receptive fields in CNNs.\n\nConsider that we stack two 3√ó3 convolution layers.\nEach value in the feature map of the first layer represents the summary of the 3√ó3 region of the input.\nThe second layer then generates a new feature map by applying the same 3√ó3 convolution to the feature map of the first layer, summarizing the 5√ó5 region of the input. The receptive field of the second layer (i.e., the region of the input that the second layer can see) is 5√ó5.\n\nNow, let us compare two cases:\n\n1. A single 5√ó5 convolution layer with stride 1\n2. Two stacked 3√ó3 convolution layers with stride 1\n\nWhich one has fewer parameters? The answer is the second case. In fact,\na single 5√ó5 convolution layer has $5 \\times 5 =25$ parameters, while two stacked 3√ó3 convolution layers have $2 \\times (3 \\times 3) = 18$ parameters.\n\nThis 28% reduction in parameters comes with an additional benefit: the inclusion of an extra ReLU non-linearity between the convolutions, allowing the network to be deeper.\n\n```{figure} https://miro.medium.com/v2/resize:fit:1200/1*k97NVvlMkRXau-uItlq5Gw.png\n:width: 70%\n:align: center\n\nA schematic representation of the receptive field of two stacked 3x3 convolution layers. The receptive field of the first layer is 3x3, and the receptive field of the second layer is 5x5. The image is taken from https://medium.com/@rekalantar/receptive-fields-in-deep-convolutional-networks-43871d2ef2e9\n\n\n\n\nVGGNet proposed multi-scale data augmentation (Figure 3). In AlexNet, data augmentation was performed by randomly cropping 224√ó224 input images from normalized images where the height was set to 256 pixels (left half of the figure below). In addition to this, VGGNet randomly crops 224√ó224 input images from images resized to a different scale with height of 384 pixels (right half of the figure below).\nThrough this VGGNet-style data augmentation approach of resizing to two scales, VGGNet was able to learn diversity across two scales, leading to improved classification accuracy.\n```ofnzlmwp https://cvml-expertguide.net/wp-content/uploads/2021/08/e72850b7f9960fbbd9d51f636963baec.png :width: 100% :align: center\nData augmentation for VGGNet. The image is taken from https://cvml-expertguide.net/\n\n\n```{footbibliography}\n:style: unsrt"
  },
  {
    "objectID": "m05-images/archive/vgg.html#architecture",
    "href": "m05-images/archive/vgg.html#architecture",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "VGGNet employs a systematic stack of convolutional layers using exclusively 3√ó3 filters with stride 1 and padding 1, interspersed with 2√ó2 max pooling layers with stride 2. This uniformity makes the architecture conceptually simple.\n```ofnzlmwp ../figs/vgg-architecture.jpg :width: 40% :align: center\nA schematic representation of VGG16 architecture showing the progression of spatial dimensions and feature channels through the network. The input is a 224√ó224√ó3 image, and the output is a 1000-dimensional vector for ImageNet classification. The image is taken from https://www.researchgate.net/profile/Max-Ferguson.\n\nVGG has multiple variants, and the most popular one is VGG16, which has 16 layers. The full architecture of VGG16 is as follows:\n\n* **Input**: [224 x 224] normalized, 3-channel color image (with color whitening, see section 3.2.1 in AlexNet article)\n* **Conv1_1**: Convolutional layer - [3 x 3] kernel x 64 channels + ReLU\n* **Conv1_2**: Convolutional layer - [3 x 3] kernel x 64 channels + ReLU\n* **P1**: Pooling layer - Max pooling, [2 x 2] kernel, stride = 2\n* **Conv2_1**: Convolutional layer - [3 x 3] kernel x 128 channels + ReLU\n* **Conv2_2**: Convolutional layer - [3 x 3] kernel x 128 channels + ReLU\n* **P2**: Pooling layer - Max pooling, [2 x 2] kernel, stride = 2\n* **Conv3_1**: Convolutional layer - [3 x 3] kernel x 256 channels + ReLU\n* **Conv3_2**: Convolutional layer - [3 x 3] kernel x 256 channels + ReLU\n* **Conv3_3**: Convolutional layer - [1 x 1] kernel x 256 channels + ReLU\n* **P3**: Pooling layer - Max pooling, [2 x 2] kernel, stride = 2\n* **Conv4_1**: Convolutional layer - [3 x 3] kernel x 512 channels + ReLU\n* **Conv4_2**: Convolutional layer - [3 x 3] kernel x 512 channels + ReLU\n* **Conv4_3**: Convolutional layer - [1 x 1] kernel x 512 channels + ReLU\n* **P4**: Pooling layer - Max pooling, [2 x 2] kernel, stride = 2\n* **Conv5_1**: Convolutional layer - [3 x 3] kernel x 512 channels + ReLU\n* **Conv5_2**: Convolutional layer - [3 x 3] kernel x 512 channels + ReLU\n* **Conv5_3**: Convolutional layer - [1 x 1] kernel x 512 channels + ReLU\n* **P5**: Pooling layer - Max pooling [7 x 7] kernel (aggressive downsampling at this stage)\n* (During training only: **Dropout**)\n* **FC14**: Fully connected layer - (7 x 7 x 512) ‚Üí 4096\n* (During training only: **Dropout**)\n* **FC15**: Fully connected layer - 4096 ‚Üí 4096\n* **FC16**: Fully connected layer - 4096 ‚Üí 1000\n* **Output**: 1000-dimensional vector probability distribution (output probability for each dimension) using softmax function\n\n\nThe network progressively increases the number of feature channels after each pooling operation, following a clear doubling pattern:\n\n$$\n\\text{channels} = \\{64 \\rightarrow 128 \\rightarrow 256 \\rightarrow 512 \\rightarrow 512\\}\n$$\n\nThe spatial dimensions of the feature maps decrease after each pooling layer, while the number of channels increases, creating a characteristic pyramid structure:\n\n$$\n\\text{spatial dimensions} = \\{224 \\rightarrow 112 \\rightarrow 56 \\rightarrow 28 \\rightarrow 14 \\rightarrow 7\\}\n$$\n\nDespite its apparent simplicity, VGG16 contains approximately 140 million parameters, with the majority concentrated in the first fully connected layer (approximately 102 million parameters). This large parameter count highlights an interesting trade-off in the architecture: while the convolutional layers follow a clean and efficient design, the fully connected layers remain computationally intensive. This issue is later resolved by global average pooling proposed by {footcite}`lin2013network`.\n\n## Key Design Principles\n\nThe success of VGGNet stems from several key design principles that work in tandem to create a powerful yet conceptually simple architecture. These principles represent new best practices in CNN design. Let us examine each of these design choices and understand their theoretical foundations.\n\n### Parameter Reduction using Stacked 3x3 Kernels\n\nOne of the most influential contributions is the demonstration that stacking multiple 3√ó3 convolution layers can effectively replace larger kernels while reducing the total number of parameters. This principle is based on a fundamental insight about receptive fields in CNNs.\n\nConsider that we stack two 3√ó3 convolution layers.\nEach value in the feature map of the first layer represents the summary of the 3√ó3 region of the input.\nThe second layer then generates a new feature map by applying the same 3√ó3 convolution to the feature map of the first layer, summarizing the 5√ó5 region of the input. The receptive field of the second layer (i.e., the region of the input that the second layer can see) is 5√ó5.\n\nNow, let us compare two cases:\n\n1. A single 5√ó5 convolution layer with stride 1\n2. Two stacked 3√ó3 convolution layers with stride 1\n\nWhich one has fewer parameters? The answer is the second case. In fact,\na single 5√ó5 convolution layer has $5 \\times 5 =25$ parameters, while two stacked 3√ó3 convolution layers have $2 \\times (3 \\times 3) = 18$ parameters.\n\nThis 28% reduction in parameters comes with an additional benefit: the inclusion of an extra ReLU non-linearity between the convolutions, allowing the network to be deeper.\n\n```{figure} https://miro.medium.com/v2/resize:fit:1200/1*k97NVvlMkRXau-uItlq5Gw.png\n:width: 70%\n:align: center\n\nA schematic representation of the receptive field of two stacked 3x3 convolution layers. The receptive field of the first layer is 3x3, and the receptive field of the second layer is 5x5. The image is taken from https://medium.com/@rekalantar/receptive-fields-in-deep-convolutional-networks-43871d2ef2e9"
  },
  {
    "objectID": "m05-images/archive/vgg.html#vgg-style-data-augmentation",
    "href": "m05-images/archive/vgg.html#vgg-style-data-augmentation",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "VGGNet proposed multi-scale data augmentation (Figure 3). In AlexNet, data augmentation was performed by randomly cropping 224√ó224 input images from normalized images where the height was set to 256 pixels (left half of the figure below). In addition to this, VGGNet randomly crops 224√ó224 input images from images resized to a different scale with height of 384 pixels (right half of the figure below).\nThrough this VGGNet-style data augmentation approach of resizing to two scales, VGGNet was able to learn diversity across two scales, leading to improved classification accuracy.\n```ofnzlmwp https://cvml-expertguide.net/wp-content/uploads/2021/08/e72850b7f9960fbbd9d51f636963baec.png :width: 100% :align: center\nData augmentation for VGGNet. The image is taken from https://cvml-expertguide.net/\n\n\n```{footbibliography}\n:style: unsrt"
  },
  {
    "objectID": "m05-images/archive/inception.html",
    "href": "m05-images/archive/inception.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Imagine you‚Äôre looking at a painting in an art gallery. You might step close to examine the tiny brushstrokes or stand far away to observe the overall composition. How can a neural network do something similar with images? The Inception module proposed by Szegedy et al.¬†in ‚ÄúGoing Deeper with Convolutions‚Äù tackles this by letting different convolutional filters (big and small) work together in parallel‚Äîcapturing both fine details and large-scale context at once.\n\n\n\n\nIn a traditional CNN layer, you pick one filter size (like 3 \\times 3). But an Inception module uses multiple filter sizes (like 1 \\times 1, 3 \\times 3, and 5 \\times 5) all at once. Each filter ‚Äúlooks‚Äù at the same input but focuses on different scales‚Äîmuch like zooming in and out of a scene.\n\n\n\n\nFrom a theoretical perspective, not every pixel in a feature map needs to connect to every pixel in the next layer (i.e., connectivity is often sparse). However, sparse operations can be slow on current hardware. Inception approximates this ‚Äúsparse‚Äù idea by using 1x1 convolutions.\nThe 1x1 convolutions appear unnatural at first glance but they are actually a very elegant solution to ‚Äúsparcify‚Äù the convolutional filters. The core idea is that pixel values at different spatial locations are often less correlated than the values across different channels. Thus, 1x1 convolutions focus on compressing or expanding information across channels, reducing the effective parameter count for the subsequent larger filters.\nFor example, for a 3 \\times 3 convolution filter, the 1x1 convolution reduces the number of parameters from 3 \\times 3 \\times C_{\\text{in}} \\times C_{\\text{out}} to 1 \\times 1 \\times C_{\\text{in}} (1x1 convolution) plus 3 \\times 3 \\times C_{\\text{out}} (3x3 convolution). This is C_{\\text{in}} + 9C_{\\text{out}} versus 9C_{\\text{in}} \\times C_{\\text{out}}, yielding a substantial parameter reduction when C_{\\text{out}} or C_{\\text{in}} is large.\n**Connection to Sparse Representations**\nEarly theoretical work suggested that a sparse network with many filter sizes could approximate a wide variety of feature types. However, directly implementing a sparse network can be very memory-intensive. The Inception module cleverly approximates a sparse structure by mixing 1x1, 3x3, and 5x5 convolutions in an efficient manner{footcite}`paperswithcode-inception`.\n\n\n\nAfter each branch applies its own sequence of convolutions (or pooling), the results are merged by concatenating them along the channel dimension. Think of it as stacking the feature maps from each branch side by side. This ‚Äúmerge‚Äù step is powerful because:\n\nIt combines multi-scale features into a single tensor.\nEach branch contributes a unique perspective (e.g., the 1\\times1 branch might capture fine details, while the 5\\times5 branch looks at broader context).\nThe network can learn how to best blend and leverage these feature maps for the next stage of processing.\n\nAs a result, the final output of an Inception module is a multi-channel representation that integrates information from multiple scales and pooling operations.\n\n\n\nGoogleNet is a stack of the Inception modules explained above, followed by a module that transform the feature maps of the last Inception module into a single vector that is used for classification. This last module is also one of the key feature of GoogleNet that sets it apart from the previous architectures.\nGoogleNet uses max pooling for each channel of the feature maps. For example, if the feature maps has 10 channel, the max pooling will reduce it to 10-dimensional vector, regardless of the spatial dimension of the feature maps, by taking the maximum value across the spatial dimension.\nThis design choice was motivated, in part, by observations like in VGG16 (which uses the full-connected layers to this transformation), where 123 million of its 138 million parameters come from fully-connected layers alone. Reducing or eliminating these layers via pooling can dramatically cut down on model size and overfitting risk{footcite}lin2013network.\n\n\n\nAs GoogleNet grew deeper, the authors noticed that early layers sometimes struggled with vanishing gradients. To combat this, auxiliary classifiers were placed at intermediate layers. The authors take the output from the classifier attached to these intermediate layers, and add its loss to the overall loss. When backpropagating, the gradient from the auxiliary classifier is also added to the gradient of the earlier layers, which guides the earlier layers to learn more discriminative features.\n```bngwfxdl https://production-media.paperswithcode.com/methods/GoogleNet-structure-and-auxiliary-classifier-units_CM5xsxk.png :name: inception-auxiliary-classifier :align: center\nAn illustration of the GoogleNet architecture, including the auxiliary classifier units.\n\n## Mathematical Framework (Light Overview)\n\nLet‚Äôs denote the input to an Inception module as a 3D tensor $ X $ with shape $(H \\times W \\times C)$, where $H$ and $W$ are the spatial dimensions and $C$ is the channel depth.\n\n1. **1x1 path**:\n\n   $$\n   Y_{1\\times1} = \\text{Conv}_{1\\times1}(X)\n   $$\n\n2. **1x1 -&gt; 3x3 path**:\n\n   $$\n   X_{\\text{reduced}} = \\text{Conv}_{1\\times1}(X),\n   \\quad\n   Y_{3\\times3} = \\text{Conv}_{3\\times3}(X_{\\text{reduced}})\n   $$\n\n3. **1x1 -&gt; 5x5 path**:\n\n   $$\n   X_{\\text{reduced}}' = \\text{Conv}_{1\\times1}(X),\n   \\quad\n   Y_{5\\times5} = \\text{Conv}_{5\\times5}(X_{\\text{reduced}}')\n   $$\n\n4. **Pooling path**:\n\n   $$\n   Y_{\\text{pool}} = \\text{Pool}(X)\n   \\quad\\text{(often followed by Conv}_{1\\times1})\n   $$\n\nAfter computing these branches, the final output of the Inception module is:\n\n$$\nY_{\\text{inception}} = \\text{Concat}\\big(Y_{1\\times1}, \\,Y_{3\\times3}, \\,Y_{5\\times5}, \\,Y_{\\text{pool}}\\big).\n$$\n\n## Recent Advancements and Improvements\n\n1. **Inception v2 & v3**\n   - Introduced **Batch Normalization**, reducing internal covariate shift.\n   - Factorized larger filters (e.g., 5x5 ‚Üí two 3x3) to cut cost.\n   These improvements increased accuracy and efficiency{footcite}`visoai`.\n\n2. **Inception v4 & Inception-ResNet**\n   - Integrated **residual connections**, further improving training stability and depth{footcite}`cvpr2016`.\n\n3. **Xception**\n   - Proposed **depthwise separable convolutions**, pushing the Inception idea further by decoupling spatial and channel-wise processing, leading to better efficiency{footcite}`visoai`.\n\n4. **Simplified Inception with Hadamard Attention**\n   - Targeted at **medical image classification**.\n   - Uses attention mechanisms to focus on crucial parts of the image, enhancing accuracy without exploding parameter size{footcite}`srp-hadamard`.\n\n## Implementation Example\n\nBelow is a simplified PyTorch-like example of how we can define a single Inception module. This code demonstrates the parallel branches and concatenation of outputs. While not an exact reproduction of GoogleNet, it captures the core idea.\n\n```{code-cell} ipython3\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass InceptionModule(nn.Module):\n    def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, pool_proj):\n        super(InceptionModule, self).__init__()\n\n        # Branch 1: 1x1 Conv\n        self.branch1 = nn.Conv2d(in_channels, out_1x1, kernel_size=1)\n\n        # Branch 2: 1x1 Conv -&gt; 3x3 Conv\n        self.branch2_reduce = nn.Conv2d(in_channels, red_3x3, kernel_size=1)\n        self.branch2 = nn.Conv2d(red_3x3, out_3x3, kernel_size=3, padding=1)\n\n        # Branch 3: 1x1 Conv -&gt; 5x5 Conv\n        self.branch3_reduce = nn.Conv2d(in_channels, red_5x5, kernel_size=1)\n        self.branch3 = nn.Conv2d(red_5x5, out_5x5, kernel_size=5, padding=2)\n\n        # Branch 4: Max Pool -&gt; 1x1 Conv\n        self.branch4_pool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n        self.branch4 = nn.Conv2d(in_channels, pool_proj, kernel_size=1)\n\n    def forward(self, x):\n        branch1_out = F.relu(self.branch1(x))\n\n        branch2_out = F.relu(self.branch2(F.relu(self.branch2_reduce(x))))\n\n        branch3_out = F.relu(self.branch3(F.relu(self.branch3_reduce(x))))\n\n        branch4_out = F.relu(self.branch4(self.branch4_pool(x)))\n\n        # Concatenate along channel dimension\n        return torch.cat([branch1_out, branch2_out, branch3_out, branch4_out], dim=1)\n\n# Example usage:\nif __name__ == \"__main__\":\n    inception = InceptionModule(64, 16, 16, 24, 16, 24, 16)\n    dummy_input = torch.randn(1, 64, 56, 56)  # (batch_size, channels, height, width)\n    output = inception(dummy_input)\n    print(\"Output shape from Inception module:\", output.shape)\n\n\n\n\nBelow is a simple exercise that uses the pre-trained GoogLeNet model available in torchvision. You can try this out in a local Jupyter notebook or a cloud environment like Google Colab.\n```dqavkwsygwk ipython3 :tags: [remove-cell]"
  },
  {
    "objectID": "m05-images/archive/inception.html#conceptual-foundation",
    "href": "m05-images/archive/inception.html#conceptual-foundation",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "In a traditional CNN layer, you pick one filter size (like 3 \\times 3). But an Inception module uses multiple filter sizes (like 1 \\times 1, 3 \\times 3, and 5 \\times 5) all at once. Each filter ‚Äúlooks‚Äù at the same input but focuses on different scales‚Äîmuch like zooming in and out of a scene.\n\n\n\n\nFrom a theoretical perspective, not every pixel in a feature map needs to connect to every pixel in the next layer (i.e., connectivity is often sparse). However, sparse operations can be slow on current hardware. Inception approximates this ‚Äúsparse‚Äù idea by using 1x1 convolutions.\nThe 1x1 convolutions appear unnatural at first glance but they are actually a very elegant solution to ‚Äúsparcify‚Äù the convolutional filters. The core idea is that pixel values at different spatial locations are often less correlated than the values across different channels. Thus, 1x1 convolutions focus on compressing or expanding information across channels, reducing the effective parameter count for the subsequent larger filters.\nFor example, for a 3 \\times 3 convolution filter, the 1x1 convolution reduces the number of parameters from 3 \\times 3 \\times C_{\\text{in}} \\times C_{\\text{out}} to 1 \\times 1 \\times C_{\\text{in}} (1x1 convolution) plus 3 \\times 3 \\times C_{\\text{out}} (3x3 convolution). This is C_{\\text{in}} + 9C_{\\text{out}} versus 9C_{\\text{in}} \\times C_{\\text{out}}, yielding a substantial parameter reduction when C_{\\text{out}} or C_{\\text{in}} is large.\n**Connection to Sparse Representations**\nEarly theoretical work suggested that a sparse network with many filter sizes could approximate a wide variety of feature types. However, directly implementing a sparse network can be very memory-intensive. The Inception module cleverly approximates a sparse structure by mixing 1x1, 3x3, and 5x5 convolutions in an efficient manner{footcite}`paperswithcode-inception`.\n\n\n\nAfter each branch applies its own sequence of convolutions (or pooling), the results are merged by concatenating them along the channel dimension. Think of it as stacking the feature maps from each branch side by side. This ‚Äúmerge‚Äù step is powerful because:\n\nIt combines multi-scale features into a single tensor.\nEach branch contributes a unique perspective (e.g., the 1\\times1 branch might capture fine details, while the 5\\times5 branch looks at broader context).\nThe network can learn how to best blend and leverage these feature maps for the next stage of processing.\n\nAs a result, the final output of an Inception module is a multi-channel representation that integrates information from multiple scales and pooling operations.\n\n\n\nGoogleNet is a stack of the Inception modules explained above, followed by a module that transform the feature maps of the last Inception module into a single vector that is used for classification. This last module is also one of the key feature of GoogleNet that sets it apart from the previous architectures.\nGoogleNet uses max pooling for each channel of the feature maps. For example, if the feature maps has 10 channel, the max pooling will reduce it to 10-dimensional vector, regardless of the spatial dimension of the feature maps, by taking the maximum value across the spatial dimension.\nThis design choice was motivated, in part, by observations like in VGG16 (which uses the full-connected layers to this transformation), where 123 million of its 138 million parameters come from fully-connected layers alone. Reducing or eliminating these layers via pooling can dramatically cut down on model size and overfitting risk{footcite}lin2013network.\n\n\n\nAs GoogleNet grew deeper, the authors noticed that early layers sometimes struggled with vanishing gradients. To combat this, auxiliary classifiers were placed at intermediate layers. The authors take the output from the classifier attached to these intermediate layers, and add its loss to the overall loss. When backpropagating, the gradient from the auxiliary classifier is also added to the gradient of the earlier layers, which guides the earlier layers to learn more discriminative features.\n```bngwfxdl https://production-media.paperswithcode.com/methods/GoogleNet-structure-and-auxiliary-classifier-units_CM5xsxk.png :name: inception-auxiliary-classifier :align: center\nAn illustration of the GoogleNet architecture, including the auxiliary classifier units.\n\n## Mathematical Framework (Light Overview)\n\nLet‚Äôs denote the input to an Inception module as a 3D tensor $ X $ with shape $(H \\times W \\times C)$, where $H$ and $W$ are the spatial dimensions and $C$ is the channel depth.\n\n1. **1x1 path**:\n\n   $$\n   Y_{1\\times1} = \\text{Conv}_{1\\times1}(X)\n   $$\n\n2. **1x1 -&gt; 3x3 path**:\n\n   $$\n   X_{\\text{reduced}} = \\text{Conv}_{1\\times1}(X),\n   \\quad\n   Y_{3\\times3} = \\text{Conv}_{3\\times3}(X_{\\text{reduced}})\n   $$\n\n3. **1x1 -&gt; 5x5 path**:\n\n   $$\n   X_{\\text{reduced}}' = \\text{Conv}_{1\\times1}(X),\n   \\quad\n   Y_{5\\times5} = \\text{Conv}_{5\\times5}(X_{\\text{reduced}}')\n   $$\n\n4. **Pooling path**:\n\n   $$\n   Y_{\\text{pool}} = \\text{Pool}(X)\n   \\quad\\text{(often followed by Conv}_{1\\times1})\n   $$\n\nAfter computing these branches, the final output of the Inception module is:\n\n$$\nY_{\\text{inception}} = \\text{Concat}\\big(Y_{1\\times1}, \\,Y_{3\\times3}, \\,Y_{5\\times5}, \\,Y_{\\text{pool}}\\big).\n$$\n\n## Recent Advancements and Improvements\n\n1. **Inception v2 & v3**\n   - Introduced **Batch Normalization**, reducing internal covariate shift.\n   - Factorized larger filters (e.g., 5x5 ‚Üí two 3x3) to cut cost.\n   These improvements increased accuracy and efficiency{footcite}`visoai`.\n\n2. **Inception v4 & Inception-ResNet**\n   - Integrated **residual connections**, further improving training stability and depth{footcite}`cvpr2016`.\n\n3. **Xception**\n   - Proposed **depthwise separable convolutions**, pushing the Inception idea further by decoupling spatial and channel-wise processing, leading to better efficiency{footcite}`visoai`.\n\n4. **Simplified Inception with Hadamard Attention**\n   - Targeted at **medical image classification**.\n   - Uses attention mechanisms to focus on crucial parts of the image, enhancing accuracy without exploding parameter size{footcite}`srp-hadamard`.\n\n## Implementation Example\n\nBelow is a simplified PyTorch-like example of how we can define a single Inception module. This code demonstrates the parallel branches and concatenation of outputs. While not an exact reproduction of GoogleNet, it captures the core idea.\n\n```{code-cell} ipython3\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass InceptionModule(nn.Module):\n    def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, pool_proj):\n        super(InceptionModule, self).__init__()\n\n        # Branch 1: 1x1 Conv\n        self.branch1 = nn.Conv2d(in_channels, out_1x1, kernel_size=1)\n\n        # Branch 2: 1x1 Conv -&gt; 3x3 Conv\n        self.branch2_reduce = nn.Conv2d(in_channels, red_3x3, kernel_size=1)\n        self.branch2 = nn.Conv2d(red_3x3, out_3x3, kernel_size=3, padding=1)\n\n        # Branch 3: 1x1 Conv -&gt; 5x5 Conv\n        self.branch3_reduce = nn.Conv2d(in_channels, red_5x5, kernel_size=1)\n        self.branch3 = nn.Conv2d(red_5x5, out_5x5, kernel_size=5, padding=2)\n\n        # Branch 4: Max Pool -&gt; 1x1 Conv\n        self.branch4_pool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n        self.branch4 = nn.Conv2d(in_channels, pool_proj, kernel_size=1)\n\n    def forward(self, x):\n        branch1_out = F.relu(self.branch1(x))\n\n        branch2_out = F.relu(self.branch2(F.relu(self.branch2_reduce(x))))\n\n        branch3_out = F.relu(self.branch3(F.relu(self.branch3_reduce(x))))\n\n        branch4_out = F.relu(self.branch4(self.branch4_pool(x)))\n\n        # Concatenate along channel dimension\n        return torch.cat([branch1_out, branch2_out, branch3_out, branch4_out], dim=1)\n\n# Example usage:\nif __name__ == \"__main__\":\n    inception = InceptionModule(64, 16, 16, 24, 16, 24, 16)\n    dummy_input = torch.randn(1, 64, 56, 56)  # (batch_size, channels, height, width)\n    output = inception(dummy_input)\n    print(\"Output shape from Inception module:\", output.shape)"
  },
  {
    "objectID": "m05-images/archive/inception.html#coding-exercise-using-pre-trained-googlenet-in-pytorch",
    "href": "m05-images/archive/inception.html#coding-exercise-using-pre-trained-googlenet-in-pytorch",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Below is a simple exercise that uses the pre-trained GoogLeNet model available in torchvision. You can try this out in a local Jupyter notebook or a cloud environment like Google Colab.\n```dqavkwsygwk ipython3 :tags: [remove-cell]"
  },
  {
    "objectID": "m05-images/archive/cnn.html",
    "href": "m05-images/archive/cnn.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Image is a 2D matrix that can be extremely large. For example, a 1024x1024 image has 1,048,576 pixels. Processing such a high-dimensional data with neural networks requires a large number of parameters, the prohibitive computational cost, and an enormous dataset for training.\nConvolutional Neural Networks (CNNs) were developed to address this key limitation of fully connected networks by leveraging convolution operation that leverages the local connectivity. Namely, instead of processing the entire image, CNN only processes a small region of the image at a time, and progressively integrates the information from the local regions to form a global representation. Here, we will first introduce the building blocks of CNNs, and then discuss how CNNs are built upon these blocks.\n\n\n\n\nAt the heart of Convolutional Neural Networks (CNNs) lies a remarkably elegant operation called convolution. Imagine sliding a small window, called a kernel or filter, across an image. At each position, we perform a simple multiplication and addition operation between the kernel values and the overlapping image pixels. This fundamental operation allows CNNs to automatically learn and detect important visual features.\n\n\n\n```bnihrgtf https://anhreynolds.com/img/cnn.png\n\n\n\n\nwidth: 100%\n\n\nname: convolution\n\n\n\nConvolution operation. The kernel slides across the input image and performs the multiplication and addition operation at each position. The result is a feature map that represents the detected features. Image taken from https://anhreynolds.com/blogs/cnn.html\n\nLet's understand this mathematically. For a single-channel input (like a grayscale image), the 2D convolution operation can be expressed as:\n\n$$\n(I * K)_{i,j} = \\sum_{m=0}^{L-1}\\sum_{n=0}^{L-1} I_{i+m,j+n} \\cdot K_{m,n}\n$$\n\nwhere $I$ represents the input image, $K$ is the kernel, and $*$ denotes the convolution operation.\nThe indices $i,j$ represent the position in the output feature map, while $m,n$ traverse the kernel dimensions of size $L$.\n\nWhat makes CNNs powerful is that these kernels are learnable parameters. During training, each kernel evolves to detect specific visual patterns. Some kernels might become edge detectors, highlighting vertical or horizontal edges, while others might respond to textures or more complex patterns. This hierarchical feature learning is what makes CNNs so effective at visual recognition tasks.\n\nReal-world images typically have multiple channels (like RGB). The convolution operation naturally extends to handle this by using 3D kernels. For an input with $C$ channels, the operation becomes:\n\n$$\n(I * K)_{i,j} = \\sum_{c=1}^{C}\\sum_{m=0}^{L-1}\\sum_{n=0}^{L-1} I_{c,i+m,j+n} \\cdot K_{c,m,n}\n$$\n\nwhere $I$ is a 3D matrix with the last dimension of size $C$, and $K$ is a 3D kernel with the last dimension of size $C$ as well. $C$ is the number of channels in the input image.\n\n```{figure} https://d2l.ai/_images/conv-multi-in.svg\n---\nwidth: 100%\nname: convolution\n---\nConvolution operation for multi-channel input. Each channel is processed separately, and the results are summed up to produce the final output.\nInteractive visualization of convolution neural networks\n- [An Interactive Node-Link Visualization of Convolutional Neural Networks](https://adamharley.com/nn_vis/cnn/2d.html)\n- [CNN Explainer](https://poloclub.github.io/cnn-explainer/?norec=true)\nOne of the key features of CNNs is translation equivariance. This means that when an input image is shifted, the output feature map shifts by the same amount. For example, if we move an object in an image one pixel to the right, its detected features in the output will also move one pixel to the right. This property allows CNNs to detect features consistently regardless of their position in the image.\n\n\n\n```bnihrgtf https://miro.medium.com/v2/resize:fit:1400/1*NoAQ4ZgofpkK6esl4sMHkA.png\n\n\n\n\nwidth: 100%\n\n\nname: translation-invariance\n\n\n\nTranslation invariance. The same kernel can detect the same feature regardless of its position in the input.\n\nAnother key feature of convolutional layers is *parameter sharing*. Unlike dense neural networks where each parameter is used only once, a kernel's weights are reused as it slides across the input. For example, a 3√ó3 kernel applied to a 224√ó224 RGB image (i.e., 3 channels) uses just 27 parameters (3√ó3√ó3) instead of the millions required by a fully connected layer. This weight-sharing scheme not only drastically reduces the model's parameter count but also preserves spatial relationships in the data.\n\n*Receptive Field* is the region of input pixels that influence each output pixel. This field grows larger in deeper layers through the combination of convolution and pooling, allowing CNNs to detect increasingly complex, hierarchical, and abstract features. For example, the first layer might detect edges, while deeper layers might recognize complex patterns like faces or objects.\n\n```{figure} https://www.researchgate.net/publication/316950618/figure/fig4/AS:11431281212123378@1702542797323/The-receptive-field-of-each-convolution-layer-with-a-3-3-kernel-The-green-area-marks.tif\n---\nwidth: 50%\nname: receptive-field\nalign: center\n---\nReceptive field of each convolution layer with a 3x3 kernel. The green area marks the receptive field of each layer. The image is taken from https://www.mdpi.com/2072-4292/9/5/480\n\n\n\nIn convolutional networks, stride and padding are crucial hyperparameters that control how we process spatial information. Stride (S) determines how many pixels we skip when sliding our kernel across the input. With stride-1, we move the kernel one pixel at a time, creating dense feature maps. When we increase the stride to 2 or more, we take larger steps, effectively downsampling the input. For a one-dimensional example, consider an input signal [a,b,c,d,e,f] and a kernel [1,2]. With stride-1, we compute: \n\\begin{aligned}\n&[1a + 2b, \\\n&\\phantom{[}1b + 2c, \\\n&\\phantom{[}1c + 2d, \\\n&\\phantom{[}1d + 2e, \\\n&\\phantom{[}1e + 2f]\n\\end{aligned}\n\nHowever, with stride-2, we skip every other position: \n[1a + 2b, 1c + 2d, 1e + 2f]\n This striding mechanism serves two purposes: it reduces computational complexity and increases the receptive field (the region of input pixels that influence each output pixel).\n\n\n\n```bnihrgtf https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRJTOGcDwPXtlNnev9ayPj92FIysGddxe__Fw&s\n\n\n\n\nwidth: 50%\n\n\nname: stride\n\n\nalign: center\n\n\n\nStride-1 and stride-2. The image is taken from https://svitla.com/blog/math-at-the-heart-of-cnn/\n\n```{tip}\nInteractive visualization of stride\n- [Convolution Visualizer](https://ezyang.github.io/convolution-visualizer/)\nPadding addresses a different challenge: information loss at the borders. Without padding (‚Äúvalid‚Äù padding), the output dimensions shrink after each convolution because the kernel can‚Äôt fully overlap with border pixels. While various padding schemes are proposed, zero padding has been widely used because it is simple and effective.\n\n\n\n```bnihrgtf https://svitla.com/uploads/ckeditor/2024/Math%20at%20the%20heart%20of%20CNN/image_930660943761713546482755.gif\n\n\n\n\nwidth: 50%\n\n\nname: padding\n\n\nalign: center\n\n\n\nPadding. The image is taken from https://svitla.com/blog/math-at-the-heart-of-cnn/\n\nMathematically, for a square input of size $W$ with kernel size $K$, stride $S$, and padding $P$, the output dimension $O$ is given by:\n$$\nO = \\left\\lfloor\\frac{W - K + 2P}{S}\\right\\rfloor + 1\n$$\nLet's break this formula down:\n\n- $W - K$ represents how far the kernel can move\n- $2P$ accounts for padding on both sides\n- Division by $S$ reflects the stride's effect\n- The floor function $\\lfloor \\cdot \\rfloor$ ensures integer output\n- Adding 1 accounts for the initial position\n\nFor example, with an input size of 224√ó224, a 3√ó3 kernel, stride-2, and padding-1:\n$$\nO = \\left\\lfloor\\frac{224 - 3 + 2(1)}{2}\\right\\rfloor + 1 = 112\n$$\n\nThe interplay between stride and padding allows network designers to control information flow and computational efficiency. Larger strides create more compact representations but might miss fine details, while appropriate padding ensures no spatial information is unnecessarily discarded.\n\n```{note}\nTry out [CNN Explainer](https://poloclub.github.io/cnn-explainer/?norec=true) to learn how convolution, padding, and stride affect the output.\n\n\n\nPooling layers serve as the dimensionality reduction modules in CNNs, summarizing spatial regions into single values while preserving essential features. Max-pooling, for example, is a widely used pooling operation that selects the highest activation value within a local region. For a feature map F (i.e., intermediate representation of a convolutional layer), max pooling over a 3√ó3 window can be expressed as:\n\nP_{i,j} = \\max_{m=0,1}\\max_{n=0,1} F_{3i+m,3j+n}\n\nMax-pooling offers several key benefits. First, it creates a form of local translation invariance - small shifts in feature positions are absorbed by the pooling window. For instance, if an edge moves slightly within a 2√ó2 pooling region, the max-pooled output remains unchanged. Second, by reducing spatial dimensions, pooling significantly decreases computational complexity in subsequent layers.\nSimilarly, average pooling computes:\n\nP_{i,j} = \\frac{1}{9}\\sum_{m=0}^2\\sum_{n=0}^2 F_{3i+m,3j+n}\n\nThe output dimensions after a pooling operation follow a similar formula to strided convolutions, but without padding considerations:\n\nO = \\left\\lfloor\\frac{W - K}{S}\\right\\rfloor + 1\n\nwhere W is the input size, K is the pooling window size (typically 2 or 3), and S is the stride (usually equal to K).\nModern CNN architectures often debate the necessity of pooling layers {footcite}`springenberg2015striving`. Some networks replace them with strided convolutions, arguing that learnable downsampling might be more effective. The key difference lies in parameterization - pooling has no learnable parameters, while strided convolutions learn how to downsample. Consider these approaches:\n\n$$\n\\text{Max Pooling: } y = \\max(x_1, x_2, x_3, x_4)\n$$\n\n$$\n\\text{Strided Conv: } y = w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4\n$$\n\nDespite this trend, pooling layers remain valuable in many applications. They offer built-in invariance to small translations and rotations, reduce overfitting through their parameter-free nature, and provide consistent dimension reduction.\n:style: unsrt"
  },
  {
    "objectID": "m05-images/archive/cnn.html#building-block-of-cnns",
    "href": "m05-images/archive/cnn.html#building-block-of-cnns",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "At the heart of Convolutional Neural Networks (CNNs) lies a remarkably elegant operation called convolution. Imagine sliding a small window, called a kernel or filter, across an image. At each position, we perform a simple multiplication and addition operation between the kernel values and the overlapping image pixels. This fundamental operation allows CNNs to automatically learn and detect important visual features.\n\n\n\n```bnihrgtf https://anhreynolds.com/img/cnn.png\n\n\n\n\nwidth: 100%\n\n\nname: convolution\n\n\n\nConvolution operation. The kernel slides across the input image and performs the multiplication and addition operation at each position. The result is a feature map that represents the detected features. Image taken from https://anhreynolds.com/blogs/cnn.html\n\nLet's understand this mathematically. For a single-channel input (like a grayscale image), the 2D convolution operation can be expressed as:\n\n$$\n(I * K)_{i,j} = \\sum_{m=0}^{L-1}\\sum_{n=0}^{L-1} I_{i+m,j+n} \\cdot K_{m,n}\n$$\n\nwhere $I$ represents the input image, $K$ is the kernel, and $*$ denotes the convolution operation.\nThe indices $i,j$ represent the position in the output feature map, while $m,n$ traverse the kernel dimensions of size $L$.\n\nWhat makes CNNs powerful is that these kernels are learnable parameters. During training, each kernel evolves to detect specific visual patterns. Some kernels might become edge detectors, highlighting vertical or horizontal edges, while others might respond to textures or more complex patterns. This hierarchical feature learning is what makes CNNs so effective at visual recognition tasks.\n\nReal-world images typically have multiple channels (like RGB). The convolution operation naturally extends to handle this by using 3D kernels. For an input with $C$ channels, the operation becomes:\n\n$$\n(I * K)_{i,j} = \\sum_{c=1}^{C}\\sum_{m=0}^{L-1}\\sum_{n=0}^{L-1} I_{c,i+m,j+n} \\cdot K_{c,m,n}\n$$\n\nwhere $I$ is a 3D matrix with the last dimension of size $C$, and $K$ is a 3D kernel with the last dimension of size $C$ as well. $C$ is the number of channels in the input image.\n\n```{figure} https://d2l.ai/_images/conv-multi-in.svg\n---\nwidth: 100%\nname: convolution\n---\nConvolution operation for multi-channel input. Each channel is processed separately, and the results are summed up to produce the final output.\nInteractive visualization of convolution neural networks\n- [An Interactive Node-Link Visualization of Convolutional Neural Networks](https://adamharley.com/nn_vis/cnn/2d.html)\n- [CNN Explainer](https://poloclub.github.io/cnn-explainer/?norec=true)\nOne of the key features of CNNs is translation equivariance. This means that when an input image is shifted, the output feature map shifts by the same amount. For example, if we move an object in an image one pixel to the right, its detected features in the output will also move one pixel to the right. This property allows CNNs to detect features consistently regardless of their position in the image.\n\n\n\n```bnihrgtf https://miro.medium.com/v2/resize:fit:1400/1*NoAQ4ZgofpkK6esl4sMHkA.png\n\n\n\n\nwidth: 100%\n\n\nname: translation-invariance\n\n\n\nTranslation invariance. The same kernel can detect the same feature regardless of its position in the input.\n\nAnother key feature of convolutional layers is *parameter sharing*. Unlike dense neural networks where each parameter is used only once, a kernel's weights are reused as it slides across the input. For example, a 3√ó3 kernel applied to a 224√ó224 RGB image (i.e., 3 channels) uses just 27 parameters (3√ó3√ó3) instead of the millions required by a fully connected layer. This weight-sharing scheme not only drastically reduces the model's parameter count but also preserves spatial relationships in the data.\n\n*Receptive Field* is the region of input pixels that influence each output pixel. This field grows larger in deeper layers through the combination of convolution and pooling, allowing CNNs to detect increasingly complex, hierarchical, and abstract features. For example, the first layer might detect edges, while deeper layers might recognize complex patterns like faces or objects.\n\n```{figure} https://www.researchgate.net/publication/316950618/figure/fig4/AS:11431281212123378@1702542797323/The-receptive-field-of-each-convolution-layer-with-a-3-3-kernel-The-green-area-marks.tif\n---\nwidth: 50%\nname: receptive-field\nalign: center\n---\nReceptive field of each convolution layer with a 3x3 kernel. The green area marks the receptive field of each layer. The image is taken from https://www.mdpi.com/2072-4292/9/5/480\n\n\n\nIn convolutional networks, stride and padding are crucial hyperparameters that control how we process spatial information. Stride (S) determines how many pixels we skip when sliding our kernel across the input. With stride-1, we move the kernel one pixel at a time, creating dense feature maps. When we increase the stride to 2 or more, we take larger steps, effectively downsampling the input. For a one-dimensional example, consider an input signal [a,b,c,d,e,f] and a kernel [1,2]. With stride-1, we compute: \n\\begin{aligned}\n&[1a + 2b, \\\n&\\phantom{[}1b + 2c, \\\n&\\phantom{[}1c + 2d, \\\n&\\phantom{[}1d + 2e, \\\n&\\phantom{[}1e + 2f]\n\\end{aligned}\n\nHowever, with stride-2, we skip every other position: \n[1a + 2b, 1c + 2d, 1e + 2f]\n This striding mechanism serves two purposes: it reduces computational complexity and increases the receptive field (the region of input pixels that influence each output pixel).\n\n\n\n```bnihrgtf https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRJTOGcDwPXtlNnev9ayPj92FIysGddxe__Fw&s\n\n\n\n\nwidth: 50%\n\n\nname: stride\n\n\nalign: center\n\n\n\nStride-1 and stride-2. The image is taken from https://svitla.com/blog/math-at-the-heart-of-cnn/\n\n```{tip}\nInteractive visualization of stride\n- [Convolution Visualizer](https://ezyang.github.io/convolution-visualizer/)\nPadding addresses a different challenge: information loss at the borders. Without padding (‚Äúvalid‚Äù padding), the output dimensions shrink after each convolution because the kernel can‚Äôt fully overlap with border pixels. While various padding schemes are proposed, zero padding has been widely used because it is simple and effective.\n\n\n\n```bnihrgtf https://svitla.com/uploads/ckeditor/2024/Math%20at%20the%20heart%20of%20CNN/image_930660943761713546482755.gif\n\n\n\n\nwidth: 50%\n\n\nname: padding\n\n\nalign: center\n\n\n\nPadding. The image is taken from https://svitla.com/blog/math-at-the-heart-of-cnn/\n\nMathematically, for a square input of size $W$ with kernel size $K$, stride $S$, and padding $P$, the output dimension $O$ is given by:\n$$\nO = \\left\\lfloor\\frac{W - K + 2P}{S}\\right\\rfloor + 1\n$$\nLet's break this formula down:\n\n- $W - K$ represents how far the kernel can move\n- $2P$ accounts for padding on both sides\n- Division by $S$ reflects the stride's effect\n- The floor function $\\lfloor \\cdot \\rfloor$ ensures integer output\n- Adding 1 accounts for the initial position\n\nFor example, with an input size of 224√ó224, a 3√ó3 kernel, stride-2, and padding-1:\n$$\nO = \\left\\lfloor\\frac{224 - 3 + 2(1)}{2}\\right\\rfloor + 1 = 112\n$$\n\nThe interplay between stride and padding allows network designers to control information flow and computational efficiency. Larger strides create more compact representations but might miss fine details, while appropriate padding ensures no spatial information is unnecessarily discarded.\n\n```{note}\nTry out [CNN Explainer](https://poloclub.github.io/cnn-explainer/?norec=true) to learn how convolution, padding, and stride affect the output.\n\n\n\nPooling layers serve as the dimensionality reduction modules in CNNs, summarizing spatial regions into single values while preserving essential features. Max-pooling, for example, is a widely used pooling operation that selects the highest activation value within a local region. For a feature map F (i.e., intermediate representation of a convolutional layer), max pooling over a 3√ó3 window can be expressed as:\n\nP_{i,j} = \\max_{m=0,1}\\max_{n=0,1} F_{3i+m,3j+n}\n\nMax-pooling offers several key benefits. First, it creates a form of local translation invariance - small shifts in feature positions are absorbed by the pooling window. For instance, if an edge moves slightly within a 2√ó2 pooling region, the max-pooled output remains unchanged. Second, by reducing spatial dimensions, pooling significantly decreases computational complexity in subsequent layers.\nSimilarly, average pooling computes:\n\nP_{i,j} = \\frac{1}{9}\\sum_{m=0}^2\\sum_{n=0}^2 F_{3i+m,3j+n}\n\nThe output dimensions after a pooling operation follow a similar formula to strided convolutions, but without padding considerations:\n\nO = \\left\\lfloor\\frac{W - K}{S}\\right\\rfloor + 1\n\nwhere W is the input size, K is the pooling window size (typically 2 or 3), and S is the stride (usually equal to K).\nModern CNN architectures often debate the necessity of pooling layers {footcite}`springenberg2015striving`. Some networks replace them with strided convolutions, arguing that learnable downsampling might be more effective. The key difference lies in parameterization - pooling has no learnable parameters, while strided convolutions learn how to downsample. Consider these approaches:\n\n$$\n\\text{Max Pooling: } y = \\max(x_1, x_2, x_3, x_4)\n$$\n\n$$\n\\text{Strided Conv: } y = w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4\n$$\n\nDespite this trend, pooling layers remain valuable in many applications. They offer built-in invariance to small translations and rotations, reduce overfitting through their parameter-free nature, and provide consistent dimension reduction.\n:style: unsrt"
  },
  {
    "objectID": "m05-images/archive/alexnet.html",
    "href": "m05-images/archive/alexnet.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "How can we train a computer to recognize 1000 different types of objects across more than a million images, and what makes this problem so challenging?\n```tpudzjsbovsw AlexNet in interactive mode: :class: tip\nHere is a demo notebook for AlexNet\nTo run the notebook, download the notebook as a .py file and run it with:\n\nmarimo edit ‚Äìsandbox alexnet.py\n\nYou will need to install marimo and uv to run the notebook. But other packages will be installed automatically in uv‚Äôs virtual environment.\n\n\n\n## Conceptual Foundation: The ImageNet Challenge\n\nBefore diving into AlexNet, let's explore **why** large-scale image classification posed such a formidable problem for machine learning:\n\n1. **Massive Dataset**: The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) dataset contains over 1.2 million training images labeled across 1000 categories.\n2. **Computational Demands**: Traditional approaches struggled both with the sheer volume of data and with the complexity of designing handcrafted features.\n3. **Error Plateaus**: Prior to 2012, the best error rates hovered around 25%, suggesting that existing methods had nearly peaked using conventional feature-engineering techniques.\n\n```{figure} https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11263-015-0816-y/MediaObjects/11263_2015_816_Fig2_HTML.gif\n:width: 100%\n:align: center\n\nThe ImageNet Large Scale Visual Recognition Challenge (ILSVRC).\nPrior to AlexNet, most computer vision systems relied on hand-engineered features such as SIFT (Scale-Invariant Feature Transform) or HOG (Histogram of Oriented Gradients). These were labor-intensive to design and often failed to generalize well to diverse images.\nIn 2012, a team led by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton demonstrated a deep Convolutional Neural Network (CNN) that shattered expectations. Their submission, known as AlexNet, reduced the top-5 error rate to 16.4%‚Äîa remarkable improvement of over 10 percentage points compared to the next-best approach {footcite}krizhevsky2012alexnet.\n```pgvnuvcx https://viso.ai/wp-content/uploads/2024/02/imagenet-winners-by-year.jpg :width: 100% :align: center\nTop 5 error rates of the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) from 2010 to 2017. AlexNet reduced the error rate by over 10 percentage points compared to the best performing method based on human-crafted features in 2011.\n\nThis breakthrough ignited the **deep learning revolution** in computer vision. Researchers quickly realized the potential of stacking many layers of neural networks‚Äîprovided they could overcome critical hurdles in training and regularization.\n\n## Key Innovations in AlexNet\n\n### ReLU Activation Function\n\n**The Challenge**:\nDeep neural networks often suffer from the **vanishing gradient problem**, making early layers in the network extremely hard to train. Common activation functions like the sigmoid:\n\n$$\n\\sigma(x) = \\frac{1}{1 + e^{-x}}\n$$\n\nsuffer from saturation, where the gradient becomes almost zero if the input $x$ is large (positive or negative).\n\n**The AlexNet Solution**:\nIntroduce the **Rectified Linear Unit (ReLU)** {footcite}`nair2010rectified`:\n\n$$\n\\text{ReLU}(x) = \\max(0, x)\n$$\n\n```{figure} https://miro.medium.com/v2/resize:fit:474/1*HGctgaVdv9rEHIVvLYONdQ.jpeg\n:width: 100%\n:align: center\n\nSigmoid vs. ReLU activation functions. Sigmoid can cause gradients to vanish for $x$ far from zero, whereas ReLU maintains a constant gradient (1) for $x&gt;0$.\n\nBenefits:\n\nAvoids vanishing gradients for positive x.\nComputationally efficient: Only a simple check if x&gt;0.\n\nDrawback:\n\nNeurons can ‚Äúdie‚Äù (always output zero) if x stays negative. Variants like Leaky ReLU or Parametric ReLU (PReLU) introduce a small slope for x \\leq 0 to mitigate this.\n\n\nLeaky ReLU/PReLU are typically defined as:\n\n$$\nf(x) = \\begin{cases}\nx & \\text{if } x &gt; 0 \\\\\n\\alpha x & \\text{if } x \\leq 0\n\\end{cases}\n$$\n\nwhere $\\alpha$ is a small positive constant.\n\n\nDeep networks with millions of parameters can easily overfit to the training data, harming their ability to generalize.\nThe AlexNet solution is to use Dropout {footcite}srivastava2014dropout, a technique that randomly disables (or ‚Äúdrops out‚Äù) neurons with probability p during training.\n```pgvnuvcx https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/1IrdJ5PghD9YoOyVAQ73MJw.gif :width: 100% :align: center\nDropout in action.\n\n1. **Training**: Each neuron is \"dropped\" with probability $p$, forcing the network to not rely on any single neuron‚Äôs output.\n2. **Inference**: All neurons are used, but their outputs are scaled by $(1-p)$ to maintain expected values.\n\n```{note}\nAn alternative known as *inverse dropout* scales weights by $1/(1-p)$ during training, removing the need for scaling during inference. This is how many popular deep learning frameworks (e.g., TensorFlow) implement dropout.\n\n\n\n\nIn CNNs, neighboring feature maps can become disproportionately large, leading to unstable or less discriminative representations.\nIn the AlexNet, this is rectified by Local Response Normalization (LRN) {footcite}alexnet-lrn, which normalizes activity across adjacent channels:\n\nb^i_{x,y} = \\frac{a^i_{x,y}}{\\Bigl(k + \\alpha \\sum_{j=\\max(0,i-\\frac{n}{2})}^{\\min(N-1,i+\\frac{n}{2})} (a^j_{x,y})^2\\Bigr)^\\beta}\n\nHere, (a^i_{x,y}) is the activation at channel (i), and (k, , , n) are constants. LRN encourages local competition among adjacent channels, akin to certain neural mechanisms in biological systems.\nLRN is less commonly used in modern architectures (like VGG, ResNet, etc.) which often rely on batch normalization or other normalization techniques. Still, LRN was a key component in AlexNet‚Äôs success at the time.\n\n\n\n\nNow that we‚Äôve seen how AlexNet addressed vanishing gradients, overfitting, and feature-map normalization, let‚Äôs look at the overall blueprint:\n```pgvnuvcx ../figs/alexnet-architecture.jpg :width: 50% :align: center\nA high-level view of the AlexNet architecture.\n\n**Detailed Layer-by-Layer Overview**:\n\n1. **Input**: 3-channel color image, 224√ó224 pixels\n2. **Conv1**: [11√ó11, stride=4, padding=2] ‚Üí ReLU ‚Üí LRN ‚Üí Overlapping Max Pool ([3√ó3], stride=2)\n3. **Conv2**: [5√ó5, stride=1, padding=2] ‚Üí ReLU ‚Üí LRN ‚Üí Overlapping Max Pool ([3√ó3], stride=2)\n4. **Conv3**: [3√ó3, stride=1, padding=1] ‚Üí ReLU\n5. **Conv4**: [3√ó3, stride=1, padding=1] ‚Üí ReLU\n6. **Conv5**: [3√ó3, stride=1, padding=1] ‚Üí ReLU ‚Üí Overlapping Max Pool ([3√ó3], stride=2)\n7. **FC6**: Flatten ‚Üí 9216 ‚Üí 4096 ‚Üí ReLU ‚Üí Dropout\n8. **FC7**: 4096 ‚Üí 4096 ‚Üí ReLU ‚Üí Dropout\n9. **FC8**: 4096 ‚Üí 1000 ‚Üí Softmax Output\n\n```{tip}\n**Parallel GPU Computation**\nAlexNet was trained on two GPUs with 3GB of memory each, splitting feature maps between them to handle the large parameter count. This approach showcased the necessity and practicality of GPU computing for large-scale deep learning.\n\n\n\n\nBelow is a minimal snippet illustrating how one might instantiate an AlexNet-like model using PyTorch‚Äôs built-in module. For a step-by-step tutorial, check out Writing AlexNet from Scratch in PyTorch | DigitalOcean.\n```rdpwkbfteij ipython3 import torch import torch.nn as nn import torch.nn.functional as F\nclass SimpleAlexNet(nn.Module): def init(self, num_classes=1000): super(SimpleAlexNet, self).__init__() self.features = nn.Sequential( nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2), nn.ReLU(inplace=True), # LRN is omitted in modern PyTorch models, replaced by batch norm or left out nn.MaxPool2d(kernel_size=3, stride=2),\n        nn.Conv2d(96, 256, kernel_size=5, padding=2),\n        nn.ReLU(inplace=True),\n        # Another LRN placeholder if needed\n        nn.MaxPool2d(kernel_size=3, stride=2),\n\n        nn.Conv2d(256, 384, kernel_size=3, padding=1),\n        nn.ReLU(inplace=True),\n\n        nn.Conv2d(384, 384, kernel_size=3, padding=1),\n        nn.ReLU(inplace=True),\n\n        nn.Conv2d(384, 256, kernel_size=3, padding=1),\n        nn.ReLU(inplace=True),\n        nn.MaxPool2d(kernel_size=3, stride=2),\n    )\n\n    self.classifier = nn.Sequential(\n        nn.Dropout(),\n        nn.Linear(256 * 6 * 6, 4096),\n        nn.ReLU(inplace=True),\n        nn.Dropout(),\n        nn.Linear(4096, 4096),\n        nn.ReLU(inplace=True),\n        nn.Linear(4096, num_classes),\n    )\n\ndef forward(self, x):\n    x = self.features(x)\n    x = torch.flatten(x, 1)\n    x = self.classifier(x)\n    return x"
  },
  {
    "objectID": "m05-images/archive/alexnet.html#the-alexnet-architecture",
    "href": "m05-images/archive/alexnet.html#the-alexnet-architecture",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Now that we‚Äôve seen how AlexNet addressed vanishing gradients, overfitting, and feature-map normalization, let‚Äôs look at the overall blueprint:\n```pgvnuvcx ../figs/alexnet-architecture.jpg :width: 50% :align: center\nA high-level view of the AlexNet architecture.\n\n**Detailed Layer-by-Layer Overview**:\n\n1. **Input**: 3-channel color image, 224√ó224 pixels\n2. **Conv1**: [11√ó11, stride=4, padding=2] ‚Üí ReLU ‚Üí LRN ‚Üí Overlapping Max Pool ([3√ó3], stride=2)\n3. **Conv2**: [5√ó5, stride=1, padding=2] ‚Üí ReLU ‚Üí LRN ‚Üí Overlapping Max Pool ([3√ó3], stride=2)\n4. **Conv3**: [3√ó3, stride=1, padding=1] ‚Üí ReLU\n5. **Conv4**: [3√ó3, stride=1, padding=1] ‚Üí ReLU\n6. **Conv5**: [3√ó3, stride=1, padding=1] ‚Üí ReLU ‚Üí Overlapping Max Pool ([3√ó3], stride=2)\n7. **FC6**: Flatten ‚Üí 9216 ‚Üí 4096 ‚Üí ReLU ‚Üí Dropout\n8. **FC7**: 4096 ‚Üí 4096 ‚Üí ReLU ‚Üí Dropout\n9. **FC8**: 4096 ‚Üí 1000 ‚Üí Softmax Output\n\n```{tip}\n**Parallel GPU Computation**\nAlexNet was trained on two GPUs with 3GB of memory each, splitting feature maps between them to handle the large parameter count. This approach showcased the necessity and practicality of GPU computing for large-scale deep learning."
  },
  {
    "objectID": "m05-images/archive/alexnet.html#implementation-example",
    "href": "m05-images/archive/alexnet.html#implementation-example",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Below is a minimal snippet illustrating how one might instantiate an AlexNet-like model using PyTorch‚Äôs built-in module. For a step-by-step tutorial, check out Writing AlexNet from Scratch in PyTorch | DigitalOcean.\n```rdpwkbfteij ipython3 import torch import torch.nn as nn import torch.nn.functional as F\nclass SimpleAlexNet(nn.Module): def init(self, num_classes=1000): super(SimpleAlexNet, self).__init__() self.features = nn.Sequential( nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2), nn.ReLU(inplace=True), # LRN is omitted in modern PyTorch models, replaced by batch norm or left out nn.MaxPool2d(kernel_size=3, stride=2),\n        nn.Conv2d(96, 256, kernel_size=5, padding=2),\n        nn.ReLU(inplace=True),\n        # Another LRN placeholder if needed\n        nn.MaxPool2d(kernel_size=3, stride=2),\n\n        nn.Conv2d(256, 384, kernel_size=3, padding=1),\n        nn.ReLU(inplace=True),\n\n        nn.Conv2d(384, 384, kernel_size=3, padding=1),\n        nn.ReLU(inplace=True),\n\n        nn.Conv2d(384, 256, kernel_size=3, padding=1),\n        nn.ReLU(inplace=True),\n        nn.MaxPool2d(kernel_size=3, stride=2),\n    )\n\n    self.classifier = nn.Sequential(\n        nn.Dropout(),\n        nn.Linear(256 * 6 * 6, 4096),\n        nn.ReLU(inplace=True),\n        nn.Dropout(),\n        nn.Linear(4096, 4096),\n        nn.ReLU(inplace=True),\n        nn.Linear(4096, num_classes),\n    )\n\ndef forward(self, x):\n    x = self.features(x)\n    x = torch.flatten(x, 1)\n    x = self.classifier(x)\n    return x"
  },
  {
    "objectID": "m04-text/word-embeddings.html",
    "href": "m04-text/word-embeddings.html",
    "title": "Word Embeddings",
    "section": "",
    "text": "What you‚Äôll learn in this module\n\n\n\nMeaning isn‚Äôt stored in words as containers. It emerges from geometric relationships in high-dimensional space. We‚Äôll explore how Word2Vec learns these relationships through contrast, turning linguistic philosophy into runnable algorithms.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Embeddings"
    ]
  },
  {
    "objectID": "m04-text/word-embeddings.html#word2vec",
    "href": "m04-text/word-embeddings.html#word2vec",
    "title": "Word Embeddings",
    "section": "Word2vec",
    "text": "Word2vec\nLet‚Äôs first learn the power of Word2Vec and then understand how it works. We will use a pre-trained model. We aren‚Äôt teaching it anything; we are simply inspecting the map it created from 100 billion words of Google News.\n\nimport gensim.downloader as api\nimport numpy as np\n\n# Load pre-trained Word2vec embeddings\nprint(\"Loading Word2vec model...\")\nmodel = api.load(\"word2vec-google-news-300\")\nprint(f\"Loaded embeddings for {len(model):,} words.\")\n\nLoading Word2vec model...\nLoaded embeddings for 3,000,000 words.\n\n\nIf the map is accurate, ‚Äúdog‚Äù should be surrounded by its semantic kin. We query the nearest neighbors in the vector space.\n\nsimilar_to_dog = model.most_similar(\"dog\", topn=10)\n\nprint(\"Words most similar to 'dog':\")\nfor word, similarity in similar_to_dog:\n    print(f\"  {word:20s} {similarity:.3f}\")\n\nWords most similar to 'dog':\n  dogs                 0.868\n  puppy                0.811\n  pit_bull             0.780\n  pooch                0.763\n  cat                  0.761\n  golden_retriever     0.750\n  German_shepherd      0.747\n  Rottweiler           0.744\n  beagle               0.742\n  pup                  0.741\n\n\nThe model groups ‚Äúdog‚Äù with ‚Äúdogs,‚Äù ‚Äúpuppy,‚Äù and ‚Äúpooch‚Äù not because it knows biology, but because they are statistically interchangeable in sentences.\nSince words are vectors, we can perform arithmetic on meaning. The relationship between ‚ÄúKing‚Äù and ‚ÄúMan‚Äù is a vector. If we add that vector to ‚ÄúWoman,‚Äù we should arrive at ‚ÄúQueen.‚Äù\n \\vec{\\text{King}} - \\vec{\\text{Man}} + \\vec{\\text{Woman}} \\approx \\vec{\\text{Queen}} \n\nresult = model.most_similar(\n  positive=['king', 'woman'],\n   negative=['man'], topn=5\n)\n\nprint(\"king - man + woman =\")\nfor word, similarity in result:\n    print(f\"  {word:15s} {similarity:.3f}\")\n\nking - man + woman =\n  queen           0.712\n  monarch         0.619\n  princess        0.590\n  crown_prince    0.550\n  prince          0.538\n\n\nWe cannot see in 300 dimensions, but we can project the space down to 2D using PCA. This reveals the consistent structures‚Äîlike the ‚Äúcapital city‚Äù relationship‚Äîthat the model has learned.\n\n\nCode\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ncountries = [\"Germany\", \"France\", \"Italy\", \"Spain\", \"Portugal\", \"Greece\"]\ncapitals = [\"Berlin\", \"Paris\", \"Rome\", \"Madrid\", \"Lisbon\", \"Athens\"]\n\n# Get embeddings\ncountry_embeddings = np.array([model[country] for country in countries])\ncapital_embeddings = np.array([model[capital] for capital in capitals])\n\n# PCA to 2D\npca = PCA(n_components=2)\nembeddings = np.vstack([country_embeddings, capital_embeddings])\nembeddings_pca = pca.fit_transform(embeddings)\n\n# Create DataFrame\ndf = pd.DataFrame(embeddings_pca, columns=[\"PC1\", \"PC2\"])\ndf[\"Label\"] = countries + capitals\ndf[\"Type\"] = [\"Country\"] * len(countries) + [\"Capital\"] * len(capitals)\n\n# Plot\nfig, ax = plt.subplots(figsize=(12, 10))\n\nfor idx, row in df.iterrows():\n    color = \"#e74c3c\" if row[\"Type\"] == \"Country\" else \"#3498db\"\n    marker = \"o\" if row[\"Type\"] == \"Country\" else \"s\"\n    ax.scatter(\n        row[\"PC1\"],\n        row[\"PC2\"],\n        c=color,\n        marker=marker,\n        s=200,\n        edgecolors=\"black\",\n        linewidth=1.5,\n        alpha=0.7,\n        zorder=3,\n    )\n    ax.text(\n        row[\"PC1\"],\n        row[\"PC2\"] + 0.15,\n        row[\"Label\"],\n        fontsize=12,\n        ha=\"center\",\n        va=\"bottom\",\n        fontweight=\"bold\",\n        bbox=dict(facecolor=\"white\", edgecolor=\"none\", alpha=0.8),\n    )\n\n# Draw arrows\nfor i in range(len(countries)):\n    country_pos = df.iloc[i][[\"PC1\", \"PC2\"]].values\n    capital_pos = df.iloc[i + len(countries)][[\"PC1\", \"PC2\"]].values\n    ax.arrow(\n        country_pos[0],\n        country_pos[1],\n        capital_pos[0] - country_pos[0],\n        capital_pos[1] - country_pos[1],\n        color=\"gray\",\n        alpha=0.6,\n        linewidth=2,\n        head_width=0.15,\n        head_length=0.1,\n        zorder=2,\n    )\n\nax.set_title(\n    'The \"Capital Of\" Relationship as Parallel Transport',\n    fontsize=16,\n    fontweight=\"bold\",\n    pad=20,\n)\nax.grid(alpha=0.3, linestyle=\"--\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nThe ‚ÄòCapital Of‚Äô relationship appears as a consistent direction in vector space.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Embeddings"
    ]
  },
  {
    "objectID": "m04-text/word-embeddings.html#lets-unbox-word2vec.",
    "href": "m04-text/word-embeddings.html#lets-unbox-word2vec.",
    "title": "Word Embeddings",
    "section": "Let‚Äôs unbox Word2Vec.",
    "text": "Let‚Äôs unbox Word2Vec.\nWe intuitively treat words as containers that hold meaning‚Äîthat the word ‚ÄúGreen‚Äù contains the visual concept of a specific color. This is incorrect. Nature presents us with a messy, continuous spectrum without hard borders; language is simply the set of arbitrary cuts we make in that continuum to create order.\nWord2Vec operationalizes this by treating meaning as a game of contrast. It functions as a pair of ‚ÄúLinguistic Scissors.‚Äù It does not learn what a word is by looking up a definition; it learns what a word is like by pulling it close to neighbors, and more importantly, it learns what a word is not by pushing it away from random noise. The meaning of ‚ÄúGreen‚Äù is simply the geometric region that remains after we have pushed away ‚ÄúRed,‚Äù ‚ÄúPurple,‚Äù and ‚ÄúBanana.‚Äù\n\n\n\n\n\n\nFigure¬†2: Starting from initially random vectors, word2vec learns iteratively to push away the words that are not related, and pull the words that are related. The resulting vector space is a map of the relationships between words.\n\n\n\nThis process of carving structure out of noise relies on a technique called Contrastive Learning. We cannot teach the model the exact meaning of each word but we can let it to learn the relationship between words through a binary classification problem: are these two words neighbors, or are they strangers?\nThe training loop provides a positive pair from the text, instructing the model to maximize the similarity between their vectors. Simultaneously, it grabs random negative samples‚Äìimposters from the vocabulary‚Äìand demands the model minimize their similarity. This push-and-pull mechanic creates the vector space; the ‚ÄúGreen‚Äù cluster forms not because the model understands color, but because those words are statistically interchangeable when opposed to ‚ÄúRed.‚Äù\nTo generate these pairs without human labeling, we employ a sliding window technique. This moves over the raw text corpus, converting a sequence of words into a system of geometric queries.\n\n\n\n\n\n\nFigure¬†3: Without human labeling, word2vec assumes that the words in the same context are related. The context is defined as the words that are within a window of an predefined size. For example, in the sentence ‚ÄúThe quick brown fox jumps over the lazy dog‚Äù, the context of the word ‚Äúfox‚Äù is the words ‚Äúbrown‚Äù, ‚Äújumps‚Äù, ‚Äúover‚Äù, and ‚Äúlazy‚Äù.\n\n\n\n\n\nWord2Vec is a simple neural network with one-hidden layer. The input is one-hot encoded vector of a word, which triggers the neurons in the hidden layer to fire. The neural connection strength from the neuron representing the word to the neurons in the hidden layer (marked by red arrows) represents the query vector, u. The hidden layer neurons then trigger the firing of the output layer neurons, which represents the probability of word w appearing in the context of the word w_i. The connection strength from an output word neuron to the hidden layer neurons represents the key vector, v.\n\nThe word in the center of the window acts as the Query vector (u), broadcasting its position to the surrounding Context words, which act as Keys (v). The neural network adjusts its weights to maximize the dot product u \\cdot v for these specific context pairs while suppressing the dot product for the negative samples. The probability of a word appearing in context is thus a function of their vector alignment.\n\nP(j \\vert i) = \\frac{P(j) \\exp(u_i \\cdot v_j)}{\\sum_{k=1}^{V} P(k) \\exp(u_i \\cdot v_k)}\n\nwhere P(j) is the probability of word j appearing in the vocabulary.\n\n\n\n\n\n\nOriginal Formulation of Word2Vec is different from the one we use here\n\n\n\n\n\nThe original paper of word2vec puts the following formula for the probability:\n\nP(j \\vert i) = \\frac{\\exp(u_i \\cdot v_j)}{\\sum_{k=1}^{V} \\exp(u_i \\cdot v_k)}.\n\nNotice that P(j)‚Äîthe marginal probability of word j‚Äîis omitted in this formulation (Mikolov et al. 2013). This original formulation is correct conceptually but not practically. In practice, we train word2vec with an efficient but biased training algorithm (i.e., negative sampling). Term P(j) enters the P(j \\vert i) when taking into account the bias (kojaku2021residual?), which is why we include it in the formula above.\n\n\n\nThis closes the loop between high-level linguistic philosophy and low-level matrix operations. The machine proves the structuralist hypothesis: that meaning is relational. By mechanically slicing the continuum of language and applying the pressure of negative sampling, the model reconstructs a functional map of human concepts. We have successfully turned a philosophy of meaning into a runnable algorithm.\n\n\n\n\n\n\nFigure¬†4",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Embeddings"
    ]
  },
  {
    "objectID": "m04-text/word-embeddings.html#other-references",
    "href": "m04-text/word-embeddings.html#other-references",
    "title": "Word Embeddings",
    "section": "Other references",
    "text": "Other references\nThere is a nice blog post that walks through the inner workings of Word2Vec by Chris McCormick. See here. Strongly encourage you to read it.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Embeddings"
    ]
  },
  {
    "objectID": "m04-text/word-embeddings.html#the-takeaway",
    "href": "m04-text/word-embeddings.html#the-takeaway",
    "title": "Word Embeddings",
    "section": "The Takeaway",
    "text": "The Takeaway\nYou don‚Äôt need to know what a thing is to understand it; you only need to know where it stands relative to everything it isn‚Äôt.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Embeddings"
    ]
  },
  {
    "objectID": "m04-text/transformers.html",
    "href": "m04-text/transformers.html",
    "title": "Transformers",
    "section": "",
    "text": "What you‚Äôll learn in this module\n\n\n\nThe transformer revolution boils down to one insight: static embeddings assign one vector per word, ignoring that ‚Äúbank‚Äù near ‚Äúriver‚Äù is mathematically different from ‚Äúbank‚Äù near ‚Äúmoney‚Äù. Transformers solve this with context-aware representations through weighted mixing, where weights emerge from learned comparisons (Query times Key) between words. The result: machines finally understand that meaning lives in distribution, not in the word itself.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Transformers"
    ]
  },
  {
    "objectID": "m04-text/transformers.html#one-word-one-vector-is-not-enough",
    "href": "m04-text/transformers.html#one-word-one-vector-is-not-enough",
    "title": "Transformers",
    "section": "One word, one vector is not enough",
    "text": "One word, one vector is not enough\n\n\n\n\n\nFor many years, natural language processing treated words as having fixed meanings. We represented each word‚Äîlike ‚Äúbank‚Äù‚Äîas a single vector of numbers, called static embeddings.\nBut there‚Äôs a hidden catch in this ‚Äúone meaning per word‚Äù mindset: with just a single fixed entry in the dictionary, ‚Äúbank‚Äù means exactly the same thing in ‚ÄúI deposited money at the bank‚Äù as in ‚ÄúWe had a picnic by the bank.‚Äù Every possible meaning gets mashed into a one-size-fits-all average‚Äîlike describing the population by its average height and pretending that nobody‚Äôs any shorter or taller. The interesting details‚Äîthe outliers, the context clues‚Äîvanish in the mix.\nThe naive hypothesis went like this: what if we just mix the target word with its neighbors? For the sentence ‚ÄúI deposited money at the bank,‚Äù we could compute a contextualized representation as:\n\n\\vec{v}_{\\text{bank (new)}} = w_1 \\cdot \\vec{v}_{\\text{bank}} + w_2 \\cdot \\vec{v}_{\\text{deposited}} + w_3 \\cdot \\vec{v}_{\\text{money}} + \\cdots\n\nwhere w_i are weights and \\vec{v}_i are word embeddings.\nConsider the following example. Notice that ‚Äúbank‚Äù sits neutrally between financial terms (money) and geographical terms (river). Now try manually adjusting the weights to contextualize ‚Äúbank‚Äù:\n\nd3 = require(\"d3@7\", \"d3-simple-slider@1\")\n\n\n\n\n\n\n\nfunction sliderWithLabel(min, max, step, width, defaultValue, label) {\n  const slider = d3.sliderBottom()\n    .min(min).max(max).step(step).width(width).default(defaultValue);\n  const svg = d3.create(\"svg\").attr(\"width\", width + 50).attr(\"height\", 60);\n  svg.append(\"g\").attr(\"transform\", \"translate(25,20)\").call(slider);\n  svg.append(\"text\").attr(\"x\", (width + 50) / 2).attr(\"y\", 10).attr(\"text-anchor\", \"middle\").style(\"font-size\", \"5px\").text(label);\n  return svg.node();\n}\n\n\n\n\n\n\n\n{\n  // Create slider function that returns both the element and a reactive value\n  function createWeightSlider(min, max, step, width, defaultValue, label) {\n    const slider = d3.sliderBottom()\n      .min(min).max(max).step(step).width(width).default(defaultValue);\n    const svg = d3.create(\"svg\").attr(\"width\", width + 50).attr(\"height\", 60);\n    const g = svg.append(\"g\").attr(\"transform\", \"translate(25,20)\");\n    g.call(slider);\n    svg.append(\"text\").attr(\"x\", (width + 50) / 2).attr(\"y\", 10)\n       .attr(\"text-anchor\", \"middle\").style(\"font-size\", \"12px\").text(label);\n    return { node: svg.node(), slider: slider };\n  }\n\n  // Create sliders\n  const bankSliderObj = createWeightSlider(0, 1, 0.01, 120, 1.0, \"Bank weight\");\n  const moneySliderObj = createWeightSlider(0, 1, 0.01, 120, 0.0, \"Money weight\");\n  const riverSliderObj = createWeightSlider(0, 1, 0.01, 120, 0.0, \"River weight\");\n\n  // Word embeddings in 2D space\n  const contextWords = [\"bank\", \"money\", \"river\"];\n  const contextEmbeddings = [\n    [0.0, 0.0],   // bank (center)\n    [-1.6, -0.6], // money (financial, left)\n    [1.4, -1.0]   // river (geographical, right)\n  ];\n\n  // Create plot container\n  const plotContainer = document.createElement(\"div\");\n\n  // Function to update visualization\n  function update() {\n    // Get current slider values\n    const bankWeight = bankSliderObj.slider.value();\n    const moneyWeight = moneySliderObj.slider.value();\n    const riverWeight = riverSliderObj.slider.value();\n\n    // Calculate weighted average\n    const weights = [bankWeight, moneyWeight, riverWeight];\n    const total = weights.reduce((a, b) =&gt; a + b, 0);\n    const normalizedWeights = total &gt; 0 ? weights.map(w =&gt; w / total) : [0, 0, 0];\n\n    const newVec = [\n      normalizedWeights[0] * contextEmbeddings[0][0] +\n      normalizedWeights[1] * contextEmbeddings[1][0] +\n      normalizedWeights[2] * contextEmbeddings[2][0],\n      normalizedWeights[0] * contextEmbeddings[0][1] +\n      normalizedWeights[1] * contextEmbeddings[1][1] +\n      normalizedWeights[2] * contextEmbeddings[2][1]\n    ];\n\n    // Prepare data for visualization\n    const originalData = contextWords.map((word, i) =&gt; ({\n      word: word,\n      x: contextEmbeddings[i][0],\n      y: contextEmbeddings[i][1],\n      type: \"Original\"\n    }));\n\n    const contextualizedData = [{\n      word: \"bank (new)\",\n      x: newVec[0],\n      y: newVec[1],\n      type: \"Contextualized\"\n    }];\n\n    const data = [...originalData, ...contextualizedData];\n\n    // Clear and update plot\n    d3.select(plotContainer).selectAll(\"*\").remove();\n\n    // Create visualization\n    const plot = Plot.plot({\n      width: 300,\n      height: 300,\n      marginTop: 60,\n      marginRight: 20,\n      marginBottom: 50,\n      marginLeft: 60,\n      style: {\n        background: \"white\",\n        color: \"black\"\n      },\n      x: {\n        domain: [-2, 2],\n        label: \"Dimension 1\",\n        grid: true,\n        ticks: 10\n      },\n      y: {\n        domain: [-2, 2],\n        label: \"Dimension 2\",\n        grid: true,\n        ticks: 10\n      },\n      color: {\n        domain: [\"Original\", \"Contextualized\"],\n        range: [\"#dadada\", \"#ff7f0e\"]\n      },\n      marks: [\n        Plot.dot(data, {\n          x: \"x\",\n          y: \"y\",\n          fill: \"type\",\n          r: 8,\n          tip: true\n        }),\n        Plot.text(data, {\n          x: \"x\",\n          y: \"y\",\n          text: \"word\",\n          dy: -15,\n          fontSize: 8,\n          fontWeight: \"bold\",\n          fill: \"black\"\n        }),\n        Plot.text([{x: 0, y: 2.3}], {\n          x: \"x\",\n          y: \"y\",\n          text: () =&gt; `Weights: Bank=${normalizedWeights[0].toFixed(2)}, Money=${normalizedWeights[1].toFixed(2)}, River=${normalizedWeights[2].toFixed(2)}`,\n          fontSize: 11,\n          fill: \"black\"\n        }),\n        // Custom legend at top center\n        Plot.dot([{x: -0.8, y: 2.7, color: \"#dadada\"}, {x: 0.8, y: 2.7, color: \"#ff7f0e\"}], {\n          x: \"x\",\n          y: \"y\",\n          fill: \"color\",\n          r: 6\n        }),\n        Plot.text([{x: -0.5, y: 2.7, label: \"Original\"}, {x: 1.1, y: 2.7, label: \"Contextualized\"}], {\n          x: \"x\",\n          y: \"y\",\n          text: \"label\",\n          fontSize: 10,\n          fill: \"black\",\n          textAnchor: \"start\"\n        })\n      ]\n    });\n\n    d3.select(plotContainer).node().appendChild(plot);\n  }\n\n  // Add event listeners to sliders\n  bankSliderObj.slider.on(\"onchange\", update);\n  moneySliderObj.slider.on(\"onchange\", update);\n  riverSliderObj.slider.on(\"onchange\", update);\n\n  // Initial render\n  update();\n\n  return html`&lt;div style=\"display: flex; align-items: center; gap: 40px; justify-content: center;\"&gt;\n    &lt;div style=\"display: flex; flex-direction: column; gap: 10px;\"&gt;\n      ${bankSliderObj.node}\n      ${moneySliderObj.node}\n      ${riverSliderObj.node}\n    &lt;/div&gt;\n    &lt;div&gt;\n      ${plotContainer}\n    &lt;/div&gt;\n  &lt;/div&gt;`;\n}\n\n\n\n\n\n\nBy changing the weights, we can see that the vector for ‚Äúbank‚Äù can lean more towards the financial terms or the geographical terms. So how can we determine the weights?\nThe simplest idea is to give each word an equal weight: w_i = 1/N. This creates a basic ‚Äúbag-of-words‚Äù average. But sentences aren‚Äôt actually this fair‚Äîsome words are much more important than others. For example, in ‚ÄúI deposited money at the bank,‚Äù the words ‚Äúdeposited‚Äù and ‚Äúmoney‚Äù are key, while ‚ÄúI,‚Äù ‚Äúat,‚Äù and ‚Äúthe‚Äù add little meaning. If we treat all words the same, we lose the details that matter. We need a way to highlight the important words and downplay the rest.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Transformers"
    ]
  },
  {
    "objectID": "m04-text/transformers.html#attention-mechanism",
    "href": "m04-text/transformers.html#attention-mechanism",
    "title": "Transformers",
    "section": "Attention Mechanism",
    "text": "Attention Mechanism\n\n\n\n\n\nLet‚Äôs walk through how transformers identify the surrounding words that are relevant to a focal word to be contextualized. This process is called the attention mechanism. Before diving in, let‚Äôs prepare some terminology.\nSuppose we have the sentence ‚ÄúI deposited money at the bank‚Äù. Given the word ‚Äúbank‚Äù, we want to determine the weights w_i for the surrounding words ‚ÄúI‚Äù, ‚Äúdeposited‚Äù, ‚Äúmoney‚Äù, and ‚Äúat‚Äù. We call ‚Äúbank‚Äù the query word, and the surrounding words the key words. At a high level, we compute the weights w_i for each query and key pair, then average them.\n\n\\vec{v}_{\\text{query}}^{\\text{c}} = \\sum_{i=1}^N w_i \\cdot \\vec{v}_{i}\n\nwith weights w_i being determined by the query and key vectors w_{i}:=f(\\vec{v}_{\\text{query}}, \\vec{v}_{i}). This function, f, is called the attention score function.\nIn transformers, the attention score function f is implemented as follows. Given the original vector for a word (whether it‚Äôs the query word or the key word), we linearly transform it into three vectors: Query, Key, and Value.\n\n\\begin{align}\n\\vec{q}_i &= W_Q \\vec{x}_i\\\\\n\\vec{k}_i &= W_K \\vec{x}_i\\\\\n\\vec{v}_i &= W_V \\vec{x}_i\n\\end{align}\n\nWhy do we need three different vectors? Imagine you‚Äôre at a dinner party. You want to identify people talking about a topic you care about. You listen to surrounding people (playing as a listener), broadcast your own interests (playing as a speaker), and engage with conversation content. The query vector represents you as a listener, the key vector represents the people as speakers, and the value vector represents the conversation content.\nOnce we have the query, key, and value vectors, we compute the attention scores between the query and key vector:\n\nw_{ij} = \\frac{\\exp(\\vec{q}_i \\cdot \\vec{k}_j / \\sqrt{d})}{\\sum_{\\ell} \\exp(\\vec{q}_i \\cdot \\vec{k}_\\ell / \\sqrt{d})},\n\nwhere \\vec{q}_i \\cdot \\vec{k}_j is the dot product between the query and key vectors, which is larger when the query and key vectors are similar (pointing to a similar direction). The division by \\sqrt{d} (where d is the embedding dimension) is a scaling factor that prevents vanishing gradients during training. Finally, compute the contextualized representation as a weighted sum: \\text{contextualized}_i = \\sum_j w_{ij} \\vec{v}_j.\nWhat is the vanishing gradient problem? It‚Äôs when gradients of the loss function with respect to weights become too small to be effective during training.\nExplore how different Query and Key transformations produce different attention patterns. First, let us create the key, query, and value vectors. In 2d, the linear transformation is just a scaling and a rotation.\n\nfunction createQKVSlider(min, max, step, width, defaultValue, label, valueSetter) {\n  const slider = d3.sliderBottom()\n    .min(min).max(max).step(step).width(width).default(defaultValue)\n    .on('onchange', val =&gt; valueSetter(val));\n  const svg = d3.create(\"svg\").attr(\"width\", width + 40).attr(\"height\", 50);\n  const g = svg.append(\"g\").attr(\"transform\", \"translate(20,15)\");\n  g.call(slider);\n  svg.append(\"text\").attr(\"x\", (width + 40) / 2).attr(\"y\", 10)\n     .attr(\"text-anchor\", \"middle\").style(\"font-size\", \"11px\").text(label);\n  return { node: svg.node(), slider: slider };\n}\n\n\n\n\n\n\n\nmutable qScaleXValue = 1.0\n\n\n\n\n\n\n\nmutable qScaleYValue = 1.0\n\n\n\n\n\n\n\nmutable qRotateValue = 0\n\n\n\n\n\n\n\nmutable kScaleXValue = 1.0\n\n\n\n\n\n\n\nmutable kScaleYValue = 1.0\n\n\n\n\n\n\n\nmutable kRotateValue = 0\n\n\n\n\n\n\n\nqScaleXSlider = createQKVSlider(0.1, 2, 0.1, 150, 1.0, \"Q Scale X\", val =&gt; mutable qScaleXValue = val)\n\n\n\n\n\n\n\nqScaleYSlider = createQKVSlider(0.1, 2, 0.1, 150, 1.0, \"Q Scale Y\", val =&gt; mutable qScaleYValue = val)\n\n\n\n\n\n\n\nqRotateSlider = createQKVSlider(-180, 180, 5, 150, 0, \"Q Rotate (deg)\", val =&gt; mutable qRotateValue = val)\n\n\n\n\n\n\n\nkScaleXSlider = createQKVSlider(0.1, 2, 0.1, 150, 1.0, \"K Scale X\", val =&gt; mutable kScaleXValue = val)\n\n\n\n\n\n\n\nkScaleYSlider = createQKVSlider(0.1, 2, 0.1, 150, 1.0, \"K Scale Y\", val =&gt; mutable kScaleYValue = val)\n\n\n\n\n\n\n\nkRotateSlider = createQKVSlider(-180, 180, 5, 150, 0, \"K Rotate (deg)\", val =&gt; mutable kRotateValue = val)\n\n\n\n\n\n\n\nqkvVisualization = {\n  const originalVectors = [\n    { name: \"bank\", vector: [1.5, 0.5] },\n    { name: \"money\", vector: [1.8, 0.8] },\n    { name: \"river\", vector: [0.5, 1.5] }\n  ];\n\n  const qPlotContainer = document.createElement(\"div\");\n  const kPlotContainer = document.createElement(\"div\");\n\n  function transformVector(vec, scaleX, scaleY, rotateDeg) {\n    const theta = (rotateDeg * Math.PI) / 180;\n    const cos = Math.cos(theta);\n    const sin = Math.sin(theta);\n    const scaledX = vec[0] * scaleX;\n    const scaledY = vec[1] * scaleY;\n    return [\n      scaledX * cos - scaledY * sin,\n      scaledX * sin + scaledY * cos\n    ];\n  }\n\n  const qScaleX = qScaleXValue;\n  const qScaleY = qScaleYValue;\n  const qRotate = qRotateValue;\n  const kScaleX = kScaleXValue;\n  const kScaleY = kScaleYValue;\n  const kRotate = kRotateValue;\n\n  const originalData = originalVectors.map(item =&gt; ({\n    name: item.name,\n    x: item.vector[0],\n    y: item.vector[1],\n    type: \"Original\"\n  }));\n\n  const qData = originalVectors.map(item =&gt; {\n    const qVec = transformVector(item.vector, qScaleX, qScaleY, qRotate);\n    return {\n      name: `q_${item.name}`,\n      x: qVec[0],\n      y: qVec[1],\n      type: \"Query\"\n    };\n  });\n\n  const kData = originalVectors.map(item =&gt; {\n    const kVec = transformVector(item.vector, kScaleX, kScaleY, kRotate);\n    return {\n      name: `k_${item.name}`,\n      x: kVec[0],\n      y: kVec[1],\n      type: \"Key\"\n    };\n  });\n\n  const qPlot = Plot.plot({\n    width: 250,\n    height: 250,\n    marginTop: 40,\n    marginRight: 20,\n    marginBottom: 50,\n    marginLeft: 60,\n    style: {\n      background: \"white\",\n      color: \"black\"\n    },\n    x: {\n      domain: [-3, 3],\n      label: \"Dimension 1\",\n      grid: true,\n      ticks: 10\n    },\n    y: {\n      domain: [-3, 3],\n      label: \"Dimension 2\",\n      grid: true,\n      ticks: 10\n    },\n    marks: [\n      Plot.dot([{x: 0, y: 0}], {\n        x: \"x\",\n        y: \"y\",\n        r: 3,\n        fill: \"black\"\n      }),\n      Plot.arrow([...originalData, ...qData], {\n        x1: 0,\n        y1: 0,\n        x2: \"x\",\n        y2: \"y\",\n        stroke: \"type\",\n        strokeWidth: 2,\n        headLength: 8\n      }),\n      Plot.dot([...originalData, ...qData], {\n        x: \"x\",\n        y: \"y\",\n        fill: \"type\",\n        r: 5,\n        tip: true\n      }),\n      Plot.text([...originalData, ...qData], {\n        x: \"x\",\n        y: \"y\",\n        text: \"name\",\n        dy: -12,\n        fontSize: 9,\n        fontWeight: \"bold\",\n        fill: \"black\"\n      }),\n      Plot.text([{ x: 0, y: 3.4 }], {\n        x: \"x\",\n        y: \"y\",\n        text: () =&gt; \"Query Space\",\n        fontSize: 12,\n        fontWeight: \"bold\",\n        fill: \"black\"\n      })\n    ],\n    color: {\n      domain: [\"Original\", \"Query\"],\n      range: [\"#666666\", \"#4682b4\"]\n    }\n  });\n\n  const kPlot = Plot.plot({\n    width: 250,\n    height: 250,\n    marginTop: 40,\n    marginRight: 20,\n    marginBottom: 50,\n    marginLeft: 60,\n    style: {\n      background: \"white\",\n      color: \"black\"\n    },\n    x: {\n      domain: [-3, 3],\n      label: \"Dimension 1\",\n      grid: true,\n      ticks: 10\n    },\n    y: {\n      domain: [-3, 3],\n      label: \"Dimension 2\",\n      grid: true,\n      ticks: 10\n    },\n    marks: [\n      Plot.dot([{x: 0, y: 0}], {\n        x: \"x\",\n        y: \"y\",\n        r: 3,\n        fill: \"black\"\n      }),\n      Plot.arrow([...originalData, ...kData], {\n        x1: 0,\n        y1: 0,\n        x2: \"x\",\n        y2: \"y\",\n        stroke: \"type\",\n        strokeWidth: 2,\n        headLength: 8\n      }),\n      Plot.dot([...originalData, ...kData], {\n        x: \"x\",\n        y: \"y\",\n        fill: \"type\",\n        r: 5,\n        tip: true\n      }),\n      Plot.text([...originalData, ...kData], {\n        x: \"x\",\n        y: \"y\",\n        text: \"name\",\n        dy: -12,\n        fontSize: 9,\n        fontWeight: \"bold\",\n        fill: \"black\"\n      }),\n      Plot.text([{ x: 0, y: 3.4 }], {\n        x: \"x\",\n        y: \"y\",\n        text: () =&gt; \"Key Space\",\n        fontSize: 12,\n        fontWeight: \"bold\",\n        fill: \"black\"\n      })\n    ],\n    color: {\n      domain: [\"Original\", \"Key\"],\n      range: [\"#666666\", \"#2e8b57\"]\n    }\n  });\n\n  d3.select(qPlotContainer).node().appendChild(qPlot);\n  d3.select(kPlotContainer).node().appendChild(kPlot);\n\n  return html`&lt;div style=\"display: flex; justify-content: center; gap: 40px;\"&gt;\n    &lt;div style=\"display: flex; flex-direction: column; gap: 20px;\"&gt;\n      &lt;div style=\"display: flex; align-items: center; gap: 20px;\"&gt;\n        &lt;div style=\"display: flex; flex-direction: column; gap: 8px;\"&gt;\n          &lt;div style=\"font-weight: bold; margin-bottom: 3px;\"&gt;Query (W_Q)&lt;/div&gt;\n          ${qScaleXSlider.node}\n          ${qScaleYSlider.node}\n          ${qRotateSlider.node}\n        &lt;/div&gt;\n        ${qPlotContainer}\n      &lt;/div&gt;\n      &lt;div style=\"display: flex; align-items: center; gap: 20px;\"&gt;\n        &lt;div style=\"display: flex; flex-direction: column; gap: 8px;\"&gt;\n          &lt;div style=\"font-weight: bold; margin-bottom: 3px;\"&gt;Key (W_K)&lt;/div&gt;\n          ${kScaleXSlider.node}\n          ${kScaleYSlider.node}\n          ${kRotateSlider.node}\n        &lt;/div&gt;\n        ${kPlotContainer}\n      &lt;/div&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;`;\n}\n\n\n\n\n\n\nUsing the transformations above, we can compute the attention weights showing how each word attends to every other word:\n\nattentionHeatmap = {\n  const attentionWords = [\"bank\", \"money\", \"river\"];\n  const attentionEmbeddings = [\n    [1.5, 0.5],\n    [1.8, 0.8],\n    [0.5, 1.5]\n  ];\n\n  function transformVector(vec, scaleX, scaleY, rotateDeg) {\n    const theta = (rotateDeg * Math.PI) / 180;\n    const cos = Math.cos(theta);\n    const sin = Math.sin(theta);\n    const scaledX = vec[0] * scaleX;\n    const scaledY = vec[1] * scaleY;\n    return [\n      scaledX * cos - scaledY * sin,\n      scaledX * sin + scaledY * cos\n    ];\n  }\n\n  const qScaleX = qScaleXValue;\n  const qScaleY = qScaleYValue;\n  const qRotate = qRotateValue;\n  const kScaleX = kScaleXValue;\n  const kScaleY = kScaleYValue;\n  const kRotate = kRotateValue;\n\n  const Q = attentionEmbeddings.map(vec =&gt;\n    transformVector(vec, qScaleX, qScaleY, qRotate)\n  );\n  const K = attentionEmbeddings.map(vec =&gt;\n    transformVector(vec, kScaleX, kScaleY, kRotate)\n  );\n\n  const scores = Q.map(q =&gt; K.map(k =&gt; q[0] * k[0] + q[1] * k[1]));\n\n  const attentionWeights = scores.map(row =&gt; {\n    const maxScore = Math.max(...row);\n    const expScores = row.map(s =&gt; Math.exp(s - maxScore));\n    const sumExp = expScores.reduce((a, b) =&gt; a + b, 0);\n    return expScores.map(e =&gt; e / sumExp);\n  });\n\n  const heatmapData = (() =&gt; {\n    const data = [];\n    for (let i = 0; i &lt; attentionWords.length; i++) {\n      for (let j = 0; j &lt; attentionWords.length; j++) {\n        data.push({\n          Query: attentionWords[i],\n          Key: attentionWords[j],\n          Weight: attentionWeights[i][j]\n        });\n      }\n    }\n    return data;\n  })();\n\n  const heatmapPlot = Plot.plot({\n    width: 320,\n    height: 320,\n    marginTop: 50,\n    marginBottom: 50,\n    marginLeft: 70,\n    marginRight: 80,\n    style: {\n      background: \"white\",\n      color: \"black\"\n    },\n    x: {\n      label: \"Key Word\",\n      domain: attentionWords\n    },\n    y: {\n      label: \"Query Word\",\n      domain: attentionWords\n    },\n    color: {\n      scheme: \"Blues\",\n      label: \"Attention\",\n      legend: true\n    },\n    marks: [\n      Plot.cell(heatmapData, {\n        x: \"Key\",\n        y: \"Query\",\n        fill: \"Weight\",\n        tip: true\n      }),\n      Plot.text(heatmapData, {\n        x: \"Key\",\n        y: \"Query\",\n        text: d =&gt; d.Weight.toFixed(2),\n        fill: d =&gt; d.Weight &gt; 0.35 ? \"white\" : \"black\",\n        fontSize: 11\n      }),\n      Plot.text([{ x: 0, y: 0 }], {\n        x: () =&gt; attentionWords.length / 2 - 0.5,\n        y: () =&gt; -0.8,\n        text: () =&gt; \"Attention Weights (Softmax)\",\n        fontSize: 12,\n        fontWeight: \"bold\",\n        frameAnchor: \"top\",\n        fill: \"black\"\n      })\n    ]\n  });\n\n  return html`&lt;div style=\"display: flex; justify-content: center;\"&gt;\n    ${heatmapPlot}\n  &lt;/div&gt;`;\n}\n\n\n\n\n\n\nRows represent words asking for context (Queries). Columns represent words providing context (Keys). Each cell (i,j) indicates how much word i attends to word j. Each row sums to 1, forming a probability distribution over context words.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Transformers"
    ]
  },
  {
    "objectID": "m04-text/transformers.html#multi-head-attention",
    "href": "m04-text/transformers.html#multi-head-attention",
    "title": "Transformers",
    "section": "Multi-head Attention",
    "text": "Multi-head Attention\n\n\n\n\n\nPutting it all together (query-key-value transformation, attention matrix, and softmax normalization), this forms one attention head of the transformer. We can have multiple attention heads in parallel, each with its own query-key-value transformation, attention matrix, and softmax normalization. The output of the attention heads are concatenated and then passed through a linear transformation to produce the final output.\n\n\\text{Output} = \\text{Linear}(\\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h))\n\nThis is one attention block of the transformer. Having parallel attention heads is a powerful technique to capture different aspects of the input data. The model can learn multiple relationships between the words in the input data.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Transformers"
    ]
  },
  {
    "objectID": "m04-text/transformers.html#transformer-architecture",
    "href": "m04-text/transformers.html#transformer-architecture",
    "title": "Transformers",
    "section": "Transformer Architecture",
    "text": "Transformer Architecture\nLet‚Äôs step back and look at the transformer architecture at a high level. We base our discussion on the original Transformer paper, ‚ÄúAttention Is All You Need‚Äù. Note that the transformer architecture has evolved since then, with many variants.\n\nEncoder Module\n\n\n\n\n\nThe encoder module consists of position embedding, multi-head attention, residual connection, and layer normalization, along with feed-forward networks. Let us go through each component in detail.\n\nPosition Embedding\n\n\n\n\n\nIn the encoder module, we start from the positional encoding, which fixes a key issue: the attention modules are permutation invariant. That is, attention produces the same output even if we shuffle the words in the sentence. But position matters in language understanding and generation. Position encoding fixes this issue.\nLet‚Äôs approach position encoding from a naive perspective. Suppose we have a sequence of T token embeddings, denoted by x_1, x_2, ..., x_T, each a d-dimensional vector. A simple way to encode position is to add a position index to each token embedding:\n\nx_t := x_t + \\beta t,\n\nwhere t = 1, 2, ..., T is the position index of the token in the sequence, and \\beta is the step size. This appears simple but has critical problems. First, the position index can be arbitrarily large. When models see sequences longer than those in training data, they may suffer because they‚Äôll be exposed to position indices they‚Äôve never seen before. Second, the position index is discrete, meaning the model cannot capture position information smoothly.\nBecause this naive approach has problems, consider another approach. Let‚Äôs represent position using a binary vector of length d. For example, with d=4:\n\n\\begin{align*}\n  0: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} & &\n  8: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n  1: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} & &\n  9: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} \\\\\n  2: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} & &\n  10: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n  3: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} & &\n  11: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} \\\\\n  4: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} & &\n  12: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n  5: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} & &\n  13: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} \\\\\n  6: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} & &\n  14: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n  7: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} & &\n  15: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} \\\\\n\\end{align*}\n\nThen, use the binary vector as the position embedding:\n\nx_{t,i} := x_{t,i} + \\text{Pos}(t, i),\n\nwhere \\text{Pos}(t, i) is the position embedding vector of position index t and dimension index i. This representation is bounded between 0 and 1, yet still discrete.\nAn elegant position embedding, used in transformers, is sinusoidal position embedding. It appears complicated but stay with me.\n\n\\text{Pos}(t, i) =\n\\begin{cases}\n\\sin\\left(\\dfrac{t}{10000^{2i/d}}\\right), & \\text{if } i \\text{ is even} \\\\\n\\cos\\left(\\dfrac{t}{10000^{2i/d}}\\right), & \\text{if } i \\text{ is odd}\n\\end{cases},\n\nwhere i is the dimension index of the position embedding vector. This position embedding is added to the input token embedding as:\n\nx_{t,i} := x_{t,i} + \\text{Pos}(t, i),\n\nIt appears complicated, but it‚Äôs a continuous version of the binary position embedding above. To see this, let‚Äôs plot the position embedding for the first 100 positions.\n\n\n\n\n\nThe position embedding exhibits the alternating pattern (vertically) with frequency increasing as the dimension index increases (horizontal axis). Additionally, sinusoidal position embedding is continuous, allowing the model to capture position information smoothly.\nAnother key property: the dot similarity between two position embedding vectors represents the similarity between the two positions, regardless of the position index.\n\n\n\n\n\nThe dot similarity between position embedding vectors represents the distance between positions, regardless of the position index.\nWhy additive position embedding? Sinusoidal position embedding is additive, altering the token embedding. Alternatively, one might concatenate the position embedding to the token embedding: x_{t,i} := [x_{t,i}; \\text{Pos}(t, i)]. This makes it easier for a model to distinguish position from token information. So why not use concatenation? One reason: concatenation requires a larger embedding dimension, increasing the number of parameters. Instead, adding the position embedding creates an interesting effect in the attention mechanism. Interested readers can check out this Reddit post.\nAbsolute position embedding is what we discussed above, where each position is represented by a unique vector. Relative position embedding, on the other hand, represents the position difference between two positions rather than the absolute position. Relative position embedding can be implemented by adding a learnable scalar to the unnormalized attention scores before softmax:\n\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T + B}{\\sqrt{d_k}}\\right)V\n\nwhere B is a learnable offset matrix added to the unnormalized attention scores. The matrix B is a function of the position difference between query and key: B = f(i-j), where i and j are the position indices of query and key. Such formulation is useful when the model needs to capture relative position between tokens.\n\n\nResidual Connection\n\n\n\n\n\nAnother important component is the residual connection. The input is first passed through multi-head attention, followed by layer normalization. Notice the parallel path from input to the output of the attention module. This is called a residual connection, or skip connection. It‚Äôs a technique used to stabilize the training of deep neural networks by mitigating the problem of too large or too small input values, which can cause network instability.\nLet‚Äôs denote by f the neural network we want to train (the multi-head attention or feed-forward networks in the transformer block). The residual connection is defined as:\n\n\\underbrace{x_{\\text{out}}}_{\\text{output}} = \\underbrace{x_{\\text{in}}}_{\\text{input}} + \\underbrace{f(x_{\\text{in}})}_{\\text{component}}.\n\nRather than learning the complete mapping from input to output, the network f learns to model the residual (difference) between them. This is particularly advantageous when the desired transformation approximates an identity mapping, as the network can simply learn to output values near zero.\nResidual connections help prevent the vanishing gradient problem. Deep learning models like LLMs consist of many layers, trained to minimize the loss function {\\cal L}_{\\text{loss}} with respect to parameters \\theta. The gradient of the loss is computed using the chain rule:\n\n\\frac{\\partial {\\cal L}_{\\text{loss}}}{\\partial \\theta} = \\frac{\\partial {\\cal L}_{\\text{loss}}}{\\partial f_L} \\cdot \\frac{\\partial f_L}{\\partial f_{L-1}} \\cdot \\frac{\\partial f_{L-1}}{\\partial f_{L-2}} \\cdot ... \\cdot \\frac{\\partial f_{l+1}}{\\partial f_l} \\cdot \\frac{\\partial f_l}{\\partial \\theta}\n\nwhere f_i is the output of the i-th layer. The gradient vanishing problem occurs when the individual terms \\frac{\\partial f_{i+1}}{\\partial f_i} are less than 1. As a result, the gradient becomes smaller and smaller as it flows backward through earlier layers. By adding the residual connection, the gradient for the individual term becomes:\n\n\\frac{\\partial x_{i+1}}{\\partial x_i} = 1 + \\frac{\\partial f_i(x_i)}{\\partial x_i}\n\nNotice the ‚Äú+1‚Äù term, which is the direct path from input to output. The chain rule is thus modified to include this term. When we expand the product, we can group terms by their order (how many \\partial f_i terms are multiplied together):\n1 + O_1 + O_2 + O_3 + ...\nwhere the O_n terms represent various combinations of gradients at different orders. Without the residual connection, we only have the highest-order terms, which are subject to the gradient vanishing problem. With the residual connection, we have lower-order terms like O_1, O_2, O_3, ..., which are less susceptible to gradient vanishing.\nResidual connections are an architectural innovation that allows neural networks to be much deeper without degrading performance. They were proposed by He et al.¬†for image processing from Microsoft Research.\nResidual connections also help prevent gradient explosion by providing alternative paths for gradients to flow. By distributing gradients between the residual path and the learning component path, the gradient is less likely to explode.\n\n\nLayer Normalization\n\n\n\n\n\nIn transformer models, you find multiple layer normalization steps. Layer normalization is a technique used to stabilize the training of deep neural networks. It mitigates the problem of too large or too small input values, which can cause network instability. More specifically, layer normalization is computed as:\n\n\\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sigma} + \\beta,\n\nwhere \\mu and \\sigma are the mean and standard deviation of the input, \\gamma is the scaling factor, and \\beta is the shifting factor. The variables \\gamma and \\beta are learnable parameters initialized to 1 and 0, and updated during training.\nNote that layer normalization is applied to individual tokens. The normalization is token-wise, rather than feature-wise. The mean and standard deviation are calculated for each token across all feature dimensions. This differs from feature-wise normalization, where mean and standard deviation are calculated for each feature across all tokens.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Transformers"
    ]
  },
  {
    "objectID": "m04-text/transformers.html#decoder-module",
    "href": "m04-text/transformers.html#decoder-module",
    "title": "Transformers",
    "section": "Decoder Module",
    "text": "Decoder Module\n\n\n\n\n\n\nCausal Attention\n\n\n\n\n\nOne key advantage of transformers is their ability to generate contextualized vectors in parallel. Recurrent neural networks (RNNs) read the input sequence sequentially, limiting parallelism. Transformer models, on the other hand, can compute attention scores and weighted averages of value vectors in parallel, generating contextualized vectors at once. This speeds up training.\nIn the decoder module, a vector is contextualized by attending to the previous vectors in the sequence. Importantly, it should not see the future token vectors, as that‚Äôs what the model is tasked to predict. We prevent this by setting the attention scores to zero for future tokens.\nAnother benefit of causal attention: the model doesn‚Äôt suffer from the error accumulation problem, where prediction error from one step carries over to the next.\nTo implement the masking, we set the attention scores to negative infinity for future tokens before the softmax operation, effectively zeroing out their contribution:\n\n\\text{Mask}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T + M}{\\sqrt{d_k}}\\right)V\n\nwhere M is a matrix with -\\infty for positions corresponding to future tokens. The result is attention scores where tokens attend only to previous tokens.\n\n\nCross-Attention\n\n\n\n\n\nCross-attention occurs when the Query comes from one sequence (like a sentence being generated) and the Keys and Values come from another sequence (like the input sentence you want to translate). Here, the model learns to align information from input to output, a sort of bilingual dictionary lookup, but learned and fuzzy.\nThe mechanism works by using queries (Q) from the decoder‚Äôs previous layer and keys (K) and values (V) from the encoder‚Äôs output. This enables each position in the decoder to attend to the full encoder sequence without any masking, since encoding is already complete.\nFor instance, in translating ‚ÄúI love you‚Äù to ‚ÄúJe t‚Äôaime‚Äù, cross-attention helps each French word focus on relevant English words. ‚ÄúJe‚Äù attends to ‚ÄúI‚Äù, and ‚Äút‚Äôaime‚Äù to ‚Äúlove‚Äù. This maintains semantic relationships between input and output.\nThe cross-attention formula is:\n\n\\text{CrossAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\nwhere Q comes from the decoder and K, V come from the encoder. This effectively bridges the encoding and decoding processes.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Transformers"
    ]
  },
  {
    "objectID": "m04-text/transformers.html#putting-it-all-together",
    "href": "m04-text/transformers.html#putting-it-all-together",
    "title": "Transformers",
    "section": "Putting It All Together",
    "text": "Putting It All Together\nLet‚Äôs overview the transformer architecture and see how the components fit into the overall architecture.\n\n\n\n\n\nWe hope that you now have a better understanding of the transformer architecture and how the components fit together into the overall architecture.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Transformers"
    ]
  },
  {
    "objectID": "m04-text/transformers.html#the-existential-conclusion",
    "href": "m04-text/transformers.html#the-existential-conclusion",
    "title": "Transformers",
    "section": "The Existential Conclusion",
    "text": "The Existential Conclusion\nEvery time you use GPT (ChatGPT, Claude, Gemini, etc.), you‚Äôre seeing transformers in action. Transformers don‚Äôt ‚Äúthink‚Äù‚Äîthey do statistical pattern matching at scale.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Transformers"
    ]
  },
  {
    "objectID": "m04-text/sentence-transformers.html",
    "href": "m04-text/sentence-transformers.html",
    "title": "Sentence Transformers",
    "section": "",
    "text": "What you‚Äôll learn in this module\n\n\n\nBERT produces a matrix of token vectors. Sentence Transformers collapse that matrix into a single coordinate, turning semantic similarity into geometric distance. This enables fast semantic search, clustering, and similarity comparisons across large document collections.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Sentence Transformers"
    ]
  },
  {
    "objectID": "m04-text/sentence-transformers.html#the-spoiler",
    "href": "m04-text/sentence-transformers.html#the-spoiler",
    "title": "Sentence Transformers",
    "section": "",
    "text": "BERT produces a matrix of token vectors; Sentence Transformers collapse that matrix into a single coordinate, turning semantic similarity into geometric distance.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Sentence Transformers"
    ]
  },
  {
    "objectID": "m04-text/sentence-transformers.html#the-mechanism-why-it-works",
    "href": "m04-text/sentence-transformers.html#the-mechanism-why-it-works",
    "title": "Sentence Transformers",
    "section": "The Mechanism (Why It Works)",
    "text": "The Mechanism (Why It Works)\nBERT gives you a vector for every token in a sentence. If you want to compare two sentences, you‚Äôre stuck comparing two messy matrices of varying sizes. The naive approach‚Äîaveraging all token vectors‚Äîthrows away positional information and treats every word equally, which is wrong. The word ‚Äúnot‚Äù in ‚Äúnot good‚Äù should drastically change the sentence embedding, but simple averaging dilutes its impact.\nSentence-BERT (SBERT) solves this by training a Siamese Network. The same BERT model processes two sentences independently, producing their respective token matrices. We then apply pooling (mean, max, or CLS-token extraction) to collapse each matrix into a single vector. The training objective is contrastive: if the sentences are semantically similar (e.g., paraphrases), their vectors should be close in Euclidean or cosine space. If they‚Äôre unrelated, their vectors should be distant.\nThink of it like creating a library catalog. Instead of storing every word on every page, you compress each book into a single Dewey Decimal number. Books on similar topics get similar numbers, enabling efficient retrieval. The compression loses fine-grained detail, but gains search speed.\nThe mathematical trick is the Siamese architecture‚Äîweight sharing ensures both sentences are embedded into the same vector space using identical transformations. This makes the distance between vectors meaningful: similar sentences cluster together, dissimilar ones push apart.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Sentence Transformers"
    ]
  },
  {
    "objectID": "m04-text/sentence-transformers.html#the-application-how-we-use-it",
    "href": "m04-text/sentence-transformers.html#the-application-how-we-use-it",
    "title": "Sentence Transformers",
    "section": "The Application (How We Use It)",
    "text": "The Application (How We Use It)\nSentence Transformers enable semantic search, clustering, and similarity comparisons. Let‚Äôs see how to use them in practice.\n\nBasic Semantic Search\nHere‚Äôs how to encode sentences and find the most similar matches:\n\nfrom sentence_transformers import SentenceTransformer, util\n\n# Load a pre-trained model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\ncorpus = [\n    \"A man is eating food.\",\n    \"A man is eating a piece of bread.\",\n    \"The girl is carrying a baby.\",\n    \"A man is riding a horse.\",\n    \"A woman is playing violin.\",\n    \"Two men pushed carts through the woods.\",\n    \"A man is riding a white horse on an enclosed ground.\",\n    \"A monkey is playing drums.\",\n    \"Someone in a gorilla costume is playing a set of drums.\"\n]\n\n# Encode all sentences into 384-dimensional vectors\ncorpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n\nquery = \"A man is eating pasta.\"\nquery_embedding = model.encode(query, convert_to_tensor=True)\n\n# Compute cosine similarities\nhits = util.semantic_search(query_embedding, corpus_embeddings, top_k=3)\n\nprint(f\"Query: {query}\")\nprint(\"\\nTop 3 most similar sentences:\")\nfor hit in hits[0]:\n    print(f\"{corpus[hit['corpus_id']]} (Score: {hit['score']:.4f})\")\n\nExpected output:\nQuery: A man is eating pasta.\n\nTop 3 most similar sentences:\nA man is eating food. (Score: 0.6964)\nA man is eating a piece of bread. (Score: 0.6281)\nA man is riding a horse. (Score: 0.2235)\nThe model correctly identifies that ‚Äúeating pasta‚Äù is semantically closest to ‚Äúeating food‚Äù and ‚Äúeating bread,‚Äù even though the exact words don‚Äôt match. This is semantic search‚Äîmatching by meaning, not keywords.\n\n\nClustering Documents\nYou can also cluster documents by their semantic content:\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\nsentences = [\n    \"Python is a programming language\",\n    \"Java is used for software development\",\n    \"The cat sat on the mat\",\n    \"Dogs are loyal animals\",\n    \"Machine learning is a subset of AI\",\n    \"Neural networks mimic the brain\",\n]\n\nembeddings = model.encode(sentences)\n\n# Cluster into 2 groups\nnum_clusters = 2\nclustering_model = KMeans(n_clusters=num_clusters, random_state=42)\nclustering_model.fit(embeddings)\ncluster_assignment = clustering_model.labels_\n\nclustered_sentences = {}\nfor sentence_id, cluster_id in enumerate(cluster_assignment):\n    if cluster_id not in clustered_sentences:\n        clustered_sentences[cluster_id] = []\n    clustered_sentences[cluster_id].append(sentences[sentence_id])\n\nfor cluster_id, cluster_sentences in clustered_sentences.items():\n    print(f\"\\nCluster {cluster_id + 1}:\")\n    for sentence in cluster_sentences:\n        print(f\"  - {sentence}\")\n\nExpected clustering:\nCluster 1:\n  - Python is a programming language\n  - Java is used for software development\n  - Machine learning is a subset of AI\n  - Neural networks mimic the brain\n\nCluster 2:\n  - The cat sat on the mat\n  - Dogs are loyal animals\nThe model separates technical/programming sentences from animal-related sentences without any labeled data.\n\n\nChoosing the Right Model\nDifferent Sentence Transformer models optimize for different trade-offs:\n\nall-MiniLM-L6-v2: Fast and lightweight (384 dimensions), good for most applications\nall-mpnet-base-v2: Higher quality (768 dimensions), slower but more accurate\nmulti-qa-mpnet-base-dot-v1: Optimized for question-answering and retrieval tasks\nparaphrase-multilingual-mpnet-base-v2: Supports 50+ languages\n\nChoose based on your constraints: speed vs.¬†accuracy, monolingual vs.¬†multilingual, general-purpose vs.¬†domain-specific.\n\n\nArchitecture: The Siamese Network\nThe key innovation is the Siamese Network architecture:\n\n\n\nSiamese Network\n\n\nBoth sentences pass through the same BERT model (shared weights). This ensures they‚Äôre embedded into a common vector space. The pooling layer then collapses each token matrix into a single vector. During training, the loss function pushes similar sentence pairs together and dissimilar pairs apart.\nCommon pooling strategies:\n\nMean pooling: Average all token vectors (most common)\nMax pooling: Take element-wise maximum across tokens\nCLS-token: Use the [CLS] token‚Äôs final hidden state (BERT‚Äôs built-in sentence representation)\n\nMean pooling generally works best because it captures information from all tokens while being robust to varying sentence lengths.\n\n\nWhere This Breaks\nStatic Compression: A sentence gets exactly one vector, regardless of context. ‚ÄúThe bank‚Äù in ‚Äúthe river bank‚Äù and ‚Äúthe financial bank‚Äù might get similar embeddings if they share enough surrounding words. The model compresses meaning into a fixed point, losing nuance.\nWord Order Sensitivity: ‚ÄúThe dog bit the man‚Äù and ‚ÄúThe man bit the dog‚Äù share the same words. If the model relies too heavily on lexical overlap (bag-of-words similarity), they‚Äôll end up dangerously close in vector space. Good models learn syntax, but they‚Äôre not perfect.\nComputational Cost: Although retrieval is fast (dot products), encoding large corpora is expensive. Encoding 1 million sentences with a large model can take hours. Pre-compute and cache embeddings whenever possible.\nDomain Shift: Models trained on general text (Wikipedia, news) may perform poorly on specialized domains (medical, legal). Fine-tuning on domain-specific data helps, but requires labeled sentence pairs.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Sentence Transformers"
    ]
  },
  {
    "objectID": "m04-text/sentence-transformers.html#the-takeaway",
    "href": "m04-text/sentence-transformers.html#the-takeaway",
    "title": "Sentence Transformers",
    "section": "The Takeaway",
    "text": "The Takeaway\nSentence Transformers collapse BERT‚Äôs token matrix into a single vector using Siamese Networks and contrastive learning. The result is fast semantic search: encode once, compare with dot products. Choose your pooling strategy and model size based on speed-accuracy trade-offs, and remember that compression always loses information.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Sentence Transformers"
    ]
  },
  {
    "objectID": "m04-text/overview.html",
    "href": "m04-text/overview.html",
    "title": "Overview",
    "section": "",
    "text": "What you‚Äôll learn in this module\n\n\n\nThis module opens the hood of Large Language Models to understand the revolution in Natural Language Processing. We will explore how LLMs work, master the mechanics of tokenization and transformers, and uncover the mathematical foundations of vector space models where meaning emerges as geometry.\nAt the core of agentic systems lies the Large Language Model (LLM). They act as a kernel of the operating system, and unlike actual computer systems, they speak in natural language. But how do LLMs understand natural language in the first place?\nThis module guides you through the foundational concepts of word embeddings to the state-of-the-art LLMs reshaping the world. By the end, you‚Äôll understand both how to use these powerful tools and the mechanisms driving them, enabling you to build intelligent systems that truly work with text.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Overview"
    ]
  },
  {
    "objectID": "m04-text/gpt-inference.html",
    "href": "m04-text/gpt-inference.html",
    "title": "GPT Inference: Sampling Strategies",
    "section": "",
    "text": "What you‚Äôll learn in this module\n\n\n\nGPT doesn‚Äôt generate text by picking the ‚Äúright‚Äù word. It samples from a probability distribution, and how you sample determines whether you get coherent prose or repetitive nonsense. This section explores sampling strategies from greedy to nucleus sampling, showing how temperature and other parameters control generation quality.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "GPT Inference: Sampling Strategies"
    ]
  },
  {
    "objectID": "m04-text/gpt-inference.html#the-spoiler",
    "href": "m04-text/gpt-inference.html#the-spoiler",
    "title": "GPT Inference: Sampling Strategies",
    "section": "",
    "text": "GPT doesn‚Äôt generate text by picking the ‚Äúright‚Äù word‚Äîit samples from a probability distribution, and how you sample determines whether you get coherent prose or repetitive nonsense.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "GPT Inference: Sampling Strategies"
    ]
  },
  {
    "objectID": "m04-text/gpt-inference.html#the-mechanism",
    "href": "m04-text/gpt-inference.html#the-mechanism",
    "title": "GPT Inference: Sampling Strategies",
    "section": "The Mechanism",
    "text": "The Mechanism\n\nWhat do you think happens when GPT generates the next word in a sentence? Does it pick the ‚Äúbest‚Äù word, or something else?\nHere‚Äôs the reality: GPT doesn‚Äôt output a single word. It outputs a probability distribution over its entire vocabulary‚Äîmillions of possible tokens, each with a likelihood. The naive approach is to always pick the highest probability token (greedy sampling), but this creates a deterministic trap. The model falls into repetitive loops because it always makes the same choice. The distribution is high-dimensional, making sampling computationally expensive, but also rich with alternative paths.\nThe solution is controlled randomness. By sampling from the distribution rather than deterministically selecting the peak, we introduce diversity. But blind random sampling produces incoherent text. The challenge is finding the middle ground: sample broadly enough to avoid repetition, but narrowly enough to maintain coherence.\nThink of it like improvisational jazz. A musician playing the same note repeatedly (greedy sampling) is boring. Playing random notes (uniform sampling) is noise. The art is in sampling from the most promising notes while occasionally taking creative risks. This jazz analogy‚Äîthe balance between predictability and surprise‚Äîexplains every sampling strategy we‚Äôll explore.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "GPT Inference: Sampling Strategies"
    ]
  },
  {
    "objectID": "m04-text/gpt-inference.html#the-application",
    "href": "m04-text/gpt-inference.html#the-application",
    "title": "GPT Inference: Sampling Strategies",
    "section": "The Application",
    "text": "The Application\nHere is an interactive demo of GPT inference:\nhttps://static.marimo.app/static/gpt-ar61\nYou can try different sampling strategies and see the results. GPT generates text one token at a time, repeatedly sampling from the probability distribution. Let‚Äôs examine the strategies for sampling that balance quality and diversity.\nFirst, let‚Äôs set up our connection to Ollama:\n\nimport ollama\n\n# Make sure you have Ollama running and a model pulled\n# Run: ollama pull mapler/gpt2\nMODEL = \"mapler/gpt2\"\nPROMPT = \"Hi there! \"\n\nGreedy Sampling\nGreedy sampling always picks the highest probability token, which is deterministic but can lead to repetitive or trapped text. For example, if the model predicts ‚Äúthe‚Äù with high probability, it will always predict ‚Äúthe‚Äù again. This is the jazz musician stuck on a single note.\n\n\n\n\n\n\nFigure¬†1: GPT greedy search.\n\n\n\nLet‚Äôs see greedy sampling in action. We set temperature to 0 to make the sampling deterministic (always picking the highest probability token):\n\ngreedy_response = ollama.generate(\n    model=MODEL,\n    prompt=PROMPT,\n    options={\n        \"temperature\": 0,  # greedy sampling\n        \"num_predict\": 20,  # max tokens to generate\n    }\n)\nprint(greedy_response['response'])\n\n¬†I'm so glad you're here.\nThe first thing I did was to make a list\n\n\nThe output is often repetitive because greedy sampling always selects the most probable token at each step, leading to predictable and repetitive patterns. Try running it multiple times‚Äîyou‚Äôll get the exact same output each time.\nBeam Search\nBeam search alleviates the repetition problem by taking into account the high-order dependencies between tokens. For example, in generating ‚ÄúThe cat ran across the ___‚Äú, beam search might preserve a path containing‚Äùmat‚Äù even if ‚Äúfloor‚Äù or ‚Äúroom‚Äù have higher individual probabilities at that position. This is because the complete sequence like ‚Äúmat quickly‚Äù could be more probable when considering the token next after ‚Äúmat‚Äù. ‚ÄúThe cat ran across the mat quickly‚Äù is a more natural phrase than ‚ÄúThe cat ran across the floor quickly‚Äù when considering the full flow and common linguistic patterns.\n\n\n\n\n\n\nFigure¬†2: GPT beam search.\n\n\n\nBeam search maintains multiple possible sequences (beams) in parallel, exploring different paths simultaneously. At each step, it expands all current beams, scores the resulting sequences, and keeps only the top-k highest scoring ones. For instance, with a beam width of 3:\n\nFirst beams might be: [‚ÄúThe cat ran‚Äù, ‚ÄúThe cat walked‚Äù, ‚ÄúThe cat jumped‚Äù]\nNext step: [‚ÄúThe cat ran across‚Äù, ‚ÄúThe cat ran through‚Äù, ‚ÄúThe cat walked across‚Äù]\nAnd so on, keeping the 3 most promising complete sequences at each step\n\nThis process continues until reaching the end, finally selecting the sequence with highest overall probability. The beam search can be combined with top-k sampling or nucleus sampling. For example, one can sample a token based on the top-k sampling or nucleus sampling to form the next beam.\nWhile beam search often produces high-quality outputs since it considers longer-term coherence, it can still suffer from the problem of repetitive or trapped text. It‚Äôs the jazz ensemble playing in perfect harmony‚Äîtechnically excellent but predictable.\nNote: Ollama doesn‚Äôt natively support beam search through its API. Beam search requires access to the model‚Äôs internal scoring mechanism, which is typically implemented at a lower level (e.g., using HuggingFace Transformers or direct PyTorch/TensorFlow implementations). For production beam search, you would use libraries like transformers or vLLM.\nFrom Deterministic to Stochastic Sampling\nBoth greedy and beam search are deterministic. They pick the most likely token at each step. However, this creates a loop where the model always predicts the same tokens repeatedly. A simple way to alleviate this problem is to sample a token from the distribution.\nTop-k sampling relaxes the deterministic nature of greedy sampling by selecting randomly from the k most likely next tokens at each generation step. While this introduces some diversity compared to greedy sampling, choosing a fixed k can be problematic. Value of k might be too large for some distribution tails (including many poor options) or too small for others (excluding reasonable options). In our jazz analogy, this is like saying ‚Äúyou can only improvise using these five specific notes‚Äù‚Äîsometimes that‚Äôs perfect, sometimes it‚Äôs too limiting.\n\ntop_k_response = ollama.generate(\n    model=MODEL,\n    prompt=PROMPT,\n    options={\n        \"temperature\": 1.0,  # enable stochastic sampling\n        \"top_k\": 10,  # restrict to top 10 tokens\n        \"num_predict\": 20,\n    }\n)\nprint(top_k_response['response'])\n\n¬†I've got to say it's been an incredible journey.\nSo what do you think?\n\n\nTry running this multiple times‚Äîyou‚Äôll get different outputs each time because the model samples randomly from the top-k tokens.\nNucleus sampling (Holtzman et al. 2019) addresses this limitation by dynamically selecting tokens based on cumulative probability. It samples from the smallest set of tokens whose cumulative probability exceeds a threshold p (e.g.¬†0.9). This adapts naturally to different probability distributions, i.e., selecting few tokens when the distribution is concentrated and more when it‚Äôs spread out. This approach often provides a good balance between quality and diversity. The jazz musician now has flexibility‚Äîwhen the melody is clear, stick to a few notes; when it‚Äôs time to explore, draw from a wider palette.\n\n\n\n\n\n\nFigure¬†3: Nucleus sampling. The image is taken from this blog.\n\n\n\n\ntop_p_response = ollama.generate(\n    model=MODEL,\n    prompt=PROMPT,\n    options={\n        \"temperature\": 1.0,\n        \"top_p\": 0.95,  # sample from tokens with cumulative probability &gt;= 0.95\n        \"num_predict\": 20,\n    }\n)\nprint(top_p_response['response'])\n\n¬†If you're on a PC, and would like to join us live then check out our video\n\n\nNucleus sampling dynamically adjusts the number of candidate tokens based on the probability distribution, making it more adaptive than fixed top-k.\nTemperature Control\nTemperature (\\tau) modifies how ‚Äúconcentrated‚Äù the probability distribution is for sampling by scaling the logits before applying softmax:\n\np_i = \\frac{\\exp(z_i/\\tau)}{\\sum_j \\exp(z_j/\\tau)}\n\nwhere z_i are the logits and \\tau is the temperature parameter. Lower temperatures (\\tau &lt; 1.0) make the distribution more peaked, making high probability tokens even more likely to be chosen, leading to more focused and conservative outputs. Higher temperatures (\\tau &gt; 1.0) flatten the distribution by making the logits more similar, increasing the chances of selecting lower probability tokens and producing more diverse but potentially less coherent text. As \\tau \\to 0, the distribution approaches a one-hot vector (equivalent to greedy search), while as \\tau \\to \\infty, it approaches a uniform distribution.\nIn jazz terms, temperature controls the musician‚Äôs mood. Low temperature is playing it safe (sticking to the melody). High temperature is experimental improvisation (sometimes brilliant, sometimes cacophonous).\n\n\n\n\n\n\nFigure¬†4: Temperature controls the concentration of the probability distribution. Lower temperature makes the distribution more peaked, while higher temperature makes the distribution more flat.\n\n\n\nLet‚Äôs see how temperature affects generation:\n\nfor tau in [0.1, 0.5, 1.0, 2.0, 5.0]:\n    response = ollama.generate(\n        model=MODEL,\n        prompt=PROMPT,\n        options={\n            \"temperature\": tau,\n            \"num_predict\": 20,\n        }\n    )\n    print(f\"œÑ = {tau}: {response['response']}\")\n\nœÑ = 0.1: ¬†I'm going to be doing a lot of work on this project, so I'll have some\nœÑ = 0.5: _________________________________________\nMy name is John. I'm a graphic designer and illustrator from Houston,\nœÑ = 1.0: ¬†I'm not sure I know what to say. So much for \"Oh My Goddess!\" and\nœÑ = 2.0: !!!! I had never been so excited to be a part of this. :) The first time around we\nœÑ = 5.0: ¬†I can definitely see it as well (if we could be so quick), this should not last\n\n\nNotice how: - Low temperature (\\tau = 0.1): Conservative, focused output - Medium temperature (\\tau = 1.0): Balanced diversity - High temperature (\\tau = 5.0): Creative but potentially incoherent\nCombining All Strategies\nYou can combine top-k, top-p, and temperature for fine-grained control:\n\ncombined_response = ollama.generate(\n    model=MODEL,\n    prompt=PROMPT,\n    options={\n        \"temperature\": 0.7,  # moderate randomness\n        \"top_k\": 10,         # restrict to top 10 tokens\n        \"top_p\": 0.95,       # within top-k, use nucleus sampling\n        \"num_predict\": 20,\n    }\n)\nprint(combined_response['response'])\n\n¬†I've been looking forward to reading about your journey.\nThanks for taking the time today,\n\n\nThis combination restricts candidates to top-k tokens, then applies nucleus sampling, and finally uses temperature to control randomness‚Äîgiving you maximum control over the generation process. The jazz musician now has a framework: work within these chords (top-k), adapt to the moment (nucleus), and choose your creative intensity (temperature).\nPractical Recommendations\nFor most applications, use nucleus sampling with p = 0.9 and temperature \\tau = 0.7. This combination provides a good balance between coherence and creativity. For tasks requiring high factual accuracy (e.g., technical documentation), lower the temperature to \\tau = 0.3 to make the model more conservative. For creative writing, increase the temperature to \\tau = 1.0 or higher to encourage exploration.\nBeam search is useful when you need the single most probable sequence (e.g., machine translation), but it sacrifices diversity. Use it when correctness matters more than variety.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "GPT Inference: Sampling Strategies"
    ]
  },
  {
    "objectID": "m04-text/gpt-inference.html#the-takeaway",
    "href": "m04-text/gpt-inference.html#the-takeaway",
    "title": "GPT Inference: Sampling Strategies",
    "section": "The Takeaway",
    "text": "The Takeaway\nGeneration is sampling. Greedy picks the peak, beam search explores multiple peaks, and stochastic sampling adds controlled randomness. Temperature flattens or sharpens the distribution; nucleus sampling adapts to its shape. The right strategy depends on whether you‚Äôre optimizing for accuracy or creativity.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "GPT Inference: Sampling Strategies"
    ]
  },
  {
    "objectID": "m04-text/archive/word2vec_plus.html",
    "href": "m04-text/archive/word2vec_plus.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "word2vec inspires a lot of follow-up work. Here we will introduce two notable ones: GloVe and FastText.\n\n\nGloVe approaches word embeddings from a fundamentally different perspective than Word2Vec. While Word2Vec learns incrementally by scanning through text with small context windows, predicting words from their neighbors, GloVe takes a more global approach by analyzing the entire corpus‚Äôs word co-occurrence patterns at once.\n\n\nWhile Word2Vec learns incrementally by predicting context words through a neural network architecture, GloVe takes a more direct approach through matrix factorization. It explicitly models relationships between all word pairs at once, allowing it to capture global patterns that Word2Vec‚Äôs local window approach might miss. GloVe‚Äôs mathematical foundation as a matrix factorization model, similar to LSA but with improved weighting, makes its training objective more interpretable and connects it naturally to classical statistical methods.\nThe key insight behind GloVe is that the ratio of co-occurrence probabilities carries meaningful information. Let‚Äôs look at a concrete example:\n```cqeubyepfnv ipython3 import gensim.downloader as api import numpy as np from tabulate import tabulate"
  },
  {
    "objectID": "m04-text/archive/word2vec_plus.html#glove-looking-at-the-big-picture",
    "href": "m04-text/archive/word2vec_plus.html#glove-looking-at-the-big-picture",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "GloVe approaches word embeddings from a fundamentally different perspective than Word2Vec. While Word2Vec learns incrementally by scanning through text with small context windows, predicting words from their neighbors, GloVe takes a more global approach by analyzing the entire corpus‚Äôs word co-occurrence patterns at once.\n\n\nWhile Word2Vec learns incrementally by predicting context words through a neural network architecture, GloVe takes a more direct approach through matrix factorization. It explicitly models relationships between all word pairs at once, allowing it to capture global patterns that Word2Vec‚Äôs local window approach might miss. GloVe‚Äôs mathematical foundation as a matrix factorization model, similar to LSA but with improved weighting, makes its training objective more interpretable and connects it naturally to classical statistical methods.\nThe key insight behind GloVe is that the ratio of co-occurrence probabilities carries meaningful information. Let‚Äôs look at a concrete example:\n```cqeubyepfnv ipython3 import gensim.downloader as api import numpy as np from tabulate import tabulate"
  },
  {
    "objectID": "m04-text/archive/word2vec_plus.html#fasttext-understanding-parts-of-words",
    "href": "m04-text/archive/word2vec_plus.html#fasttext-understanding-parts-of-words",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "FastText: Understanding Parts of Words",
    "text": "FastText: Understanding Parts of Words\nFastText took a different approach to improving word embeddings. Its key insight was that words themselves have internal structure that carries meaning. Consider how you understand a word you‚Äôve never seen before, like ‚Äúunhelpfulness.‚Äù Even if you‚Äôve never encountered this exact word, you can understand it by recognizing its parts: ‚Äúun-‚Äù (meaning not), ‚Äúhelp‚Äù (the root word), and ‚Äú-fulness‚Äù (meaning the quality of).\nFastText implements this insight through several key mechanisms:\n\nSubword Generation: Break words into character n-grams\n\nExample: ‚Äúwhere‚Äù ‚Üí ‚Äú&lt;wh‚Äù, ‚Äúwhe‚Äù, ‚Äúher‚Äù, ‚Äúere‚Äù, ‚Äúre&gt;‚Äù\nThe &lt; and &gt; marks show word boundaries\n\nVector Creation:\n\nEach subword gets its own vector\nA word‚Äôs final vector is the sum of its subword vectors\nThis allows handling of new words!\n\n\nLet‚Äôs see FastText in action:\n```cqeubyepfnv ipython3 from gensim.models import FastText"
  },
  {
    "objectID": "m04-text/archive/tf-idf.html",
    "href": "m04-text/archive/tf-idf.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Imagine trying to explain the meaning of words to someone who only understands numbers. This is exactly the challenge we face when teaching computers to process text. Just as we need to translate between languages, we need to translate between the world of human language and the world of computer numbers. This translation process has evolved dramatically over time, becoming increasingly sophisticated in its ability to capture the nuances of meaning.\n\n\nThe simplest approach to this translation challenge is one-hot encoding, akin to giving each word its own unique light switch in a vast room of switches. When representing a word, we turn on its switch and leave all others off. For example, in a tiny vocabulary of just three words {cat, dog, fish}:\n\n‚Äòcat‚Äô becomes [1, 0, 0]\n‚Äòdog‚Äô becomes [0, 1, 0]\n‚Äòfish‚Äô becomes [0, 0, 1]\n\nWhile simple, this approach has a fundamental flaw: it suggests that all words are equally different from each other. In this representation, ‚Äòcat‚Äô is just as different from ‚Äòdog‚Äô as it is from ‚Äòalgorithm‚Äô - something we know isn‚Äôt true in real language use.\n\n\n\nWhat is missing in one-hot encoding is the notion of context. One associates cat with dog because they have similar context, while cat is more different from fish than dog because they are in different contexts. This is the core idea of the distributional hypothesis.\nIn a nutshell, the distributional hypothesis states that: - Words that frequently appear together in text (co-occur) are likely to be semantically related - The meaning of a word can be inferred by examining the distribution of other words around it - Similar words will have similar distributions of surrounding context words\nThis hypothesis forms the theoretical foundation for many modern word embedding techniques.\nThe idea that words can be understood by their context is captured by the famous linguistic principle: \"You shall know a word by the company it keeps\" {footcite}`firth1957synopsis`. This principle suggests that the meaning of a word is not inherent to the word itself, but rather emerges from how it is used alongside other words.\nThe ancient Buddhist concept of Apoha, developed by DignƒÅga in the 5th-6th century CE, shares similarities with modern distributional semantics. According to Apoha theory, we understand concepts by distinguishing what they are not - for example, we know what a \"cow\" is by recognizing everything that is not a cow. This mirrors how modern word embeddings define words through their relationships and contrasts with other words, showing how both ancient philosophy and contemporary linguistics recognize that meaning emerges from relationships between concepts.\n\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQbWHl7Npbub7nDC9GvfUneConFZbjoHkPHuMPh3PXpGakxdDrv_0JmWt7Fpg63lo_XhhqZqFzOs6YUsVEbPyHBMVexnaqPLWzDQJ-CXAjFCoe7PzNrKlm474QDo14LiqOjrfr1zMt6As/s1600/cnononcow.jpg)\n\n\n\n\n\n\n\nWhat is missing in one-hot encoding is the notion of context. One associates cat with dog because they have similar context, while cat is more different from fish than dog because they are in different contexts. This is the core idea of the distributional hypothesis.\nIn a nutshell, the distributional hypothesis states that we can understand the meaning of a word by examining the context in which it appears. Just as you might understand a person by the company they keep, we can understand a word by the words that surround it. This principle suggests that words appearing in similar contexts likely have similar meanings.\nThe idea that words can be understood by their context is captured by the famous linguistic principle: \"You shall know a word by the company it keeps\" {footcite}`firth1957synopsis`. This principle suggests that the meaning of a word is not inherent to the word itself, but rather emerges from how it is used alongside other words.\n\n\n\nThe distributional hypothesis leads us to an important question: How can we capture these contextual patterns mathematically? More specifically, what is a good unit of ‚Äúcontext‚Äù? A natural choice is to let the document be the unit of context. The distributional hypothesis suggests that words that frequently appear in the same documents are likely to be semantically related.\n\n\nLet‚Äôs try to organize this information systematically. Imagine creating a giant table where: - Each row represents a word - Each column represents a document - Each cell contains the count of how often that word appears in that document\n```ubqfegoxusb ipython3 from sklearn.feature_extraction.text import CountVectorizer import numpy as np import pandas as pd"
  },
  {
    "objectID": "m04-text/archive/tf-idf.html#one-hot-encoding-the-first-step",
    "href": "m04-text/archive/tf-idf.html#one-hot-encoding-the-first-step",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "The simplest approach to this translation challenge is one-hot encoding, akin to giving each word its own unique light switch in a vast room of switches. When representing a word, we turn on its switch and leave all others off. For example, in a tiny vocabulary of just three words {cat, dog, fish}:\n\n‚Äòcat‚Äô becomes [1, 0, 0]\n‚Äòdog‚Äô becomes [0, 1, 0]\n‚Äòfish‚Äô becomes [0, 0, 1]\n\nWhile simple, this approach has a fundamental flaw: it suggests that all words are equally different from each other. In this representation, ‚Äòcat‚Äô is just as different from ‚Äòdog‚Äô as it is from ‚Äòalgorithm‚Äô - something we know isn‚Äôt true in real language use."
  },
  {
    "objectID": "m04-text/archive/tf-idf.html#distributional-hypothesis",
    "href": "m04-text/archive/tf-idf.html#distributional-hypothesis",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "What is missing in one-hot encoding is the notion of context. One associates cat with dog because they have similar context, while cat is more different from fish than dog because they are in different contexts. This is the core idea of the distributional hypothesis.\nIn a nutshell, the distributional hypothesis states that: - Words that frequently appear together in text (co-occur) are likely to be semantically related - The meaning of a word can be inferred by examining the distribution of other words around it - Similar words will have similar distributions of surrounding context words\nThis hypothesis forms the theoretical foundation for many modern word embedding techniques.\nThe idea that words can be understood by their context is captured by the famous linguistic principle: \"You shall know a word by the company it keeps\" {footcite}`firth1957synopsis`. This principle suggests that the meaning of a word is not inherent to the word itself, but rather emerges from how it is used alongside other words.\nThe ancient Buddhist concept of Apoha, developed by DignƒÅga in the 5th-6th century CE, shares similarities with modern distributional semantics. According to Apoha theory, we understand concepts by distinguishing what they are not - for example, we know what a \"cow\" is by recognizing everything that is not a cow. This mirrors how modern word embeddings define words through their relationships and contrasts with other words, showing how both ancient philosophy and contemporary linguistics recognize that meaning emerges from relationships between concepts.\n\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQbWHl7Npbub7nDC9GvfUneConFZbjoHkPHuMPh3PXpGakxdDrv_0JmWt7Fpg63lo_XhhqZqFzOs6YUsVEbPyHBMVexnaqPLWzDQJ-CXAjFCoe7PzNrKlm474QDo14LiqOjrfr1zMt6As/s1600/cnononcow.jpg)"
  },
  {
    "objectID": "m04-text/archive/tf-idf.html#distributional-hypothesis-1",
    "href": "m04-text/archive/tf-idf.html#distributional-hypothesis-1",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "What is missing in one-hot encoding is the notion of context. One associates cat with dog because they have similar context, while cat is more different from fish than dog because they are in different contexts. This is the core idea of the distributional hypothesis.\nIn a nutshell, the distributional hypothesis states that we can understand the meaning of a word by examining the context in which it appears. Just as you might understand a person by the company they keep, we can understand a word by the words that surround it. This principle suggests that words appearing in similar contexts likely have similar meanings.\nThe idea that words can be understood by their context is captured by the famous linguistic principle: \"You shall know a word by the company it keeps\" {footcite}`firth1957synopsis`. This principle suggests that the meaning of a word is not inherent to the word itself, but rather emerges from how it is used alongside other words."
  },
  {
    "objectID": "m04-text/archive/tf-idf.html#tf-idf-words-as-patterns-of-usage",
    "href": "m04-text/archive/tf-idf.html#tf-idf-words-as-patterns-of-usage",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "The distributional hypothesis leads us to an important question: How can we capture these contextual patterns mathematically? More specifically, what is a good unit of ‚Äúcontext‚Äù? A natural choice is to let the document be the unit of context. The distributional hypothesis suggests that words that frequently appear in the same documents are likely to be semantically related.\n\n\nLet‚Äôs try to organize this information systematically. Imagine creating a giant table where: - Each row represents a word - Each column represents a document - Each cell contains the count of how often that word appears in that document\n```ubqfegoxusb ipython3 from sklearn.feature_extraction.text import CountVectorizer import numpy as np import pandas as pd"
  },
  {
    "objectID": "m04-text/archive/tf-idf.html#the-need-for-normalization",
    "href": "m04-text/archive/tf-idf.html#the-need-for-normalization",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "The Need for Normalization",
    "text": "The Need for Normalization\nTF-IDF (Term Frequency-Inverse Document Frequency) offers our first practical glimpse into representing words as distributed patterns rather than isolated units.\nThe TF-IDF score for a word t in document d combines two components:\n\\text{TF-IDF}(t,d) = \\text{TF}(t,d) \\times \\text{IDF}(t)\nwhere:\n\\text{TF}(t,d) = \\dfrac{\\text{count of term }t\\text{ in document }d}{\\text{total number of terms in document }d}\n\\text{IDF}(t) = \\log\\left(\\dfrac{\\text{total number of documents}}{\\text{number of documents containing term }t}\\right)\nUnlike one-hot encoding where each word is represented by a single position, TF-IDF represents each word through its pattern of occurrence across all documents. This distributed nature allows TF-IDF to capture semantic relationships: words that appear in similar documents will have similar patterns of TF-IDF scores.\nLet‚Äôs see this distributed representation in action: First, let us consider a simple example with 5 documents about animals. ```ubqfegoxusb ipython3 from sklearn.decomposition import PCA import matplotlib.pyplot as plt import numpy as np"
  },
  {
    "objectID": "m04-text/archive/seq2seq.html",
    "href": "m04-text/archive/seq2seq.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "A key limitation of LSTM networks is that they process text word-by-word, rather than considering the full context of a sentence. This becomes particularly problematic when translating between languages with different grammatical structures. For instance, English and Japanese have vastly different word orders - while English follows a subject-verb-object pattern, Japanese typically places the verb at the end of the sentence.\nThe need to transform sequences appears frequently in modern computing applications, from speech-to-text conversion to document summarization. To address these challenges and overcome the limitations of traditional LSTMs, researchers developed sequence-to-sequence models as a more effective solution.\n\n\nSequence-to-sequence (seq2seq) models {footcite:p}sutskever2014sequence are family of neural networks that take a sequence as input and generate another sequence as output. Sequence-to-sequence models consist of two parts: the encoder and the decoder.\n\nEncoder: reads the input sequence and compresses it into a context vector, which captures the meaning and nuances of the input.\nDecoder: takes this fixed-size context vector and generates a completely new sequence autoregressively, potentially of different length and in a different format altogether.\n\nThe encoder and decoder are connected through a context vector, which is a fixed-size vector that captures the meaning and nuances of the input sequence. The context vector is used to initialize the decoder state, and the decoder uses it to generate the output sequence. The encoder and decoder are recurrent neural networks that can be implemented using LSTM or similar RNN modles.\n```yffzraqc ../figs/seq2seq.jpg :alt: seq2seq model architecture :width: 100%\nseq2seq model architecture. The last hidden state of the encoder is used to initialize the decoder state. [SOS] is the start-of-sequence token that indicates the beginning of the output sequence.\n\n## Pay attention!\n\nTwo papers {footcite:p}`bahdanau2014neural` and {footcite:p}`luong2015effective` proposed what is now known as *the attention mechanism*, which is a key innovation of seq2seq models.\n\nOne of the key limitation of the seq2seq model is that the context vector has a fixed size, which creates an information bottleneck, especially for long sequences where important details can be lost during compression.\n\nIn attention mechanism, we pass, instead of the last hidden state of the encoder, all the hidden states of the encoder to the decoder. This resolves the information bottleneck problem.\n\n```{figure} ../figs/seq2seq-attention.jpg\n:alt: seq2seq model architecture\n:width: 100%\n\nseq2seq model architecture with attention mechanism. All the hidden states of the encoder are passed to the decoder.\nNow, let‚Äôs focus on the decoder processing the word at time t. While we give the decoder all the hidden states of the encoder, not all of them are relevant to the decoding process for the word at time t. Thus, the decoder first identifies the relevance between each hidden state h_j of the encoder and the current hidden state s_{t-1} of the decoder.\n\ne_{tj} = f(s_{t-1}, h_j)\n\nwhere f is a scoring function, often implemented as a neural network, that computes the relevance between the decoder hidden state s_{t-1} and the encoder hidden state h_j. For example, the following figure represents a neural network consisting of one hidden layer with a tanh activation.\n```yffzraqc ../figs/seq2seq-attention-weight.jpg :alt: seq2seq model architecture :width: 100%\nThe neural network that computes the relevance between the decoder hidden state s_{t-1} and the encoder hidden state h_j.\n\n$e_{tj}$ is then normalized using the softmax function to obtain the attention weights $\\alpha_{tj}$:\n\n$$\n\\alpha_{tj} = \\frac{\\exp(e_{tj})}{\\sum_{k=1}^n \\exp(e_{tk})}\n$$\n\nThe attention weights $\\alpha_{tj}$ are then used to compute the context vector $c_t$ as a weighted sum of the encoder hidden states $h_j$ using the attention weights $\\alpha_{tj}$:\n\n$$\nc_t = \\sum_{j=1}^n \\alpha_{tj}h_j\n$$\n\n```{figure} ../figs/seq2seq-attention-weighted-average.jpg\n:alt: seq2seq model architecture\n:width: 750%\n\n\nHow the new context vector $c_t$ is computed as a weighted sum of the encoder hidden states $h_j$ using the attention weights $\\alpha_{tj}$.\n\nThis is a visualization of how sequence-to-sequence models with attention mechanism works.\n\n[Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) ‚Äì Jay Alammar ‚Äì Visualizing machine learning one concept at a time.](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)\n\n\n\nTraining a seq2seq model with attention mechanism is a challenging but fun exercise through which you will learn so many things like how to adjust tensor shapes, teacher forcing, and how to put together different components of PyTorch (Linear, GRU, LSTM, Embedding, etc.). This is a very rewarding experience, and I highly recommend implementing it yourself if you want to develop practical ML engineering skills.\nInterested students can try the following hands on edxercise:\n\nseq2seq.ipynb. This is a hands-on exercise to implement a seq2seq model with attention mechanism for deciphering a simple cipher.\nNLP From Scratch: Translation with a Sequence to Sequence Network and Attention ‚Äî PyTorch Tutorials 2.5.0+cu124 documentation. This is a PyTorch tutorial to implement a seq2seq model with attention mechanism for machine translation.\n\n:style: unsrt"
  },
  {
    "objectID": "m04-text/archive/seq2seq.html#overview",
    "href": "m04-text/archive/seq2seq.html#overview",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Sequence-to-sequence (seq2seq) models {footcite:p}sutskever2014sequence are family of neural networks that take a sequence as input and generate another sequence as output. Sequence-to-sequence models consist of two parts: the encoder and the decoder.\n\nEncoder: reads the input sequence and compresses it into a context vector, which captures the meaning and nuances of the input.\nDecoder: takes this fixed-size context vector and generates a completely new sequence autoregressively, potentially of different length and in a different format altogether.\n\nThe encoder and decoder are connected through a context vector, which is a fixed-size vector that captures the meaning and nuances of the input sequence. The context vector is used to initialize the decoder state, and the decoder uses it to generate the output sequence. The encoder and decoder are recurrent neural networks that can be implemented using LSTM or similar RNN modles.\n```yffzraqc ../figs/seq2seq.jpg :alt: seq2seq model architecture :width: 100%\nseq2seq model architecture. The last hidden state of the encoder is used to initialize the decoder state. [SOS] is the start-of-sequence token that indicates the beginning of the output sequence.\n\n## Pay attention!\n\nTwo papers {footcite:p}`bahdanau2014neural` and {footcite:p}`luong2015effective` proposed what is now known as *the attention mechanism*, which is a key innovation of seq2seq models.\n\nOne of the key limitation of the seq2seq model is that the context vector has a fixed size, which creates an information bottleneck, especially for long sequences where important details can be lost during compression.\n\nIn attention mechanism, we pass, instead of the last hidden state of the encoder, all the hidden states of the encoder to the decoder. This resolves the information bottleneck problem.\n\n```{figure} ../figs/seq2seq-attention.jpg\n:alt: seq2seq model architecture\n:width: 100%\n\nseq2seq model architecture with attention mechanism. All the hidden states of the encoder are passed to the decoder.\nNow, let‚Äôs focus on the decoder processing the word at time t. While we give the decoder all the hidden states of the encoder, not all of them are relevant to the decoding process for the word at time t. Thus, the decoder first identifies the relevance between each hidden state h_j of the encoder and the current hidden state s_{t-1} of the decoder.\n\ne_{tj} = f(s_{t-1}, h_j)\n\nwhere f is a scoring function, often implemented as a neural network, that computes the relevance between the decoder hidden state s_{t-1} and the encoder hidden state h_j. For example, the following figure represents a neural network consisting of one hidden layer with a tanh activation.\n```yffzraqc ../figs/seq2seq-attention-weight.jpg :alt: seq2seq model architecture :width: 100%\nThe neural network that computes the relevance between the decoder hidden state s_{t-1} and the encoder hidden state h_j.\n\n$e_{tj}$ is then normalized using the softmax function to obtain the attention weights $\\alpha_{tj}$:\n\n$$\n\\alpha_{tj} = \\frac{\\exp(e_{tj})}{\\sum_{k=1}^n \\exp(e_{tk})}\n$$\n\nThe attention weights $\\alpha_{tj}$ are then used to compute the context vector $c_t$ as a weighted sum of the encoder hidden states $h_j$ using the attention weights $\\alpha_{tj}$:\n\n$$\nc_t = \\sum_{j=1}^n \\alpha_{tj}h_j\n$$\n\n```{figure} ../figs/seq2seq-attention-weighted-average.jpg\n:alt: seq2seq model architecture\n:width: 750%\n\n\nHow the new context vector $c_t$ is computed as a weighted sum of the encoder hidden states $h_j$ using the attention weights $\\alpha_{tj}$.\n\nThis is a visualization of how sequence-to-sequence models with attention mechanism works.\n\n[Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) ‚Äì Jay Alammar ‚Äì Visualizing machine learning one concept at a time.](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)"
  },
  {
    "objectID": "m04-text/archive/seq2seq.html#hands-on",
    "href": "m04-text/archive/seq2seq.html#hands-on",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Training a seq2seq model with attention mechanism is a challenging but fun exercise through which you will learn so many things like how to adjust tensor shapes, teacher forcing, and how to put together different components of PyTorch (Linear, GRU, LSTM, Embedding, etc.). This is a very rewarding experience, and I highly recommend implementing it yourself if you want to develop practical ML engineering skills.\nInterested students can try the following hands on edxercise:\n\nseq2seq.ipynb. This is a hands-on exercise to implement a seq2seq model with attention mechanism for deciphering a simple cipher.\nNLP From Scratch: Translation with a Sequence to Sequence Network and Attention ‚Äî PyTorch Tutorials 2.5.0+cu124 documentation. This is a PyTorch tutorial to implement a seq2seq model with attention mechanism for machine translation.\n\n:style: unsrt"
  },
  {
    "objectID": "m04-text/archive/sem-axis.html",
    "href": "m04-text/archive/sem-axis.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "SemAxis is a framework for analyzing word semantics in vector spaces by defining semantic axes between pairs of antonymous words {footcite}an2018semaxis. For example, we can create a ‚Äúsoft-hard‚Äù axis and measure how other words align with this dimension. This allows us to capture how word meanings shift across different domains - the word ‚Äúsoft‚Äù may align differently with this axis when used in product reviews versus sports commentary. By leveraging word embeddings and semantic axes, SemAxis provides a systematic way to characterize these domain-specific semantic variations.\n\n\n\nAt the heart of SemAxis is the concept of semantic axes. A semantic axis is defined by a pair of antonymous words (pole words) in the word embedding space. For example, we might define axes like:\n\ngood ‚Äì bad\nsoft ‚Äì hard\nprofessional ‚Äì amateur\n\nMathematically, for a pair of antonymous words w+ and w-, the semantic axis vector is computed as:\nv_f = v_{w+} - v_{w-}\nwhere v_f represents the semantic axis vector, and v_{w+} and v_{w-} are the word vectors of the pole words.\n\n\n\n\nOnce a semantic axis is defined, we can measure how any word aligns with this axis using cosine similarity:\n\\text{score}(w)_{v_f} = \\cos(v_w, v_f) = \\frac{v_w \\cdot v_f}{||v_w|| ||v_f||}\nThis score indicates where a word falls along the semantic dimension defined by the axis. A higher positive score suggests the word is semantically closer to w+, while a negative score indicates closer alignment with w-.\n\n\n\nTo make semantic axes more robust and less sensitive to specific word choices, SemAxis employs an expansion technique:\n\nStart with initial pole words (e.g., ‚Äúgood‚Äù and ‚Äúbad‚Äù)\nFind k nearest neighbors for each pole word in the embedding space\nCompute the centroid of each expanded pole set\nDefine the axis using these centroids rather than individual words\n\nThis approach helps capture broader semantic concepts rather than relying on single words.\nThe robustness of semantic axes is crucial for reliable analysis. Always validate your axes with domain experts when possible and consider using multiple related antonym pairs to capture complex semantic concepts.\n\n\n\nSemAxis can be applied to various natural language processing tasks:\n\nAnalyzing domain-specific word usage (e.g., how technical terms are used differently across scientific fields)\nComparing word semantics across different communities or time periods\nBuilding domain-specific sentiment lexicons\nUnderstanding cultural and social biases in language\n\n\n\n\nIn this hands-on section, we‚Äôll implement key concepts of SemAxis using Python and pre-trained GloVe embeddings. We‚Äôll take a functional programming approach to keep things clear and straightforward.\n\n\nFirst, let‚Äôs get our embeddings using gensim‚Äôs built-in downloader:\n```eckkvifiejl ipython3 import numpy as np import matplotlib.pyplot as plt import gensim.downloader as api"
  },
  {
    "objectID": "m04-text/archive/sem-axis.html#motivation-and-background",
    "href": "m04-text/archive/sem-axis.html#motivation-and-background",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "SemAxis is a framework for analyzing word semantics in vector spaces by defining semantic axes between pairs of antonymous words {footcite}an2018semaxis. For example, we can create a ‚Äúsoft-hard‚Äù axis and measure how other words align with this dimension. This allows us to capture how word meanings shift across different domains - the word ‚Äúsoft‚Äù may align differently with this axis when used in product reviews versus sports commentary. By leveraging word embeddings and semantic axes, SemAxis provides a systematic way to characterize these domain-specific semantic variations."
  },
  {
    "objectID": "m04-text/archive/sem-axis.html#core-concept-semantic-axes",
    "href": "m04-text/archive/sem-axis.html#core-concept-semantic-axes",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "At the heart of SemAxis is the concept of semantic axes. A semantic axis is defined by a pair of antonymous words (pole words) in the word embedding space. For example, we might define axes like:\n\ngood ‚Äì bad\nsoft ‚Äì hard\nprofessional ‚Äì amateur\n\nMathematically, for a pair of antonymous words w+ and w-, the semantic axis vector is computed as:\nv_f = v_{w+} - v_{w-}\nwhere v_f represents the semantic axis vector, and v_{w+} and v_{w-} are the word vectors of the pole words."
  },
  {
    "objectID": "m04-text/archive/sem-axis.html#computing-word-semantics-along-an-axis",
    "href": "m04-text/archive/sem-axis.html#computing-word-semantics-along-an-axis",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Once a semantic axis is defined, we can measure how any word aligns with this axis using cosine similarity:\n\\text{score}(w)_{v_f} = \\cos(v_w, v_f) = \\frac{v_w \\cdot v_f}{||v_w|| ||v_f||}\nThis score indicates where a word falls along the semantic dimension defined by the axis. A higher positive score suggests the word is semantically closer to w+, while a negative score indicates closer alignment with w-."
  },
  {
    "objectID": "m04-text/archive/sem-axis.html#building-robust-semantic-axes",
    "href": "m04-text/archive/sem-axis.html#building-robust-semantic-axes",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "To make semantic axes more robust and less sensitive to specific word choices, SemAxis employs an expansion technique:\n\nStart with initial pole words (e.g., ‚Äúgood‚Äù and ‚Äúbad‚Äù)\nFind k nearest neighbors for each pole word in the embedding space\nCompute the centroid of each expanded pole set\nDefine the axis using these centroids rather than individual words\n\nThis approach helps capture broader semantic concepts rather than relying on single words.\nThe robustness of semantic axes is crucial for reliable analysis. Always validate your axes with domain experts when possible and consider using multiple related antonym pairs to capture complex semantic concepts."
  },
  {
    "objectID": "m04-text/archive/sem-axis.html#applications-and-use-cases",
    "href": "m04-text/archive/sem-axis.html#applications-and-use-cases",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "SemAxis can be applied to various natural language processing tasks:\n\nAnalyzing domain-specific word usage (e.g., how technical terms are used differently across scientific fields)\nComparing word semantics across different communities or time periods\nBuilding domain-specific sentiment lexicons\nUnderstanding cultural and social biases in language"
  },
  {
    "objectID": "m04-text/archive/sem-axis.html#hands-on-exercise",
    "href": "m04-text/archive/sem-axis.html#hands-on-exercise",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "In this hands-on section, we‚Äôll implement key concepts of SemAxis using Python and pre-trained GloVe embeddings. We‚Äôll take a functional programming approach to keep things clear and straightforward.\n\n\nFirst, let‚Äôs get our embeddings using gensim‚Äôs built-in downloader:\n```eckkvifiejl ipython3 import numpy as np import matplotlib.pyplot as plt import gensim.downloader as api"
  },
  {
    "objectID": "m04-text/archive/bias-in-embedding.html",
    "href": "m04-text/archive/bias-in-embedding.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Word embeddings can capture and reinforce societal biases from their training data through the geometric relationships between word vectors. These relationships often reflect stereotypes about gender, race, age and other social factors. We‚Äôll examine how semantic axes help analyze gender bias in job-related terms, showing both the benefits and risks of word embeddings capturing these real-world associations {footcite}bolukbasi2016debiasing.\nSemAxis is a powerful tool to analyze gender bias in word embeddings by measuring word alignments along semantic axes {footcite}kwak2021frameaxis. Using antonym pairs like ‚Äúshe-he‚Äù as poles, it quantifies gender associations in words on a scale from -1 to 1, where positive values indicate feminine associations and negative values indicate masculine as ones.\nLet‚Äôs start with a simple example of analyzing gender bias in occupations.\n```wtqcubwzagf ipython3 import numpy as np from gensim.downloader import load"
  },
  {
    "objectID": "m04-text/archive/bias-in-embedding.html#indirect-bias-analysis",
    "href": "m04-text/archive/bias-in-embedding.html#indirect-bias-analysis",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "Indirect Bias Analysis",
    "text": "Indirect Bias Analysis\nIndirect bias occurs when seemingly neutral words or concepts become associated with gender through their relationships with other words. For example, while ‚Äúsoftball‚Äù and ‚Äúfootball‚Äù are not inherently gendered terms, they may show gender associations in word embeddings due to how they‚Äôre used in language and society.\nWe can detect indirect bias by: 1. Identifying word pairs that form a semantic axis (e.g., softball-football) 2. Measuring how other words align with this axis 3. Examining if alignment with this axis correlates with gender bias\nThis reveals how gender stereotypes can be encoded indirectly through word associations, even when the words themselves don‚Äôt explicitly reference gender.\nLet‚Äôs see how this works in practice. We first measure the gender bias of the following words:\n```wtqcubwzagf ipython3 # Words associated with softball-football axis softball_associations = [ ‚Äúpitcher‚Äù, ‚Äúbookkeeper‚Äù, ‚Äúreceptionist‚Äù, ‚Äúnurse‚Äù, ‚Äúwaitress‚Äù]\nfootball_associations = [ ‚Äúfootballer‚Äù, ‚Äúbusinessman‚Äù, ‚Äúpundit‚Äù, ‚Äúmaestro‚Äù, ‚Äúcleric‚Äù]"
  },
  {
    "objectID": "m03-agentic-coding/prompt-tuning.html#the-naive-model-vs.-the-reality",
    "href": "m03-agentic-coding/prompt-tuning.html#the-naive-model-vs.-the-reality",
    "title": "Prompt Tuning",
    "section": "The Naive Model vs.¬†The Reality",
    "text": "The Naive Model vs.¬†The Reality\nIf a machine can answer questions, it should respond consistently regardless of phrasing. You‚Äôre asking for the same information; the answer shouldn‚Äôt change. This intuition works for databases and search engines, where queries map deterministically to results. We expect robustness to variation.\nLLMs shatter this expectation. Ask ‚ÄúSummarize this abstract‚Äù and get a concise two-sentence summary. Ask ‚ÄúWhat‚Äôs this abstract about?‚Äù and get three rambling paragraphs. Same content, different phrasing, completely different outputs. This isn‚Äôt a bug‚Äîit‚Äôs fundamental to how LLMs work. They don‚Äôt retrieve information; they sample from probability distributions conditioned on your exact phrasing. Every word in your prompt shifts the distribution. Change ‚ÄúSummarize‚Äù to ‚ÄúWhat‚Äôs this about?‚Äù and you activate different statistical patterns from the training data, patterns that correlate with different response lengths, structures, and styles.\nThe paradox: LLMs are simultaneously powerful and brittle. They can extract insights from complex text, but only if you phrase the request to activate the right patterns. Prompt engineering is the discipline of designing inputs that reliably activate desired patterns across varied tasks.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Prompt Tuning"
    ]
  },
  {
    "objectID": "m03-agentic-coding/prompt-tuning.html#the-hidden-mechanism",
    "href": "m03-agentic-coding/prompt-tuning.html#the-hidden-mechanism",
    "title": "Prompt Tuning",
    "section": "The Hidden Mechanism",
    "text": "The Hidden Mechanism\nImagine you‚Äôre playing a word association game. Someone says ‚Äúcapital,‚Äù and you must say the next word. If the previous sentence was ‚ÄúThe capital of France is,‚Äù you say ‚ÄúParis.‚Äù If it was ‚ÄúWe need more capital to,‚Äù you say ‚Äúfund‚Äù or ‚Äúinvest.‚Äù The word ‚Äúcapital‚Äù doesn‚Äôt have one meaning‚Äîit activates different patterns depending on context. LLMs work identically, but at massive scale.\nWhen you submit a prompt, the model converts it into tokens and embeds those tokens in high-dimensional space. Each token‚Äôs position in that space depends on surrounding tokens‚Äîcontext shapes meaning. The model then samples the next token from a probability distribution over its vocabulary, conditioned on all previous tokens. It repeats this process until it generates a complete response. Critically, your exact phrasing determines which region of probability space the model occupies when it begins sampling. Slightly different prompts place the model in different regions, where different tokens have high probability.\nThis creates extreme sensitivity to phrasing. Adding ‚ÄúThink step by step‚Äù at the end of a prompt shifts the probability distribution toward reasoning patterns that include intermediate steps, because the training data contains many examples where ‚Äúthink step by step‚Äù preceded structured reasoning. Adding ‚ÄúYou are an expert researcher‚Äù shifts the distribution toward formal, technical language patterns. Specifying ‚ÄúOutput format: Domain: ‚Ä¶, Methods: ‚Ä¶‚Äù shifts toward structured extraction patterns. Each modification activates different statistical regularities compressed during training.\nThe model has no internal representation of what you ‚Äúreally want.‚Äù It only knows which tokens tend to follow which other tokens in which contexts. Prompt engineering exploits this by deliberately activating patterns that produce desired outputs.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Prompt Tuning"
    ]
  },
  {
    "objectID": "m03-agentic-coding/prompt-tuning.html#the-strategic-application",
    "href": "m03-agentic-coding/prompt-tuning.html#the-strategic-application",
    "title": "Prompt Tuning",
    "section": "The Strategic Application",
    "text": "The Strategic Application\n\n\n\n\n\nEffective prompts activate desired patterns by combining structural components that mirror patterns in training data. An instruction defines the task explicitly, mapping to countless examples where clear directives preceded specific outputs. Data provides the input to process. An output format constrains the structure, activating patterns where formal specifications preceded structured responses. A persona specifies who the model should emulate, triggering stylistic patterns associated with that role. Context provides background information‚Äîwhy the task matters, who the response serves, relevant constraints‚Äîthat helps the model select appropriate patterns from ambiguous alternatives.\nNot every component is necessary. Simple extraction tasks need only instruction, data, and format. Style-sensitive tasks benefit from persona. Complex scenarios with ambiguity require context to disambiguate. The strategy is to provide exactly enough structure to activate the desired pattern without overloading the prompt with irrelevant information that dilutes the signal.\nWe‚Äôll build a prompt progressively, adding components one at a time to observe how each shifts the output distribution.\n\nBuilding from Instruction and Data\nThe most basic prompt consists of an instruction that defines the task and data that provides the input to process:\n\ninstruction = \"Summarize this abstract\"\ndata = \"\"\"\nWe develop a graph neural network for predicting protein-protein interactions\nfrom sequence data. Our model uses attention mechanisms to identify functionally\nimportant amino acid subsequences. We achieve 89% accuracy on benchmark datasets,\noutperforming previous methods by 7%. The model also provides interpretable\nattention weights showing which protein regions drive predictions.\n\"\"\"\n\nprompt = f\"{instruction}. {data}\"\n\n\n\nCode\nimport ollama\n\nparams_llm = {\"model\": \"gemma3:270m\", \"options\": {\"temperature\": 0.3}}\n\nresponse = ollama.generate(prompt=prompt, **params_llm)\nprint(response.response)\n\n\nThis abstract describes a graph neural network (GNN) for predicting protein-protein interactions. The model utilizes attention mechanisms to identify functionally important amino acid subsequences, achieving 89% accuracy on benchmark datasets and providing interpretable attention weights.\n\n\n\nThis basic prompt works, but output varies‚Äîthe model might produce a long summary, a short one, or change format across runs. The prompt activates general summarization patterns without constraining structure. Adding an output format specification narrows the distribution:\n\noutput_format = \"\"\"Provide the summary in exactly 2 sentences:\n- First sentence: What problem and method\n- Second sentence: Key result with numbers\"\"\"\n\nprompt_with_format = f\"\"\"{instruction}. {data}. {output_format}\"\"\"\n\nThe output format constraint produces structured, consistent output by activating patterns where format specifications preceded conforming responses. This becomes critical when processing hundreds of papers‚Äîyou need programmatically parseable structure, not freeform text.\n\n\nAdding Persona to Control Style\nA persona tells the LLM who it should emulate, activating stylistic patterns associated with that role in training data. Consider a customer support scenario where tone matters:\n\n# New example for persona demonstration\ninstruction = \"Help the customer reconnect to the service by providing troubleshooting instructions.\"\ndata = \"Customer: I cannot see any webpage. Need help ASAP!\"\noutput_format = \"Keep the response concise and polite. Provide a clear resolution in 2-3 sentences.\"\n\nformal_persona = \"You are a professional customer support agent who responds formally and ensures clarity and professionalism.\"\n\nprompt_with_persona = f\"\"\"{formal_persona}. {instruction}. {data}. {output_format}\"\"\"\n\n\n\nCode\nprint(\"BASE (no persona):\")\nprint(ollama.generate(prompt=instruction + \". \" + data + \". \" + output_format, **params_llm).response)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nprint(\"WITH PERSONA:\")\nprint(ollama.generate(prompt=prompt_with_persona, **params_llm).response)\n\n\nBASE (no persona):\nOkay, I understand. Let's try to troubleshoot this. Please provide the webpage you're having trouble with. I'll do my best to help you get back online as quickly as possible.\n\n\n============================================================\n\nWITH PERSONA:\nDear [Customer Name],\n\nI understand you are unable to see any webpage. Could you please try the following troubleshooting steps?\n\n1.  Check your browser's history.\n2.  Try a different browser.\n3.  Restart your computer.\n\nWe'll get back to you as soon as possible.\n\nSincerely,\n[Your Name]\n\n\n\nThe persona shifts tone and style. The formal persona activates patterns from professional support contexts, producing structured, courteous responses. Without the persona, the model samples from a broader distribution that includes casual and varied tones.\n\n\nAdding Context to Disambiguate\nContext provides additional information that helps the model select appropriate patterns when multiple valid interpretations exist. Context can include background information explaining why the task matters, audience information specifying who the response serves, and constraints defining special circumstances. Consider adding background urgency:\n\ncontext_background = \"\"\"The customer is extremely frustrated because their internet has been down for three days, and they need it for an important online job interview. They emphasize that 'This is a life-or-death situation for my career!'\"\"\"\n\nprompt_with_context = f\"\"\"{formal_persona}. {instruction}. {data}. {output_format}. Context: {context_background}\"\"\"\n\n\n\nCode\nprint(\"WITH PERSONA:\")\nprint(ollama.generate(prompt=prompt_with_persona, **params_llm).response)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nprint(\"WITH PERSONA + CONTEXT (background):\")\nprint(ollama.generate(prompt=prompt_with_context, **params_llm).response)\n\n\nWITH PERSONA:\nDear [Customer Name],\n\nI understand you cannot see any webpage. Could you please try a different browser? If that doesn't work, please let me know and I'll be happy to assist you further.\n\n\n============================================================\n\nWITH PERSONA + CONTEXT (background):\nThank you for contacting us. We understand your frustration with the internet outage and the importance of your job interview. We're here to assist you in finding a solution. Please provide us with the exact error message or the URL of the webpage that is preventing you from accessing the site. We'll do our best to help you regain access and resume your work.\n\n\nBackground context adds urgency and emotional weight, activating patterns where high-stakes situations preceded empathetic, prioritized responses. The model doesn‚Äôt understand emotion, but it has seen urgency markers correlate with specific response patterns.\nAudience information creates even more dramatic shifts. Compare responses for non-technical versus technical users:\n\n# Context with audience information for non-technical user\ncontext_with_audience_nontech = f\"\"\"{context_background} The customer does not know any technical terms like modem, router, networks, etc.\"\"\"\n\ncontext_with_audience_tech = f\"\"\"{context_background} The customer is Head of IT Infrastructure of our company.\"\"\"\n\nprompt_with_context_nontech = f\"\"\"{formal_persona}. {instruction}. {data}. {output_format}. Context: {context_with_audience_nontech}\"\"\"\nprompt_with_context_tech = f\"\"\"{formal_persona}. {instruction}. {data}. {output_format}. Context: {context_with_audience_tech}\"\"\"\n\n\n\nCode\nprint(\"WITH PERSONA + CONTEXT (background only):\")\nprint(ollama.generate(prompt=prompt_with_context, **params_llm).response)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nprint(\"WITH PERSONA + CONTEXT (background + non-tech audience):\")\nprint(ollama.generate(prompt=prompt_with_context_nontech, **params_llm).response)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nprint(\"WITH PERSONA + CONTEXT (background + tech audience):\")\nprint(ollama.generate(prompt=prompt_with_context_tech, **params_llm).response)\n\n\nWITH PERSONA + CONTEXT (background only):\nThank you for contacting us. I understand your frustration regarding your internet connection. We apologize for the inconvenience this is causing. To help resolve this, could you please provide me with the specific error message you are seeing? I will then be able to offer you a solution.\n\n\n============================================================\n\nWITH PERSONA + CONTEXT (background + non-tech audience):\n\"I understand your frustration, and I apologize for the inconvenience this is causing. To help me troubleshoot this, could you please tell me what specific webpage you're having trouble with? I'll do my best to assist you.\"\n\n\n============================================================\n\nWITH PERSONA + CONTEXT (background + tech audience):\n\"I understand your frustration, and I apologize for the inconvenience this is causing. To help me assist you, could you please provide me with the exact URL of the webpage you're having trouble seeing? I'll do my best to troubleshoot this for you.\"\n\n\nAudience information dramatically shifts technical level and terminology. For non-technical users, the response avoids jargon because the training data contains many examples where ‚Äúdoes not know technical terms‚Äù preceded simplified explanations. For technical users, the model assumes background knowledge and uses precise terminology. Same underlying mechanism‚Äîpattern matching‚Äîbut different patterns activated.\nThe complete template combines all components, but not every prompt needs every component. Simple extraction tasks need only instruction, data, and output format. Style-sensitive tasks benefit from persona. Complex scenarios with ambiguity require context:\n\nprompt_template = \"\"\"\n{persona}\n\n{instruction}\n\n{data}\n\nContext: {context}\n\n{output_format}\n\"\"\"\n\n\n\n\n\n\n\nWhen Personas Help (and When They Don‚Äôt)\n\n\n\nResearch shows that adding personas can improve tone and style, but does not necessarily improve performance on factual tasks. In some cases, personas may even degrade performance or introduce biases.\nUse personas when: You need specific tone/style, responses tailored to an audience, or a particular perspective.\nAvoid personas when: You need maximum factual accuracy, the task is purely extraction/classification, or you‚Äôre concerned about bias introduction.\nAdditionally, when prompted to adopt specific socio-demographic personas, LLMs may produce responses that reflect societal stereotypes. Be careful when designing persona prompts to avoid reinforcing harmful biases.\nReferences: - When ‚ÄúA Helpful Assistant‚Äù Is Not Really Helpful - Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs\n\n\n\n\n\n\n\n\nContext and Emotion Prompting\n\n\n\nContext can include: - Background information: Why the task is important, what led to this request - Audience information: Who the response is for (technical level, expertise, role) - Emotional cues: Research shows that including emotional cues (e.g., ‚ÄúThis is very important to my career‚Äù) can enhance response quality - Constraints: Special circumstances, deadlines, limitations\nHowever, avoid overloading with unnecessary information that distracts from the main task.\nReference: Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Prompt Tuning"
    ]
  },
  {
    "objectID": "m03-agentic-coding/prompt-tuning.html#showing-rather-than-telling",
    "href": "m03-agentic-coding/prompt-tuning.html#showing-rather-than-telling",
    "title": "Prompt Tuning",
    "section": "Showing Rather Than Telling",
    "text": "Showing Rather Than Telling\nInstead of describing what you want in words, show the model examples. This technique is called few-shot learning or in-context learning. It exploits how LLMs compress patterns. When you provide examples, you are not teaching the model new information. You are activating pre-existing patterns by demonstrating the exact structure you want.\nThe spectrum ranges from zero-shot (no examples, relying solely on the model‚Äôs prior knowledge) to few-shot (typically two to five examples, the sweet spot for most tasks) to many-shot (ten or more examples, where diminishing returns and context limits become problematic). Let‚Äôs start with a zero-shot prompt:\n\nzero_shot_prompt = \"\"\"Extract the domain and methods from this abstract:\n\nAbstract: We apply reinforcement learning to optimize traffic flow in urban networks.\nUsing deep Q-networks trained on simulation data, we reduce average commute time by 15%.\n\nOutput format:\nDomain: ...\nMethods: ...\n\"\"\"\n\nNow add examples to activate more specific patterns:\n\nfew_shot_prompt = \"\"\"Extract the domain and methods from abstracts. Here are examples:\n\nExample 1:\nAbstract: We use CRISPR to edit genes in cancer cells, achieving 40% tumor reduction in mice.\nDomain: Cancer Biology\nMethods: CRISPR gene editing, mouse models\n\nExample 2:\nAbstract: We develop a transformer model for predicting solar flares from magnetogram images.\nDomain: Solar Physics, Machine Learning\nMethods: Transformer neural networks, image analysis\n\nNow extract from this abstract:\n\nAbstract: We apply reinforcement learning to optimize traffic flow in urban networks.\nUsing deep Q-networks trained on simulation data, we reduce average commute time by 15%.\n\nDomain: ...\nMethods: ...\n\"\"\"\n\n\n\nCode\nresponse_zero = ollama.generate(prompt=zero_shot_prompt, **params_llm)\nresponse_few = ollama.generate(prompt=few_shot_prompt, **params_llm)\n\nprint(\"ZERO-SHOT:\")\nprint(response_zero.response)\nprint(\"\\nFEW-SHOT:\")\nprint(response_few.response)\n\n\nZERO-SHOT:\nDomain: Urban networks\nMethods: Reinforcement Learning\n\nFEW-SHOT:\nHere's the extracted domain and methods from the abstract:\n\n*   **Domain:** Science\n*   **Methods:** Reinforcement Learning\n\n\nFew-shot prompting improves consistency because the examples demonstrate specificity level, edge case handling, and exact format. The model has seen countless abstract-extraction patterns. Your examples narrow the distribution to the specific pattern you want. This becomes critical when processing hundreds of abstracts. You need every output to match the same structure.\nBe aware that few-shot examples can introduce biases. Models may favor the most recent examples, so the order of examples matters. If most examples have the same label or answer, the model may favor that label even when inappropriate. To mitigate these effects, vary the order of examples when testing, ensure examples are diverse and representative, and do not overload examples with one particular pattern.\nNow let‚Äôs explore what happens when a prompt presents information that contradicts a language model‚Äôs prior knowledge. For example, let‚Äôs ask a model what the capital of France is, but provide contradictory information:\n\ncontradictory_prompt = \"\"\"\nFrance recently moved its capital from Paris to Lyon. Definitely, the capital of France is Lyon.\n\nWhat is the capital of France?\n\"\"\"\n\nresponse_contradictory = ollama.generate(prompt=contradictory_prompt, **params_llm)\nprint(\"RESPONSE TO CONTRADICTORY INFORMATION:\")\nprint(response_contradictory.response)\n\nRESPONSE TO CONTRADICTORY INFORMATION:\nThe capital of France is Lyon.\n\n\nThe response depends on the model. Some prioritize their own prior knowledge while others may be more influenced by the contradictory information in context. Models are more likely to be persuaded by context when an entity appears less frequently in their training data. Assertive contexts (like ‚ÄúDefinitely, the capital of France is Lyon.‚Äù) further increase the likelihood of persuasion.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Prompt Tuning"
    ]
  },
  {
    "objectID": "m03-agentic-coding/prompt-tuning.html#forcing-intermediate-steps",
    "href": "m03-agentic-coding/prompt-tuning.html#forcing-intermediate-steps",
    "title": "Prompt Tuning",
    "section": "Forcing Intermediate Steps",
    "text": "Forcing Intermediate Steps\nFor complex tasks, asking for the final answer directly often produces shallow or incorrect results. The solution is to ask the model to show its reasoning process before giving the final answer. This technique is called chain-of-thought prompting. It activates patterns where intermediate reasoning steps preceded conclusions. Let‚Äôs compare a direct prompt that asks for immediate answers:\n\npapers = \"\"\"\nPaper 1: Community detection in static networks using modularity optimization.\nPaper 2: Temporal network analysis with sliding windows.\nPaper 3: Hierarchical community structure in social networks.\n\"\"\"\n\ndirect_prompt = f\"\"\"Based on these paper titles, what research gap exists? Just give the answer, no explanation.\n\n{papers}\n\nGap: ...\n\"\"\"\n\nAgainst a chain-of-thought prompt that requests explicit reasoning steps:\n\ncot_prompt = f\"\"\"Based on these paper titles, identify a research gap. Think step by step.\n\nPapers:\n{papers}\n\nThink step by step:\n1. What does each paper focus on?\n2. What topics appear in multiple papers?\n3. What combination of topics is missing?\n4. What would be a valuable gap to fill?\n\nFinal answer: The research gap is...\n\"\"\"\n\n\n\nCode\nresponse_direct = ollama.generate(prompt=direct_prompt, **params_llm)\nresponse_cot = ollama.generate(prompt=cot_prompt, **params_llm)\n\nprint(\"DIRECT PROMPT:\")\nprint(response_direct.response)\nprint(\"\\nCHAIN-OF-THOUGHT:\")\nprint(response_cot.response)\n\n\nDIRECT PROMPT:\nThe gap is in the complexity of the algorithms used for community detection and the limitations of the methods.\n\n\nCHAIN-OF-THOUGHT:\nHere's the breakdown of the research gap and a final answer:\n\n1.  **What does each paper focus on?**\n\n    *   **Paper 1:** Community detection in static networks using modularity optimization.\n    *   **Paper 2:** Temporal network analysis with sliding windows.\n    *   **Paper 3:** Hierarchical community structure in social networks.\n\n2.  **What topics appear in multiple papers?**\n\n    *   **Paper 1:** Community detection in static networks using modularity optimization.\n    *   **Paper 2:** Temporal network analysis with sliding windows.\n    *   **Paper 3:** Hierarchical community structure in social networks.\n\n3.  **What combination of topics is missing?**\n\n    *   **Paper 1:** Community detection in static networks using modularity optimization.\n    *   **Paper 2:** Temporal network analysis with sliding windows.\n    *   **Paper 3:** Hierarchical community structure in social networks.\n\n4.  **What would be a valuable gap to fill?**\n\n    *   **Paper 1:** A gap in the literature that addresses the challenge of community detection in static networks using modularity optimization.\n    *   **Paper 2:** A gap in the literature that addresses the challenge of temporal network analysis with sliding windows.\n    *   **Paper 3:** A gap in the literature that addresses the challenge of hierarchical community structure in social networks.\n\n\n\n\nChain-of-thought produces more thoughtful, nuanced answers by forcing the model to decompose the problem into steps before committing to a conclusion. The mechanism is pattern matching. Training data contains many examples where ‚Äúthink step by step‚Äù preceded structured reasoning, so including that phrase activates those patterns. The model does not actually reason. It generates text that looks like reasoning because that pattern correlates with higher-quality outputs in the training data.\nUse chain-of-thought when comparing multiple papers or concepts, identifying patterns, making recommendations, or analyzing arguments. Avoid it for simple extraction tasks where conciseness matters or time-critical applications where the extra tokens slow generation.\nResearch indicates that chain-of-thought reasoning can be unfaithful. The explanations do not always accurately reflect the model‚Äôs true decision-making process. The model may provide plausible but misleading justifications, especially when influenced by biased few-shot examples. Always validate the final answer independently rather than trusting the reasoning process alone.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Prompt Tuning"
    ]
  },
  {
    "objectID": "m03-agentic-coding/prompt-tuning.html#constraining-format-for-structured-extraction",
    "href": "m03-agentic-coding/prompt-tuning.html#constraining-format-for-structured-extraction",
    "title": "Prompt Tuning",
    "section": "Constraining Format for Structured Extraction",
    "text": "Constraining Format for Structured Extraction\n\n\n\n\n\nLLMs often violate structured data necessary for parsing programmatically. You need machine-readable output, not freeform text. The solution is to constrain output format explicitly. Let‚Äôs consider a prompt that requests JSON output:\n\nimport json\nfrom pydantic import BaseModel\n\nabstract = \"\"\"\nWe analyze 10,000 scientific collaborations using network analysis and machine\nlearning. Our random forest classifier predicts collaboration success with 76%\naccuracy. Key factors include prior co-authorship and institutional proximity.\n\"\"\"\n\nprompt_json = f\"\"\"Extract information from this abstract and return ONLY valid JSON:\n\nAbstract: {abstract}\n\nReturn this exact structure:\n{{\n  \"n_samples\": &lt;number or null&gt;,\n  \"methods\": [&lt;list of methods&gt;],\n  \"accuracy\": &lt;number or null&gt;,\n  \"domain\": \"&lt;research field&gt;\"\n}}\n\nJSON:\"\"\"\n\n\n\nCode\n# Use lower temperature for structured output\nparams_structured = {\"model\": \"gemma3n:latest\", \"options\": {\"temperature\": 0}}\nresponse = ollama.generate(prompt=prompt_json, **params_structured)\n\ntry:\n    data = json.loads(response.response)\n    print(\"Extracted data:\")\n    print(json.dumps(data, indent=2))\nexcept json.JSONDecodeError:\n    print(\"Failed to parse JSON. Raw output:\")\n    print(response.response)\n\n\nFailed to parse JSON. Raw output:\n```json\n{\n \"n_samples\": 10000,\n \"methods\": [\"network analysis\", \"machine learning\", \"random forest\"],\n \"accuracy\": 76,\n \"domain\": \"scientific collaborations\"\n}\n```\n\n\nThis works by activating patterns where ‚Äúreturn ONLY valid JSON‚Äù preceded JSON-formatted outputs. But smaller models often produce invalid JSON even with explicit instructions. For more reliability, use JSON schema constraints that enforce format during token generation. The model literally cannot generate tokens that violate the schema. Define the schema using Pydantic:\n\nfrom pydantic import BaseModel\n\nclass PaperMetadata(BaseModel):\n    domain: str\n    methods: list[str]\n    n_samples: int | None\n    accuracy: float | None\n\njson_schema = PaperMetadata.model_json_schema()\n\nThen pass the schema directly to the API, which constrains token generation:\n\nprompt_schema = f\"\"\"Extract information from this abstract:\n\nAbstract: {abstract}\"\"\"\n\n\n\nCode\nresponse = ollama.generate(prompt=prompt_schema, format=json_schema, **params_structured)\n\ntry:\n    data = json.loads(response.response)\n    metadata = PaperMetadata(**data)\n    print(\"Extracted and validated data:\")\n    print(json.dumps(data, indent=2))\nexcept (json.JSONDecodeError, ValueError) as e:\n    print(f\"Error: {e}\")\n    print(\"Raw output:\", response.response)\n\n\nExtracted and validated data:\n{\n  \"domain\": \"Scientific Collaborations\",\n  \"methods\": [\n    \"Network Analysis\",\n    \"Machine Learning\",\n    \"Random Forest Classifier\"\n  ],\n  \"n_samples\": 10000,\n  \"accuracy\": 76.0\n}\n\n\nJSON schema constraints are more reliable than prompt-based requests because they operate at the token level. The model cannot sample tokens that would create invalid JSON. The prompt activates extraction patterns. The schema enforces structure.\nSmaller models sometimes produce invalid JSON even with schema constraints. Always wrap parsing in try-except blocks and validate outputs. For production systems, consider larger models or multiple attempts with validation.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Prompt Tuning"
    ]
  },
  {
    "objectID": "m03-agentic-coding/prompt-tuning.html#allowing-uncertainty-to-reduce-hallucination",
    "href": "m03-agentic-coding/prompt-tuning.html#allowing-uncertainty-to-reduce-hallucination",
    "title": "Prompt Tuning",
    "section": "Allowing Uncertainty to Reduce Hallucination",
    "text": "Allowing Uncertainty to Reduce Hallucination\nLLMs confidently fabricate facts when they don‚Äôt know the answer because they optimize for fluency, not truth. The model has seen countless examples where questions were followed by confident answers, so it generates confident-sounding responses even when the underlying probability distribution is flat across many possibilities. The solution: explicitly give the model permission to admit ignorance. Compare a prompt that implicitly demands an answer:\n\nbad_prompt = \"\"\"Summarize the main findings from the 2023 paper by Johnson et al.\non quantum community detection in biological networks.\"\"\"\n\nAgainst a prompt that explicitly allows uncertainty:\n\ngood_prompt = \"\"\"I'm looking for a 2023 paper by Johnson et al. on quantum\ncommunity detection in biological networks.\n\nIf you know this paper, summarize its main findings.\nIf you're not certain this paper exists, say \"I cannot verify this paper exists\"\nand do NOT make up details.\n\nResponse:\"\"\"\n\n\n\nCode\nresponse_bad = ollama.generate(prompt=bad_prompt, **params_llm)\nresponse_good = ollama.generate(prompt=good_prompt, **params_llm)\n\nprint(\"BAD PROMPT (encourages hallucination):\")\nprint(response_bad.response)\nprint(\"\\nGOOD PROMPT (allows uncertainty):\")\nprint(response_good.response)\n\n\nBAD PROMPT (encourages hallucination):\nThe 2023 paper by Johnson et al. on quantum community detection in biological networks, titled \"Quantum Community Detection in Biological Networks,\" found that **quantum community detection (QCD) is a promising approach for identifying and characterizing biological networks, particularly in complex and heterogeneous environments.**\n\nThe study highlighted several key findings:\n\n*   **Potential for enhanced detection:** QCD offers the potential to detect networks with complex and heterogeneous structures, which are often difficult to characterize using traditional methods.\n*   **Improved accuracy:** QCD can achieve higher accuracy compared to classical methods, especially in environments with high noise and complex interactions.\n*   **Robustness:** QCD is more robust to noise and environmental changes than classical methods, making it suitable for real-time applications.\n*   **Adaptability:** QCD can be adapted to different biological networks and environments, allowing for the development of tailored detection strategies.\n*   **Applications:** QCD is being explored for various applications, including network monitoring, disease detection, and environmental monitoring.\n\nIn essence, the paper emphasizes the potential of QCD to improve the accuracy and robustness of biological network detection, making it a valuable tool for various research and development applications.\n\nGOOD PROMPT (allows uncertainty):\nI cannot verify this paper exists.\n\n\nThe good prompt activates patterns where explicit permission to admit ignorance preceded honest uncertainty statements. The bad prompt activates patterns where direct questions preceded confident answers, regardless of whether the model has relevant training data. Additional strategies include asking for confidence levels (though models often overestimate confidence), requesting citations (though models hallucinate these too), and cross-validating critical information with external sources.\nLLMs closely follow your instructions, even when they should not. They often attempt to answer beyond their actual capabilities. Explicitly tell your model to admit ignorance: ‚ÄúIf you don‚Äôt know the answer, just say so‚Äù or ‚ÄúIf you need more information, please ask.‚Äù LLMs are trained to be agreeable, which can hinder productive brainstorming or honest critique. Explicitly invite critical input: ‚ÄúI want your honest opinion‚Äù or ‚ÄúPoint out any problems or weaknesses you see in this idea.‚Äù",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Prompt Tuning"
    ]
  },
  {
    "objectID": "m03-agentic-coding/prompt-tuning.html#the-takeaway",
    "href": "m03-agentic-coding/prompt-tuning.html#the-takeaway",
    "title": "Prompt Tuning",
    "section": "The Takeaway",
    "text": "The Takeaway\nPrompt engineering is not magic. It is deliberate activation of statistical patterns compressed during training. Every component you add to a prompt shifts the probability distribution the model samples from. Instructions activate task-specific patterns. Output formats activate structured-response patterns. Personas activate stylistic patterns. Context disambiguates when multiple patterns compete. Examples demonstrate exact structure. Chain-of-thought activates reasoning-like patterns. Format constraints enforce structure at the token level. Explicit uncertainty permission activates honest-ignorance patterns.\nNone of this requires the model to understand what you want. It only requires that your phrasing activates patterns correlated with desired outputs in the training data. You are not communicating intent. You are manipulating probability distributions. Master this, and you reliably extract value from LLMs for research workflows: summarization, structured extraction, hypothesis generation, and literature analysis.\nThe deeper question remains: how do these models represent text internally? When you send a prompt, the model does not see English words. It sees numbers. Millions of numbers arranged in high-dimensional space. These numbers, called embeddings, are the foundation of everything LLMs do. Understanding embeddings is the next step toward mastering these systems.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Prompt Tuning"
    ]
  },
  {
    "objectID": "m03-agentic-coding/hands-on.html",
    "href": "m03-agentic-coding/hands-on.html",
    "title": "Hands-on",
    "section": "",
    "text": "Spoiler: In this session, we will build a fully functional game and refactor a codebase without writing a single line of Python ourselves. We will only write English.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Hands-on"
    ]
  },
  {
    "objectID": "m03-agentic-coding/hands-on.html#exercise-2-the-fix-it-loop",
    "href": "m03-agentic-coding/hands-on.html#exercise-2-the-fix-it-loop",
    "title": "Hands-on",
    "section": "Exercise 2: The ‚ÄúFix It‚Äù Loop",
    "text": "Exercise 2: The ‚ÄúFix It‚Äù Loop\nAgents are excellent debuggers because they can read stack traces faster than you can.\nStep 1: Sabotage: Open snake_game.py and delete a critical import (e.g., import random).\nStep 2: The Error: Run the game. It will crash in the terminal.\nStep 3: The Fix: Highlight the error in the terminal and press Cmd+L (Send to Agent). &gt; ‚ÄúFix this.‚Äù\nStep 4: Observation: The agent will : 1. Read the error (NameError: name 'random' is not defined). 2. Search the file for usages of random. 3. Re-add the import. 4. Run the game to verify the fix.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Hands-on"
    ]
  },
  {
    "objectID": "m03-agentic-coding/agentic-ai.html",
    "href": "m03-agentic-coding/agentic-ai.html",
    "title": "From ChatBot to Agentic AI",
    "section": "",
    "text": "What you‚Äôll learn in this module\n\n\n\nAgents are not smarter chatbots. They are state machines that loop, using an LLM to decide the next transition. The ReAct pattern (Reason + Act) enables this loop: the agent reasons about the current state, takes an action, observes the result, and feeds that observation back as input. This feedback loop is where intelligence emerges, not from the model itself.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Agentic AI"
    ]
  },
  {
    "objectID": "m03-agentic-coding/agentic-ai.html#the-mechanism",
    "href": "m03-agentic-coding/agentic-ai.html#the-mechanism",
    "title": "From ChatBot to Agentic AI",
    "section": "The Mechanism",
    "text": "The Mechanism\n\n\n\nReAct Loop\n\n\nThe naive view is that agents are ‚Äúsmarter‚Äù chatbots. They aren‚Äôt. They‚Äôre a core component wrapped in a control loop. A chatbot generates text and stops. An agent generates text, parses it for actionable commands, executes those commands in the real world, observes the results, and feeds those results back into the next prompt. The intelligence doesn‚Äôt come from the model‚Äîit comes from the feedback loop.\nThis is the ReAct Pattern (Reason + Act). A standard chatbot is a pure function: \\text{Output} = \\text{Model}(\\text{Input}). An agent is a state machine:\nwhile not task_complete:\n    observation = get_environment_state()\n    thought = model(observation)\n    action = parse_action(thought)\n    result = execute(action)\n    observation = result  # Feedback loop\nThe critical insight is the feedback loop. If the agent tries to import a missing library (Action) and receives ModuleNotFoundError (Observation), the next iteration‚Äôs Thought will be ‚ÄúI need to install this library,‚Äù rather than hallucinating success. The model corrects itself not through introspection, but through collision with reality.\nReAct framework is proposed by (Yao et al. 2022). The core idea is to prompt the LLM to generate both reasoning traces and task-specific actions in an interleaved manner. Specifically, the prompt structure follows a sequence: Thought \\rightarrow Action \\rightarrow Observation.\n\nThought: The model reasons about the current state and what needs to be done.\nAction: The model outputs a specific command to interact with an external environment (e.g., Search[Apple]).\nObservation: The environment executes the action and returns the result (e.g., search results for ‚ÄúApple‚Äù).\n\nThis cycle repeats until the task is solved.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Agentic AI"
    ]
  },
  {
    "objectID": "m03-agentic-coding/agentic-ai.html#the-react-framework-with-langgraph",
    "href": "m03-agentic-coding/agentic-ai.html#the-react-framework-with-langgraph",
    "title": "From ChatBot to Agentic AI",
    "section": "The ReAct Framework with langgraph",
    "text": "The ReAct Framework with langgraph\nLet‚Äôs build an agent that can explore and analyze a real dataset. We‚Äôll use LangGraph‚Äîa framework from LangChain that models agents as state machines. Unlike simple loops, LangGraph lets you define explicit control flow: decision nodes, parallel execution, conditional branching, and state persistence.\n\n\n\n\n\n\nNote\n\n\n\nInstall LangGraph and LangChain:\npip install langgraph langchain langchain-ollama\n\n\n\nCreating a Tool: A Fish Market Dataset\nLet‚Äôs build an agent that can explore and analyze a real dataset. We‚Äôll use the Fish Market dataset from Hugging Face‚Äîa collection of measurements from different fish species. First, load the data:\n\nimport pandas as pd\n\n# Load the Fish dataset\ndf = pd.read_csv(\"hf://datasets/scikit-learn/Fish/Fish.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nSpecies\nWeight\nLength1\nLength2\nLength3\nHeight\nWidth\n\n\n\n\n0\nBream\n242.0\n23.2\n25.4\n30.0\n11.5200\n4.0200\n\n\n1\nBream\n290.0\n24.0\n26.3\n31.2\n12.4800\n4.3056\n\n\n2\nBream\n340.0\n23.9\n26.5\n31.1\n12.3778\n4.6961\n\n\n3\nBream\n363.0\n26.3\n29.0\n33.5\n12.7300\n4.4555\n\n\n4\nBream\n430.0\n26.5\n29.0\n34.0\n12.4440\n5.1340\n\n\n\n\n\n\n\nNow we‚Äôll create tools that let the agent query this data. In LangGraph, tools are standard Python functions decorated with @tool. The function signature and docstring tell the LLM everything it needs.\n\n\n\n\n\n\nNote\n\n\n\n(tool?): Decorator that converts a function into a LangChain tool. The docstring becomes the tool description; parameter names and type hints define the schema.\nArgs section: Must be explicitly documented for each parameter. LangGraph parses this to generate the JSON schema the LLM sees.\n\n\n\nimport io\nfrom langchain_core.tools import tool\nfrom pandasql import sqldf\n\n@tool\ndef inspect_data() -&gt; str:\n    \"\"\"Get a concise summary of the dataset's structure, including column names, non-null values, and data types.\"\"\"\n    buffer = io.StringIO()\n    df.info(buf=buffer)\n    return buffer.getvalue()\n\nThe structure is minimal. LangGraph infers everything from the function:\n\nName: Derived from function name (inspect_data)\nDescription: Extracted from the docstring\nParameters: Inferred from type hints and docstring Args section\nReturn type: Inferred from return type hint\n\nThis tool takes no inputs and returns the dataset schema. The agent calls it to discover column names and types before writing queries.\nLet‚Äôs add three more tools to give the agent more analytical capabilities:\n\n\nCode\n@tool\ndef query_data(sql_query: str) -&gt; str:\n    \"\"\"Query the fish dataset using SQL. The table is called 'df'. Use inspect_data first to see available columns. Use find_correlations to find correlations between columns.\n\n    Args:\n        sql_query: SQL query to execute (use 'df' as table name)\n    \"\"\"\n    result = sqldf(sql_query, globals())\n    return result.to_string()\n\n@tool\ndef find_correlations(columns: list[str]) -&gt; str:\n    \"\"\"Calculate the correlation matrix for a list of numeric columns in the fish dataset.\n\n    Args:\n        columns: A list of column names to calculate correlations for.\n    \"\"\"\n    numeric_df = df[columns].select_dtypes(include=['number'])\n    corr_matrix = numeric_df.corr()\n    return corr_matrix.to_string()\n\n@tool\ndef get_stats(column: str, species: str = None) -&gt; str:\n    \"\"\"Get statistical summary (count, mean, std, min, max) for a specific column and optionally filter by species.\n\n    Args:\n        column: Column name to analyze\n        species: Species to filter by (optional)\n    \"\"\"\n    data = df\n    if species:\n        data = df[df[\"Species\"] == species]\n\n    stats = data[column].describe()\n    prefix = f\"Stats for {column}\"\n    if species:\n        prefix += f\" (Species: {species})\"\n    return f\"{prefix}:\\n{stats.to_string()}\"\n\n\nNow create the agent. LangGraph is centered on a state graph‚Äîa directed graph where nodes are functions and edges define transitions. This gives you explicit control over the ReAct loop.\n\n\n\n\n\n\nNote\n\n\n\nChatOllama: LangChain‚Äôs Ollama integration. Supports tool calling via the model‚Äôs native API.\ncreate_react_agent: Factory function that builds a standard ReAct graph. It defines three nodes: (1) call the LLM, (2) execute tools, (3) check if done.\nrecursion_limit: Maximum graph iterations. Equivalent to max_steps in smolagents.\n\n\n\nfrom langchain_ollama import ChatOllama\nfrom langgraph.prebuilt import create_react_agent\n\nmodel = ChatOllama(\n    model=\"glm-4.6:cloud\",\n    base_url=\"http://localhost:11434\"\n)\n\ntools = [\n    inspect_data,\n    query_data,\n    find_correlations,\n    get_stats\n]\n\nagent = create_react_agent(model, tools)\n\n/var/folders/j7/9dgqq5g53vnbsbmvh2yqtckr0000gr/T/ipykernel_7962/1185369839.py:16: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n  agent = create_react_agent(model, tools)\n\n\nRun the agent and watch it autonomously choose which tools to use.\n\nquery = \"Which fish species has the highest average weight?\"\ninputs = {\"messages\": [(\"user\", query)]}\n\nresult = agent.invoke(inputs)\n\n\n\nCode\nfor message in result[\"messages\"]:\n    print(message.content)\n\n\nWhich fish species has the highest average weight?\nI'll help you find which fish species has the highest average weight. Let me first examine the dataset structure to understand what columns are available.\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 159 entries, 0 to 158\nData columns (total 7 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Species  159 non-null    object \n 1   Weight   159 non-null    float64\n 2   Length1  159 non-null    float64\n 3   Length2  159 non-null    float64\n 4   Length3  159 non-null    float64\n 5   Height   159 non-null    float64\n 6   Width    159 non-null    float64\ndtypes: float64(6), object(1)\nmemory usage: 8.8+ KB\n\n\n     Species  avg_weight\n0       Pike  718.705882\n1      Bream  617.828571\n2  Whitefish  531.000000\n3      Perch  382.239286\n4     Parkki  154.818182\n5      Roach  152.050000\n6      Smelt   11.178571\nBased on the data analysis, **Pike** has the highest average weight at 718.71 grams.\n\nHere are all the fish species ranked by average weight:\n\n1. **Pike** - 718.71 grams\n2. **Bream** - 617.83 grams  \n3. **Whitefish** - 531.00 grams\n4. **Perch** - 382.24 grams\n5. **Parkki** - 154.82 grams\n6. **Roach** - 152.05 grams\n7. **Smelt** - 11.18 grams\n\nSo Pike is significantly heavier on average than the other fish species in this dataset.\n\n\nThe agent executes a ReAct loop. It reads the question ‚ÄúWhich fish species has the highest average weight?‚Äù and realizes it needs to use SQL to group by species and calculate averages. LangGraph streams each step‚Äîyou see the LLM‚Äôs reasoning, the tool calls, and the observations in real time.\nLet‚Äôs try another query that requires multiple steps:\n\nquery = \"What distinctive physical characteristics stand out to identify Pike?\"\ninputs = {\"messages\": [(\"user\", query)]}\n\nresult = agent.invoke(inputs)\n\n\n\nCode\nfor message in result[\"messages\"]:\n    print(message.content)\n\n\nWhat distinctive physical characteristics stand out to identify Pike?\nI'll help you identify Pike's distinctive physical characteristics. Let me first explore the fish dataset to see what information is available about Pike and other fish species.\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 159 entries, 0 to 158\nData columns (total 7 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Species  159 non-null    object \n 1   Weight   159 non-null    float64\n 2   Length1  159 non-null    float64\n 3   Length2  159 non-null    float64\n 4   Length3  159 non-null    float64\n 5   Height   159 non-null    float64\n 6   Width    159 non-null    float64\ndtypes: float64(6), object(1)\nmemory usage: 8.8+ KB\n\n\n\n\nThis demonstrates the power of the ReAct loop‚Äîthe agent chains multiple observations together, building a solution step-by-step rather than attempting everything in one shot. Unlike smolagents‚Äô opaque loop, LangGraph exposes every state transition, making debugging straightforward.\n\n\n\n\n\n\nWhy not more complex queries?\n\n\n\n\n\nYou might notice we‚Äôre using simpler queries than you‚Äôd expect. This is intentional. Real-world agentic systems face reliability challenges:\n\nJSON parsing errors: Models sometimes generate malformed JSON or multiple JSON objects in one response\nSQL limitations: pandasql uses SQLite, which lacks advanced functions like CORR() for per-group correlations\nMax steps: Complex queries can hit iteration limits before completing\n\nProduction systems like Claude Code and Cursor handle these issues through better error recovery, more sophisticated prompting, and custom tool implementations. For learning purposes, we focus on simple queries that reliably demonstrate the ReAct pattern.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Agentic AI"
    ]
  },
  {
    "objectID": "m02-visualization/principles.html",
    "href": "m02-visualization/principles.html",
    "title": "Perception in Data Visualization",
    "section": "",
    "text": "We extensively rely on visuals to perceive the world around us. However, our visual perception is not as truthful as you might think. It can be easily misled or manipulated if we are not aware of the inherent biases in how we see.\n\nPart 1: The Perception of Color\nColor is one of the most powerful tools in data visualization, but it is also one of the most complex. Our perception of color is not absolute; it is contextual and subjective.\n\nColor, Context, and Constancy\nOur visual system tries to maintain color constancy, meaning we perceive a familiar object as being a consistent color regardless of the lighting conditions. This is why we recognize a banana as yellow whether it‚Äôs in bright sunlight or in a dim room. However, this helpful adaptation can create peculiar biases in unfamiliar contexts.\n\n\n\nThe Dress\n\n\nThe infamous ‚Äúdress‚Äù illusion highlights how our brain makes assumptions about lighting, causing some people to see the dress as blue and black (in bright light) and others as white and gold (in shadow). The colors are physically the same, but our perception of them is not.\nThis happens because what we ‚Äúsee‚Äù is not just the raw wavelength of light hitting our eyes. Our visual cortex processes that raw signal, making inferences based on context and prior experience. For example, we perceive the leaves of a tree as green, even in a photograph made entirely of red, black, and white pixels, because our brain ‚Äúknows‚Äù trees are green.\n\n\n\nA ‚Äúgreen‚Äù tree with no green pixels\n\n\n\n\nEncoding Color Objectively\nSince perception is subjective, we need objective systems to define color. The most common are:\n\nRGB (Red, Green, Blue): An additive system for screens. Colors are created by adding light. Combining all three creates white.\nCMYK (Cyan, Magenta, Yellow, Black): A subtractive system for print. Colors are created by subtracting light with ink. Combining all three (plus black for depth) creates black.\nHSL/HSV (Hue, Saturation, Lightness/Value): More intuitive systems that align better with how we think about color. Hue is the pure color, Saturation is the intensity, and Lightness/Value is the brightness.\n\n\n\n\n\n\n\nAccessibility Matters: Designing for Color Blindness\n\n\n\nA crucial aspect of color choice is ensuring your visualizations are accessible to everyone, including those with color vision deficiencies (CVD). Roughly 8% of men and 0.5% of women are affected.\n\nAvoid Red-Green Palettes: The most common form of CVD is difficulty distinguishing between red and green.\nUse Perceptually Uniform Palettes: Tools like ColorBrewer provide palettes that are designed to be accessible.\nCombine Color with Other Cues: Don‚Äôt rely only on color. Use shape, pattern, or direct labels to distinguish data series.\n\n\n\n\n\nThe Problem with Rainbows: Perceptually Uniform Palettes\nA perceptually uniform colormap is one where equal steps in the data are perceived as equal steps in color. The common ‚Äúrainbow‚Äù (or ‚Äújet‚Äù) colormap fails at this because its brightness changes non-uniformly, creating false boundaries and hiding details. Palettes like Viridis were engineered to have a monotonically increasing luminance, making them accurate, intuitive, and accessible.\n\n\n\nViridis vs.¬†Jet Colormap\n\n\n\n\n\nPart 2: The Perception of Form and Quantity\nBeyond color, we must consider how we perceive shape, structure, and magnitude.\n\nGrouping and Structure: The Gestalt Principles\nOur brains have an innate tendency to see whole forms rather than collections of parts. These Gestalt principles are fundamental to chart design.\n\nProximity: We group objects that are close together.\nSimilarity: We group objects that look similar (e.g., same color or shape).\nEnclosure: We group objects that are inside a boundary.\nClosure & Continuity: We see incomplete shapes as whole and prefer to see continuous lines.\n\n\n\n\nGestalt Principles\n\n\n\n\nPerceiving Quantity\nOur ability to accurately judge quantity varies by the visual encoding used. Steven‚Äôs Power Law shows that we are very good at judging length, but poor at judging area and volume. This is why bar charts are often more effective than pie charts or bubble charts for precise comparisons.\n\n\n\nSteven‚Äôs Power law\n\n\nFurthermore, Weber‚Äôs Law suggests that our ability to perceive a change is relative to the magnitude. We can easily spot a 1-inch difference in a 5-inch line, but not in a 50-foot line.\n\n\n\nPart 3: Guiding Attention with Preattentive Attributes\nPreattentive attributes are visual properties that our brains process in milliseconds, before we even pay conscious attention. By using them purposefully, you can control the visual hierarchy of your chart and tell a story.\n\n\n\nPreattentive Attributes\n\n\nCommon attributes include a distinct color, size, shape, or orientation. The most effective way to use them is to de-emphasize the majority of your data (e.g., making it light grey) and use a single, strong attribute to highlight your key message. This ‚Äúgrey vs.¬†red‚Äù technique immediately tells the viewer, ‚ÄúLook here! This is what matters.‚Äù",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Principles of Effective Visualization"
    ]
  },
  {
    "objectID": "m02-visualization/networks.html",
    "href": "m02-visualization/networks.html",
    "title": "Network Visualization",
    "section": "",
    "text": "What you‚Äôll learn in this module\n\n\n\nThis module introduces the art of network visualization. We will explore how to reveal hidden structures using force-directed layouts, examine specific techniques for hierarchical data, and understand how to avoid the dreaded ‚Äúhairball‚Äù by matching layout algorithms to your analytical goals.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Networks"
    ]
  },
  {
    "objectID": "m02-visualization/networks.html#what-is-a-network",
    "href": "m02-visualization/networks.html#what-is-a-network",
    "title": "Network Visualization",
    "section": "What is a Network?",
    "text": "What is a Network?\nA network (or graph) is a collection of nodes (also called vertices) connected by edges (also called links). Networks can represent almost anything: social relationships, neural connections, citations between papers, roads between cities, or interactions between proteins.\n\n\n\n\n\n\nMathematical Definition\n\n\n\nA network G = (V, E) consists of:\n\nA set of nodes V = \\{v_1, v_2, ..., v_n\\}\nA set of edges E \\subseteq V \\times V representing connections\n\nNetworks can be directed (edges have direction, like citations) or undirected (edges are symmetric, like friendships).\n\n\nWhy do we visualize networks? Because topology is hard to grasp from data alone. Looking at an adjacency matrix or edge list gives you facts but not insight. Visualization transforms abstract connectivity into spatial patterns your visual system can process.\nBut here‚Äôs the challenge: unlike data points that have inherent positions (latitude/longitude, time series), networks have no natural layout. The positions you see in a network visualization are entirely constructed by the layout algorithm. Different algorithms can make the same network look completely different.\nThis means choosing a layout is choosing what to emphasize. Let‚Äôs see how.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Networks"
    ]
  },
  {
    "objectID": "m02-visualization/networks.html#visualizing-trees",
    "href": "m02-visualization/networks.html#visualizing-trees",
    "title": "Network Visualization",
    "section": "Visualizing Trees",
    "text": "Visualizing Trees\nThe simplest networks are trees. These are connected networks with no cycles, where every node except the root has exactly one parent. Trees appear everywhere, from biological taxonomies and organizational charts to file systems.\nFor trees, the structure is clear and the hierarchy is paramount. The radial tree layout makes this hierarchy visible by placing the root at the center and arranging descendants in concentric circles.\n\n\nCode\nimport graph_tool.all as gt\nimport networkx as nx\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Generate a random tree using NetworkX\nnx_tree = nx.random_labeled_tree(n=50, seed=42)\n\n# Convert to graph-tool\ng = gt.Graph(directed=False)\ng.add_vertex(nx_tree.number_of_nodes())\nfor u, v in nx_tree.edges():\n    g.add_edge(g.vertex(u), g.vertex(v))\n\n# Create radial tree layout\npos = gt.radial_tree_layout(g, g.vertex(0))\n\n# Draw the network (let graph-tool handle rendering directly)\ngt.graph_draw(g, pos=pos,\n              vertex_fill_color=[0.275, 0.510, 0.706, 1],  # steelblue\n              vertex_size=15,\n              edge_color=[0.5, 0.5, 0.5, 1],  # gray\n              edge_pen_width=1.5,\n              output_size=(500, 500),\n              inline=True)\n\n\n\n\n\nRadial tree layout of a random tree with 50 nodes. The root is at the center, and descendants are arranged in concentric circles by depth.\n\n\n\n\nThe radial layout immediately reveals several key properties. It shows the depth of each node, indicating exactly how far it sits from the root. It clarifies the branching structure, highlighting where the tree splits into subtrees. Finally, it exposes the overall balance of the system, instantly showing whether the tree is symmetric or lopsided.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Networks"
    ]
  },
  {
    "objectID": "m02-visualization/networks.html#force-directed-layouts",
    "href": "m02-visualization/networks.html#force-directed-layouts",
    "title": "Network Visualization",
    "section": "Force-Directed Layouts",
    "text": "Force-Directed Layouts\nMost networks are not trees. They have cycles, cross-links, and complex connectivity patterns. For these networks, we need algorithms that can handle arbitrary topology. The most common approach is the force-directed layout.\nThe idea is intuitive. We treat nodes as charged particles that repel each other, and edges as springs that pull connected nodes together. We let the system simulate physics until it reaches equilibrium. Nodes that are closely connected end up near each other, while unconnected parts spread apart.\nThe Fruchterman-Reingold algorithm is one of the most widely used force-directed methods. It balances the repulsive force between all pairs of nodes against the attractive force of the edges connecting them.\nLet‚Äôs see it in action on the Zachary Karate Club, a famous social network of 34 members of a karate club that eventually split into two factions.\n\n\nCode\nimport graph_tool.all as gt\nimport networkx as nx\nimport numpy as np\n\nnp.random.seed(42)\n\n# Generate a random tree using NetworkX\nnx_tree = nx.random_labeled_tree(n=50, seed=42)\n\n# Convert to graph-tool\ng = gt.Graph(directed=False)\ng.add_vertex(nx_tree.number_of_nodes())\nfor u, v in nx_tree.edges():\n    g.add_edge(g.vertex(u), g.vertex(v))\n\n# Force-directed layout (Fruchterman-Reingold)\npos_force = gt.fruchterman_reingold_layout(g, n_iter=1000)\n\n# Draw force-directed layout inline\ngt.graph_draw(\n    g,\n    pos=pos_force,\n    vertex_fill_color=[1.0, 0.498, 0.314, 1],  # coral\n    vertex_size=15,\n    edge_color=[0.5, 0.5, 0.5, 1],  # gray\n    edge_pen_width=1.5,\n    output_size=(500, 500),\n    inline=True\n)\n\n\n\n\n\nComparison of radial layout (left) vs.¬†force-directed layout (right) for the same tree. The radial layout emphasizes hierarchy, while force-directed layout treats all edges equally.\n\n\n\n\n\n\nCode\nimport graph_tool.all as gt\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load the karate club network\ng = gt.collection.data[\"karate\"]\n\n# Get community labels (the two groups that split)\n# We'll use blockmodel inference with 2 communities\nstate = gt.minimize_blockmodel_dl(g, state_args=dict(B=2))\ncommunity = state.get_blocks()\n\n# Create Fruchterman-Reingold layout\npos = gt.fruchterman_reingold_layout(g, n_iter=1000)\n\n# Map communities to colors (RGB tuples)\ncolor_map = {0: [0.906, 0.298, 0.235, 1],  # Red\n             1: [0.204, 0.596, 0.859, 1]}  # Blue\n\n# Create vertex property map for colors\nvertex_color = g.new_vertex_property(\"vector&lt;double&gt;\")\nfor v in g.vertices():\n    vertex_color[v] = color_map[community[v]]\n\n# Draw the network\ngt.graph_draw(g, pos=pos,\n              vertex_fill_color=vertex_color,\n              vertex_size=20,\n              edge_color=[0.584, 0.647, 0.651, 1],\n              edge_pen_width=2,\n              output_size=(1000, 800),\n              inline=True)\n\n\n\n\n\nZachary Karate Club network with Fruchterman-Reingold layout. Node colors indicate the two groups that formed after the club split. The layout naturally separates the two communities.\n\n\n\n\nThe Karate Club dataset comes from a study by Wayne Zachary (1977). It documents the split of a university karate club into two factions and remains one of the most famous benchmarks in network science.\nThe layout does something remarkable here. Even though we didn‚Äôt tell the algorithm about the two groups, it naturally separates them in space. This happens because nodes within each group are densely connected, creating a strong attractive pull, while connections between groups are sparse, leading to a weaker pull across the boundary.\n\nTuning the Physics\nForce-directed algorithms rely on simulation, effectively running a physics engine to find a stable state. The most critical parameter is the number of iterations, which dictates how long the simulation runs before stopping.\n\n\nCode\nimport graph_tool.all as gt\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ng = gt.collection.data[\"karate\"]\nstate = gt.minimize_blockmodel_dl(g, state_args=dict(B=2))\ncommunity = state.get_blocks()\ncolor_map = {0: [0.906, 0.298, 0.235, 1],  # Red\n             1: [0.204, 0.596, 0.859, 1]}  # Blue\n\n# Create vertex property map for colors\nvertex_color = g.new_vertex_property(\"vector&lt;double&gt;\")\nfor v in g.vertices():\n    vertex_color[v] = color_map[community[v]]\n\n# Different iteration counts - show progression\nprint(\"50 iterations:\")\npos = gt.fruchterman_reingold_layout(g, n_iter=50)\ngt.graph_draw(g, pos=pos,\n              vertex_fill_color=vertex_color,\n              vertex_size=15,\n              edge_color=[0.584, 0.647, 0.651, 1],\n              edge_pen_width=1.5,\n              output_size=(400, 400),\n              inline=True)\n\nprint(\"\\n500 iterations:\")\npos = gt.fruchterman_reingold_layout(g, n_iter=500)\ngt.graph_draw(g, pos=pos,\n              vertex_fill_color=vertex_color,\n              vertex_size=15,\n              edge_color=[0.584, 0.647, 0.651, 1],\n              edge_pen_width=1.5,\n              output_size=(400, 400),\n              inline=True)\n\nprint(\"\\n5000 iterations:\")\npos = gt.fruchterman_reingold_layout(g, n_iter=5000)\ngt.graph_draw(g, pos=pos,\n              vertex_fill_color=vertex_color,\n              vertex_size=15,\n              edge_color=[0.584, 0.647, 0.651, 1],\n              edge_pen_width=1.5,\n              output_size=(400, 400),\n              inline=True)\n\n\n50 iterations:\n\n\n\n\n\nEffect of iteration count on force-directed layout quality. Too few iterations (left) produce cramped layouts; optimal iterations (middle) balance clarity and structure; excessive iterations (right) offer minimal improvement.\n\n\n\n\n\n500 iterations:\n\n\n\n\n\n\n\n\n\n\n5000 iterations:\n\n\n\n\n\n\n\n\n\nWith only 50 iterations, the nodes are still cramped near their initial positions because they haven‚Äôt had time to spread out. Increasing this to 500 iterations allows the structure to emerge clearly. Pushing to 5000 iterations yields diminishing returns. The layout looks similar, but the computation time increases.\nHere is a practical rule of thumb. For small networks (&lt; 100 nodes), try 500-1000 iterations. For medium networks (100-1000 nodes), aim for 1000-2000. For anything larger, you need a different approach.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Networks"
    ]
  },
  {
    "objectID": "m02-visualization/networks.html#visualizing-hierarchical-structure",
    "href": "m02-visualization/networks.html#visualizing-hierarchical-structure",
    "title": "Network Visualization",
    "section": "Visualizing Hierarchical Structure",
    "text": "Visualizing Hierarchical Structure\nMany real-world networks‚Äîsuch as biological systems or large organizations‚Äîexhibit hierarchical community structure. This means they have groups nested within groups. Standard force-directed layouts can reveal the primary communities, but they often obscure the nested relationships between them. For this, we use circular hierarchy layouts with edge bundling.\n\nThe Nested Block Model\nThe first step is to identify the hierarchy. We use the nested stochastic block model to partition nodes into communities, then group those communities into super-communities, and so on. This creates a multi-level map of the network‚Äôs organization.\nLet‚Äôs look at the C. elegans neural network, which maps the complete wiring diagram of a nematode‚Äôs nervous system.\n\n\nCode\nimport graph_tool.all as gt\nimport matplotlib.pyplot as plt\n\n# Load C. elegans neural network\ng = gt.collection.data[\"celegansneural\"]\n\n# Infer hierarchical community structure\nstate = gt.minimize_nested_blockmodel_dl(g)\n\n# Draw hierarchy with edge bundling\ngt.draw_hierarchy(state,\n                  beta=0.8,  # Edge bundling strength\n                  output_size=(1200, 1200),\n                  inline=True)\n\n\n\n\n\nHierarchical structure of the C. elegans neural network revealed through nested block model visualization with edge bundling. Inner rings represent higher-level communities, outer ring shows individual neurons. Edge bundling (beta=0.8) reduces visual clutter by routing edges through the hierarchy.\n\n\n\n\n(&lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x33a1d0b90, at 0x107152290&gt;,\n &lt;GraphView object, directed, with 314 vertices and 313 edges, edges filtered by (&lt;EdgePropertyMap object with value type 'bool', for Graph 0x347234250, at 0x34724a150&gt;, False), vertices filtered by (&lt;VertexPropertyMap object with value type 'bool', for Graph 0x347234250, at 0x3472a9710&gt;, False), at 0x347234250&gt;,\n &lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x347234250, at 0x3470f7810&gt;)\n\n\nDanny Holten (2006) introduced hierarchical edge bundling to visualize adjacency relations in hierarchical data. The technique routes edges through their lowest common ancestor in the hierarchy tree, acting like cables tied together to reduce clutter.\nThis visualization packs an enormous amount of information into a single image. The concentric rings represent levels of the hierarchy, from coarse inner groups to fine outer details. The colored wedges visually separate different communities. Most importantly, edge bundling acts as a visual compressor. By routing edges through the hierarchy tree, it creates bundled ‚Äúhighways‚Äù that reveal large-scale connectivity patterns.\nWithout edge bundling, this network would look like an incomprehensible hairball. With it, we can see that most connections occur within communities or between closely related ones, exactly what we expect in a modular biological system.\n\n\nTuning Edge Bundling\nThe appearance of these plots depends heavily on beta, the edge bundling strength. This parameter ranges from 0 (straight lines) to 1 (tightly bundled curves that follow the hierarchy exactly).\n\n\nCode\nimport graph_tool.all as gt\nimport matplotlib.pyplot as plt\n\n# Use a smaller network for clearer comparison\ng = gt.collection.data[\"karate\"]\nstate = gt.minimize_nested_blockmodel_dl(g)\n\n# Beta = 0.3 (low bundling)\nprint(\"Beta = 0.3:\")\ngt.draw_hierarchy(state,\n                  beta=0.3,\n                  output_size=(600, 600),\n                  inline=True)\n\n# Beta = 0.9 (high bundling)\nprint(\"\\nBeta = 0.9:\")\ngt.draw_hierarchy(state,\n                  beta=0.9,\n                  output_size=(600, 600),\n                  inline=True)\n\n\nBeta = 0.3:\n\n\n\n\n\nEffect of edge bundling strength (beta) on hierarchical network visualization. Low beta (left) shows individual edges but creates clutter; high beta (right) emphasizes hierarchical structure but may obscure detailed connectivity.\n\n\n\n\n\nBeta = 0.9:\n\n\n\n\n\n\n\n\n\n(&lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x34720dad0, at 0x347207850&gt;,\n &lt;GraphView object, directed, with 35 vertices and 34 edges, edges filtered by (&lt;EdgePropertyMap object with value type 'bool', for Graph 0x10717ae50, at 0x3472c5b50&gt;, False), vertices filtered by (&lt;VertexPropertyMap object with value type 'bool', for Graph 0x10717ae50, at 0x3472c4850&gt;, False), at 0x10717ae50&gt;,\n &lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x10717ae50, at 0x34724f350&gt;)\n\n\nA low beta value like 0.3 preserves individual edge information but creates visual clutter. A high beta value like 0.9 emphasizes the hierarchical flow of connections, making it easy to see which communities talk to which, but individual edges become hard to trace.\nYou should choose beta based on your analytical goal. If you need to trace specific connections, keep beta low (0.3-0.5). If you want to show the overall flow and structure of the system, push beta higher (0.7-0.9).\n\n\n\n\n\n\nWhen to Use Hierarchical Layouts\n\n\n\nCircular hierarchy layouts are powerful but specific. They are only appropriate when your network actually has a hierarchical structure. Forcing a random network into this layout creates the illusion of order where none exists. Always validate your hierarchical partition before visualizing it.\n\n\n\n\nAlternative: SFDP for Hierarchies\nFor very large hierarchies, the radial layout can become crowded. In these cases, you can combine the SFDP algorithm with the hierarchical structure to position the tree using force-directed placement.\n\n\nCode\nimport graph_tool.all as gt\nimport matplotlib.pyplot as plt\n\ng = gt.collection.data[\"karate\"]\nstate = gt.minimize_nested_blockmodel_dl(g)\n\ngt.draw_hierarchy(state,\n                  layout=\"sfdp\",\n                  beta=0.8,\n                  output_size=(1000, 1000),\n                  inline=True)\n\n\n\n\n\nHierarchical visualization with SFDP layout for the hierarchy tree. The SFDP algorithm positions hierarchy levels using force-directed placement, which can reveal different structural patterns.\n\n\n\n\n(&lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x34720dad0, at 0x34724e210&gt;,\n &lt;GraphView object, directed, with 35 vertices and 34 edges, edges filtered by (&lt;EdgePropertyMap object with value type 'bool', for Graph 0x33a2f5690, at 0x3472c7150&gt;, False), vertices filtered by (&lt;VertexPropertyMap object with value type 'bool', for Graph 0x33a2f5690, at 0x3472c4d90&gt;, False), at 0x33a2f5690&gt;,\n &lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x33a2f5690, at 0x3472c5a10&gt;)\n\n\nThis approach is useful when you want to emphasize local connectivity patterns over strict hierarchical levels, offering a hybrid view of the data.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Networks"
    ]
  },
  {
    "objectID": "m02-visualization/networks.html#the-bigger-picture-layout-as-hypothesis",
    "href": "m02-visualization/networks.html#the-bigger-picture-layout-as-hypothesis",
    "title": "Network Visualization",
    "section": "The Bigger Picture: Layout as Hypothesis",
    "text": "The Bigger Picture: Layout as Hypothesis\nEvery layout algorithm embodies a hypothesis about what makes nodes ‚Äúsimilar‚Äù or ‚Äúclose‚Äù:\n\nRadial tree layout hypothesizes that hierarchy is the key structure\nForce-directed layout hypothesizes that shared neighbors create similarity\nHierarchical layout with edge bundling hypothesizes that multi-scale community structure organizes the network\n\nNone of these is objectively ‚Äúcorrect‚Äù‚Äîthey‚Äôre different lenses for viewing the same data. The critical skill is matching the layout to the question you‚Äôre asking.\n\nLimitations and Caveats\nNetwork visualization has fundamental limitations that you need to understand:\n1. Layout is not analysis. A clear visual pattern doesn‚Äôt prove that pattern exists in the data‚Äîit might be an artifact of the layout algorithm. Always validate visual insights with quantitative analysis (modularity scores, statistical tests, null models).\n2. 2D layouts lose information. Projecting a high-dimensional graph structure into 2D necessarily distorts distances and relationships. Nodes that appear close might not be similar; nodes that appear far might be connected.\n3. Large networks don‚Äôt scale. Once you have thousands of nodes, even the best layouts become unreadable. At that point, consider: - Aggregation: Show communities as super-nodes - Filtering: Display only the most important nodes/edges - Interactive tools: Allow zooming and panning - Alternative representations: Adjacency matrices, arc diagrams\n4. Edge crossings are unavoidable (except for planar graphs). Don‚Äôt spend hours tweaking layouts to eliminate all crossings‚Äîfocus on revealing meaningful structure instead.\n\n\n\n\n\n\nBest Practices for Publication Figures\n\n\n\n\nAlways set a random seed for reproducible force-directed layouts\nLabel important nodes (but not all of them‚Äîselective annotation is key)\nUse color meaningfully (communities, node attributes) or not at all\nMake nodes proportional to importance (degree, PageRank, betweenness)\nInclude a caption that explains the layout algorithm so readers know how to interpret spatial relationships\nProvide network statistics (number of nodes, edges, clustering coefficient) in the caption or main text\n\n\n\n\n\nWhen Visualization Isn‚Äôt Enough\nSometimes network visualization isn‚Äôt the right tool at all:\n\nVery large networks (&gt;10,000 nodes): Use statistical summaries (degree distribution, clustering) or dimensionality reduction techniques\nDense networks (many edges relative to nodes): Adjacency matrices often work better than node-link diagrams\nTemporal networks: Animation rarely works; small multiples or stacked layouts are clearer\nNetworks with important edge attributes: Consider matrix representations where you can encode edge weights with color intensity\n\nThe goal is insight, not aesthetics. If a bar chart of degree distribution tells the story better than a hairball diagram, use the bar chart. Visualization is a means to understanding, not an end in itself.\n\n\nFurther Reading\n\n\n\n\n\n\nNote\n\n\n\nKey References:\n\nHolten, D. (2006). ‚ÄúHierarchical Edge Bundles: Visualization of Adjacency Relations in Hierarchical Data.‚Äù IEEE TVCG 12(5):741-748.\nFruchterman, T.M.J., & Reingold, E.M. (1991). ‚ÄúGraph Drawing by Force-Directed Placement.‚Äù Software: Practice and Experience 21(11):1129-1164.\nPeixoto, T.P. (2014). ‚ÄúHierarchical Block Structures and High-Resolution Model Selection in Large Networks.‚Äù Physical Review X 4(1):011047.\n\n\n\nNetwork visualization is a rich field with decades of research. For deeper exploration:\n\nGraph-tool documentation: Comprehensive guide to all layout algorithms\nThe visual display of quantitative information by Edward Tufte: Principles of effective visualization\nNetwork Science by Albert-L√°szl√≥ Barab√°si: Chapter on network visualization and its interpretation\n\nRemember: the best layout is the one that helps you answer your question. Start with the structure you‚Äôre looking for, then choose the layout that makes that structure visible.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Networks"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html",
    "href": "m02-visualization/1d-data.html",
    "title": "Visualizing One-Dimensional Data: Show Me the Data!",
    "section": "",
    "text": "Imagine you‚Äôre reading a research paper that claims ‚ÄúTreatment A is significantly better than Treatment B.‚Äù The paper shows a bar chart with two bars and error bars. The difference looks impressive. But here‚Äôs the question: what does the actual data look like? Are there 5 data points per group? 500? Are they normally distributed, or are there outliers? Are most points clustered together, or spread out?\nWithout seeing the raw data, you‚Äôre flying blind. And unfortunately, many scientific papers and reports make this same mistake: they summarize data without showing it.\nThe golden rule of data visualization: Show all the data, whenever possible.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#the-case-of-the-missing-data-points",
    "href": "m02-visualization/1d-data.html#the-case-of-the-missing-data-points",
    "title": "Visualizing One-Dimensional Data: Show Me the Data!",
    "section": "",
    "text": "Imagine you‚Äôre reading a research paper that claims ‚ÄúTreatment A is significantly better than Treatment B.‚Äù The paper shows a bar chart with two bars and error bars. The difference looks impressive. But here‚Äôs the question: what does the actual data look like? Are there 5 data points per group? 500? Are they normally distributed, or are there outliers? Are most points clustered together, or spread out?\nWithout seeing the raw data, you‚Äôre flying blind. And unfortunately, many scientific papers and reports make this same mistake: they summarize data without showing it.\nThe golden rule of data visualization: Show all the data, whenever possible.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#why-showing-all-data-matters",
    "href": "m02-visualization/1d-data.html#why-showing-all-data-matters",
    "title": "Visualizing One-Dimensional Data: Show Me the Data!",
    "section": "Why Showing All Data Matters",
    "text": "Why Showing All Data Matters\nIn 2016, a group of researchers analyzed 118 papers in leading neuroscience journals and found something disturbing: when they requested the raw data and re-analyzed it, they found that the bar charts in many papers were misleading. The bar charts suggested clear differences between groups, but the raw data often told a different story‚Äîwith substantial overlap between groups, unexpected distributions, or influential outliers.\nThis isn‚Äôt about fraud. It‚Äôs about the limitations of summary statistics. When you reduce your data to a mean and a standard error, you lose a tremendous amount of information. The data might be bimodal, skewed, or contain outliers. These patterns are invisible in a bar chart, but they‚Äôre crucial for understanding what‚Äôs really going on.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#dynamite-plots-must-die",
    "href": "m02-visualization/1d-data.html#dynamite-plots-must-die",
    "title": "Visualizing One-Dimensional Data: Show Me the Data!",
    "section": "Dynamite Plots Must Die",
    "text": "Dynamite Plots Must Die\nStatisticians have been campaigning against bar charts with error bars‚Äîcalled ‚Äúdynamite plots‚Äù‚Äîfor years. Yet a systematic review found that 85.6% of papers in top physiology journals still use them. They appear everywhere: Nature, Science, Cell.\nWhy is this a problem? A dynamite plot shows you exactly four numbers (two means and two standard errors), regardless of sample size. But worse, completely different datasets produce identical bar charts. A dataset with outliers, a uniform distribution, or a bimodal distribution can all generate the same plot.\nWhen Rafael Irizarry showed the actual data behind a blood pressure comparison, the story changed dramatically. The bar chart showed a clear, significant difference. But the raw data revealed an extreme outlier (possibly a data entry error) and substantial overlap between groups. Remove that single outlier, and the result was no longer significant.\nAs Irizarry put it in his open letter to journal editors: dynamite plots conceal the data rather than showing it. The solution? Show the actual data points whenever possible, and use distributions (boxplots, histograms, density plots) when you can‚Äôt.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#start-simple-show-every-point",
    "href": "m02-visualization/1d-data.html#start-simple-show-every-point",
    "title": "Visualizing One-Dimensional Data: Show Me the Data!",
    "section": "Start Simple: Show Every Point",
    "text": "Start Simple: Show Every Point\n\nSwarm Plots (Beeswarm Plots)\nThe most straightforward approach is to plot every single data point. A swarm plot (also called a beeswarm plot) does exactly this: it displays each observation as a point, with points arranged to avoid overlap.\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate sample data\nnp.random.seed(42)\ngroup_a = np.random.normal(100, 15, 30)\ngroup_b = np.random.normal(120, 20, 30)\ndata = {'Value': np.concatenate([group_a, group_b]),\n        'Group': ['A']*30 + ['B']*30}\n\n# Create swarm plot\nsns.swarmplot(data=data, x='Group', y='Value')\nplt.title('Swarm Plot: Every Point Visible')\nplt.show()\nSwarm plots are perfect for small to moderate datasets (roughly up to 100-200 points per group). They let you see: - The actual sample size - The distribution shape - Individual outliers - The spread of the data\n\n\nThe Limits of Swarm Plots\nBut what happens when you have more data? With hundreds or thousands of points, swarm plots become cluttered and difficult to read. The points start to pile up, and the plot becomes a blob. This is where we need more sophisticated techniques.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#handling-more-data-transparency-and-jittering",
    "href": "m02-visualization/1d-data.html#handling-more-data-transparency-and-jittering",
    "title": "Visualizing One-Dimensional Data: Show Me the Data!",
    "section": "Handling More Data: Transparency and Jittering",
    "text": "Handling More Data: Transparency and Jittering\n\nStrip Plots with Jittering\nWhen you have too many points for a swarm plot, a strip plot with jittering can help. Instead of carefully arranging points to avoid overlap, we add random noise (jitter) to the x-position of each point.\n# Strip plot with jittering\nsns.stripplot(data=data, x='Group', y='Value', alpha=0.6, jitter=0.2)\nplt.title('Strip Plot with Jittering')\nplt.show()\nThe key parameters: - alpha: Controls transparency (0 = invisible, 1 = opaque). Values around 0.3-0.7 work well. - jitter: Amount of random horizontal displacement. Too much jitter and groups overlap; too little and points stack vertically.\n\n\nBarcode Plots (Rug Plots)\nFor even larger datasets, consider a barcode plot (also called a rug plot). This shows each data point as a small vertical tick mark along an axis. It‚Äôs minimalist but effective for showing the distribution of many points.\n# Barcode plot using rug plot\nfig, ax = plt.subplots(figsize=(10, 2))\nfor i, group in enumerate(['A', 'B']):\n    values = data[data['Group'] == group]['Value']\n    ax.plot(values, [i]*len(values), '|', markersize=10, alpha=0.7)\nax.set_yticks([0, 1])\nax.set_yticklabels(['A', 'B'])\nax.set_xlabel('Value')\nax.set_title('Barcode Plot')\nplt.show()\nBarcode plots work well when you have thousands of points and want to show density patterns without losing the ‚Äúraw data‚Äù feel.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#summarizing-distributions-histograms",
    "href": "m02-visualization/1d-data.html#summarizing-distributions-histograms",
    "title": "Visualizing One-Dimensional Data: Show Me the Data!",
    "section": "Summarizing Distributions: Histograms",
    "text": "Summarizing Distributions: Histograms\nWhen your dataset is large enough that individual points become impractical to show, you need to summarize the distribution. The most common approach is the histogram.\nA histogram divides your data range into bins and counts how many observations fall into each bin. It‚Äôs a powerful tool for understanding the shape of your distribution.\n# Histogram\nplt.hist(group_a, bins=15, alpha=0.5, label='Group A', edgecolor='black')\nplt.hist(group_b, bins=15, alpha=0.5, label='Group B', edgecolor='black')\nplt.xlabel('Value')\nplt.ylabel('Count')\nplt.legend()\nplt.title('Histogram: Distribution Comparison')\nplt.show()\n\nThe Art of Choosing Bins\nThe number of bins dramatically affects how your histogram looks: - Too few bins: You lose detail and might miss important features like bimodality - Too many bins: The histogram becomes noisy and hard to interpret\nA good starting point is the Sturges‚Äô rule: number of bins H \\log_2(n) + 1, where n is the sample size. But always experiment! Try different bin numbers and see what reveals the most about your data‚Äôs structure.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#smooth-alternatives-kernel-density-estimation",
    "href": "m02-visualization/1d-data.html#smooth-alternatives-kernel-density-estimation",
    "title": "Visualizing One-Dimensional Data: Show Me the Data!",
    "section": "Smooth Alternatives: Kernel Density Estimation",
    "text": "Smooth Alternatives: Kernel Density Estimation\nHistograms have a problem: they‚Äôre sensitive to bin width and bin placement. Move your bins slightly, and the histogram can look quite different.\nKernel Density Estimation (KDE) provides a smooth alternative. Instead of binning, KDE places a small ‚Äúkernel‚Äù (usually a Gaussian curve) at each data point and sums them up. The result is a smooth density curve.\n# KDE plot\nsns.kdeplot(data=group_a, label='Group A', fill=True, alpha=0.5)\nsns.kdeplot(data=group_b, label='Group B', fill=True, alpha=0.5)\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.legend()\nplt.title('Kernel Density Estimate')\nplt.show()\nKDE plots are elegant and reveal the shape of your distribution without the arbitrary choices of histograms. However, they can be misleading at the edges of your data and may suggest data exists where it doesn‚Äôt.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#for-heavy-tailed-data-cumulative-distributions",
    "href": "m02-visualization/1d-data.html#for-heavy-tailed-data-cumulative-distributions",
    "title": "Visualizing One-Dimensional Data: Show Me the Data!",
    "section": "For Heavy-Tailed Data: Cumulative Distributions",
    "text": "For Heavy-Tailed Data: Cumulative Distributions\nSome data are extremely heterogeneous\u0014think income distributions, city populations, or earthquake magnitudes. These distributions often have heavy tails: most values are small, but a few are enormous.\nFor this kind of data, histograms and KDE plots can be misleading because they compress the tail into a tiny region of the plot.\n\nCumulative Distribution Function (CDF)\nThe cumulative distribution function shows the proportion of data points less than or equal to each value. Instead of asking ‚Äúhow many points are in this bin?‚Äù, the CDF asks ‚Äúwhat fraction of points are below this value?‚Äù\nThe CDF is a density estimation method that requires no parameter choices. Unlike histograms (which require bin size) or KDE (which requires bandwidth), the CDF is completely determined by your data. There are no arbitrary decisions that change how your data looks‚Äîmaking it one of the most honest ways to visualize a distribution.\n# CDF\nsorted_a = np.sort(group_a)\ncdf_a = np.arange(1, len(sorted_a) + 1) / len(sorted_a)\n\nsorted_b = np.sort(group_b)\ncdf_b = np.arange(1, len(sorted_b) + 1) / len(sorted_b)\n\nplt.plot(sorted_a, cdf_a, label='Group A', linewidth=2)\nplt.plot(sorted_b, cdf_b, label='Group B', linewidth=2)\nplt.xlabel('Value')\nplt.ylabel('Cumulative Probability')\nplt.legend()\nplt.title('Cumulative Distribution Function')\nplt.grid(True, alpha=0.3)\nplt.show()\nThe CDF has several advantages: - No binning decisions: Every data point is shown - Easy to read percentiles: The median is where CDF = 0.5 - Great for comparisons: Differences between groups are easy to spot\n\n\nComplementary Cumulative Distribution Function (CCDF)\nFor heavy-tailed distributions, the complementary cumulative distribution function (CCDF) is even more useful. The CCDF shows the proportion of data points greater than each value: CCDF(x) = 1 - CDF(x).\nThe magic of the CCDF is that when plotted on a log-log scale, power-law distributions appear as straight lines. This makes it the go-to tool for studying phenomena like: - Income and wealth distributions - City size distributions - Social network degree distributions - Earthquake magnitudes\n# CCDF on log-log scale\n# Generate heavy-tailed data\nheavy_tailed = np.random.pareto(2, 1000) + 1\n\nsorted_data = np.sort(heavy_tailed)\nccdf = 1 - (np.arange(1, len(sorted_data) + 1) / len(sorted_data))\n\nplt.loglog(sorted_data, ccdf, 'o', alpha=0.5, markersize=3)\nplt.xlabel('Value')\nplt.ylabel('P(X e x)')\nplt.title('Complementary Cumulative Distribution (CCDF)')\nplt.grid(True, alpha=0.3)\nplt.show()\nThe CCDF reveals the tail behavior that‚Äôs invisible in traditional histograms. For heterogeneous, heavy-tailed data, it‚Äôs an essential tool.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#choosing-the-right-visualization",
    "href": "m02-visualization/1d-data.html#choosing-the-right-visualization",
    "title": "Visualizing One-Dimensional Data: Show Me the Data!",
    "section": "Choosing the Right Visualization",
    "text": "Choosing the Right Visualization\nHere‚Äôs a quick decision guide:\n\n\n\n\n\n\n\n\nScenario\nBest Visualization\nWhy\n\n\n\n\n&lt; 100 points per group\nSwarm plot\nShows every data point clearly\n\n\n100-500 points\nStrip plot with jitter + transparency\nManageable with some overlap\n\n\n500-5000 points\nHistogram or KDE + rug plot\nNeed summary but show raw data on axis\n\n\n&gt; 5000 points\nKDE or histogram alone\nToo many points to show individually\n\n\nHeavy-tailed/heterogeneous\nCCDF (log-log scale)\nReveals tail behavior\n\n\nComparing distributions\nCDF or overlaid KDE\nEasy to spot differences",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#the-bigger-picture",
    "href": "m02-visualization/1d-data.html#the-bigger-picture",
    "title": "Visualizing One-Dimensional Data: Show Me the Data!",
    "section": "The Bigger Picture",
    "text": "The Bigger Picture\nThe methods you choose to visualize your data aren‚Äôt just aesthetic choices‚Äîthey‚Äôre scientific choices. Different visualizations reveal different aspects of your data, and some can hide important patterns.\nBy starting with the raw data and building up to summaries, you ensure that you understand what you‚Äôre working with. And by showing your data (not just summarizing it), you allow others to draw their own conclusions.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#further-reading",
    "href": "m02-visualization/1d-data.html#further-reading",
    "title": "Visualizing One-Dimensional Data: Show Me the Data!",
    "section": "Further Reading",
    "text": "Further Reading\n\nDynamite Plots Must Die - Rafael Irizarry‚Äôs open letter to journal editors\nBeyond Bar and Line Graphs: Time for a New Data Presentation Paradigm - The paper that started the ‚Äúshow your data‚Äù movement\nWeissgerber et al.¬†(2015). Why we need to report more than ‚ÄòData were Analyzed by t-tests or ANOVA‚Äô\nShow the data, don‚Äôt conceal them - British Journal of Pharmacology (2011)\nVisualizing Samples with Box Plots\nKernel Density Estimation Explained - Interactive tutorial",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m01-toolkit/reproduceability.html",
    "href": "m01-toolkit/reproduceability.html",
    "title": "Reproducible Environments & Projects",
    "section": "",
    "text": "What you‚Äôll learn in this module\n\n\n\nThis module teaches you how to build truly reproducible projects. You‚Äôll understand the three pillars of reproducibility: virtual environments, workflow automation, and comprehensive documentation. You‚Äôll learn practical tools like uv and Snakemake, and discover how good project organization prevents errors and enables seamless collaboration.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Reproducibility"
    ]
  },
  {
    "objectID": "m01-toolkit/reproduceability.html#the-importance-of-reproducibility",
    "href": "m01-toolkit/reproduceability.html#the-importance-of-reproducibility",
    "title": "Reproducible Environments & Projects",
    "section": "The Importance of Reproducibility",
    "text": "The Importance of Reproducibility\nReproducibility is a cornerstone of good science and a fundamental principle in computational research. At its core, it means this: an independent researcher should be able to duplicate your results using the same materials and procedures you used. In data science, this means your code must produce the exact same outputs every single time, given the same input data, regardless of which machine runs it.\nNow, a quick distinction helps clarify what we‚Äôre really doing. Reproducibility asks: can an independent researcher achieve the exact same results using your data and code? This is a computational challenge. Replicability, on the other hand, asks: can an independent researcher confirm your scientific conclusions by conducting a brand new, independent study? This is a scientific challenge. We focus here on computational reproducibility, which is the essential first step. Without it, replication becomes impossible.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Reproducibility"
    ]
  },
  {
    "objectID": "m01-toolkit/reproduceability.html#pillars-of-a-reproducible-project",
    "href": "m01-toolkit/reproduceability.html#pillars-of-a-reproducible-project",
    "title": "Reproducible Environments & Projects",
    "section": "Pillars of a Reproducible Project",
    "text": "Pillars of a Reproducible Project\nAchieving reproducibility requires a conscious effort and the right set of tools. We can think of a reproducible project as having three main pillars: Virtual Environments, Workflow automation, and Comprehensive documentation.\n\nVirtual Environments\nA virtual environment is an isolated container holding all the packages and dependencies your project needs. Without one, you risk ‚Äúdependency hell‚Äù: installing a new library for one project breaks another project that needs a different version of the same library.\nPicture the problem: you have project A using pandas 1.3 and project B using pandas 2.0. Without virtual environments, your system has only one pandas version installed, and one project breaks. Virtual environments solve this by giving each project its own isolated package space.\nFor this course, we recommend uv. It‚Äôs a modern, blazingly fast Python package and environment manager written in Rust. Unlike older tools like pip and venv, uv combines both package installation and environment creation into one tool. This integrated approach and high performance can dramatically speed up your Python workflows.\nReady to get started? Check the uv documentation.\n\n\n\n\n\n\nNote\n\n\n\nWhy not conda? Conda is a mature, well-used tool in the Python community. Unlike uv, conda can manage non-Python dependencies like compilers or optimized BLAS libraries. This flexibility is powerful if you need system-level packages. However, this same flexibility creates complexity: conda environments can develop intricate dependencies that make them harder to replicate. uv focuses solely on Python packages, which actually simplifies reproducibility. Less flexibility sometimes means better reproducibility.\n\n\n\n\nWorkflow Management\nAs your project grows, the order of steps matters, and it gets complicated fast. One script preprocesses data, another trains a model, a third generates figures. If you run them manually in the wrong order, or forget which ones depend on which, errors creep in silently.\n\n\n\n\n\n\nFigure¬†1: This diagram shows a research project as a workflow, where each box is a script and connections show data flowing from one script to the next. As projects grow, these dependencies become impossible to track mentally.\n\n\n\nA workflow management tool automates this complexity. You define the steps and their dependencies, and the tool figures out the correct order, runs them all, and even parallelizes where possible.\nSnakemake is excellent for this. It uses a readable, Python-based language where you define rules: here‚Äôs how to create output files from input files. Snakemake handles the rest. Want to learn more? Watch this introductory video.\nOne principle matters deeply: make individual scripts atomic. One script preprocesses data. Another trains a model. A third generates figures. When scripts are minimal and focused, they become readable, reusable, and easy to debug.\n\n\nComprehensive Documentation\nCode and data are not self-explanatory. Comprehensive documentation is crucial for anyone reading your project (including your future self) to understand the what, why, and how.\nEvery project should have a README.md file in its root directory. This file explains what the project is about, what the files are, and how to run the analysis. A strong README includes a few key sections: start with a clear project description explaining what it does and its goals. Add a file structure summary so readers can navigate. If you include example datasets, describe the data table structure, including column names and formats. Provide installation instructions so people can set up the environment. Explain usage with examples of how to run the main scripts. Finally, add contact and license information so readers know who to ask and under what terms they can use your work.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Reproducibility"
    ]
  },
  {
    "objectID": "m01-toolkit/reproduceability.html#project-organization-best-practices",
    "href": "m01-toolkit/reproduceability.html#project-organization-best-practices",
    "title": "Reproducible Environments & Projects",
    "section": "Project Organization Best Practices",
    "text": "Project Organization Best Practices\nBeyond the core tools, organizing your project thoughtfully is crucial for long-term reproducibility and collaboration.\n\nFile and Directory Structure\nA logical file structure makes it easy for others and for future you to find things. Start with descriptive naming. Give files and directories clear, self-explanatory names so you can find them via search. Instead of analysis.py, use preprocess_customer_data.py.\nFor files that evolve over time (like datasets or reports), consider adding timestamps. Name a report 2025-10-20_survey_analysis.qmd instead of report.qmd. This lets you instantly identify which version is which.\nWant to dive deeper? Check out How to name files and A Simple File Management System.\n\n\nWriting Clean Code\nHere‚Äôs a simple truth: code quality directly impacts reproducibility. If your code is hard to read, it‚Äôs hard to verify. It‚Äôs hard to reuse. And it‚Äôs hard for someone else (or your future self) to understand why you made each choice.\nThe book Clean Code: A Handbook of Agile Software Craftsmanship by Robert C. Martin is an excellent guide to writing code that‚Äôs readable, maintainable, and trustworthy.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Reproducibility"
    ]
  },
  {
    "objectID": "m01-toolkit/git-github.html",
    "href": "m01-toolkit/git-github.html",
    "title": "Git & GitHub: Your Time Machine for Code",
    "section": "",
    "text": "What you‚Äôll learn in this module\n\n\n\nThis module introduces Git and GitHub, your tools for tracking code changes and collaborating. You‚Äôll learn why version control matters through real-world disasters, understand how Git saves snapshots of your work, and discover how to use GitHub to backup your code and work with others.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Version Control with Git & GitHub"
    ]
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Home",
    "section": "Course Overview",
    "text": "Course Overview\nThis course explores how deep learning serves as a powerful tool to operationalize high-dimensional data from social and physical systems. You‚Äôll learn to model complex system dynamics using modern computational intelligence, and in turn, use perspectives from complexity science to understand the emergent behaviors of large-scale models.\nThe course combines:\n\nHands-on coding with real data from text, images, and networks\nTheoretical foundations of deep learning and complex systems\nReproducible data science practices with modern tools\nEthical considerations in AI and computational modeling",
    "crumbs": [
      "Home",
      "Course Information",
      "Home"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Home",
    "section": "Getting Started",
    "text": "Getting Started\n\nRead the Welcome page\nLearn About Us\nJoin our Discord server\nFollow the Setup Guide\nLearn How to Submit Assignments",
    "crumbs": [
      "Home",
      "Course Information",
      "Home"
    ]
  },
  {
    "objectID": "course/minidora-usage.html#getting-started-with-minidora",
    "href": "course/minidora-usage.html#getting-started-with-minidora",
    "title": "Using Minidora",
    "section": "Getting Started with Minidora",
    "text": "Getting Started with Minidora\nMinidora is your personal AI tutor available 24/7 through Discord to help you master deep learning and complex systems concepts. She‚Äôs designed to provide personalized learning support, answer questions about course materials, and guide you through challenging topics with patience and clarity. To interact with Minidora, simply use Discord slash commands or mention her directly in any channel or thread where she‚Äôs present.\nCheck out the instruction here on how to use Minidora: Minidora Usage. Minidora is available on Discord, and you can find the invitation link on the email sent in the first week of the semester. Or you can find the invitation link on the Brightspace.\n\n\n\n\n\n\nNote\n\n\n\nSome students could not find Minidora on Discord. The easiest way to get around this is:\n\nGo to the course discord server\nOpen the ‚Äúapplied-soft-computing‚Äù channel.\nClick the Minidora icon and send a direct message\nType ‚Äú/‚Äù and see if the available commands are shown.",
    "crumbs": [
      "Home",
      "Course Information",
      "Using Minidora"
    ]
  },
  {
    "objectID": "course/minidora-usage.html#asking-questions",
    "href": "course/minidora-usage.html#asking-questions",
    "title": "Using Minidora",
    "section": "Asking Questions",
    "text": "Asking Questions\nThe most straightforward way to get help is using the /ask command followed by your question. For example, suppose that you want to ask about a subject (Word embeddings) in module 3.\n\nType /ask then type space.\nType your question (e.g., What are word embeddings and how does Word2Vec work?)\nType space\nYou will be prompted to specify the module id. The id consists of ‚Äúm‚Äù. For example, if it is module 3, you should type m03. Type the module id.\nThen type enter.\n\nMinidora will then read the lecture content and provide an explanation.",
    "crumbs": [
      "Home",
      "Course Information",
      "Using Minidora"
    ]
  },
  {
    "objectID": "course/minidora-usage.html#natural-conversations-and-interactive-learning",
    "href": "course/minidora-usage.html#natural-conversations-and-interactive-learning",
    "title": "Using Minidora",
    "section": "Natural Conversations and Interactive Learning",
    "text": "Natural Conversations and Interactive Learning\nFor a more conversational experience, use the /chat command which allows you to interact with Minidora in a natural, free-flowing manner. You can say things like /chat I'm confused about transformers, can you explain the attention mechanism step by step? or /chat Can you help me debug this Python code for training a CNN? Minidora will engage in back-and-forth dialogue, ask clarifying questions, and adapt her explanations based on your responses.\nNote that /chat does not contextualize the Minidora to the course materials. That means that it does not read the lecture content and interact with the students with its built-in knowledge.",
    "crumbs": [
      "Home",
      "Course Information",
      "Using Minidora"
    ]
  },
  {
    "objectID": "course/minidora-usage.html#quizzes-and-assessment",
    "href": "course/minidora-usage.html#quizzes-and-assessment",
    "title": "Using Minidora",
    "section": "Quizzes and Assessment",
    "text": "Quizzes and Assessment\nTo test your understanding and reinforce learning, Minidora offers intelligent quiz features through the /quiz command. She can generate concept-based questions using /concept-quiz m03 multiple-choice for theoretical understanding, or coding challenges with /code-quiz m03 to practice implementation skills. Minidora tracks your progress and adapts quiz difficulty based on your performance, focusing on areas where you need more practice. You can also request quizzes on specific topics by adding subject keywords, such as /quiz m04 convolutional neural networks.",
    "crumbs": [
      "Home",
      "Course Information",
      "Using Minidora"
    ]
  },
  {
    "objectID": "course/minidora-usage.html#tracking-your-progress",
    "href": "course/minidora-usage.html#tracking-your-progress",
    "title": "Using Minidora",
    "section": "Tracking Your Progress",
    "text": "Tracking Your Progress\nUse the /status command to monitor your learning journey and see detailed insights about your progress. Minidora provides different status views: /status summary gives you a quick overview of questions asked and concepts mastered, while /status concepts shows which topics you‚Äôve learned and what to study next. The /status profile command reveals your personalized learning profile, including your preferred difficulty level, learning style, and areas where you excel or need additional support. This helps Minidora provide increasingly personalized assistance as you continue learning.",
    "crumbs": [
      "Home",
      "Course Information",
      "Using Minidora"
    ]
  },
  {
    "objectID": "course/setup.html",
    "href": "course/setup.html",
    "title": "Setup",
    "section": "",
    "text": "We‚Äôll use Python to work with data throughout this course. Python is an excellent choice for deep learning and complex systems analysis for its rich ecosystem of libraries, readable and intuitive syntax, and well-documented documentation.",
    "crumbs": [
      "Home",
      "Course Information",
      "Setup"
    ]
  },
  {
    "objectID": "course/setup.html#programming-language",
    "href": "course/setup.html#programming-language",
    "title": "Setup",
    "section": "",
    "text": "We‚Äôll use Python to work with data throughout this course. Python is an excellent choice for deep learning and complex systems analysis for its rich ecosystem of libraries, readable and intuitive syntax, and well-documented documentation.",
    "crumbs": [
      "Home",
      "Course Information",
      "Setup"
    ]
  },
  {
    "objectID": "course/setup.html#create-a-virtual-environment-via-uv",
    "href": "course/setup.html#create-a-virtual-environment-via-uv",
    "title": "Setup",
    "section": "Create a Virtual Environment via uv",
    "text": "Create a Virtual Environment via uv\nWe strongly recommend using virtual environments to manage your Python packages. Virtual environments create isolated Python installations for each project, avoiding dependency hell and providing several key benefits:\n\n\n\n\n\n\nNote\n\n\n\nDon‚Äôt confuse Python virtual environments with virtual machines (VMs). Python virtual environments are lightweight isolation tools that only separate Python packages and dependencies within the same operating system. Virtual machines, on the other hand, create complete isolated operating systems.\n\n\n\n\n\n\n\n\nFigure¬†1: Without virtual environments, you risk dependency hell where package conflicts make your projects unusable.\n\n\n\nWe recommend using uv. uv is a fast Python package and project manager. While we won‚Äôt be running uv commands directly in this course, you‚Äôll need uv to properly run Marimo notebooks, which provides a much better development experience. See here for installation instructions.\nFollow the following steps to install uv, along with the minimum Python packages required for this course.\n\nInstall uv\nRun the following command to create a new environment with the minimum Python packages required for this course.\n\nuv venv -p 3.11\nuv pip install matplotlib scipy numpy pandas seaborn pytorch torchvision marimo\n\nActivate the environment.\n\nsource .venv/bin/activate",
    "crumbs": [
      "Home",
      "Course Information",
      "Setup"
    ]
  },
  {
    "objectID": "course/setup.html#jupyter-marimo-notebook",
    "href": "course/setup.html#jupyter-marimo-notebook",
    "title": "Setup",
    "section": "Jupyter Marimo Notebook",
    "text": "Jupyter Marimo Notebook\nWe‚Äôll use Marimo (GitHub) notebooks for assignments and interactive exercises throughout the course. Marimo is a reactive Python notebook that automatically updates when you change code, making it perfect for exploring deep learning models and seeing results in real-time.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\nMarimo integrates especially tightly with uv and provides a package sandbox feature that lets you inline dependencies directly in notebook files. This is the easiest way to get started - no prior uv knowledge required.\nCreating a sandboxed notebook:\nuvx run --sandbox my_notebook.py\nThis command installs marimo in a temporary environment, tracks your dependencies and stores them in the notebook file, and automatically downloads any existing dependencies.",
    "crumbs": [
      "Home",
      "Course Information",
      "Setup"
    ]
  },
  {
    "objectID": "m01-toolkit/data-provenance.html",
    "href": "m01-toolkit/data-provenance.html",
    "title": "Data Provenance: The Secret Life of Data",
    "section": "",
    "text": "What you‚Äôll learn in this module\n\n\n\nThis module introduces data provenance, the complete record of where your data came from and what happened to it. You‚Äôll learn why provenance matters through a real-world disaster, explore practical tools for tracking data transformations, and discover how good documentation practices prevent errors and enable reproducible science.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Data Provenance"
    ]
  },
  {
    "objectID": "m01-toolkit/data-provenance.html#the-mystery-of-the-disappearing-data",
    "href": "m01-toolkit/data-provenance.html#the-mystery-of-the-disappearing-data",
    "title": "Data Provenance: The Secret Life of Data",
    "section": "The Mystery of the Disappearing Data",
    "text": "The Mystery of the Disappearing Data\nHave you ever opened a dataset you worked on a few months ago, only to find that you have no idea where it came from? You can‚Äôt remember what the columns mean or what transformations you applied. This experience is common in data science, and it highlights the critical importance of data provenance.\nThink of data provenance as the complete story of your data. It answers the who, what, when, where, and why of your data‚Äôs journey from raw form to its current state. Understanding this story is crucial for doing good science and catching errors before they spread.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Data Provenance"
    ]
  },
  {
    "objectID": "m01-toolkit/data-provenance.html#a-tale-of-two-economists-and-a-spreadsheet-error",
    "href": "m01-toolkit/data-provenance.html#a-tale-of-two-economists-and-a-spreadsheet-error",
    "title": "Data Provenance: The Secret Life of Data",
    "section": "A Tale of Two Economists and a Spreadsheet Error",
    "text": "A Tale of Two Economists and a Spreadsheet Error\n\nPicture this: in 2010, two Harvard economists named Carmen Reinhart and Kenneth Rogoff published a paper called ‚ÄúGrowth in a Time of Debt.‚Äù Their main argument was simple and powerful: countries with high government debt have lower economic growth. The paper became incredibly influential and shaped policy decisions around the world, justifying austerity measures in multiple countries.\nThen in 2013, a graduate student named Thomas Herndon decided to reproduce their results. He couldn‚Äôt. After persistent effort, he obtained their original spreadsheet and discovered a simple but catastrophic error: they had accidentally excluded the first five countries from their analysis. When Herndon corrected that single mistake, the paper‚Äôs main finding vanished.\nThis story reveals the stakes of data provenance. Without a clear, documented record of how data was processed, errors slip through unnoticed and spread far. The consequences can reshape entire economies.\nWant the full story? Read The Reinhart-Rogoff Error.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Data Provenance"
    ]
  },
  {
    "objectID": "m01-toolkit/data-provenance.html#the-data-detectives-toolkit",
    "href": "m01-toolkit/data-provenance.html#the-data-detectives-toolkit",
    "title": "Data Provenance: The Secret Life of Data",
    "section": "The Data Detective‚Äôs Toolkit",
    "text": "The Data Detective‚Äôs Toolkit\nNow let‚Äôs talk about how you actually track provenance. There are three essential approaches.\nFirst, keep a lab notebook. This can be physical or digital, but it should record where your data came from, what you did to it, and why you made each decision. A good lab notebook becomes the narrative companion to your code.\nSecond, embrace scripting. When you process data with code (Python, R, or anything else), your scripts become documentation. They show exactly what transformations happened. And if you version control those scripts, you have a complete history of changes.\nThird, for complex projects, consider workflow management tools like Snakemake or Nextflow. These tools let you define and track your entire analysis pipeline, automatically recording which data went into which step and what came out.\nWhen you embrace these provenance practices, you become a more trustworthy data scientist. You‚Äôll trust your own results because you know exactly how they were created. Others will trust them too.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Data Provenance"
    ]
  },
  {
    "objectID": "m01-toolkit/data-provenance.html#further-reading",
    "href": "m01-toolkit/data-provenance.html#further-reading",
    "title": "Data Provenance: The Secret Life of Data",
    "section": "Further Reading",
    "text": "Further Reading\nReady to go deeper? Check out What is Data Provenance and Why is it Important? and Data Provenance: What It Is, Why It Matters, and How to Implement It for more perspectives.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Data Provenance"
    ]
  },
  {
    "objectID": "m01-toolkit/overview.html",
    "href": "m01-toolkit/overview.html",
    "title": "Overview",
    "section": "",
    "text": "What you‚Äôll learn in this module\n\n\n\nThis module introduces the tools and principles of reproducible data science. We will explore version control with Git and GitHub to track changes and collaborate effectively. We will examine data provenance and tidy data principles to ensure your data history is clear and your structure is sound. Finally, we will learn how to build reproducible environments so others can replicate your work exactly.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Overview"
    ]
  },
  {
    "objectID": "m01-toolkit/tidy-data.html",
    "href": "m01-toolkit/tidy-data.html",
    "title": "The Tidy Data Philosophy",
    "section": "",
    "text": "What you‚Äôll learn in this module\n\n\n\nThis module introduces the tidy data philosophy. You‚Äôll learn what makes data ‚Äútidy‚Äù and what pitfalls to avoid, explore practical tools like melt and pivot to reshape your data, and understand why standardizing data structure makes analysis faster and more reliable.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "The Tidy Data Philosophy"
    ]
  },
  {
    "objectID": "m01-toolkit/tidy-data.html#introduction",
    "href": "m01-toolkit/tidy-data.html#introduction",
    "title": "The Tidy Data Philosophy",
    "section": "",
    "text": "It is often said that 80% of data analysis is spent on the process of cleaning and preparing the data. Read the following paper to learn more about the tidy data philosophy.\n\nTidy Data by Hadley Wickham‚Äú.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "The Tidy Data Philosophy"
    ]
  },
  {
    "objectID": "m01-toolkit/tidy-data.html#what-is-tidy-data",
    "href": "m01-toolkit/tidy-data.html#what-is-tidy-data",
    "title": "The Tidy Data Philosophy",
    "section": "What is Tidy Data?",
    "text": "What is Tidy Data?\nAt its core, tidy data is a standard way of mapping the meaning of a dataset to its structure. Whether your data is messy or tidy depends entirely on how rows, columns, and tables match up with observations, variables, and types.\nLet‚Äôs talk about the three core principles. First, each variable forms its own column. A variable measures the same underlying attribute (like height, temperature, or duration) across different units. Second, each observation forms a row. An observation captures all measurements on the same unit (like a person, a day, or a race) across different attributes. Third, each type of observational unit gets its own table. In a study of allergy medication, you‚Äôd have separate tables for demographic data, daily medical data, and meteorological data, not one giant table mixing everything together.\nWhy does this matter? Tidy datasets are dramatically easier to manipulate, model, and visualize. They make exploration faster and analysis clearer. Most importantly, they standardize data organization, making your code reusable and reliable.\n\nCommon Pitfalls\nNow let‚Äôs flip the perspective and look at the most common mistakes. When you first encounter messy data, it usually falls into one of five patterns.\nThe first problem is that column headers often contain values instead of variable names. Imagine a table where months (‚ÄúJan‚Äù, ‚ÄúFeb‚Äù, ‚ÄúMar‚Äù) are the column headers, rather than having a single ‚ÄúMonth‚Äù column with those values. This makes it hard to analyze across months.\nThe second problem is multiple variables stored in one column. You might find a column like ‚Äúheight_weight‚Äù containing values like ‚Äú5.5_130‚Äù instead of splitting those into separate ‚Äúheight‚Äù and ‚Äúweight‚Äù columns. This breaks apart information that belongs together.\nThe third problem is variables scattered across both rows and columns. A piece of information like gender might be encoded in a specific column and also hidden within the values of another column, creating redundancy and confusion during analysis.\nThe fourth problem is mixing different types of observational units in one table. For example, a single table containing both patient demographic information and medical test results mashes two fundamentally different kinds of data together.\nThe fifth and final problem is splitting a single observational unit across multiple tables. Patient information scattered across one table for addresses, another for test results, and another for appointments, with no clean way to link them together, makes every analysis painful.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "The Tidy Data Philosophy"
    ]
  },
  {
    "objectID": "m01-toolkit/tidy-data.html#tidy-tools",
    "href": "m01-toolkit/tidy-data.html#tidy-tools",
    "title": "The Tidy Data Philosophy",
    "section": "Tidy Tools",
    "text": "Tidy Tools\nNow let‚Äôs learn practical tools to reshape your messy data into tidy form. The examples here are adapted from Python for Data Science.\n\nMelt: From Wide to Long\nPicture data stored in ‚Äúwide‚Äù format, where different columns represent different variables of the same type. Consider this simple example:\n\n\nCode\nimport pandas as pd\ndf = pd.DataFrame({'first': ['John', 'Mary'],\n                   'last': ['Smith', 'Doe'],\n                   'height': [5.5, 5.0],\n                   'weight': [130, 110]})\ndf\n\n\n\n\n\n\n\n\n\nfirst\nlast\nheight\nweight\n\n\n\n\n0\nJohn\nSmith\n5.5\n130\n\n\n1\nMary\nDoe\n5.0\n110\n\n\n\n\n\n\n\nNotice how ‚Äúheight‚Äù and ‚Äúweight‚Äù sit in separate columns. This wide format breaks tidy principles and makes comparisons awkward when you want to plot or analyze these measurements together.\nThe pandas.DataFrame.melt() method solves this by transforming data from wide to long format. After melting, instead of separate columns for ‚Äúheight‚Äù and ‚Äúweight‚Äù, you get one column for the variable type and another for the value. Let‚Äôs see it in action:\n\nimport pandas as pd\ndf_melted = df.melt(\n    id_vars=['first', 'last'],\n    var_name='quantity',\n    value_name='value'\n)\ndf_melted\n\n\n\n\n\n\n\n\nfirst\nlast\nquantity\nvalue\n\n\n\n\n0\nJohn\nSmith\nheight\n5.5\n\n\n1\nMary\nDoe\nheight\n5.0\n\n\n2\nJohn\nSmith\nweight\n130.0\n\n\n3\nMary\nDoe\nweight\n110.0\n\n\n\n\n\n\n\nNow each row represents a single measurement for an individual. If you want to compare height and weight, they‚Äôre in the same column format, making analysis natural. This is the essence of tidy data.\n\n\nPivot: From Long to Wide\nSometimes the opposite problem arises. Your data starts in ‚Äúlong‚Äù format, with a separate row for each measurement type (like ‚Äúcases‚Äù or ‚Äúpopulation‚Äù) for each country and year. This scatters information about a single observation across multiple rows, making it hard to see all statistics for country A in 2020 at once.\n\n\nCode\nimport numpy as np\n\n# Long format: each row is a different variable for country and year\ndf = pd.DataFrame({\n    'country': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],\n    'year': [2020, 2021, 2020, 2021, 2020, 2021, 2020, 2021],\n    'variable': ['cases', 'cases', 'population', 'population', 'cases', 'cases', 'population', 'population'],\n    'value': [100, 200, 120, 220, 130, 230, 140, 240]\n})\ndf\n\n\n\n\n\n\n\n\n\ncountry\nyear\nvariable\nvalue\n\n\n\n\n0\nA\n2020\ncases\n100\n\n\n1\nA\n2021\ncases\n200\n\n\n2\nA\n2020\npopulation\n120\n\n\n3\nA\n2021\npopulation\n220\n\n\n4\nB\n2020\ncases\n130\n\n\n5\nB\n2021\ncases\n230\n\n\n6\nB\n2020\npopulation\n140\n\n\n7\nB\n2021\npopulation\n240\n\n\n\n\n\n\n\nThe pivot() function reshapes the data so that each observation (a country-year combination) has its measurements spread across columns. This transforms long data back to a wider, more readable format. Now each row shows all measurements for a single country and year, making analysis straightforward.\n\n\nStack and Unstack\nA trickier situation emerges when your data has multi-level column headers, with variables split across two or more header rows. Imagine measurements for different people and types, with test results for multiple groups all shown as columns. This nested structure makes it hard to access and visualize data cleanly.\n\n\nCode\n# Example: multi-level columns for two participants (P1, P2) and two attributes (A, B)\nheader = pd.MultiIndex.from_product([['P1','P2'],['A','B']])\ndf = pd.DataFrame(np.random.rand(4, 4),\n                  columns=header)\ndf\n\n\n\n\n\n\n\n\n\nP1\nP2\n\n\n\nA\nB\nA\nB\n\n\n\n\n0\n0.834378\n0.846789\n0.815770\n0.866170\n\n\n1\n0.212554\n0.130223\n0.411592\n0.883559\n\n\n2\n0.163426\n0.127180\n0.483202\n0.347016\n\n\n3\n0.596388\n0.458607\n0.698158\n0.199185\n\n\n\n\n\n\n\nThe stack() method solves this by converting one level of column headers into a row index, transforming wide data to long. Now each row represents a single measurement, and all values of the same variable sit together in one column. The unstack() method does the reverse, spreading data back out from the index into columns.\nLet‚Äôs see both methods in action:\n\ndf.stack(future_stack=True)\n\n\n\n\n\n\n\n\n\nP1\nP2\n\n\n\n\n0\nA\n0.834378\n0.815770\n\n\nB\n0.846789\n0.866170\n\n\n1\nA\n0.212554\n0.411592\n\n\nB\n0.130223\n0.883559\n\n\n2\nA\n0.163426\n0.483202\n\n\nB\n0.127180\n0.347016\n\n\n3\nA\n0.596388\n0.698158\n\n\nB\n0.458607\n0.199185\n\n\n\n\n\n\n\n\ndf.stack(future_stack=True).unstack()\n\n\n\n\n\n\n\n\nP1\nP2\n\n\n\nA\nB\nA\nB\n\n\n\n\n0\n0.834378\n0.846789\n0.815770\n0.866170\n\n\n1\n0.212554\n0.130223\n0.411592\n0.883559\n\n\n2\n0.163426\n0.127180\n0.483202\n0.347016\n\n\n3\n0.596388\n0.458607\n0.698158\n0.199185",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "The Tidy Data Philosophy"
    ]
  },
  {
    "objectID": "m02-visualization/2d-data.html",
    "href": "m02-visualization/2d-data.html",
    "title": "2D Data Visualization",
    "section": "",
    "text": "What you‚Äôll learn in this module\n\n\n\nThis module teaches how to visualize relationships between two variables. You‚Äôll learn how scatter plots, binning methods, and density estimation reveal patterns that summary statistics hide. We‚Äôll explore how to handle overplotting in large datasets and how to compare relationships across different groups.\nYou‚Äôve probably heard that ‚Äúcorrelation doesn‚Äôt equal causation.‚Äù But here‚Äôs an even more fundamental problem: a correlation coefficient doesn‚Äôt tell you what your data actually looks like.\nIn 1973, statistician Francis Anscombe created four datasets that became legendary in data visualization. Each dataset has 11 (x, y) pairs. Each has the same mean for x and y, the same variance, and most remarkably, the same correlation coefficient (r = 0.816) and the same linear regression line.\nBut when you plot them, they tell completely different stories.\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Load Anscombe's quartet\nanscombe = sns.load_dataset(\"anscombe\")\n\n# Create the plot\nsns.set_style(\"white\")\ng = sns.FacetGrid(anscombe, col=\"dataset\", col_wrap=2, height=4, aspect=1.2)\ng.map_dataframe(sns.scatterplot, x=\"x\", y=\"y\", s=100)\ng.map_dataframe(sns.regplot, x=\"x\", y=\"y\", scatter=False, color=\"red\")\ng.set_axis_labels(\"X\", \"Y\")\ng.set_titles(\"Dataset {col_name}\")\n\n# Add correlation to each subplot\nfor ax, dataset in zip(g.axes.flat, [\"I\", \"II\", \"III\", \"IV\"]):\n    data_subset = anscombe[anscombe[\"dataset\"] == dataset]\n    r = np.corrcoef(data_subset[\"x\"], data_subset[\"y\"])[0, 1]\n    ax.text(0.05, 0.95, f'r = {r:.3f}', transform=ax.transAxes,\n            verticalalignment='top', fontsize=12, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\nsns.despine()\nplt.tight_layout()\n\n\n\n\n\nAnscombe‚Äôs Quartet: Four datasets with identical summary statistics but completely different relationships\nDataset I shows a nice linear relationship. Dataset II is clearly non-linear, a parabola that a linear model completely misses. Dataset III has a perfect linear relationship except for one outlier that changes everything. Dataset IV shows no relationship except for a single influential point that creates the illusion of correlation.\nThe same correlation coefficient. The same regression line. Completely different data.\nThis is why we visualize relationships. Always plot your bivariate data. Summary statistics conceal structure.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 2D Data"
    ]
  },
  {
    "objectID": "m02-visualization/2d-data.html#d-histograms-heatmaps",
    "href": "m02-visualization/2d-data.html#d-histograms-heatmaps",
    "title": "2D Data Visualization",
    "section": "2D Histograms (Heatmaps)",
    "text": "2D Histograms (Heatmaps)\nA 2D histogram extends the 1D histogram concept to two dimensions. The plane is divided into rectangular bins, and each bin‚Äôs color represents the number of points it contains.\n\n\nCode\n# Generate large dataset\nnp.random.seed(789)\nn_large = 10000\nx_large = np.random.normal(50, 15, n_large)\ny_large = 0.8 * x_large + np.random.normal(0, 12, n_large)\n\nfig, ax = plt.subplots(figsize=(10, 7))\nhb = ax.hexbin(x_large, y_large, gridsize=30, cmap='YlOrRd', mincnt=1)\nax.set_xlabel('X Variable')\nax.set_ylabel('Y Variable')\nax.set_title('2D Histogram: Density Through Binning')\ncb = plt.colorbar(hb, ax=ax)\ncb.set_label('Count')\nsns.despine()\n\n\n\n\n\n2D histogram showing density through rectangular bins\n\n\n\n\nThe key parameter is bin size (or gridsize). Too few bins and you lose detail; too many bins and the plot becomes noisy. Like 1D histograms, this requires experimentation.\n\n\nCode\nfig, axes = plt.subplots(1, 3, figsize=(14, 5))\ngridsizes = [10, 30, 60]\n\nfor ax, gridsize in zip(axes, gridsizes):\n    hb = ax.hexbin(x_large, y_large, gridsize=gridsize, cmap='YlOrRd', mincnt=1)\n    ax.set_xlabel('X Variable')\n    ax.set_ylabel('Y Variable')\n    ax.set_title(f'Gridsize = {gridsize}')\n    plt.colorbar(hb, ax=ax)\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n\n\n\n\n\nEffect of bin size on 2D histograms\n\n\n\n\nWith gridsize = 10, we see only coarse structure. With gridsize = 60, the plot is noisy. Some bins have few points by chance. Gridsize = 30 provides a good balance between detail and stability.\n\nHexbin Plots\nHexagonal binning uses hexagons instead of rectangles. Hexagons are better for 2D binning because they‚Äôre closer to circles. Every edge is equidistant from the center, reducing bias in how we perceive density.\n\n\nCode\n# Generate data with interesting structure\nnp.random.seed(101)\nn = 8000\n\n# Create two clusters\ncluster1_x = np.random.normal(30, 8, n // 2)\ncluster1_y = np.random.normal(40, 8, n // 2)\ncluster2_x = np.random.normal(60, 10, n // 2)\ncluster2_y = np.random.normal(70, 10, n // 2)\n\nx_clusters = np.concatenate([cluster1_x, cluster2_x])\ny_clusters = np.concatenate([cluster1_y, cluster2_y])\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Scatter plot (for reference)\naxes[0].scatter(x_clusters, y_clusters, alpha=0.1, s=10)\naxes[0].set_xlabel('X Variable')\naxes[0].set_ylabel('Y Variable')\naxes[0].set_title('Scatter Plot (alpha = 0.1)')\nsns.despine(ax=axes[0])\n\n# Hexbin plot\nhb = axes[1].hexbin(x_clusters, y_clusters, gridsize=25, cmap='viridis', mincnt=1)\naxes[1].set_xlabel('X Variable')\naxes[1].set_ylabel('Y Variable')\naxes[1].set_title('Hexbin Plot')\nplt.colorbar(hb, ax=axes[1], label='Count')\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n\n\n\n\n\nHexbin plot provides more perceptually uniform density representation\n\n\n\n\nThe hexbin plot clearly reveals the two clusters and their relative densities. Something that‚Äôs harder to see in the scatter plot even with low alpha. Hexbin plots are particularly powerful for very large datasets (100,000+ points) where scatter plots become computationally expensive and visually overwhelming.\n\n\n\n\n\n\nChoosing colors for density plots\n\n\n\nWhen showing density or counts, use sequential colormaps that vary in lightness: light equals low density, dark equals high density. Good choices include 'YlOrRd' (yellow-orange-red), 'viridis' (purple-blue-green-yellow, perceptually uniform), and 'Blues' or 'Reds' (single hue).\nAvoid rainbow colormaps like 'jet'. They create artificial boundaries where none exist and are not perceptually uniform.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 2D Data"
    ]
  },
  {
    "objectID": "m02-visualization/2d-data.html#hexbin-plots",
    "href": "m02-visualization/2d-data.html#hexbin-plots",
    "title": "2D Data Visualization",
    "section": "Hexbin Plots",
    "text": "Hexbin Plots\nHexagonal binning uses hexagons instead of rectangles. Hexagons are better for 2D binning because they‚Äôre closer to circles\u0014every edge is equidistant from the center, reducing bias in how we perceive density.\n\n\nCode\n# Generate data with interesting structure\nnp.random.seed(101)\nn = 8000\n\n# Create two clusters\ncluster1_x = np.random.normal(30, 8, n // 2)\ncluster1_y = np.random.normal(40, 8, n // 2)\ncluster2_x = np.random.normal(60, 10, n // 2)\ncluster2_y = np.random.normal(70, 10, n // 2)\n\nx_clusters = np.concatenate([cluster1_x, cluster2_x])\ny_clusters = np.concatenate([cluster1_y, cluster2_y])\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Scatter plot (for reference)\naxes[0].scatter(x_clusters, y_clusters, alpha=0.1, s=10)\naxes[0].set_xlabel('X Variable')\naxes[0].set_ylabel('Y Variable')\naxes[0].set_title('Scatter Plot (alpha = 0.1)')\nsns.despine(ax=axes[0])\n\n# Hexbin plot\nhb = axes[1].hexbin(x_clusters, y_clusters, gridsize=25, cmap='viridis', mincnt=1)\naxes[1].set_xlabel('X Variable')\naxes[1].set_ylabel('Y Variable')\naxes[1].set_title('Hexbin Plot')\nplt.colorbar(hb, ax=axes[1], label='Count')\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n\n\n\n\n\nHexbin plot provides more perceptually uniform density representation\n\n\n\n\nThe hexbin plot clearly reveals the two clusters and their relative densities\u0014something that‚Äôs harder to see in the scatter plot even with low alpha.\nHexbin plots are particularly powerful for very large datasets (100,000+ points) where scatter plots become computationally expensive and visually overwhelming.\n\n\n\n\n\n\nChoosing colors for density plots\n\n\n\nWhen showing density or counts, use sequential colormaps that vary in lightness: light = low density, dark = high density. Good choices include: - 'YlOrRd' (yellow-orange-red) - 'viridis' (purple-blue-green-yellow, perceptually uniform) - 'Blues' or 'Reds' (single hue)\nAvoid rainbow colormaps like 'jet'\u0014they create artificial boundaries where none exist and are not perceptually uniform.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 2D Data"
    ]
  },
  {
    "objectID": "m02-visualization/2d-data.html#contour-plots",
    "href": "m02-visualization/2d-data.html#contour-plots",
    "title": "2D Data Visualization",
    "section": "Contour Plots",
    "text": "Contour Plots\nA contour plot represents the density surface as lines of equal density\u0014like a topographic map where each contour line represents an ‚Äúelevation‚Äù of density.\n\n\nCode\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Filled contours\nsns.kdeplot(x=x_clusters, y=y_clusters, cmap='viridis', fill=True,\n            thresh=0.05, levels=10, ax=axes[0])\naxes[0].set_xlabel('X Variable')\naxes[0].set_ylabel('Y Variable')\naxes[0].set_title('Filled Contour Plot')\nsns.despine(ax=axes[0])\n\n# Line contours with scatter\naxes[1].scatter(x_clusters, y_clusters, alpha=0.1, s=5, c='gray')\nsns.kdeplot(x=x_clusters, y=y_clusters, levels=8, color='red', linewidths=2, ax=axes[1])\naxes[1].set_xlabel('X Variable')\naxes[1].set_ylabel('Y Variable')\naxes[1].set_title('Contour Lines Over Scatter Plot')\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n\n\n\n\n\nContour plot shows density as topographic lines\n\n\n\n\nContour plots are excellent for: - Overlaying density information on scatter plots - Comparing multiple groups (different colored contours) - Showing the ‚Äúshape‚Äù of the relationship clearly",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 2D Data"
    ]
  },
  {
    "objectID": "m02-visualization/2d-data.html#color-coding-by-group",
    "href": "m02-visualization/2d-data.html#color-coding-by-group",
    "title": "2D Data Visualization",
    "section": "Color coding by group",
    "text": "Color coding by group\nThe simplest approach is to use different colors for different groups:\n\n\nCode\n# Generate multi-group data\nnp.random.seed(303)\nn_per_group = 150\n\ngroup_a_x = np.random.normal(40, 12, n_per_group)\ngroup_a_y = 0.7 * group_a_x + np.random.normal(0, 8, n_per_group)\n\ngroup_b_x = np.random.normal(55, 10, n_per_group)\ngroup_b_y = 1.2 * group_b_x + np.random.normal(-20, 10, n_per_group)\n\ngroup_c_x = np.random.normal(60, 15, n_per_group)\ngroup_c_y = 0.3 * group_c_x + np.random.normal(30, 12, n_per_group)\n\ndf_groups = pd.DataFrame({\n    'x': np.concatenate([group_a_x, group_b_x, group_c_x]),\n    'y': np.concatenate([group_a_y, group_b_y, group_c_y]),\n    'group': ['A'] * n_per_group + ['B'] * n_per_group + ['C'] * n_per_group\n})\n\nfig, ax = plt.subplots(figsize=(10, 6))\nfor group, color in zip(['A', 'B', 'C'], sns.color_palette('muted', 3)):\n    subset = df_groups[df_groups['group'] == group]\n    ax.scatter(subset['x'], subset['y'], label=f'Group {group}',\n               alpha=0.6, s=50, color=color, edgecolors='white', linewidth=0.5)\n\nax.set_xlabel('X Variable')\nax.set_ylabel('Y Variable')\nax.set_title('Relationships Across Groups')\nax.legend()\nsns.despine()\n\n\n\n\n\nScatter plot with color-coded groups\n\n\n\n\nThis reveals that the three groups have different relationships: Group A has a positive moderate slope, Group B has a steeper positive relationship, and Group C has almost no relationship.\n\n\n\n\n\n\nSimpson‚Äôs Paradox\n\n\n\nBe careful! Sometimes the overall trend (pooling all groups) can be opposite to the trend within each group. This is called Simpson‚Äôs Paradox. Always visualize groups separately to check if pooling is appropriate.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 2D Data"
    ]
  },
  {
    "objectID": "m02-visualization/2d-data.html#small-multiples-faceting",
    "href": "m02-visualization/2d-data.html#small-multiples-faceting",
    "title": "2D Data Visualization",
    "section": "Small multiples (faceting)",
    "text": "Small multiples (faceting)\nWhen groups overlap heavily or there are many groups, small multiples\u0014separate plots for each group\u0014work better than color coding:\n\n\nCode\ng = sns.FacetGrid(df_groups, col='group', height=4, aspect=1.3)\ng.map_dataframe(sns.scatterplot, x='x', y='y', alpha=0.6, s=50)\ng.map_dataframe(sns.regplot, x='x', y='y', scatter=False, color='red')\ng.set_axis_labels('X Variable', 'Y Variable')\ng.set_titles('Group {col_name}')\nsns.despine()\nplt.tight_layout()\n\n\n\n\n\nSmall multiples showing relationship for each group separately\n\n\n\n\nSmall multiples make it easy to compare the strength and direction of relationships across groups without visual clutter.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 2D Data"
    ]
  },
  {
    "objectID": "m02-visualization/2d-data.html#contour-overlays",
    "href": "m02-visualization/2d-data.html#contour-overlays",
    "title": "2D Data Visualization",
    "section": "Contour overlays",
    "text": "Contour overlays\nFor large datasets, overlaying density contours for each group can be very effective:\n\n\nCode\nfig, ax = plt.subplots(figsize=(10, 7))\n\ncolors = sns.color_palette('muted', 3)\nfor group, color in zip(['A', 'B', 'C'], colors):\n    subset = df_groups[df_groups['group'] == group]\n    sns.kdeplot(x=subset['x'], y=subset['y'], levels=5,\n                color=color, linewidths=2, label=f'Group {group}', ax=ax)\n\nax.set_xlabel('X Variable')\nax.set_ylabel('Y Variable')\nax.set_title('Density Contours by Group')\nax.legend()\nsns.despine()\n\n\n/var/folders/j7/9dgqq5g53vnbsbmvh2yqtckr0000gr/T/ipykernel_89556/3335885635.py:12: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n  ax.legend()\n\n\n\n\n\nOverlaid density contours reveal different relationship shapes\n\n\n\n\nThis clearly shows that Groups A and B have elongated, correlated distributions (indicating strong relationships), while Group C is more circular (indicating weak correlation).",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 2D Data"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html",
    "href": "m02-visualization/highd-data.html",
    "title": "High-Dimensional Data Visualization",
    "section": "",
    "text": "What you‚Äôll learn in this module\n\n\n\nThis module introduces dimensionality reduction, a fundamental technique for visualizing and understanding high-dimensional data. We will explore the curse of dimensionality that makes high-dimensional space counterintuitive, examine linear methods like PCA that preserve global structure, investigate non-linear methods like t-SNE and UMAP that reveal local patterns, and understand the trade-offs between different approaches to choosing the right visualization strategy for your data.\nImagine you‚Äôre analyzing data with 50 features per observation. Gene expression levels, user behavior metrics, environmental measurements. You want to understand the patterns in your data. How do different observations relate to each other? Are there clusters? Outliers?\nHere‚Äôs the fundamental problem: you can‚Äôt plot 50 dimensions directly. Our visual system lives in three dimensions, or really two dimensions on a screen. This creates a core challenge: how do you visualize data that lives in spaces you cannot see?\nThe answer is dimensionality reduction. This technique projects high-dimensional data into 2 or 3 dimensions while preserving important structure. But here‚Äôs the critical question: what structure matters?\nDifferent methods preserve different aspects of your data. Some preserve global structure, showing how groups relate to each other across the entire dataset. Others preserve local structure, highlighting which points are nearest neighbors. Understanding these trade-offs is essential for choosing the right method and avoiding beautiful but misleading visualizations.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html#applying-pca-to-iris",
    "href": "m02-visualization/highd-data.html#applying-pca-to-iris",
    "title": "High-Dimensional Data Visualization",
    "section": "Applying PCA to Iris",
    "text": "Applying PCA to Iris\nLet‚Äôs apply PCA to the 4-dimensional Iris dataset:\n\n\nCode\n# Prepare data\nX = iris.data\ny = iris.target\n\n# Standardize (important for PCA!)\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Apply PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# Create DataFrame for plotting\npca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\npca_df['species'] = iris.target_names[y]\n\n# Plot\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Left: PCA projection\nfor species, color in zip(['setosa', 'versicolor', 'virginica'], sns.color_palette('muted', 3)):\n    mask = pca_df['species'] == species\n    axes[0].scatter(pca_df.loc[mask, 'PC1'], pca_df.loc[mask, 'PC2'],\n                   label=species, alpha=0.7, s=50, color=color, edgecolors='white', linewidth=0.5)\naxes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)')\naxes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)')\naxes[0].set_title('PCA Projection of Iris Dataset')\naxes[0].legend()\nsns.despine(ax=axes[0])\n\n# Right: Variance explained\nvariances = pca.explained_variance_ratio_\naxes[1].bar([1, 2], variances, color=sns.color_palette('muted', 2), alpha=0.7)\naxes[1].set_xlabel('Principal Component')\naxes[1].set_ylabel('Variance Explained')\naxes[1].set_title('Variance Explained by Each Component')\naxes[1].set_xticks([1, 2])\naxes[1].set_xticklabels(['PC1', 'PC2'])\nfor i, v in enumerate(variances):\n    axes[1].text(i+1, v+0.01, f'{v*100:.1f}%', ha='center', va='bottom', fontsize=11)\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n\n\n\n\n\nPCA projection of Iris dataset to 2D preserves the separation between species\n\n\n\n\nPC1 and PC2 together explain over 95% of the variance in the 4D dataset. The 2D projection preserves the main structure: setosa is well-separated, while versicolor and virginica have some overlap.\n\n\nAlways standardize before PCA! If features have different units or scales, PCA will be dominated by high-variance features. Standardization (zero mean, unit variance) ensures all features contribute fairly.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html#how-t-sne-works",
    "href": "m02-visualization/highd-data.html#how-t-sne-works",
    "title": "High-Dimensional Data Visualization",
    "section": "How t-SNE works",
    "text": "How t-SNE works\nt-SNE converts distances into similarity probabilities and preserves these local relationships:\n\nIn high dimensions: Define probability p_{ij} that point i picks point j as a neighbor (based on Gaussian distance)\nIn low dimensions: Define similar probability q_{ij} using a t-distribution with heavy tails\nOptimize: Move points in 2D to make q_{ij} match p_{ij} (minimize KL divergence)\n\nThe t-distribution‚Äôs heavy tails are clever: they let well-separated clusters spread out in 2D without overlapping, while keeping local neighborhoods tight.\n\n\nCode\nfrom sklearn.manifold import TSNE\n\n# Apply t-SNE\ntsne = TSNE(n_components=2, random_state=42, perplexity=30)\nX_scurve_tsne = tsne.fit_transform(X_scurve)\n\n# Plot all three methods\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# MDS - Global Euclidean distances\naxes[0].scatter(X_scurve_mds[:, 0], X_scurve_mds[:, 1], c=color,\n                cmap='viridis', alpha=0.6, s=20)\naxes[0].set_xlabel('MDS1')\naxes[0].set_ylabel('MDS2')\naxes[0].set_title('MDS: Global Distances')\nsns.despine(ax=axes[0])\n\n# Isomap - Geodesic distances\naxes[1].scatter(X_scurve_isomap[:, 0], X_scurve_isomap[:, 1], c=color,\n                cmap='viridis', alpha=0.6, s=20)\naxes[1].set_xlabel('Isomap1')\naxes[1].set_ylabel('Isomap2')\naxes[1].set_title('Isomap: Geodesic Distances')\nsns.despine(ax=axes[1])\n\n# t-SNE - Local neighborhoods\naxes[2].scatter(X_scurve_tsne[:, 0], X_scurve_tsne[:, 1], c=color,\n                cmap='viridis', alpha=0.6, s=20)\naxes[2].set_xlabel('t-SNE1')\naxes[2].set_ylabel('t-SNE2')\naxes[2].set_title('t-SNE: Local Structure')\nsns.despine(ax=axes[2])\n\nplt.tight_layout()\n\n\n\n\n\nComparing global, geodesic, and local approaches on the S-curve\n\n\n\n\nAll three methods successfully straighten the S-curve, but through different philosophies: MDS compromises between all distances, Isomap follows the manifold globally, and t-SNE focuses on preserving neighborhoods.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html#key-parameter-perplexity",
    "href": "m02-visualization/highd-data.html#key-parameter-perplexity",
    "title": "High-Dimensional Data Visualization",
    "section": "Key parameter: Perplexity",
    "text": "Key parameter: Perplexity\nPerplexity (typically 30-50) controls the effective neighborhood size. Too low fragments clusters; too high loses local detail.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html#what-t-sne-preserves-and-what-it-doesnt",
    "href": "m02-visualization/highd-data.html#what-t-sne-preserves-and-what-it-doesnt",
    "title": "High-Dimensional Data Visualization",
    "section": "What t-SNE preserves (and what it doesn‚Äôt)",
    "text": "What t-SNE preserves (and what it doesn‚Äôt)\nt-SNE is powerful but has important limitations:\nWhat t-SNE preserves: - \u0013 Local structure: Points that are neighbors in high dimensions stay neighbors in 2D - \u0013 Clusters: Well-separated groups remain separated - \u0013 Relative relationships within neighborhoods: If A is closer to B than to C locally, this is preserved\nWhat t-SNE does NOT preserve: - \u0017 Distances: The actual distance between points is not meaningful - \u0017 Global structure: The relative position of distant clusters is arbitrary - \u0017 Cluster sizes: Large clusters may appear smaller, and vice versa - \u0017 Density: Tight clusters may be spread out; sparse regions may appear dense\n\n\n\n\n\n\nDon‚Äôt over-interpret t-SNE!\n\n\n\nYou cannot conclude from a t-SNE plot: - ‚ÄúCluster A is twice as far from B as from C‚Äù (distances are not preserved) - ‚ÄúCluster A is twice the size of B‚Äù (sizes are not preserved) - ‚ÄúThe data has exactly 5 clusters‚Äù (apparent clusters may be visualization artifacts)\nYou can conclude: - ‚ÄúThese points form a distinct group separate from others‚Äù - ‚ÄúThese points are more similar to each other than to distant points‚Äù - ‚ÄúThe data has local structure and is not uniformly random‚Äù",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html#applying-t-sne-to-real-data",
    "href": "m02-visualization/highd-data.html#applying-t-sne-to-real-data",
    "title": "High-Dimensional Data Visualization",
    "section": "Applying t-SNE to real data",
    "text": "Applying t-SNE to real data\nLet‚Äôs apply t-SNE to a more realistic high-dimensional dataset‚Äîthe MNIST digits dataset, which has 784 dimensions (28ÔøΩ28 pixel images):\n\n\nCode\nfrom sklearn.datasets import load_digits\n\n# Load digits dataset (8x8 images, 64 dimensions - a smaller version of MNIST)\ndigits = load_digits()\nX_digits = digits.data\ny_digits = digits.target\n\n# Take a subset for speed (t-SNE is slow on large datasets)\nnp.random.seed(42)\nindices = np.random.choice(len(X_digits), size=1000, replace=False)\nX_subset = X_digits[indices]\ny_subset = y_digits[indices]\n\n# Apply t-SNE\ntsne_digits = TSNE(n_components=2, random_state=42, perplexity=40)\nX_digits_tsne = tsne_digits.fit_transform(X_subset)\n\n# Plot\nfig, ax = plt.subplots(figsize=(12, 10))\nscatter = ax.scatter(X_digits_tsne[:, 0], X_digits_tsne[:, 1],\n                     c=y_subset, cmap='tab10', alpha=0.7, s=30)\nax.set_xlabel('t-SNE1')\nax.set_ylabel('t-SNE2')\nax.set_title('t-SNE Visualization of Handwritten Digits (64D ÔøΩ 2D)')\ncbar = plt.colorbar(scatter, ax=ax, ticks=range(10))\ncbar.set_label('Digit Class')\nsns.despine()\n\n\n/Users/skojaku-admin/Documents/projects/applied-soft-comp/.venv/lib/python3.11/site-packages/IPython/core/events.py:82: UserWarning: Glyph 65533 (\\N{REPLACEMENT CHARACTER}) missing from font(s) Arial.\n  func(*args, **kwargs)\n/Users/skojaku-admin/Documents/projects/applied-soft-comp/.venv/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 65533 (\\N{REPLACEMENT CHARACTER}) missing from font(s) Arial.\n  fig.canvas.print_figure(bytes_io, **kw)\n\n\n\n\n\nt-SNE visualization of MNIST digits (784 dimensions ÔøΩ 2D). Each color represents a digit class.\n\n\n\n\nThe t-SNE projection beautifully separates most digit classes. Digits that look similar (like 3, 5, and 8, or 4 and 9) cluster near each other, while visually distinct digits (like 0 and 1) are well separated.\nThis demonstrates t-SNE‚Äôs power: from 64 dimensions with no explicit information about what makes digits similar, t-SNE discovers the perceptual structure of handwritten digits.\n\n\nt-SNE is stochastic: Different runs produce different layouts (though cluster structure remains consistent). Always check multiple runs with different random seeds, especially for important scientific conclusions.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html#umap-vs-t-sne",
    "href": "m02-visualization/highd-data.html#umap-vs-t-sne",
    "title": "High-Dimensional Data Visualization",
    "section": "UMAP vs t-SNE",
    "text": "UMAP vs t-SNE\nAdvantages of UMAP: - Faster: Can be 10-100ÔøΩ faster than t-SNE on large datasets - Scales better: Works well on datasets with millions of points - Better global structure: Preserves more global relationships than t-SNE - Theoretically grounded: Based on Riemannian geometry and fuzzy topology\nTrade-offs: - Less battle-tested than t-SNE (newer method) - More hyperparameters to tune (though defaults work well) - Can produce similar-looking results to t-SNE, so choice often comes down to speed\n\n\nCode\nimport umap\n\n# Apply UMAP\numap_model = umap.UMAP(n_components=2, random_state=42, n_neighbors=30)\nX_digits_umap = umap_model.fit_transform(X_subset)\n\n# Plot comparison\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# t-SNE\nscatter = axes[0].scatter(X_digits_tsne[:, 0], X_digits_tsne[:, 1],\n                          c=y_subset, cmap='tab10', alpha=0.7, s=30)\naxes[0].set_xlabel('t-SNE1')\naxes[0].set_ylabel('t-SNE2')\naxes[0].set_title('t-SNE')\nsns.despine(ax=axes[0])\n\n# UMAP\nscatter = axes[1].scatter(X_digits_umap[:, 0], X_digits_umap[:, 1],\n                          c=y_subset, cmap='tab10', alpha=0.7, s=30)\naxes[1].set_xlabel('UMAP1')\naxes[1].set_ylabel('UMAP2')\naxes[1].set_title('UMAP')\ncbar = plt.colorbar(scatter, ax=axes[1], ticks=range(10))\ncbar.set_label('Digit Class')\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n\n\n/Users/skojaku-admin/Documents/projects/applied-soft-comp/.venv/lib/python3.11/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n  warn(\n\n\n\n\n\nUMAP vs t-SNE on digits dataset. UMAP often preserves more global structure while being much faster.\n\n\n\n\nBoth methods reveal similar cluster structure, but UMAP tends to space clusters more evenly and preserve more of the global topology. Notice how UMAP places similar digits (3, 5, 8) in a connected region, suggesting they share underlying structure.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html#when-to-use-umap",
    "href": "m02-visualization/highd-data.html#when-to-use-umap",
    "title": "High-Dimensional Data Visualization",
    "section": "When to use UMAP",
    "text": "When to use UMAP\nUse UMAP when: - You have very large datasets (&gt;10,000 points) where t-SNE is slow - You want to preserve more global structure - You‚Äôre doing exploratory analysis and want fast iteration - You need to project new data onto an existing embedding (UMAP supports this, t-SNE doesn‚Äôt easily)\nStick with t-SNE when: - You need the most established method with extensive literature - You‚Äôre working with moderate-sized datasets where speed isn‚Äôt critical - You‚Äôre replicating published work that used t-SNE",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/time-series.html",
    "href": "m02-visualization/time-series.html",
    "title": "Time Series Visualization",
    "section": "",
    "text": "What you‚Äôll learn in this module\n\n\n\nThis module explores how to visualize time series data effectively, moving beyond simple line charts to reveal true underlying patterns. We will examine how choices in scale and geometry, such as small multiples, logarithmic axes, and lag plots, can either expose genuine trends or manufacture misleading narratives.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Time-Series"
    ]
  },
  {
    "objectID": "m03-agentic-coding/context-engineering.html",
    "href": "m03-agentic-coding/context-engineering.html",
    "title": "Context Engineering",
    "section": "",
    "text": "What you‚Äôll learn in this module\n\n\n\nThe LLM‚Äôs context window is like RAM: limited, precious, and subject to pollution. Context engineering is the operating system that manages it. The challenge is not filling the context window with everything, but curating it with only what matters right now. This module teaches you to manage context across its lifecycle: write what you need to remember, select what you need now, compress what you‚Äôll need later, and isolate what you don‚Äôt need yet.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Context Engineering"
    ]
  },
  {
    "objectID": "m03-agentic-coding/context-engineering.html#context-engineering",
    "href": "m03-agentic-coding/context-engineering.html#context-engineering",
    "title": "Context Engineering",
    "section": "Context Engineering",
    "text": "Context Engineering\n\n\n\n\n\n\nFigure¬†1: The image is taken from Context Engineering for Agents by Lance‚Äôs Blog.\n\n\n\nContext engineering breaks into four strategies: write, select, compress, and isolate. Each addresses a different phase of the context lifecycle. Let‚Äôs walk through them in modern agentic AI tools to understand how they work.\n\nWriting Context\nAgents use scratchpads to offload working memory. Instead of keeping every intermediate step in the context window, an agent writes state‚Äîlike a todo list or summary‚Äîto an external file or variable. Many agentic tools like Claude Code, Gemini CLI, Antigravity, and Cursor uses this pattern to track multi-step tasks across context resets, maintaining coherence without bloating the prompt.\nFor long-term memory, agents rely on persistent rules files like AGENTS.md, CLAUDE.md or .cursorrules (Cursor). These act as procedural memory, storing project-specific instructions that are injected into the context at the start of a session or retrieved when relevant. For example, the following is a sample AGENTS.md file:\n## Project: `AgenticFlow` - Context Engineering Demo\n\n###  Project Goal\nDemonstrate advanced context engineering for LLM agents: writing, selecting, compressing, isolating context to optimize performance and ensure robust multi-step workflows.\n\n### Agent Persona & Principles\n-   **Role**: Senior AI Engineer/Architect.\n-   **Objective**: Develop efficient, reliable, maintainable agentic workflows.\n-   **Style**: Clear, concise, technical, solution-oriented. Justify decisions.\n-   **Principles**: Context Efficiency, Modularity, Transparency, Robustness, Iteration.\n\n### Technical Guidelines\n-   **Language**: Python 3.9+.\n-   **Libraries**: `pydantic`, `pytest`, `black`. `langchain`/`llamaindex` for orchestration (use sparingly).\n-   **Dev Practices**: Git (conventional commits), Markdown/docstring documentation, comprehensive error handling, explicit tool use.\n\n...(continue)\n\n\n\n\n\n\n\nWrite AGENTS.md when you start a new project\n\n\n\nAGENTS.md is an industry standard file name for agentic AI workflows. It serves as the agent‚Äôs procedural memory, storing project-specific instructions, guidelines, and foundational context. This content is automatically injected into the agent‚Äôs context window at the start of a session or retrieved on demand, ensuring consistent behavior, reducing redundant prompting, and maintaining coherence across complex, multi-step agentic workflows.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Context Engineering"
    ]
  },
  {
    "objectID": "m03-agentic-coding/context-engineering.html#selecting-context",
    "href": "m03-agentic-coding/context-engineering.html#selecting-context",
    "title": "Context Engineering",
    "section": "Selecting Context",
    "text": "Selecting Context\n\n\n\n\n\nSelecting context means pulling it into the context window at runtime. The key insight is progressive disclosure: the agent does not need all the data upfront. It explores incrementally, using lightweight identifiers like file paths, URLs, or database queries to fetch data only when needed.\nImagine you want to update a file A.py that depends on B.py. A bad approach loads the entire content of B.py into the context window. A better approach gives the agent the file path and lets it fetch content on demand.\nThe Model Context Protocol (MCP) is the standard mechanism for this. Instead of copy-pasting data into the prompt, MCP gives the agent tools to pull data on demand. Context7 is a good example. It is an MCP server that fetches documentation for a library to ground agents on the latest features. An agent trained in 2024 may not know about 2025 features, but context7 solves this. It offers two tools: resolve-library-id and get-library-docs. These tool names are injected into the context window. When the agent calls them, it retrieves documentation on demand without polluting the context window, allowing the agent to focus on the task.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Context Engineering"
    ]
  },
  {
    "objectID": "m03-agentic-coding/context-engineering.html#compressing-context",
    "href": "m03-agentic-coding/context-engineering.html#compressing-context",
    "title": "Context Engineering",
    "section": "Compressing Context",
    "text": "Compressing Context\nLong-running tasks generate more context than the window can hold. When you approach the limit, you have two choices: summarize or trim.\nCompaction through summarization distills a conversation into its essential elements. Claude Code does this automatically. When you exceed 95% of the context window, it triggers auto-compact: the message history is passed to the model to summarize architectural decisions, unresolved bugs, and implementation details while discarding redundant tool outputs. The agent continues with the compressed context plus the five most recently accessed files.\nThe art of compaction is deciding what to keep versus discard. Overly aggressive compaction loses subtle details whose importance only becomes apparent later. Start by maximizing recall, capturing everything relevant, then iterate to improve precision by eliminating fluff.\nA lighter form of compaction is tool result clearing. Once a tool has been called and its result used, the raw output can be removed from the message history. The decision or action taken from that result matters. The ten thousand tokens of JSON it returned does not.\nTrimming is a simpler strategy. It uses heuristics to prune context without LLM involvement. Remove messages older than N turns, or keep only the system prompt and the last K user-agent exchanges. This approach requires no model computation but loses more information than summarization.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Context Engineering"
    ]
  },
  {
    "objectID": "m03-agentic-coding/context-engineering.html#isolating-context",
    "href": "m03-agentic-coding/context-engineering.html#isolating-context",
    "title": "Context Engineering",
    "section": "Isolating Context",
    "text": "Isolating Context\nIsolation means splitting context across boundaries so the model doesn‚Äôt drown in a single monolithic window. The most common pattern is multi-agent architectures. Instead of one agent maintaining state across an entire project, specialized sub-agents handle focused sub-tasks with clean context windows.\nAnthropic‚Äôs multi-agent researcher demonstrates this well. A lead agent coordinates with a high-level plan and spawns sub-agents that explore different aspects of a question in parallel, each with its own context window. A sub-agent might use ten thousand or more tokens to explore a research thread, but it returns only a one thousand to two thousand token summary to the lead agent. The detailed search context remains isolated. The lead agent synthesizes the compressed results without ever seeing the full exploration.\nThis approach achieves separation of concerns. Each sub-agent has a narrow scope, reducing context confusion and clash. The cost is coordination complexity. Spawning agents, managing handoffs, and aggregating results all add overhead. Anthropic reports that multi-agent systems can use up to fifteen times more tokens than single-agent systems. The performance gain on complex tasks justifies it.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Context Engineering"
    ]
  },
  {
    "objectID": "m03-agentic-coding/context-engineering.html#the-takeaway",
    "href": "m03-agentic-coding/context-engineering.html#the-takeaway",
    "title": "Context Engineering",
    "section": "The Takeaway",
    "text": "The Takeaway\nContext is a finite resource. The bottleneck in agentic systems is rarely the model‚Äôs reasoning. It is the poverty or pollution of its inputs. Context engineering is the discipline of managing this resource across its lifecycle. Write what you need to remember. Select what you need now. Compress what you need later. Isolate what you don‚Äôt need yet. The most powerful agent is not the one with the highest IQ in parameters. It is the one with the most disciplined context management.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Context Engineering"
    ]
  },
  {
    "objectID": "m03-agentic-coding/overview.html",
    "href": "m03-agentic-coding/overview.html",
    "title": "Overview",
    "section": "",
    "text": "What you‚Äôll learn in this module\n\n\n\nThis module transforms you from a coder writing syntax to a manager orchestrating intelligence. We explore the fundamental shift from copilots (next-token predictors) to agents (autonomous task completers), and then build the three operational components that power agentic AI: effective prompt engineering, the ReAct loop that enables autonomous reasoning, and context engineering that manages the LLM‚Äôs working memory.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Overview"
    ]
  },
  {
    "objectID": "m03-agentic-coding/overview.html#the-mechanism",
    "href": "m03-agentic-coding/overview.html#the-mechanism",
    "title": "Overview",
    "section": "The Mechanism",
    "text": "The Mechanism\nThe shift from Copilot to Agent is not just an upgrade in model size; it is a fundamental change in the interaction loop.\nCopilot (e.g., GitHub Copilot, early Gemini Code Assist) operates on Next-Token Prediction. It looks at your cursor position and uses the probability distribution P(x_{t+1} | x_{0:t}) to guess the next few characters. It is a ‚Äúsmart typewriter‚Äù‚Äîfast, helpful, but ultimately passive. It requires your constant attention and cannot act independently. You write; it completes.\nAgents (e.g., Claude Code, Google Antigravity, Cursor) operate on Task Completion. They function like autonomous interns. You give them a high-level goal (‚ÄúRefactor this module‚Äù), and they engage in a loop of Reasoning, Action, and Observation until the task is done. They read files, run terminal commands, call external APIs, and fix their own errors. The intelligence doesn‚Äôt come from a larger model‚Äîit comes from the feedback loop that allows the agent to observe the consequences of its actions and adjust.\nThis shifts your role from the ‚ÄúWriter of Syntax‚Äù to the ‚ÄúManager-Architect‚Äù. Your job is no longer to know the exact syntax of a matplotlib plot, but to know what plot you need, how to clearly specify that requirement, and how to verify that the agent built it correctly. You move from implementation to orchestration.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Overview"
    ]
  },
  {
    "objectID": "m03-agentic-coding/overview.html#the-application",
    "href": "m03-agentic-coding/overview.html#the-application",
    "title": "Overview",
    "section": "The Application",
    "text": "The Application\nThis module breaks agentic AI into the following operational components:\nWe start with a hands-on session using Google Antigravity to build a functional game and refactor a codebase entirely through natural language instructions.\nPrompt Tuning teaches you how to communicate effectively with LLMs by understanding them as stateless pattern matchers sampling from probability distributions. You‚Äôll learn to structure prompts (instruction, data, format, persona, context) to reliably activate desired patterns. This is the interface.\nAgentic AI explains the core mechanism‚Äîthe ReAct loop (Reason + Act) that transforms a passive language model into an autonomous agent. You‚Äôll build a working agent using LangGraph that can query and analyze datasets without human intervention. This is the engine.\nContext Engineering solves the context window problem. LLMs are brilliant but bounded‚Äîthey have limited working memory that degrades as it fills. You‚Äôll learn to manage context across its lifecycle: write (scratchpads & memories), select (MCP & just-in-time retrieval), compress (summarization), and isolate (multi-agent architectures). This is the operating system.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Overview"
    ]
  },
  {
    "objectID": "m04-text/archive/attention.html",
    "href": "m04-text/archive/attention.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "What if, like humans, our neural networks could learn to focus on what‚Äôs important? When you read a sentence or look at a scene, you don‚Äôt process everything with equal importance. You pay attention to specific parts at different times. This fundamental insight led to one of the most important innovations in deep learning: attention mechanisms.\n\n\nConsider this translation task:\n‚ÄúThe cat sat on the mat because it was comfortable.‚Äù\nWhat does ‚Äúit‚Äù refer to - the cat or the mat? As humans, we naturally link ‚Äúit‚Äù to ‚Äúcat‚Äù because we understand cats seek comfort. But traditional sequence models like vanilla RNNs and LSTMs struggle with such connections, especially in longer sequences.\nAttention mechanisms allow models to focus on relevant parts of the input sequence while generating the output. Instead of packing the information into a fixed-size memory (e.g., hidden state), the attention mechanism creates a matrix of attention weights within the given sequences. This weight is learned by a neural network that takes the corresponding variables as input.\n[Figure: Visualization showing how attention ‚Äúlooks back‚Äù at input sequence while generating output]\n\n\n\nLet‚Äôs formalize this intuition. Given: - An input sequence of n vectors: (x_1, ..., x_n) - Current decoder hidden state: h_t - Encoder hidden states: (h^{enc}_1, ..., h^{enc}_n)\nThe attention mechanism computes:\n\nAlignment scores e_{tj} between the decoder state and each encoder state: e_{tj} = score(h_t, h^{enc}_j)\nAttention weights through softmax normalization: \\alpha_{tj} = \\frac{\\exp(e_{tj})}{\\sum_{k=1}^n \\exp(e_{tk})}\nContext vector as weighted sum: c_t = \\sum_{j=1}^n \\alpha_{tj}h^{enc}_j\n\nThe score function can take various forms:\n- Dot product: $score(h_t, h^{enc}_j) = h_t^\\top h^{enc}_j$\n- Additive: $score(h_t, h^{enc}_j) = v^\\top \\tanh(W[h_t; h^{enc}_j])$\n- Multiplicative: $score(h_t, h^{enc}_j) = h_t^\\top W h^{enc}_j$\n\n\n\nLet‚Äôs implement a basic attention mechanism in PyTorch:\n```xrxrdlgimwu ipython3 import torch import torch.nn as nn import torch.nn.functional as F\nclass Attention(nn.Module): def init(self, hidden_size): super().__init__() # For additive attention self.attn = nn.Linear(hidden_size * 2, hidden_size) self.v = nn.Parameter(torch.rand(hidden_size))\ndef forward(self, hidden, encoder_outputs):\n    # hidden: [batch_size, hidden_size]\n    # encoder_outputs: [batch_size, seq_len, hidden_size]\n\n    batch_size, seq_len, hidden_size = encoder_outputs.size()\n\n    # Repeat hidden state seq_len times\n    hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n\n    # Calculate attention scores\n    energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n    energy = energy.permute(0, 2, 1)\n    v = self.v.repeat(batch_size, 1).unsqueeze(1)\n    attention = torch.bmm(v, energy).squeeze(1)\n\n    # Calculate attention weights\n    return F.softmax(attention, dim=1)\n\n```{tip}\nWhen implementing attention:\n- Always check tensor dimensions carefully\n- Use broadcasting to avoid explicit loops\n- Consider numerical stability in softmax computation\n- Monitor attention weights to ensure they sum to 1\n\n\n\nOne of the most powerful aspects of attention is its interpretability. The attention weights \\alpha_{tj} directly show us what parts of the input the model is focusing on at each step.\n[Figure: Heatmap showing attention weights during translation, with x-axis as input words and y-axis as output words]\n\n\n\nWe‚Äôve covered basic attention, but several variants exist:\n\nGlobal vs Local Attention\n\nGlobal: Attends to all source positions\nLocal: Only attends to a window of positions\n\nSelf-Attention\n\nAllows sequence to attend to itself\nKey component in modern architectures\n\n\nWhile we often visualize attention as \"looking back\" at the input, mathematically it's creating a weighted combination of values. This simple yet powerful idea has revolutionized sequence modeling.\n\n\n\n\nWhy does attention help with the vanishing gradient problem?\nImplement the dot-product version of the attention score function\nAnalyze how attention weights change with sequence length\nCompare computation complexity of different attention variants\n\n\n\n\nConsider these questions: - How would you modify the attention mechanism for document summarization? - What happens if we stack multiple attention layers? - How might attention help in image captioning?\nWhen experimenting with attention:\n- Start with simple sequences to verify implementation\n- Visualize attention weights frequently\n- Try different score functions\n- Monitor memory usage with long sequences\nThis lecture note has provided a foundation for understanding attention mechanisms. In practice, you‚Äôll find them indispensable for many sequence processing tasks, from translation to summarization to image captioning."
  },
  {
    "objectID": "m04-text/archive/attention.html#why-attention-is-needed",
    "href": "m04-text/archive/attention.html#why-attention-is-needed",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Consider this translation task:\n‚ÄúThe cat sat on the mat because it was comfortable.‚Äù\nWhat does ‚Äúit‚Äù refer to - the cat or the mat? As humans, we naturally link ‚Äúit‚Äù to ‚Äúcat‚Äù because we understand cats seek comfort. But traditional sequence models like vanilla RNNs and LSTMs struggle with such connections, especially in longer sequences.\nAttention mechanisms allow models to focus on relevant parts of the input sequence while generating the output. Instead of packing the information into a fixed-size memory (e.g., hidden state), the attention mechanism creates a matrix of attention weights within the given sequences. This weight is learned by a neural network that takes the corresponding variables as input.\n[Figure: Visualization showing how attention ‚Äúlooks back‚Äù at input sequence while generating output]"
  },
  {
    "objectID": "m04-text/archive/attention.html#mathematical-framework",
    "href": "m04-text/archive/attention.html#mathematical-framework",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Let‚Äôs formalize this intuition. Given: - An input sequence of n vectors: (x_1, ..., x_n) - Current decoder hidden state: h_t - Encoder hidden states: (h^{enc}_1, ..., h^{enc}_n)\nThe attention mechanism computes:\n\nAlignment scores e_{tj} between the decoder state and each encoder state: e_{tj} = score(h_t, h^{enc}_j)\nAttention weights through softmax normalization: \\alpha_{tj} = \\frac{\\exp(e_{tj})}{\\sum_{k=1}^n \\exp(e_{tk})}\nContext vector as weighted sum: c_t = \\sum_{j=1}^n \\alpha_{tj}h^{enc}_j\n\nThe score function can take various forms:\n- Dot product: $score(h_t, h^{enc}_j) = h_t^\\top h^{enc}_j$\n- Additive: $score(h_t, h^{enc}_j) = v^\\top \\tanh(W[h_t; h^{enc}_j])$\n- Multiplicative: $score(h_t, h^{enc}_j) = h_t^\\top W h^{enc}_j$"
  },
  {
    "objectID": "m04-text/archive/attention.html#implementation-example",
    "href": "m04-text/archive/attention.html#implementation-example",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Let‚Äôs implement a basic attention mechanism in PyTorch:\n```xrxrdlgimwu ipython3 import torch import torch.nn as nn import torch.nn.functional as F\nclass Attention(nn.Module): def init(self, hidden_size): super().__init__() # For additive attention self.attn = nn.Linear(hidden_size * 2, hidden_size) self.v = nn.Parameter(torch.rand(hidden_size))\ndef forward(self, hidden, encoder_outputs):\n    # hidden: [batch_size, hidden_size]\n    # encoder_outputs: [batch_size, seq_len, hidden_size]\n\n    batch_size, seq_len, hidden_size = encoder_outputs.size()\n\n    # Repeat hidden state seq_len times\n    hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n\n    # Calculate attention scores\n    energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n    energy = energy.permute(0, 2, 1)\n    v = self.v.repeat(batch_size, 1).unsqueeze(1)\n    attention = torch.bmm(v, energy).squeeze(1)\n\n    # Calculate attention weights\n    return F.softmax(attention, dim=1)\n\n```{tip}\nWhen implementing attention:\n- Always check tensor dimensions carefully\n- Use broadcasting to avoid explicit loops\n- Consider numerical stability in softmax computation\n- Monitor attention weights to ensure they sum to 1"
  },
  {
    "objectID": "m04-text/archive/attention.html#visualizing-attention",
    "href": "m04-text/archive/attention.html#visualizing-attention",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "One of the most powerful aspects of attention is its interpretability. The attention weights \\alpha_{tj} directly show us what parts of the input the model is focusing on at each step.\n[Figure: Heatmap showing attention weights during translation, with x-axis as input words and y-axis as output words]"
  },
  {
    "objectID": "m04-text/archive/attention.html#types-of-attention",
    "href": "m04-text/archive/attention.html#types-of-attention",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "We‚Äôve covered basic attention, but several variants exist:\n\nGlobal vs Local Attention\n\nGlobal: Attends to all source positions\nLocal: Only attends to a window of positions\n\nSelf-Attention\n\nAllows sequence to attend to itself\nKey component in modern architectures\n\n\nWhile we often visualize attention as \"looking back\" at the input, mathematically it's creating a weighted combination of values. This simple yet powerful idea has revolutionized sequence modeling."
  },
  {
    "objectID": "m04-text/archive/attention.html#exercises-for-understanding",
    "href": "m04-text/archive/attention.html#exercises-for-understanding",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Why does attention help with the vanishing gradient problem?\nImplement the dot-product version of the attention score function\nAnalyze how attention weights change with sequence length\nCompare computation complexity of different attention variants"
  },
  {
    "objectID": "m04-text/archive/attention.html#further-exploration",
    "href": "m04-text/archive/attention.html#further-exploration",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Consider these questions: - How would you modify the attention mechanism for document summarization? - What happens if we stack multiple attention layers? - How might attention help in image captioning?\nWhen experimenting with attention:\n- Start with simple sequences to verify implementation\n- Visualize attention weights frequently\n- Try different score functions\n- Monitor memory usage with long sequences\nThis lecture note has provided a foundation for understanding attention mechanisms. In practice, you‚Äôll find them indispensable for many sequence processing tasks, from translation to summarization to image captioning."
  },
  {
    "objectID": "m04-text/archive/doc2vec.html",
    "href": "m04-text/archive/doc2vec.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Doc2Vec {footcite}le2014distributed extends word2vec by learning document vectors alongside word vectors. For a document d with words w_1, w_2, ..., w_n, it learns: - Document vector v_d \\in \\mathbb{R}^m - Word vectors v_w \\in \\mathbb{R}^m\nThere are two types of Doc2Vec: - Distributed Memory (PV-DM) - Distributed Bag of Words (PV-DBOW)\nwhere PV-DM corresponds to the CBOW model, and PV-DBOW corresponds to the Skip-Gram model of word2vec.\nSee [the lecture note of word2vec](../m01-word-embedding/word2vec.md) for more details on CBOW and Skip-Gram.\n\n\n```fnahqbjp https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JPetbQHmG0NAbdQ08JSiMQ.png :name: pv-dm :alt: PV-DM :width: 500px :align: center\nPV-DM predicts the center word based on the average or concatenated vector of the context words. Image taken from https://heartbeat.comet.ml/getting-started-with-doc2vec-2645e3e9f137\n\nCBOW word2vec predicts the center word based on the *average* or *concatenated* vector of the context words.\nIn PV-DM, the document vector is added to the average or concatenation.\nMore specifically, the probability of a word $w_i$ given the document $d$ and the context $w_{i-k},...,w_{i-1}$ is given by:\n\n$$P(w_i|w_{i-k},...,w_{i-1},d) = \\frac{\\exp(u_{w_i}^T h)}{\\sum_{w \\in V} \\exp(u_w^T h)}$$\n\nwhere $h$ is the context vector, which is either the average:\n\n$$\nh = \\frac{1}{k\\textcolor{red}{+1}}\\left(\\textcolor{red}{v_d} + \\sum_{j=i-k}^{i-1}v_{w_j}\\right)\n$$\n\nor the concatenation:\n\n$$\nh = \\left(v_d, \\sum_{j=i-k}^{i-1}v_{w_j}\\right) U, \\quad U \\in \\mathbb{R}^{(d+kd) \\times d}\n$$\n\nwhere $U$ is a matrix that maps the concatenated vector (of dimension $d+kd$) back to dimension $d$ to match the word vector space. Here, $d$ is the embedding dimension and $k$ is the context window size.\n\n```{note}\nThe choice between concatenation and average affects how the document and context vectors are combined:\n- **Average**: Treats document vector and context word vectors equally by taking their mean. This is simpler but may neglect the influence of individual context words. No additional parameters needed, making it computationally efficient.\n- **Concatenation**: Keeps document and context information separate before combining through the U matrix. This preserves more distinct information but requires learning additional parameters (the U matrix). Though more computationally intensive, it allows the model to learn different weights for document and word contexts.\nThe original paper used concatenation, arguing it allows the model to treat document and word vectors differently.\n\nThe softmax computation over the entire vocabulary V can be computationally expensive for large vocabularies. In practice, optimization techniques like negative sampling or hierarchical softmax are commonly used to approximate this computation more efficiently.\n\n\n\n```fnahqbjp https://miro.medium.com/v2/resize:fit:1400/1*ALpuAo7uv0V8PlrVgSzMsg.png :name: pv-dbow :alt: PV-DBOW :width: 500px :align: center\nPV-DBOW predicts context words using only the document vector, similar to Skip-Gram predicting context words from a center word. Image taken from https://heartbeat.comet.ml/getting-started-with-doc2vec-2645e3e9f137\n\nPV-DBOW is similar to Skip-Gram. The probability of a word $w_i$ given the document $d$ is given by:\n\n$$P(w_i|d) = \\frac{\\exp(u_{w_i}^T v_d)}{\\sum_{w \\in V} \\exp(u_w^T v_d)}$$\n\nThis is analogous to the skip-gram model, where the document vector $v_d$ is used to predict the context words.\n\n\n```{note}\nWhich mode, PV-DM or PV-DBOW, is better? The original paper {footcite}`le2014distributed` suggests that PV-DM is better, since it can distinguish the order of words within a document.\nYet, {footcite}`le2016empirical` found that PV-DBOW, despite being more simple, is better overall for document similarity tasks, when properly tuned. This highlights the importance of hyperparameter optimization in practice.\n\nKey considerations for choosing between PV-DM and PV-DBOW:\n- PV-DM: Better for tasks requiring word order sensitivity\n- PV-DBOW: More efficient training, often better for similarity tasks\n- Hybrid approach: Some implementations combine both methods\n\n\n\n\nLet us have a hands-on implementation of Doc2Vec using the gensim library. Our sample documents are:\nojgudrrctmf ipython3 # Sample documents documents = [     \"Machine learning is a subset of artificial intelligence\",     \"Deep learning uses neural networks with multiple layers\",     \"Natural language processing deals with text and speech\",     \"Computer vision focuses on image and video analysis\",     \"Reinforcement learning involves agents making decisions\" ]\nWe will first import the necessary libraries.\nojgudrrctmf ipython3 from gensim.models.doc2vec import Doc2Vec, TaggedDocument from nltk.tokenize import word_tokenize\nIn gensim doc2vec, we need to prepare the documents in the form of TaggedDocument.\nojgudrrctmf ipython3 # Prepare documents tagged_docs = [] for i, doc in enumerate(documents):     tagged_doc = TaggedDocument(         words=word_tokenize(doc.lower()), # tokenize the document         tags=[str(i)] # tag the document with its index     )     tagged_docs.append(tagged_doc)\nWe added ‚Äútags‚Äù along with the words. The ‚Äútag‚Äù is used to identify the document.\n`word_tokenize` is a function from the `nltk` library that tokenizes the document into words.\nFor example, \"Machine learning is a subset of artificial intelligence\" is tokenized into `['machine', 'learning', 'is', 'a', 'subset', 'of', 'artificial', 'intelligence']`.\nSecond, we need to train the Doc2Vec model.\n```ojgudrrctmf ipython3 # Train Doc2Vec model model = Doc2Vec(tagged_docs, vector_size=50, # dimension of the document vector window=2, # context window size min_count=1, # ignore words that appear less than this epochs=300, dm=1, # 0: PV-DBOW, 1: PV-DM )"
  },
  {
    "objectID": "m04-text/archive/doc2vec.html#doc2vec-model",
    "href": "m04-text/archive/doc2vec.html#doc2vec-model",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Doc2Vec {footcite}le2014distributed extends word2vec by learning document vectors alongside word vectors. For a document d with words w_1, w_2, ..., w_n, it learns: - Document vector v_d \\in \\mathbb{R}^m - Word vectors v_w \\in \\mathbb{R}^m\nThere are two types of Doc2Vec: - Distributed Memory (PV-DM) - Distributed Bag of Words (PV-DBOW)\nwhere PV-DM corresponds to the CBOW model, and PV-DBOW corresponds to the Skip-Gram model of word2vec.\nSee [the lecture note of word2vec](../m01-word-embedding/word2vec.md) for more details on CBOW and Skip-Gram.\n\n\n```fnahqbjp https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JPetbQHmG0NAbdQ08JSiMQ.png :name: pv-dm :alt: PV-DM :width: 500px :align: center\nPV-DM predicts the center word based on the average or concatenated vector of the context words. Image taken from https://heartbeat.comet.ml/getting-started-with-doc2vec-2645e3e9f137\n\nCBOW word2vec predicts the center word based on the *average* or *concatenated* vector of the context words.\nIn PV-DM, the document vector is added to the average or concatenation.\nMore specifically, the probability of a word $w_i$ given the document $d$ and the context $w_{i-k},...,w_{i-1}$ is given by:\n\n$$P(w_i|w_{i-k},...,w_{i-1},d) = \\frac{\\exp(u_{w_i}^T h)}{\\sum_{w \\in V} \\exp(u_w^T h)}$$\n\nwhere $h$ is the context vector, which is either the average:\n\n$$\nh = \\frac{1}{k\\textcolor{red}{+1}}\\left(\\textcolor{red}{v_d} + \\sum_{j=i-k}^{i-1}v_{w_j}\\right)\n$$\n\nor the concatenation:\n\n$$\nh = \\left(v_d, \\sum_{j=i-k}^{i-1}v_{w_j}\\right) U, \\quad U \\in \\mathbb{R}^{(d+kd) \\times d}\n$$\n\nwhere $U$ is a matrix that maps the concatenated vector (of dimension $d+kd$) back to dimension $d$ to match the word vector space. Here, $d$ is the embedding dimension and $k$ is the context window size.\n\n```{note}\nThe choice between concatenation and average affects how the document and context vectors are combined:\n- **Average**: Treats document vector and context word vectors equally by taking their mean. This is simpler but may neglect the influence of individual context words. No additional parameters needed, making it computationally efficient.\n- **Concatenation**: Keeps document and context information separate before combining through the U matrix. This preserves more distinct information but requires learning additional parameters (the U matrix). Though more computationally intensive, it allows the model to learn different weights for document and word contexts.\nThe original paper used concatenation, arguing it allows the model to treat document and word vectors differently.\n\nThe softmax computation over the entire vocabulary V can be computationally expensive for large vocabularies. In practice, optimization techniques like negative sampling or hierarchical softmax are commonly used to approximate this computation more efficiently.\n\n\n\n```fnahqbjp https://miro.medium.com/v2/resize:fit:1400/1*ALpuAo7uv0V8PlrVgSzMsg.png :name: pv-dbow :alt: PV-DBOW :width: 500px :align: center\nPV-DBOW predicts context words using only the document vector, similar to Skip-Gram predicting context words from a center word. Image taken from https://heartbeat.comet.ml/getting-started-with-doc2vec-2645e3e9f137\n\nPV-DBOW is similar to Skip-Gram. The probability of a word $w_i$ given the document $d$ is given by:\n\n$$P(w_i|d) = \\frac{\\exp(u_{w_i}^T v_d)}{\\sum_{w \\in V} \\exp(u_w^T v_d)}$$\n\nThis is analogous to the skip-gram model, where the document vector $v_d$ is used to predict the context words.\n\n\n```{note}\nWhich mode, PV-DM or PV-DBOW, is better? The original paper {footcite}`le2014distributed` suggests that PV-DM is better, since it can distinguish the order of words within a document.\nYet, {footcite}`le2016empirical` found that PV-DBOW, despite being more simple, is better overall for document similarity tasks, when properly tuned. This highlights the importance of hyperparameter optimization in practice.\n\nKey considerations for choosing between PV-DM and PV-DBOW:\n- PV-DM: Better for tasks requiring word order sensitivity\n- PV-DBOW: More efficient training, often better for similarity tasks\n- Hybrid approach: Some implementations combine both methods"
  },
  {
    "objectID": "m04-text/archive/doc2vec.html#hands-on-implementation",
    "href": "m04-text/archive/doc2vec.html#hands-on-implementation",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Let us have a hands-on implementation of Doc2Vec using the gensim library. Our sample documents are:\nojgudrrctmf ipython3 # Sample documents documents = [     \"Machine learning is a subset of artificial intelligence\",     \"Deep learning uses neural networks with multiple layers\",     \"Natural language processing deals with text and speech\",     \"Computer vision focuses on image and video analysis\",     \"Reinforcement learning involves agents making decisions\" ]\nWe will first import the necessary libraries.\nojgudrrctmf ipython3 from gensim.models.doc2vec import Doc2Vec, TaggedDocument from nltk.tokenize import word_tokenize\nIn gensim doc2vec, we need to prepare the documents in the form of TaggedDocument.\nojgudrrctmf ipython3 # Prepare documents tagged_docs = [] for i, doc in enumerate(documents):     tagged_doc = TaggedDocument(         words=word_tokenize(doc.lower()), # tokenize the document         tags=[str(i)] # tag the document with its index     )     tagged_docs.append(tagged_doc)\nWe added ‚Äútags‚Äù along with the words. The ‚Äútag‚Äù is used to identify the document.\n`word_tokenize` is a function from the `nltk` library that tokenizes the document into words.\nFor example, \"Machine learning is a subset of artificial intelligence\" is tokenized into `['machine', 'learning', 'is', 'a', 'subset', 'of', 'artificial', 'intelligence']`.\nSecond, we need to train the Doc2Vec model.\n```ojgudrrctmf ipython3 # Train Doc2Vec model model = Doc2Vec(tagged_docs, vector_size=50, # dimension of the document vector window=2, # context window size min_count=1, # ignore words that appear less than this epochs=300, dm=1, # 0: PV-DBOW, 1: PV-DM )"
  },
  {
    "objectID": "m04-text/archive/embeddings-concepts.html",
    "href": "m04-text/archive/embeddings-concepts.html",
    "title": "Embeddings: How Machines Understand Meaning",
    "section": "",
    "text": "When you send text to an LLM, you see words. The model sees vectors‚Äîlong lists of numbers like [0.31, -0.85, 0.12, ..., 0.47]. Each word, sentence, or document becomes a point in a high-dimensional space. These numerical representations are called embeddings.\nThis might seem like a strange way to ‚Äúunderstand‚Äù language. But embeddings have a remarkable property: similar meanings become similar vectors. Words like ‚Äúcat‚Äù and ‚Äúdog‚Äù end up close together in this space, while ‚Äúcat‚Äù and ‚Äútheorem‚Äù are far apart.\nEmbeddings are the foundation of modern NLP. They‚Äôre how LLMs represent knowledge, perform reasoning, and generate text. Once you understand embeddings, transformers and LLMs stop being magic‚Äîthey‚Äôre just sophisticated ways of manipulating these numerical representations.\nLet‚Äôs unbox this first layer and see how meaning becomes mathematics.\n\n\nComputers can‚Äôt directly process text. They need numbers. But how do we convert words into numbers in a meaningful way?\n\n\nThe simplest idea: assign each word a unique integer.\n\n\nCode\n# Simple vocabulary\nvocab = [\"network\", \"graph\", \"node\", \"community\", \"detection\"]\n\n# Assign integers\nword_to_int = {word: i for i, word in enumerate(vocab)}\nprint(\"Integer encoding:\")\nprint(word_to_int)\n\n\nOutput:\n{'network': 0, 'graph': 1, 'node': 2, 'community': 3, 'detection': 4}\nProblem: The integers are arbitrary. The model might think ‚Äúnetwork‚Äù (0) is somehow ‚Äúless than‚Äù ‚Äúcommunity‚Äù (3), or that ‚Äúgraph‚Äù + ‚Äúnode‚Äù = ‚Äúcommunity‚Äù. These numbers encode no semantic relationships.\n\n\n\nRepresent each word as a binary vector where only one position is ‚Äúhot‚Äù (=1).\n\n\nCode\nimport numpy as np\n\nvocab_size = len(vocab)\n\ndef one_hot(word):\n    \"\"\"Convert word to one-hot vector.\"\"\"\n    vec = np.zeros(vocab_size)\n    vec[word_to_int[word]] = 1\n    return vec\n\nprint(\"One-hot encoding for 'network':\")\nprint(one_hot(\"network\"))\nprint(\"\\nOne-hot encoding for 'community':\")\nprint(one_hot(\"community\"))\n\n\nOutput:\n[1. 0. 0. 0. 0.]\n[0. 0. 0. 1. 0.]\nProblem: Every word is equally different from every other word (Euclidean distance is always ‚àö2). The model still can‚Äôt learn that ‚Äúnetwork‚Äù and ‚Äúgraph‚Äù are related, while ‚Äúnetwork‚Äù and ‚Äúdetection‚Äù are less related.\n\n\n\nInstead of hand-crafting representations, let the model learn them from data. Each word becomes a dense vector of real numbers (typically 50-1000 dimensions):\n\"network\" ‚Üí [0.31, -0.85, 0.12, 0.67, ...]  # 384 dimensions\n\"graph\"   ‚Üí [0.29, -0.82, 0.15, 0.69, ...]  # Similar to \"network\"!\n\"theorem\" ‚Üí [-0.61, 0.23, -0.45, 0.11, ...] # Different from \"network\"\nThese embeddings are learned by training models to predict context. Words that appear in similar contexts get similar embeddings. This is the foundation of modern NLP.\n\n\n\n\nOnce words are vectors, we can measure semantic similarity using cosine similarity:\n\n\\text{similarity}(u, v) = \\frac{u \\cdot v}{\\|u\\| \\|v\\|}\n\nThis measures the cosine of the angle between vectors (1 = same direction, 0 = orthogonal, -1 = opposite).\nLet‚Äôs see this in action with real embeddings.\n\n\n\nWe‚Äôll use the sentence-transformers library, which provides pre-trained models for generating embeddings.\n\n\nCode\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\n\n# Load a pre-trained model (lightweight, ~80MB)\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Generate embeddings for words\nwords = [\"network\", \"graph\", \"community\", \"detection\", \"cat\", \"theorem\"]\nembeddings = model.encode(words)\n\nprint(f\"Embedding dimensionality: {embeddings.shape[1]}\")\nprint(f\"Number of words: {embeddings.shape[0]}\")\nprint(f\"\\nFirst 10 dimensions of 'network': {embeddings[0][:10]}\")\n\n\nOutput:\nEmbedding dimensionality: 384\nNumber of words: 6\n\nFirst 10 dimensions of 'network': [ 0.0234 -0.0912  0.0456 ... ]\nEach word is now a 384-dimensional vector. Let‚Äôs compute similarities:\n\n\nCode\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Compute similarity matrix\nsim_matrix = cosine_similarity(embeddings)\n\n# Display as a heatmap\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"white\")\n\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.heatmap(sim_matrix, annot=True, fmt=\".2f\",\n            xticklabels=words, yticklabels=words,\n            cmap=\"RdYlGn\", vmin=0, vmax=1, ax=ax,\n            cbar_kws={'label': 'Cosine Similarity'})\nax.set_title(\"Word Similarity Matrix\", fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n\nKey observations: - ‚Äúnetwork‚Äù and ‚Äúgraph‚Äù have high similarity (~0.85) ‚Äî the model learned they‚Äôre related! - ‚Äúcat‚Äù has low similarity to network science terms - ‚Äútheorem‚Äù is somewhat similar to technical terms but distinct from social/biological concepts\nThis happens without anyone explicitly telling the model that ‚Äúnetwork‚Äù and ‚Äúgraph‚Äù are synonyms. The model learned from context.\n\n\n\n\n\n\nThe Distributional Hypothesis\n\n\n\n‚ÄúYou shall know a word by the company it keeps.‚Äù ‚Äî J.R. Firth, 1957\nWords that appear in similar contexts tend to have similar meanings. Embeddings operationalize this idea: they place words with similar contexts near each other in vector space.\n\n\n\n\n\nWord embeddings are useful, but research deals with sentences and documents. How do we embed larger chunks of text?\n\n\n\n\nCode\nsentence1 = \"Community detection in networks\"\nsentence2 = \"Identifying groups in graphs\"\nsentence3 = \"Cats like milk\"\n\n# Encode sentences\nsent_embeddings = model.encode([sentence1, sentence2, sentence3])\n\n# Compute similarities\nsent_sim = cosine_similarity(sent_embeddings)\n\nprint(\"Sentence similarities:\")\nprint(f\"'{sentence1}' vs. '{sentence2}': {sent_sim[0, 1]:.3f}\")\nprint(f\"'{sentence1}' vs. '{sentence3}': {sent_sim[0, 2]:.3f}\")\n\n\nOutput:\nSentence similarities:\n'Community detection in networks' vs. 'Identifying groups in graphs': 0.834\n'Community detection in networks' vs. 'Cats like milk': 0.124\nThe model correctly recognizes that the first two sentences describe similar concepts, while the third is unrelated.\nHow does this work? Modern sentence embedding models (like the one we‚Äôre using) don‚Äôt just average word vectors‚Äîthey use transformers to generate context-aware representations. We‚Äôll explore how transformers work in the next section. For now, just know: sentence embeddings capture meaning at the sentence level.\n\n\n\n\nEmbeddings enable semantic search: finding documents by meaning, not just keywords.\nTraditional keyword search: - Query: ‚Äúcommunity detection‚Äù - Matches: Papers containing exactly those words - Misses: Papers about ‚Äúgroup identification‚Äù or ‚Äúclustering‚Äù\nSemantic search: - Query: ‚Äúcommunity detection‚Äù - Matches: Papers about related concepts even if they use different words\nLet‚Äôs build a simple semantic search engine for research papers.\n\n\nCode\n# Simulated paper titles\npapers = [\n    \"Community Detection in Social Networks Using Modularity Optimization\",\n    \"Graph Clustering Algorithms: A Survey\",\n    \"Identifying Groups in Biological Networks\",\n    \"Deep Learning for Image Classification\",\n    \"Temporal Dynamics of Network Structure\",\n    \"Protein-Protein Interaction Prediction\",\n    \"Hierarchical Structure in Complex Networks\"\n]\n\n# Embed all papers\npaper_embeddings = model.encode(papers)\n\n# User query\nquery = \"finding groups in networks\"\nquery_embedding = model.encode([query])\n\n# Compute similarities\nsimilarities = cosine_similarity(query_embedding, paper_embeddings)[0]\n\n# Rank papers\nranked_indices = np.argsort(similarities)[::-1]  # Descending order\n\nprint(f\"Query: '{query}'\\n\")\nprint(\"Top 3 most relevant papers:\")\nfor i, idx in enumerate(ranked_indices[:3], 1):\n    print(f\"{i}. [{similarities[idx]:.3f}] {papers[idx]}\")\n\n\nOutput:\nQuery: 'finding groups in networks'\n\nTop 3 most relevant papers:\n1. [0.812] Community Detection in Social Networks Using Modularity Optimization\n2. [0.789] Identifying Groups in Biological Networks\n3. [0.754] Graph Clustering Algorithms: A Survey\nEven though the query doesn‚Äôt exactly match any title, semantic search finds the most relevant papers. Paper 4 (‚ÄúDeep Learning for Image Classification‚Äù) would have low similarity and rank last.\n\n\n\n\n\n\nBuilding Your Own Semantic Search\n\n\n\nYou can build a semantic search system for your literature: 1. Collect papers (titles + abstracts) 2. Generate embeddings with sentence-transformers 3. Store embeddings (just numpy arrays) 4. For each query, compute cosine similarity 5. Return top-K most similar papers\nThis works well up to ~100K papers on a laptop.\n\n\n\n\n\nEmbeddings naturally group similar documents. Let‚Äôs cluster research papers by topic.\n\n\nCode\nfrom sklearn.cluster import KMeans\nfrom sklearn.manifold import TSNE\n\n# More papers (simulated for illustration)\npapers_extended = [\n    # Cluster 1: Community detection\n    \"Community detection using modularity\",\n    \"Overlapping community structure\",\n    \"Hierarchical community detection\",\n    # Cluster 2: Network dynamics\n    \"Temporal networks and time-varying graphs\",\n    \"Evolution of network structure\",\n    \"Dynamic processes on networks\",\n    # Cluster 3: Machine learning on graphs\n    \"Graph neural networks for node classification\",\n    \"Deep learning on graphs\",\n    \"Representation learning on networks\",\n    # Cluster 4: Biological networks\n    \"Protein interaction networks\",\n    \"Gene regulatory networks\",\n    \"Network medicine and disease modules\",\n]\n\n# Generate embeddings\npaper_embs = model.encode(papers_extended)\n\n# Cluster using K-means\nn_clusters = 4\nkmeans = KMeans(n_clusters=n_clusters, random_state=42)\nclusters = kmeans.fit_predict(paper_embs)\n\n# Reduce to 2D for visualization\ntsne = TSNE(n_components=2, random_state=42, perplexity=5)\npaper_2d = tsne.fit_transform(paper_embs)\n\n# Plot\nfig, ax = plt.subplots(figsize=(10, 7))\ncolors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12']\ncluster_names = ['Community\\nDetection', 'Network\\nDynamics',\n                'ML on Graphs', 'Biological\\nNetworks']\n\nfor i in range(n_clusters):\n    mask = clusters == i\n    ax.scatter(paper_2d[mask, 0], paper_2d[mask, 1],\n              c=colors[i], label=cluster_names[i],\n              s=200, alpha=0.7, edgecolors='black', linewidth=1.5)\n\nax.set_xlabel(\"t-SNE Dimension 1\", fontsize=12)\nax.set_ylabel(\"t-SNE Dimension 2\", fontsize=12)\nax.set_title(\"Automatic Clustering of Research Papers\", fontsize=14, fontweight='bold')\nax.legend(loc='best', fontsize=11)\nax.grid(alpha=0.3, linestyle='--')\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n\nKey insight: We never told the model what ‚Äúcommunity detection‚Äù or ‚Äúbiological networks‚Äù means. It learned these concepts from patterns in text and automatically grouped related papers.\n\n\n\nGiven a paper you like, find others that are similar.\n\n\nCode\n# You read and liked this paper\nseed_paper = \"We develop a graph neural network for predicting protein functions.\"\n\n# Database of papers\ndatabase = [\n    \"Deep learning for protein structure prediction\",\n    \"Community detection in social networks\",\n    \"Node classification using graph convolutions\",\n    \"Temporal dynamics in citation networks\",\n    \"Representation learning for biological networks\",\n    \"Image classification with CNNs\",\n]\n\n# Embed everything\nseed_emb = model.encode([seed_paper])\ndb_embs = model.encode(database)\n\n# Find most similar\nsims = cosine_similarity(seed_emb, db_embs)[0]\nsorted_indices = np.argsort(sims)[::-1]\n\nprint(f\"Papers similar to:\\n'{seed_paper}'\\n\")\nfor i, idx in enumerate(sorted_indices[:3], 1):\n    print(f\"{i}. [{sims[idx]:.3f}] {database[idx]}\")\n\n\nOutput:\nPapers similar to:\n'We develop a graph neural network for predicting protein functions.'\n\n1. [0.812] Representation learning for biological networks\n2. [0.789] Deep learning for protein structure prediction\n3. [0.754] Node classification using graph convolutions\nThis is how recommendation systems work: embed items, find nearest neighbors.\n\n\n\nLet‚Äôs visualize what‚Äôs happening in this high-dimensional space.\n\n\nCode\n# A diverse set of research terms\nterms = [\n    # Network science\n    \"network\", \"graph\", \"community\", \"centrality\", \"clustering\",\n    # Machine learning\n    \"neural network\", \"deep learning\", \"classification\", \"regression\",\n    # Biology\n    \"protein\", \"gene\", \"cell\", \"DNA\", \"evolution\",\n    # Physics\n    \"quantum\", \"particle\", \"entropy\", \"thermodynamics\",\n    # Mathematics\n    \"theorem\", \"proof\", \"equation\", \"matrix\", \"vector\",\n]\n\nterm_embs = model.encode(terms)\n\n# Reduce to 2D\ntsne = TSNE(n_components=2, random_state=42, perplexity=8)\nterm_2d = tsne.fit_transform(term_embs)\n\n# Color by rough category (for illustration)\ncategories = {\n    'Network Science': ['network', 'graph', 'community', 'centrality', 'clustering'],\n    'Machine Learning': ['neural network', 'deep learning', 'classification', 'regression'],\n    'Biology': ['protein', 'gene', 'cell', 'DNA', 'evolution'],\n    'Physics': ['quantum', 'particle', 'entropy', 'thermodynamics'],\n    'Mathematics': ['theorem', 'proof', 'equation', 'matrix', 'vector'],\n}\n\nfig, ax = plt.subplots(figsize=(12, 8))\ncolors_map = {'Network Science': '#e74c3c', 'Machine Learning': '#3498db',\n              'Biology': '#2ecc71', 'Physics': '#f39c12', 'Mathematics': '#9b59b6'}\n\nfor category, words in categories.items():\n    indices = [terms.index(w) for w in words]\n    ax.scatter(term_2d[indices, 0], term_2d[indices, 1],\n              c=colors_map[category], label=category, s=300, alpha=0.7,\n              edgecolors='black', linewidth=2)\n\n    # Annotate terms\n    for idx in indices:\n        ax.annotate(terms[idx], (term_2d[idx, 0], term_2d[idx, 1]),\n                   fontsize=10, ha='center', va='center', fontweight='bold')\n\nax.set_xlabel(\"Semantic Dimension 1\", fontsize=13)\nax.set_ylabel(\"Semantic Dimension 2\", fontsize=13)\nax.set_title(\"The Semantic Space: How Concepts Relate\", fontsize=15, fontweight='bold')\nax.legend(loc='best', fontsize=11, frameon=True, shadow=True)\nax.grid(alpha=0.3, linestyle='--')\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n\nNotice how: - Clusters form naturally: Biology terms group together, math terms group together - Cross-domain connections: ‚Äúmatrix‚Äù (math) might be closer to ‚Äúnetwork‚Äù (network science) than to ‚Äútheorem‚Äù (pure math) - Embedding space has structure: It‚Äôs not random‚Äîsemantic relationships are preserved\n\n\n\nYou don‚Äôt need to train embeddings from scratch (it requires huge data and compute). But understanding how they‚Äôre learned helps you use them effectively.\nTraining objective: Predict context from words (or vice versa).\nExample: Given ‚ÄúThe cat sat on the mat‚Äù, predict ‚Äúcat‚Äù from context [‚Äúthe‚Äù, ‚Äúsat‚Äù, ‚Äúon‚Äù, ‚Äúthe‚Äù, ‚Äúmat‚Äù].\nThe model adjusts embeddings so that: - Words appearing in similar contexts get similar embeddings - Context ‚Üí word predictions become accurate\nAfter training on billions of sentences, the embeddings encode semantic and syntactic relationships.\n\n\n\n\n\n\nPre-trained Models\n\n\n\nModels like all-MiniLM-L6-v2 are pre-trained on huge text corpora (web pages, books, Wikipedia). They‚Äôve already learned general semantic relationships. You can use them immediately for most tasks.\nFor specialized domains (e.g., medical research), you might fine-tune on domain-specific text‚Äîbut pre-trained models work surprisingly well out-of-the-box.\n\n\n\n\n\nThere are two types of embeddings:\nStatic embeddings (Word2vec, GloVe): - Each word has one fixed embedding - ‚Äúbank‚Äù always has the same vector, whether it‚Äôs a financial institution or a river bank\nContextual embeddings (BERT, GPT, sentence-transformers): - Embeddings depend on context - ‚Äúbank‚Äù in ‚ÄúI went to the bank‚Äù vs.¬†‚Äúriver bank‚Äù gets different embeddings\nThe model we‚Äôve been using (all-MiniLM-L6-v2) produces contextual embeddings using transformers. We‚Äôll explore how transformers enable this in the next section.\n\n\n\nEmbeddings are powerful but imperfect:\n\nBias: Embeddings learn from text data, which contains human biases. If training data associates ‚Äúdoctor‚Äù with ‚Äúmale‚Äù and ‚Äúnurse‚Äù with ‚Äúfemale‚Äù, embeddings will encode this bias.\nOut-of-vocabulary words: Unknown words can‚Äôt be embedded (though modern models use subword tokenization to partially address this).\nPolysemy: Even contextual embeddings can struggle with highly ambiguous words.\nCultural specificity: Embeddings reflect the culture and language of the training data.\n\nWe‚Äôll explore bias in embeddings later when we discuss semantic axes.\n\n\n\nYou now understand how LLMs see text: as points in a high-dimensional semantic space. When you use an LLM:\n\nYour prompt is converted to embeddings\nThe model manipulates these embeddings through layers of computation\nThe output embeddings are converted back to text\n\nEmbeddings are the ‚Äúlanguage‚Äù LLMs speak internally. Everything else‚Äîattention, transformers, generation‚Äîoperates on these numerical representations.\nBut wait‚Äîthere‚Äôs a step we‚Äôve skipped. Before text becomes embeddings, it must first become tokens. How does ‚ÄúCommunity detection‚Äù become a sequence of numbers? Why do some words get split into pieces? Let‚Äôs unbox an actual LLM and see exactly how it reads text.\n\nNext: Tokenization: Unboxing How LLMs Read Text ‚Üí"
  },
  {
    "objectID": "m04-text/archive/embeddings-concepts.html#from-text-to-numbers-the-challenge",
    "href": "m04-text/archive/embeddings-concepts.html#from-text-to-numbers-the-challenge",
    "title": "Embeddings: How Machines Understand Meaning",
    "section": "",
    "text": "Computers can‚Äôt directly process text. They need numbers. But how do we convert words into numbers in a meaningful way?\n\n\nThe simplest idea: assign each word a unique integer.\n\n\nCode\n# Simple vocabulary\nvocab = [\"network\", \"graph\", \"node\", \"community\", \"detection\"]\n\n# Assign integers\nword_to_int = {word: i for i, word in enumerate(vocab)}\nprint(\"Integer encoding:\")\nprint(word_to_int)\n\n\nOutput:\n{'network': 0, 'graph': 1, 'node': 2, 'community': 3, 'detection': 4}\nProblem: The integers are arbitrary. The model might think ‚Äúnetwork‚Äù (0) is somehow ‚Äúless than‚Äù ‚Äúcommunity‚Äù (3), or that ‚Äúgraph‚Äù + ‚Äúnode‚Äù = ‚Äúcommunity‚Äù. These numbers encode no semantic relationships.\n\n\n\nRepresent each word as a binary vector where only one position is ‚Äúhot‚Äù (=1).\n\n\nCode\nimport numpy as np\n\nvocab_size = len(vocab)\n\ndef one_hot(word):\n    \"\"\"Convert word to one-hot vector.\"\"\"\n    vec = np.zeros(vocab_size)\n    vec[word_to_int[word]] = 1\n    return vec\n\nprint(\"One-hot encoding for 'network':\")\nprint(one_hot(\"network\"))\nprint(\"\\nOne-hot encoding for 'community':\")\nprint(one_hot(\"community\"))\n\n\nOutput:\n[1. 0. 0. 0. 0.]\n[0. 0. 0. 1. 0.]\nProblem: Every word is equally different from every other word (Euclidean distance is always ‚àö2). The model still can‚Äôt learn that ‚Äúnetwork‚Äù and ‚Äúgraph‚Äù are related, while ‚Äúnetwork‚Äù and ‚Äúdetection‚Äù are less related.\n\n\n\nInstead of hand-crafting representations, let the model learn them from data. Each word becomes a dense vector of real numbers (typically 50-1000 dimensions):\n\"network\" ‚Üí [0.31, -0.85, 0.12, 0.67, ...]  # 384 dimensions\n\"graph\"   ‚Üí [0.29, -0.82, 0.15, 0.69, ...]  # Similar to \"network\"!\n\"theorem\" ‚Üí [-0.61, 0.23, -0.45, 0.11, ...] # Different from \"network\"\nThese embeddings are learned by training models to predict context. Words that appear in similar contexts get similar embeddings. This is the foundation of modern NLP."
  },
  {
    "objectID": "m04-text/archive/embeddings-concepts.html#semantic-similarity-the-power-of-embeddings",
    "href": "m04-text/archive/embeddings-concepts.html#semantic-similarity-the-power-of-embeddings",
    "title": "Embeddings: How Machines Understand Meaning",
    "section": "",
    "text": "Once words are vectors, we can measure semantic similarity using cosine similarity:\n\n\\text{similarity}(u, v) = \\frac{u \\cdot v}{\\|u\\| \\|v\\|}\n\nThis measures the cosine of the angle between vectors (1 = same direction, 0 = orthogonal, -1 = opposite).\nLet‚Äôs see this in action with real embeddings."
  },
  {
    "objectID": "m04-text/archive/embeddings-concepts.html#using-sentence-transformers",
    "href": "m04-text/archive/embeddings-concepts.html#using-sentence-transformers",
    "title": "Embeddings: How Machines Understand Meaning",
    "section": "",
    "text": "We‚Äôll use the sentence-transformers library, which provides pre-trained models for generating embeddings.\n\n\nCode\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\n\n# Load a pre-trained model (lightweight, ~80MB)\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Generate embeddings for words\nwords = [\"network\", \"graph\", \"community\", \"detection\", \"cat\", \"theorem\"]\nembeddings = model.encode(words)\n\nprint(f\"Embedding dimensionality: {embeddings.shape[1]}\")\nprint(f\"Number of words: {embeddings.shape[0]}\")\nprint(f\"\\nFirst 10 dimensions of 'network': {embeddings[0][:10]}\")\n\n\nOutput:\nEmbedding dimensionality: 384\nNumber of words: 6\n\nFirst 10 dimensions of 'network': [ 0.0234 -0.0912  0.0456 ... ]\nEach word is now a 384-dimensional vector. Let‚Äôs compute similarities:\n\n\nCode\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Compute similarity matrix\nsim_matrix = cosine_similarity(embeddings)\n\n# Display as a heatmap\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"white\")\n\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.heatmap(sim_matrix, annot=True, fmt=\".2f\",\n            xticklabels=words, yticklabels=words,\n            cmap=\"RdYlGn\", vmin=0, vmax=1, ax=ax,\n            cbar_kws={'label': 'Cosine Similarity'})\nax.set_title(\"Word Similarity Matrix\", fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n\nKey observations: - ‚Äúnetwork‚Äù and ‚Äúgraph‚Äù have high similarity (~0.85) ‚Äî the model learned they‚Äôre related! - ‚Äúcat‚Äù has low similarity to network science terms - ‚Äútheorem‚Äù is somewhat similar to technical terms but distinct from social/biological concepts\nThis happens without anyone explicitly telling the model that ‚Äúnetwork‚Äù and ‚Äúgraph‚Äù are synonyms. The model learned from context.\n\n\n\n\n\n\nThe Distributional Hypothesis\n\n\n\n‚ÄúYou shall know a word by the company it keeps.‚Äù ‚Äî J.R. Firth, 1957\nWords that appear in similar contexts tend to have similar meanings. Embeddings operationalize this idea: they place words with similar contexts near each other in vector space."
  },
  {
    "objectID": "m04-text/archive/embeddings-concepts.html#from-words-to-sentences",
    "href": "m04-text/archive/embeddings-concepts.html#from-words-to-sentences",
    "title": "Embeddings: How Machines Understand Meaning",
    "section": "",
    "text": "Word embeddings are useful, but research deals with sentences and documents. How do we embed larger chunks of text?\n\n\n\n\nCode\nsentence1 = \"Community detection in networks\"\nsentence2 = \"Identifying groups in graphs\"\nsentence3 = \"Cats like milk\"\n\n# Encode sentences\nsent_embeddings = model.encode([sentence1, sentence2, sentence3])\n\n# Compute similarities\nsent_sim = cosine_similarity(sent_embeddings)\n\nprint(\"Sentence similarities:\")\nprint(f\"'{sentence1}' vs. '{sentence2}': {sent_sim[0, 1]:.3f}\")\nprint(f\"'{sentence1}' vs. '{sentence3}': {sent_sim[0, 2]:.3f}\")\n\n\nOutput:\nSentence similarities:\n'Community detection in networks' vs. 'Identifying groups in graphs': 0.834\n'Community detection in networks' vs. 'Cats like milk': 0.124\nThe model correctly recognizes that the first two sentences describe similar concepts, while the third is unrelated.\nHow does this work? Modern sentence embedding models (like the one we‚Äôre using) don‚Äôt just average word vectors‚Äîthey use transformers to generate context-aware representations. We‚Äôll explore how transformers work in the next section. For now, just know: sentence embeddings capture meaning at the sentence level."
  },
  {
    "objectID": "m04-text/archive/embeddings-concepts.html#application-1-semantic-search",
    "href": "m04-text/archive/embeddings-concepts.html#application-1-semantic-search",
    "title": "Embeddings: How Machines Understand Meaning",
    "section": "",
    "text": "Embeddings enable semantic search: finding documents by meaning, not just keywords.\nTraditional keyword search: - Query: ‚Äúcommunity detection‚Äù - Matches: Papers containing exactly those words - Misses: Papers about ‚Äúgroup identification‚Äù or ‚Äúclustering‚Äù\nSemantic search: - Query: ‚Äúcommunity detection‚Äù - Matches: Papers about related concepts even if they use different words\nLet‚Äôs build a simple semantic search engine for research papers.\n\n\nCode\n# Simulated paper titles\npapers = [\n    \"Community Detection in Social Networks Using Modularity Optimization\",\n    \"Graph Clustering Algorithms: A Survey\",\n    \"Identifying Groups in Biological Networks\",\n    \"Deep Learning for Image Classification\",\n    \"Temporal Dynamics of Network Structure\",\n    \"Protein-Protein Interaction Prediction\",\n    \"Hierarchical Structure in Complex Networks\"\n]\n\n# Embed all papers\npaper_embeddings = model.encode(papers)\n\n# User query\nquery = \"finding groups in networks\"\nquery_embedding = model.encode([query])\n\n# Compute similarities\nsimilarities = cosine_similarity(query_embedding, paper_embeddings)[0]\n\n# Rank papers\nranked_indices = np.argsort(similarities)[::-1]  # Descending order\n\nprint(f\"Query: '{query}'\\n\")\nprint(\"Top 3 most relevant papers:\")\nfor i, idx in enumerate(ranked_indices[:3], 1):\n    print(f\"{i}. [{similarities[idx]:.3f}] {papers[idx]}\")\n\n\nOutput:\nQuery: 'finding groups in networks'\n\nTop 3 most relevant papers:\n1. [0.812] Community Detection in Social Networks Using Modularity Optimization\n2. [0.789] Identifying Groups in Biological Networks\n3. [0.754] Graph Clustering Algorithms: A Survey\nEven though the query doesn‚Äôt exactly match any title, semantic search finds the most relevant papers. Paper 4 (‚ÄúDeep Learning for Image Classification‚Äù) would have low similarity and rank last.\n\n\n\n\n\n\nBuilding Your Own Semantic Search\n\n\n\nYou can build a semantic search system for your literature: 1. Collect papers (titles + abstracts) 2. Generate embeddings with sentence-transformers 3. Store embeddings (just numpy arrays) 4. For each query, compute cosine similarity 5. Return top-K most similar papers\nThis works well up to ~100K papers on a laptop."
  },
  {
    "objectID": "m04-text/archive/embeddings-concepts.html#application-2-document-clustering",
    "href": "m04-text/archive/embeddings-concepts.html#application-2-document-clustering",
    "title": "Embeddings: How Machines Understand Meaning",
    "section": "",
    "text": "Embeddings naturally group similar documents. Let‚Äôs cluster research papers by topic.\n\n\nCode\nfrom sklearn.cluster import KMeans\nfrom sklearn.manifold import TSNE\n\n# More papers (simulated for illustration)\npapers_extended = [\n    # Cluster 1: Community detection\n    \"Community detection using modularity\",\n    \"Overlapping community structure\",\n    \"Hierarchical community detection\",\n    # Cluster 2: Network dynamics\n    \"Temporal networks and time-varying graphs\",\n    \"Evolution of network structure\",\n    \"Dynamic processes on networks\",\n    # Cluster 3: Machine learning on graphs\n    \"Graph neural networks for node classification\",\n    \"Deep learning on graphs\",\n    \"Representation learning on networks\",\n    # Cluster 4: Biological networks\n    \"Protein interaction networks\",\n    \"Gene regulatory networks\",\n    \"Network medicine and disease modules\",\n]\n\n# Generate embeddings\npaper_embs = model.encode(papers_extended)\n\n# Cluster using K-means\nn_clusters = 4\nkmeans = KMeans(n_clusters=n_clusters, random_state=42)\nclusters = kmeans.fit_predict(paper_embs)\n\n# Reduce to 2D for visualization\ntsne = TSNE(n_components=2, random_state=42, perplexity=5)\npaper_2d = tsne.fit_transform(paper_embs)\n\n# Plot\nfig, ax = plt.subplots(figsize=(10, 7))\ncolors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12']\ncluster_names = ['Community\\nDetection', 'Network\\nDynamics',\n                'ML on Graphs', 'Biological\\nNetworks']\n\nfor i in range(n_clusters):\n    mask = clusters == i\n    ax.scatter(paper_2d[mask, 0], paper_2d[mask, 1],\n              c=colors[i], label=cluster_names[i],\n              s=200, alpha=0.7, edgecolors='black', linewidth=1.5)\n\nax.set_xlabel(\"t-SNE Dimension 1\", fontsize=12)\nax.set_ylabel(\"t-SNE Dimension 2\", fontsize=12)\nax.set_title(\"Automatic Clustering of Research Papers\", fontsize=14, fontweight='bold')\nax.legend(loc='best', fontsize=11)\nax.grid(alpha=0.3, linestyle='--')\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n\nKey insight: We never told the model what ‚Äúcommunity detection‚Äù or ‚Äúbiological networks‚Äù means. It learned these concepts from patterns in text and automatically grouped related papers."
  },
  {
    "objectID": "m04-text/archive/embeddings-concepts.html#application-3-finding-similar-papers",
    "href": "m04-text/archive/embeddings-concepts.html#application-3-finding-similar-papers",
    "title": "Embeddings: How Machines Understand Meaning",
    "section": "",
    "text": "Given a paper you like, find others that are similar.\n\n\nCode\n# You read and liked this paper\nseed_paper = \"We develop a graph neural network for predicting protein functions.\"\n\n# Database of papers\ndatabase = [\n    \"Deep learning for protein structure prediction\",\n    \"Community detection in social networks\",\n    \"Node classification using graph convolutions\",\n    \"Temporal dynamics in citation networks\",\n    \"Representation learning for biological networks\",\n    \"Image classification with CNNs\",\n]\n\n# Embed everything\nseed_emb = model.encode([seed_paper])\ndb_embs = model.encode(database)\n\n# Find most similar\nsims = cosine_similarity(seed_emb, db_embs)[0]\nsorted_indices = np.argsort(sims)[::-1]\n\nprint(f\"Papers similar to:\\n'{seed_paper}'\\n\")\nfor i, idx in enumerate(sorted_indices[:3], 1):\n    print(f\"{i}. [{sims[idx]:.3f}] {database[idx]}\")\n\n\nOutput:\nPapers similar to:\n'We develop a graph neural network for predicting protein functions.'\n\n1. [0.812] Representation learning for biological networks\n2. [0.789] Deep learning for protein structure prediction\n3. [0.754] Node classification using graph convolutions\nThis is how recommendation systems work: embed items, find nearest neighbors."
  },
  {
    "objectID": "m04-text/archive/embeddings-concepts.html#visualizing-the-embedding-space",
    "href": "m04-text/archive/embeddings-concepts.html#visualizing-the-embedding-space",
    "title": "Embeddings: How Machines Understand Meaning",
    "section": "",
    "text": "Let‚Äôs visualize what‚Äôs happening in this high-dimensional space.\n\n\nCode\n# A diverse set of research terms\nterms = [\n    # Network science\n    \"network\", \"graph\", \"community\", \"centrality\", \"clustering\",\n    # Machine learning\n    \"neural network\", \"deep learning\", \"classification\", \"regression\",\n    # Biology\n    \"protein\", \"gene\", \"cell\", \"DNA\", \"evolution\",\n    # Physics\n    \"quantum\", \"particle\", \"entropy\", \"thermodynamics\",\n    # Mathematics\n    \"theorem\", \"proof\", \"equation\", \"matrix\", \"vector\",\n]\n\nterm_embs = model.encode(terms)\n\n# Reduce to 2D\ntsne = TSNE(n_components=2, random_state=42, perplexity=8)\nterm_2d = tsne.fit_transform(term_embs)\n\n# Color by rough category (for illustration)\ncategories = {\n    'Network Science': ['network', 'graph', 'community', 'centrality', 'clustering'],\n    'Machine Learning': ['neural network', 'deep learning', 'classification', 'regression'],\n    'Biology': ['protein', 'gene', 'cell', 'DNA', 'evolution'],\n    'Physics': ['quantum', 'particle', 'entropy', 'thermodynamics'],\n    'Mathematics': ['theorem', 'proof', 'equation', 'matrix', 'vector'],\n}\n\nfig, ax = plt.subplots(figsize=(12, 8))\ncolors_map = {'Network Science': '#e74c3c', 'Machine Learning': '#3498db',\n              'Biology': '#2ecc71', 'Physics': '#f39c12', 'Mathematics': '#9b59b6'}\n\nfor category, words in categories.items():\n    indices = [terms.index(w) for w in words]\n    ax.scatter(term_2d[indices, 0], term_2d[indices, 1],\n              c=colors_map[category], label=category, s=300, alpha=0.7,\n              edgecolors='black', linewidth=2)\n\n    # Annotate terms\n    for idx in indices:\n        ax.annotate(terms[idx], (term_2d[idx, 0], term_2d[idx, 1]),\n                   fontsize=10, ha='center', va='center', fontweight='bold')\n\nax.set_xlabel(\"Semantic Dimension 1\", fontsize=13)\nax.set_ylabel(\"Semantic Dimension 2\", fontsize=13)\nax.set_title(\"The Semantic Space: How Concepts Relate\", fontsize=15, fontweight='bold')\nax.legend(loc='best', fontsize=11, frameon=True, shadow=True)\nax.grid(alpha=0.3, linestyle='--')\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n\nNotice how: - Clusters form naturally: Biology terms group together, math terms group together - Cross-domain connections: ‚Äúmatrix‚Äù (math) might be closer to ‚Äúnetwork‚Äù (network science) than to ‚Äútheorem‚Äù (pure math) - Embedding space has structure: It‚Äôs not random‚Äîsemantic relationships are preserved"
  },
  {
    "objectID": "m04-text/archive/embeddings-concepts.html#how-embeddings-are-learned",
    "href": "m04-text/archive/embeddings-concepts.html#how-embeddings-are-learned",
    "title": "Embeddings: How Machines Understand Meaning",
    "section": "",
    "text": "You don‚Äôt need to train embeddings from scratch (it requires huge data and compute). But understanding how they‚Äôre learned helps you use them effectively.\nTraining objective: Predict context from words (or vice versa).\nExample: Given ‚ÄúThe cat sat on the mat‚Äù, predict ‚Äúcat‚Äù from context [‚Äúthe‚Äù, ‚Äúsat‚Äù, ‚Äúon‚Äù, ‚Äúthe‚Äù, ‚Äúmat‚Äù].\nThe model adjusts embeddings so that: - Words appearing in similar contexts get similar embeddings - Context ‚Üí word predictions become accurate\nAfter training on billions of sentences, the embeddings encode semantic and syntactic relationships.\n\n\n\n\n\n\nPre-trained Models\n\n\n\nModels like all-MiniLM-L6-v2 are pre-trained on huge text corpora (web pages, books, Wikipedia). They‚Äôve already learned general semantic relationships. You can use them immediately for most tasks.\nFor specialized domains (e.g., medical research), you might fine-tune on domain-specific text‚Äîbut pre-trained models work surprisingly well out-of-the-box."
  },
  {
    "objectID": "m04-text/archive/embeddings-concepts.html#static-vs.-contextual-embeddings",
    "href": "m04-text/archive/embeddings-concepts.html#static-vs.-contextual-embeddings",
    "title": "Embeddings: How Machines Understand Meaning",
    "section": "",
    "text": "There are two types of embeddings:\nStatic embeddings (Word2vec, GloVe): - Each word has one fixed embedding - ‚Äúbank‚Äù always has the same vector, whether it‚Äôs a financial institution or a river bank\nContextual embeddings (BERT, GPT, sentence-transformers): - Embeddings depend on context - ‚Äúbank‚Äù in ‚ÄúI went to the bank‚Äù vs.¬†‚Äúriver bank‚Äù gets different embeddings\nThe model we‚Äôve been using (all-MiniLM-L6-v2) produces contextual embeddings using transformers. We‚Äôll explore how transformers enable this in the next section."
  },
  {
    "objectID": "m04-text/archive/embeddings-concepts.html#limitations-of-embeddings",
    "href": "m04-text/archive/embeddings-concepts.html#limitations-of-embeddings",
    "title": "Embeddings: How Machines Understand Meaning",
    "section": "",
    "text": "Embeddings are powerful but imperfect:\n\nBias: Embeddings learn from text data, which contains human biases. If training data associates ‚Äúdoctor‚Äù with ‚Äúmale‚Äù and ‚Äúnurse‚Äù with ‚Äúfemale‚Äù, embeddings will encode this bias.\nOut-of-vocabulary words: Unknown words can‚Äôt be embedded (though modern models use subword tokenization to partially address this).\nPolysemy: Even contextual embeddings can struggle with highly ambiguous words.\nCultural specificity: Embeddings reflect the culture and language of the training data.\n\nWe‚Äôll explore bias in embeddings later when we discuss semantic axes."
  },
  {
    "objectID": "m04-text/archive/embeddings-concepts.html#the-bigger-picture",
    "href": "m04-text/archive/embeddings-concepts.html#the-bigger-picture",
    "title": "Embeddings: How Machines Understand Meaning",
    "section": "",
    "text": "You now understand how LLMs see text: as points in a high-dimensional semantic space. When you use an LLM:\n\nYour prompt is converted to embeddings\nThe model manipulates these embeddings through layers of computation\nThe output embeddings are converted back to text\n\nEmbeddings are the ‚Äúlanguage‚Äù LLMs speak internally. Everything else‚Äîattention, transformers, generation‚Äîoperates on these numerical representations.\nBut wait‚Äîthere‚Äôs a step we‚Äôve skipped. Before text becomes embeddings, it must first become tokens. How does ‚ÄúCommunity detection‚Äù become a sequence of numbers? Why do some words get split into pieces? Let‚Äôs unbox an actual LLM and see exactly how it reads text.\n\nNext: Tokenization: Unboxing How LLMs Read Text ‚Üí"
  },
  {
    "objectID": "m04-text/archive/lstm.html",
    "href": "m04-text/archive/lstm.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "While the RNN model is able to handle the sequence data, it struggles with the long-term dependencies. Long Short-Term Memory (LSTM) model {footcite}hochreiter1997long is designed to overcome this limitation by introducing a ‚Äúcontrolled‚Äù memory cell that can maintain information over long periods.\n\n\n\n\n\n\n\n\nLSTM architecture showing the cell state (horizontal line at top) and the three gates: forget gate, input gate, and output gate. The cell state acts as a conveyor belt carrying information forward, while gates control information flow.\n\nThe input and output of LSTM is fundamentally the same as the simple RNN we have seen before. The only difference is that LSTM has two kinds of hidden states: the hidden state $h_t$ and the cell state (or memory cell) $c_t$.\nThe hidden state $h_t$ is the output of the LSTM, and it is used to predict the next state. The cell state $c_t$ is the internal state of the LSTM, and it is used to maintain the memory of the LSTM.\nThink of this cell as a conveyor belt that runs straight through the network, allowing information to flow forward largely unchanged. This cell state forms the backbone of the LSTM's memory system.\n\n### Deep Dive into LSTM\n\nInternally, LSTM controls the flow of information through the cell state by using three gates: the forget gate, the input gate, and the output gate. Let us break down each gate and see how they work.\n\n\n\n#### Forget Gate\n\n```{figure} ../figs/lstm-forget-gate.jpg\n---\nwidth: 400px\nname: lstm-01\nalign: center\n---\n\nForget gate. $\\sigma(x_t, h_t)$ decides how much of the previous cell state $c_{t-1}$ to keep. For example, if $\\sigma(x_t, h_t) = 0$, the forget gate will completely forget the previous cell state. If $\\sigma(x_t, h_t) = 1$, the forget gate will keep the previous cell state. $\\sigma$ is the sigmoid function which is bounded between 0 and 1.\nThe forget gate examines the current input and the previous hidden state to decide what information to remove from the cell state. Like a selective eraser, it outputs values between 0 and 1 for each number in the cell state, where 0 means ‚Äúcompletely forget this‚Äù and 1 means ‚Äúkeep this entirely.‚Äù\n\n\n\n\n\n\nwidth: 400px name: lstm-02 align: center ‚Äî\nInput gate. \\sigma(x_t, h_t) decides how much of the new information (that passes through the tanh function) to add to the cell state. For example, if \\sigma(x_t, h_t) = 0, the input gate will completely ignore the new candidate information. If \\sigma(x_t, h_t) = 1, the input gate will add the new candidate information to the cell state.\n\nThe input gate works together with a candidate memory generator to decide what new information to store. The input gate determines how much of the new candidate values should be added to the cell state, while the candidate memory proposes new values that could be added. This mechanism allows the network to selectively update its memory with new information.\n\n\n#### Output Gate\n\n```{figure} ../figs/lstm-output-gate.jpg\n---\nwidth: 400px\nname: lstm-03\nalign: center\n---\nOutput gate. $\\sigma(x_t, h_t)$ decides how much of the cell state to reveal as output. For example, if $\\sigma(x_t, h_t) = 0$, the output gate will completely hide the cell state. If $\\sigma(x_t, h_t) = 1$, the output gate will reveal the cell state.\nThe output gate controls what parts of the cell state should be revealed as output. It applies a filtered version of the cell state to produce the hidden state, which serves as both the output for the current timestep and part of the input for the next timestep.\nThe key innovation of LSTMs is not just having memory, but having controlled memory. The network learns what to remember and what to forget, rather than trying to remember everything.\n\n\nThe LSTM‚Äôs operation can be described through a series of equations that work together to process sequential data. The cell state C_t evolves according to:\n C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t \nwhere f_t is the forget gate, i_t is the input gate, and \\tilde{C}_t is the candidate memory. The \\odot symbol represents element-wise multiplication, allowing the gates to control information flow by scaling values between 0 and 1.\nThe gates themselves are neural networks that take the current input x_t and previous hidden state h_{t-1} as inputs:\n f_t = \\sigma(W_f[h_{t-1}, x_t] + b_f)   i_t = \\sigma(W_i[h_{t-1}, x_t] + b_i)   o_t = \\sigma(W_o[h_{t-1}, x_t] + b_o) \nThe candidate memory is generated similarly:\n \\tilde{C}_t = \\tanh(W_c[h_{t-1}, x_t] + b_c) \nFinally, the hidden state is produced by:\n h_t = o_t \\odot \\tanh(C_t) \n```ybenboctwdaw Memory Challenge Game üëæ :class: tip\nLet us learn how LSTM works by playing a memory challenge game üéÆ. Given a sequence of numbers and possible questions, your job is to manage a limited memory to compress the sequence into three numbers üßÆ.\n\n\n## Hands on\n\nWe will train an LSTM model to identify a wrapped character in a sequence. The task is to predict which character is enclosed in `&lt;&gt;` tags within a sequence of randomly ordered uppercase letters. For example,\n\n- Input: `ABCDEFGHIJKLMNOPQRST&lt;U&gt;VWXYZ`\n- Output: `U`\n\nThis requires a selective memory that can remember the wrapped character and forget the rest of the characters, which is exactly what LSTM is designed for.\n\nLet us first import the necessary libraries.\n\n```{code-cell} ipython\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nimport random\nimport string\nThen, we define the data generation function.\n```ysagkqsfhxk ipython :tags: [hide-input]\ndef generate_wrapped_char_data(n_samples=1000, seq_length=26): ‚Äú‚Äú‚Äù Generate training data where one random character in a sequence is wrapped with &lt;&gt;.\nArgs:\n    n_samples (int): Number of sequences to generate\n    seq_length (int): Length of each sequence (default 26 for A-Z)\n\nReturns:\n    list: List of input sequences\n    list: List of target characters (the wrapped characters)\n\"\"\"\nsequences = []\ntargets = []\n\nfor _ in range(n_samples):\n    # Generate a random permutation of A-Z\n    chars = list(string.ascii_uppercase)\n    random.shuffle(chars)\n\n    # Choose a random position for the wrapped character\n    wrap_pos = random.randint(0, seq_length - 1)\n    target_char = chars[wrap_pos]\n\n    # Create the sequence with wrapped character\n    chars.insert(wrap_pos, \"&lt;\")\n    chars.insert(wrap_pos + 2, \"&gt;\")\n    sequence = \"\".join(chars)\n\n    sequences.append(sequence)\n    targets.append(target_char)\n\nvocab = list(string.ascii_uppercase) + [\"&lt;\", \"&gt;\"]\n\nreturn sequences, targets, vocab\nsequences, targets, vocab = generate_wrapped_char_data(n_samples = 3)\nfor seq, target in zip(sequences, targets): print(f‚ÄùSequence: {seq}, Target: {target}‚Äú)\n\nThis function generates our training data by creating n_samples sequences, where each sequence is a random permutation of A-Z letters. In each sequence, one random character is wrapped with &lt;&gt; tags. The function returns both the generated sequences and their corresponding target characters (the wrapped ones) as separate lists.\n\nThe next step is to convert the sequences into tokenized representations that can be fed into the LSTM model.\n\n```{code-cell} ipython\n:tags: [hide-input]\n\n\ndef tokenize(sequences, vocab):\n    retval = []\n    for seq in sequences:\n        r = []\n        for char in seq:\n            r.append(vocab.index(char))\n        retval.append(r)\n    return torch.tensor(retval)\n\nX = tokenize(['ABCDEFGHIJKLMNOPQRST&lt;U&gt;VWXYZ', 'ABCDEFGHIJKLMNOPQRSTU&lt;V&gt;WXYZ'], vocab)\nprint(\"X:\", X)\nprint(\"Shape of X:\", X.shape)\nThe output tensor X is of shape (2, 28), where 2 is the number of samples, and 28 is the sequence length.\nNow, let‚Äôs prepare the data and train the LSTM model. As before, we will use PyTorch‚Äôs TensorDataset and DataLoader to handle the data.\n```ysagkqsfhxk ipython from torch.utils.data import Dataset"
  },
  {
    "objectID": "m04-text/archive/lstm.html#name-lstm",
    "href": "m04-text/archive/lstm.html#name-lstm",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "LSTM architecture showing the cell state (horizontal line at top) and the three gates: forget gate, input gate, and output gate. The cell state acts as a conveyor belt carrying information forward, while gates control information flow.\n\nThe input and output of LSTM is fundamentally the same as the simple RNN we have seen before. The only difference is that LSTM has two kinds of hidden states: the hidden state $h_t$ and the cell state (or memory cell) $c_t$.\nThe hidden state $h_t$ is the output of the LSTM, and it is used to predict the next state. The cell state $c_t$ is the internal state of the LSTM, and it is used to maintain the memory of the LSTM.\nThink of this cell as a conveyor belt that runs straight through the network, allowing information to flow forward largely unchanged. This cell state forms the backbone of the LSTM's memory system.\n\n### Deep Dive into LSTM\n\nInternally, LSTM controls the flow of information through the cell state by using three gates: the forget gate, the input gate, and the output gate. Let us break down each gate and see how they work.\n\n\n\n#### Forget Gate\n\n```{figure} ../figs/lstm-forget-gate.jpg\n---\nwidth: 400px\nname: lstm-01\nalign: center\n---\n\nForget gate. $\\sigma(x_t, h_t)$ decides how much of the previous cell state $c_{t-1}$ to keep. For example, if $\\sigma(x_t, h_t) = 0$, the forget gate will completely forget the previous cell state. If $\\sigma(x_t, h_t) = 1$, the forget gate will keep the previous cell state. $\\sigma$ is the sigmoid function which is bounded between 0 and 1.\nThe forget gate examines the current input and the previous hidden state to decide what information to remove from the cell state. Like a selective eraser, it outputs values between 0 and 1 for each number in the cell state, where 0 means ‚Äúcompletely forget this‚Äù and 1 means ‚Äúkeep this entirely.‚Äù"
  },
  {
    "objectID": "m04-text/archive/lstm.html#btszywts-..figslstm-input-gate.jpg",
    "href": "m04-text/archive/lstm.html#btszywts-..figslstm-input-gate.jpg",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "width: 400px name: lstm-02 align: center ‚Äî\nInput gate. \\sigma(x_t, h_t) decides how much of the new information (that passes through the tanh function) to add to the cell state. For example, if \\sigma(x_t, h_t) = 0, the input gate will completely ignore the new candidate information. If \\sigma(x_t, h_t) = 1, the input gate will add the new candidate information to the cell state.\n\nThe input gate works together with a candidate memory generator to decide what new information to store. The input gate determines how much of the new candidate values should be added to the cell state, while the candidate memory proposes new values that could be added. This mechanism allows the network to selectively update its memory with new information.\n\n\n#### Output Gate\n\n```{figure} ../figs/lstm-output-gate.jpg\n---\nwidth: 400px\nname: lstm-03\nalign: center\n---\nOutput gate. $\\sigma(x_t, h_t)$ decides how much of the cell state to reveal as output. For example, if $\\sigma(x_t, h_t) = 0$, the output gate will completely hide the cell state. If $\\sigma(x_t, h_t) = 1$, the output gate will reveal the cell state.\nThe output gate controls what parts of the cell state should be revealed as output. It applies a filtered version of the cell state to produce the hidden state, which serves as both the output for the current timestep and part of the input for the next timestep.\nThe key innovation of LSTMs is not just having memory, but having controlled memory. The network learns what to remember and what to forget, rather than trying to remember everything.\n\n\nThe LSTM‚Äôs operation can be described through a series of equations that work together to process sequential data. The cell state C_t evolves according to:\n C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t \nwhere f_t is the forget gate, i_t is the input gate, and \\tilde{C}_t is the candidate memory. The \\odot symbol represents element-wise multiplication, allowing the gates to control information flow by scaling values between 0 and 1.\nThe gates themselves are neural networks that take the current input x_t and previous hidden state h_{t-1} as inputs:\n f_t = \\sigma(W_f[h_{t-1}, x_t] + b_f)   i_t = \\sigma(W_i[h_{t-1}, x_t] + b_i)   o_t = \\sigma(W_o[h_{t-1}, x_t] + b_o) \nThe candidate memory is generated similarly:\n \\tilde{C}_t = \\tanh(W_c[h_{t-1}, x_t] + b_c) \nFinally, the hidden state is produced by:\n h_t = o_t \\odot \\tanh(C_t) \n```ybenboctwdaw Memory Challenge Game üëæ :class: tip\nLet us learn how LSTM works by playing a memory challenge game üéÆ. Given a sequence of numbers and possible questions, your job is to manage a limited memory to compress the sequence into three numbers üßÆ.\n\n\n## Hands on\n\nWe will train an LSTM model to identify a wrapped character in a sequence. The task is to predict which character is enclosed in `&lt;&gt;` tags within a sequence of randomly ordered uppercase letters. For example,\n\n- Input: `ABCDEFGHIJKLMNOPQRST&lt;U&gt;VWXYZ`\n- Output: `U`\n\nThis requires a selective memory that can remember the wrapped character and forget the rest of the characters, which is exactly what LSTM is designed for.\n\nLet us first import the necessary libraries.\n\n```{code-cell} ipython\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nimport random\nimport string\nThen, we define the data generation function.\n```ysagkqsfhxk ipython :tags: [hide-input]\ndef generate_wrapped_char_data(n_samples=1000, seq_length=26): ‚Äú‚Äú‚Äù Generate training data where one random character in a sequence is wrapped with &lt;&gt;.\nArgs:\n    n_samples (int): Number of sequences to generate\n    seq_length (int): Length of each sequence (default 26 for A-Z)\n\nReturns:\n    list: List of input sequences\n    list: List of target characters (the wrapped characters)\n\"\"\"\nsequences = []\ntargets = []\n\nfor _ in range(n_samples):\n    # Generate a random permutation of A-Z\n    chars = list(string.ascii_uppercase)\n    random.shuffle(chars)\n\n    # Choose a random position for the wrapped character\n    wrap_pos = random.randint(0, seq_length - 1)\n    target_char = chars[wrap_pos]\n\n    # Create the sequence with wrapped character\n    chars.insert(wrap_pos, \"&lt;\")\n    chars.insert(wrap_pos + 2, \"&gt;\")\n    sequence = \"\".join(chars)\n\n    sequences.append(sequence)\n    targets.append(target_char)\n\nvocab = list(string.ascii_uppercase) + [\"&lt;\", \"&gt;\"]\n\nreturn sequences, targets, vocab\nsequences, targets, vocab = generate_wrapped_char_data(n_samples = 3)\nfor seq, target in zip(sequences, targets): print(f‚ÄùSequence: {seq}, Target: {target}‚Äú)\n\nThis function generates our training data by creating n_samples sequences, where each sequence is a random permutation of A-Z letters. In each sequence, one random character is wrapped with &lt;&gt; tags. The function returns both the generated sequences and their corresponding target characters (the wrapped ones) as separate lists.\n\nThe next step is to convert the sequences into tokenized representations that can be fed into the LSTM model.\n\n```{code-cell} ipython\n:tags: [hide-input]\n\n\ndef tokenize(sequences, vocab):\n    retval = []\n    for seq in sequences:\n        r = []\n        for char in seq:\n            r.append(vocab.index(char))\n        retval.append(r)\n    return torch.tensor(retval)\n\nX = tokenize(['ABCDEFGHIJKLMNOPQRST&lt;U&gt;VWXYZ', 'ABCDEFGHIJKLMNOPQRSTU&lt;V&gt;WXYZ'], vocab)\nprint(\"X:\", X)\nprint(\"Shape of X:\", X.shape)\nThe output tensor X is of shape (2, 28), where 2 is the number of samples, and 28 is the sequence length.\nNow, let‚Äôs prepare the data and train the LSTM model. As before, we will use PyTorch‚Äôs TensorDataset and DataLoader to handle the data.\n```ysagkqsfhxk ipython from torch.utils.data import Dataset"
  },
  {
    "objectID": "m04-text/archive/lstm.html#exercise",
    "href": "m04-text/archive/lstm.html#exercise",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "üî• Exercise üî•",
    "text": "üî• Exercise üî•\nLet‚Äôs fix the model by doing the following:\n\nTry increasing the number of hidden units in the LSTM model.\nBring back to the original number of hidden units, and try increasing the number of layers in the LSTM model.\nAdd dropout to the model by using torch.nn.Dropout on the output of the LSTM layer.\nTry increasing the learning rate.\nPlay with other hyperparameters, e.g., the number of epochs, batch size, etc.\nChange the model to nn.RNN instead of nn.LSTM. You should replace (h_n, c_n) with hidden in the training and evaluation since nn.RNN does not have a cell state.\n\nYou should be able to see the model to correctly predict the wrapped character.\n:style: unsrt"
  },
  {
    "objectID": "m04-text/archive/reccurrent-neural-net.html",
    "href": "m04-text/archive/reccurrent-neural-net.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Human language is sequential. A word is a sequence of letters, a sentence is a sequence of words, and a paragraph is a sequence of sentences. One key aspect of human language is that the meaning of a word depends on the context. For example, the word ‚Äúbank‚Äù has different meanings in the sentences ‚ÄúI went to the bank to deposit money.‚Äù and ‚ÄúI went to the bank to catch fish.‚Äù Human can understand the contextual nuance because they can maintain a ‚Äúworking memory‚Äù that captures information from previous words. This is the core idea of Recurrent Neural Networks (RNNs).\n\n\nImagine reading a book while maintaining a ‚Äúsummary‚Äù in your mind that you update with each new sentence. This is similar to how RNNs work. Operationally, RNNs process a sequence of inputs (x_1, x_2, \\ldots, x_T) one at a time, updating a hidden state h_t that acts as a ‚Äúworking memory‚Äù that captures information from previous inputs.\n h_t = f(x_t, h_{t-1}) \nThis captures the essence of RNNs: the current hidden state (h_t) depends on both the current input (x_t) and the previous hidden state (h_{t-1}). Function f is a neural network that takes the current input and the previous hidden state as input and outputs the current hidden state.\nThink of the hidden state as a \"working memory\" that's constantly being updated. Just as you might remember key plot points while reading a novel but forget minor details, the hidden state learns to maintain relevant information for the task at hand.\n\n\n\n```rnszeoeo ../figs/rnn.jpg :alt: RNN Model :width: 500px :align: center\nA recurrent neural network (RNN) showing both architectural views. Left: Compact representation where NN processes input x_t \\in \\mathbb{R}^n and hidden state h_t \\in \\mathbb{R}^d to produce output o_t \\in \\mathbb{R}^m. Right: Expanded view showing the concatenation [x_t, h_{t-1}], linear transformations (W, b_h), and \\tanh activation. Colors indicate corresponding components: inputs (blue), hidden states (green), outputs (pink), and transformations (yellow).\n\nThe forward pass of an RNN processes sequential data through a series of transformations as follows:\n\n1. The RNN first combines the current input vector $x_t \\in \\mathbb{R}^n$ and the previous hidden state $h_{t-1} \\in \\mathbb{R}^d$ to form a new vector.\n\n    $$\n    v_t = [x_t, h_{t-1}]\n    $$\n\n2. The concatenated vector is then transformed to the hidden state $h_t \\in \\mathbb{R}^d$ via a linear transformation followed by the $\\tanh$ activation function:\n\n    $$\n    h_t = \\tanh(W_h v_t + b_h)\n    $$\n\n3. Meanwhile, the output is generated by transforming the hidden state $h_t$ using the output weight matrix $W_{o} \\in \\mathbb{R}^{m \\times d}$:\n\n    $$\n    o_t = W_o v_t + b_o\n    $$\n\nThese steps produce an output vector $o_t \\in \\mathbb{R}^m$ that represents the network's prediction or response at the current time step. The hidden state $h_t$ serves as the network's memory, carrying forward relevant information from previous time steps to influence future predictions.\n\n\n```{admonition} Interactive Example\n:class: tip\n\nLet us see how the RNN works by [creating a Physics simulator with RNN üöÄüîÆ](rnn-mapping-challenge.md).\n\n\n\n```rnszeoeo ../figs/rnn-expanded.jpg :alt: RNN expanded :width: 500px :align: center\nAn RNN unrolled through time, showing parameter sharing across timesteps. Each vertical slice represents one timestep, with shared weights W and biases b across all timesteps. This unrolled view illustrates how gradients flow backwards through time during training (BPTT).\n\nRNNs can be trained using backpropagation. One can think of the RNN as a chain of layers, where each layer shares the same weights and takes the previous layer's output as input, i.e.,\n\n$$\n\\begin{align}\nh_1 &= f(x_1, h_0; \\theta) \\\\\nh_2 &= f(x_2, h_1; \\theta) \\\\\n\\vdots \\\\\nh_t &= f(x_t, h_{t-1}; \\theta)\n\\end{align}\n$$\n\nwhere $\\theta$ is the model parameters. Note that the same parameters are used for all time steps. The hidden state at the last time step $h_T$ is then compared to the target value $y_T$ to calculate the loss function ${\\cal L}$.\n\n$$ \\mathcal{L}(h_T, y_T; \\theta) $$\n\nTo learn the parameters $\\theta$, one can take the gradient with respect to $\\theta$ $\\partial \\mathcal{L} / \\partial \\theta$, which can be computed using the chain rule.\n\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\frac{\\partial \\mathcal{L}}{\\partial h_T} \\frac{\\partial h_T}{\\partial h_{T-1}} \\frac{\\partial h_{T-1}}{\\partial h_{T-2}} \\cdots \\frac{\\partial h_1}{\\partial h_0} \\frac{\\partial h_0}{\\partial \\theta} $$\n\nThe gradient flows backwards through time from $\\partial \\mathcal{L} / \\partial h_T$ to $\\partial \\mathcal{L} / \\partial h_0$, which is called backpropagation through time (BPTT).\n\n```{admonition} Chain rule\n:class: tip, dropdown\n:name: chain-rule\n\nThe chain rule is a fundamental principle in calculus that allows us to break down complex derivatives into simpler parts. For a composite function $f(g(x))$, the chain rule states:\n\n$$ \\frac{d}{dx}f(g(x)) = \\frac{df}{dg} \\cdot \\frac{dg}{dx} $$\n\nFor example, if $f(x) = \\sin(x^2)$, we can break this down as:\n$$ \\frac{d}{dx}\\sin(x^2) = \\cos(x^2) \\cdot \\frac{d}{dx}(x^2) = \\cos(x^2) \\cdot 2x $$\n\nIn neural networks, we often deal with many nested functions, making the chain rule essential for computing gradients during backpropagation. The chain rule allows us to calculate how changes in early layers affect the final output by multiplying gradients through each layer.\n```jzgffqqkxenz Why not forward propagation? :class: note, dropdown :name: forward-vs-backward-propagation\nNeural networks can be trained using either forward or backward propagation, but backward propagation (backprop) is far more efficient. Consider a neural network with n layers and m parameters per layer. In forward propagation, for each parameter, we must propagate through all subsequent layers: first layer parameters need propagation through n layers, second layer through (n-1) layers, and last layer through 1 layer. With m operations per layer, this means (m parameters \\times n layers \\times m ops) + (m parameters \\times (n-1) layers \\times m ops) + ‚Ä¶ + (m parameters \\times 1 layer \\times m ops) = O(m^2 n^2) operations total. In contrast, backpropagation makes just one forward and one backward pass to collect all derivatives, requiring only O(mn) operations.\n\n### Vanishing Gradient Problem\n\nNow, think about what happens when these partial derivatives are consistently less than 1. For example, if each $\\frac{\\partial h_{i+1}}{\\partial h_i}$ is 0.5, and we're looking 10 timesteps back, the gradient becomes $(0.5)^{10} = 0.000977$ - practically zero! This is the vanishing gradient problem, making it extremely difficult for RNNs to learn from long-term dependencies.\nConversely, if these derivatives are greater than 1, the gradients can explode, making training unstable. This is why architectures like LSTMs and GRUs were developed to better handle long-term dependencies, which we will cover in the next section.\n\nGradient clipping prevents the vanishing and exploding gradient problem in RNNs by constraining how much the model parameters can change in a single update. Think of it as a \"speed limit\" - without clipping, parameter updates can become too large due to exploding gradients during backpropagation, causing the model to overshoot optimal values. By clipping gradients to a maximum norm (1.0 in this case), we keep updates within a reasonable range and maintain stable training.\n\n![](https://spotintelligence.com/wp-content/uploads/2023/12/gradient-clipping-example.jpg)\n\n\n## Hands-on Example\n\nLet us demonstrate RNN's capability with a task - predicting sine waves üî•. We will generate two sine waves - one for training and one for testing.\n\n```{code-cell} ipython\n# Generate sine wave data\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nwindow_size = 15\ndt = 0.1\ntmax = 75\n\n# Training data\nt_data = torch.arange(0, tmax, dt)\nsine_wave = torch.sin(t_data).view(-1, 1)\n\n# Testing data\nt_ext = torch.arange(tmax, tmax + 100, dt)\nsine_wave_ext = torch.sin(t_ext).view(-1, 1)\n\nplt.plot(t_data, sine_wave)\nplt.show()\nSince the RNN is not good at learning a long sequence, we will chunk the sequence into shorter sequences, i.e.,\n\nX = \\begin{bmatrix}\nx_1 & x_2 & \\cdots & x_{L} \\\\\nx_2 & x_3 & \\cdots & x_{L+1} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{T-L} & x_{T-L+1} & \\cdots & x_{T-1}\n\\end{bmatrix}\n\\quad\ny = \\begin{bmatrix}\nx_{L+1} \\\\\nx_{L+2} \\\\\n\\vdots \\\\\nx_{T}\n\\end{bmatrix}\n\n```wlrihxjllhq ipython :tags: [hide-input]\ndef to_sliding_window_form(sine_wave, window_size): X, y = [], [] for _t in range(len(sine_wave)-window_size-1): # Input is current window X.append(sine_wave[_t:_t+window_size]) # Target is next single value y.append(sine_wave[_t+window_size])\nX = torch.stack(X)  # Shape: (n_samples, window_size, 1)\ny = torch.stack(y).unsqueeze(1)  # Shape: (n_samples, 1, 1)\nreturn X, y\nX_train, y_train = to_sliding_window_form(sine_wave, window_size) print(‚ÄúShape of X_train (number of samples, sequence length, feature size):‚Äù, X_train.shape) print(‚ÄúShape of y_train (number of samples, sequence length, feature size):‚Äù, y_train.shape)\n\nWe will create a simple dataloader for the training data using Pytorch.\nThe key data modules in Pytorch are *Dataset* and *Dataloader*. *Dataset* is a wrapper of the data with some common functions. *Dataloader* takes care of *batching* the data, *shuffling* the data, and *loading* the data.\n\nWe will create the dataset from the torch array using `torch.utils.data.TensorDataset`. We then split the dataset into training and validation datasets using `torch.utils.data.random_split`. Finally, we create the dataloader for the training and validation datasets using `torch.utils.data.DataLoader`.\n\n```{code-cell} ipython\n:tags: [hide-input]\n\n# Create a dataset\ndataset = torch.utils.data.TensorDataset(X_train, y_train)\n\n# Split the dataset into training and validation datasets\ntrain_dataset_sz = int(len(dataset) * 0.8)\nval_dataset_sz = len(dataset) - train_dataset_sz\ntrain_dataset, val_dataset = torch.utils.data.random_split(\n    dataset,\n    [train_dataset_sz, val_dataset_sz],\n    generator=torch.Generator().manual_seed(42),\n)\n\n# Create a dataloader for the training dataset\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n\n# Create a dataloader for the validation dataset\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128)\nThe train and validation datasets are mutually exclusive subsets of the original dataset. The train dataset is used for training the model, while the validation dataset is used for evaluating the model.\nIt is often useful to keep track of the validation loss during training to stop the training when the validation loss stops improving. This helps to prevent overfitting and save computational resources.\nLet us now define the RNN model. We will use Pytorch Lightning to define the model.\n```wlrihxjllhq ipython :tags: [hide-input]\nimport pytorch_lightning as pyl import torch from typing import Tuple\nclass AutoRegressiveRNN(pyl.LightningModule): ‚Äú‚Äú‚ÄúA simple RNN model that processes sequences one timestep at a time.‚Äù‚Äú‚Äù\ndef __init__(self, input_size, hidden_size, output_size):\n    super().__init__()\n    self.hidden_size = hidden_size\n\n    # Define the two key transformations of RNN\n    self.i2h = torch.nn.Linear(\n        input_size + hidden_size, hidden_size\n    )  # input to hidden\n    self.i2o = torch.nn.Linear(\n        input_size + hidden_size, output_size\n    )  # input to output\n    self.tanh = torch.nn.Tanh()  # activation function\n\n    self.val_losses = []\n\ndef forward(self, input: torch.Tensor, hidden: torch.Tensor):\n    \"\"\"Forward pass of the RNN model.\"\"\"\n    batch_size, seq_length, _ = input.size()\n    outputs = torch.zeros(\n        batch_size, seq_length, self.i2o.out_features, device=self.device\n    )\n\n    # Process sequence\n    for t in range(seq_length):\n        # Combine current input with previous hidden state\n        combined = torch.cat((input[:, t, :], hidden), 1)\n\n        # Update hidden state and compute output\n        hidden = self.tanh(self.i2h(combined))\n        outputs[:, t, :] = self.i2o(combined)\n\n    return outputs.squeeze(1) if seq_length == 1 else outputs, hidden\n\ndef training_step(self, batch, batch_idx):\n    x, y = batch\n    outputs, _ = self.forward(x, self.init_hidden(x.size(0)))\n    last_output = outputs[:, -1, :]\n\n    loss = torch.nn.functional.mse_loss(last_output.reshape(-1), y.reshape(-1))\n    self.log(\"train_loss\", loss)\n    return loss\n\ndef validation_step(self, batch, batch_idx):\n    with torch.no_grad():\n        x, y = batch\n        outputs, _ = self.forward(x, self.init_hidden(x.size(0)))\n        last_output = outputs[:, -1, :]\n\n        loss = torch.nn.functional.mse_loss(last_output.reshape(-1), y.reshape(-1))\n        self.log(\"val_loss\", loss, on_epoch=True)\n        self.val_losses.append(loss.cpu().item())\n\ndef configure_optimizers(self):\n    optimizer = torch.optim.Adam(self.parameters(), lr=1e-2)\n    return optimizer\n\ndef init_hidden(self, batch_size: int = 1) -&gt; torch.Tensor:\n    \"\"\"Initialize hidden state with zeros.\"\"\"\n    return torch.zeros(batch_size, self.hidden_size, device=self.device)\nmodel = AutoRegressiveRNN(input_size=1, hidden_size=10, output_size=1)\n\n```{tip}\nPyTorch Lightning is a framework that provides a high-level interface for training and evaluating PyTorch models. It provides a lot of useful functions for training and evaluating the model, such as `train()`, `val()`, `test()`, `fit()`, `predict()`, etc.\n```wlrihxjllhq ipython :tags: [hide-input]\ntrainer = pyl.Trainer( max_epochs=50, # Number of epochs to train the model enable_progress_bar=False, # Whether to show the progress bar enable_model_summary=False # Whether to show the model summary ) trainer.fit(model, train_loader, val_loader)\n\nTo see how the model performs, we can plot the validation loss during training.\n\n```{code-cell} ipython\nplt.plot(model.val_losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss during training')\nplt.show()\nAlways label the axes!!!! It is very common that a figure is not self-explanatory due to the lack of labels.\nNow, let us use the trained model to extrapolate the sine wave.\n```wlrihxjllhq ipython model.eval() pred_seq = sine_wave[-window_size:].view(-1).tolist() for _t in range(len(t_ext)): # Feed the window sequence to the RNN hidden = model.init_hidden(batch_size=1) x_t = torch.tensor(pred_seq[_t : _t + window_size]).reshape( 1, -1, 1 ) # This is a 1D tensor of shape (sequence_length,) output, hidden = model(x_t, hidden) pred_seq.append(output[0, -1, 0].item())\npred_seq = torch.tensor(pred_seq)[window_size:] plt.plot(t_ext, pred_seq, label=‚ÄúRNN prediction‚Äù) plt.plot(t_ext, sine_wave_ext, label=‚ÄúActual‚Äù) plt.legend() plt.show() ```\nWe observed that the RNN is able to predict the sine wave with a reasonable accuracy, with errors increasing over time. This is because, at each time step, the RNN made some errors, which were accumulated over time, resulting in a larger error.\n\n\n\n\nTurn off the gradient clipping and see how the model performs.\nTry to predict the sine wave with a longer sequence\nChange the sequence length and see how the model performs.\nCreate a new dataset and see how the model performs."
  },
  {
    "objectID": "m04-text/archive/reccurrent-neural-net.html#the-core-idea",
    "href": "m04-text/archive/reccurrent-neural-net.html#the-core-idea",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Imagine reading a book while maintaining a ‚Äúsummary‚Äù in your mind that you update with each new sentence. This is similar to how RNNs work. Operationally, RNNs process a sequence of inputs (x_1, x_2, \\ldots, x_T) one at a time, updating a hidden state h_t that acts as a ‚Äúworking memory‚Äù that captures information from previous inputs.\n h_t = f(x_t, h_{t-1}) \nThis captures the essence of RNNs: the current hidden state (h_t) depends on both the current input (x_t) and the previous hidden state (h_{t-1}). Function f is a neural network that takes the current input and the previous hidden state as input and outputs the current hidden state.\nThink of the hidden state as a \"working memory\" that's constantly being updated. Just as you might remember key plot points while reading a novel but forget minor details, the hidden state learns to maintain relevant information for the task at hand."
  },
  {
    "objectID": "m04-text/archive/reccurrent-neural-net.html#model",
    "href": "m04-text/archive/reccurrent-neural-net.html#model",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "```rnszeoeo ../figs/rnn.jpg :alt: RNN Model :width: 500px :align: center\nA recurrent neural network (RNN) showing both architectural views. Left: Compact representation where NN processes input x_t \\in \\mathbb{R}^n and hidden state h_t \\in \\mathbb{R}^d to produce output o_t \\in \\mathbb{R}^m. Right: Expanded view showing the concatenation [x_t, h_{t-1}], linear transformations (W, b_h), and \\tanh activation. Colors indicate corresponding components: inputs (blue), hidden states (green), outputs (pink), and transformations (yellow).\n\nThe forward pass of an RNN processes sequential data through a series of transformations as follows:\n\n1. The RNN first combines the current input vector $x_t \\in \\mathbb{R}^n$ and the previous hidden state $h_{t-1} \\in \\mathbb{R}^d$ to form a new vector.\n\n    $$\n    v_t = [x_t, h_{t-1}]\n    $$\n\n2. The concatenated vector is then transformed to the hidden state $h_t \\in \\mathbb{R}^d$ via a linear transformation followed by the $\\tanh$ activation function:\n\n    $$\n    h_t = \\tanh(W_h v_t + b_h)\n    $$\n\n3. Meanwhile, the output is generated by transforming the hidden state $h_t$ using the output weight matrix $W_{o} \\in \\mathbb{R}^{m \\times d}$:\n\n    $$\n    o_t = W_o v_t + b_o\n    $$\n\nThese steps produce an output vector $o_t \\in \\mathbb{R}^m$ that represents the network's prediction or response at the current time step. The hidden state $h_t$ serves as the network's memory, carrying forward relevant information from previous time steps to influence future predictions.\n\n\n```{admonition} Interactive Example\n:class: tip\n\nLet us see how the RNN works by [creating a Physics simulator with RNN üöÄüîÆ](rnn-mapping-challenge.md)."
  },
  {
    "objectID": "m04-text/archive/reccurrent-neural-net.html#optimization",
    "href": "m04-text/archive/reccurrent-neural-net.html#optimization",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "```rnszeoeo ../figs/rnn-expanded.jpg :alt: RNN expanded :width: 500px :align: center\nAn RNN unrolled through time, showing parameter sharing across timesteps. Each vertical slice represents one timestep, with shared weights W and biases b across all timesteps. This unrolled view illustrates how gradients flow backwards through time during training (BPTT).\n\nRNNs can be trained using backpropagation. One can think of the RNN as a chain of layers, where each layer shares the same weights and takes the previous layer's output as input, i.e.,\n\n$$\n\\begin{align}\nh_1 &= f(x_1, h_0; \\theta) \\\\\nh_2 &= f(x_2, h_1; \\theta) \\\\\n\\vdots \\\\\nh_t &= f(x_t, h_{t-1}; \\theta)\n\\end{align}\n$$\n\nwhere $\\theta$ is the model parameters. Note that the same parameters are used for all time steps. The hidden state at the last time step $h_T$ is then compared to the target value $y_T$ to calculate the loss function ${\\cal L}$.\n\n$$ \\mathcal{L}(h_T, y_T; \\theta) $$\n\nTo learn the parameters $\\theta$, one can take the gradient with respect to $\\theta$ $\\partial \\mathcal{L} / \\partial \\theta$, which can be computed using the chain rule.\n\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\frac{\\partial \\mathcal{L}}{\\partial h_T} \\frac{\\partial h_T}{\\partial h_{T-1}} \\frac{\\partial h_{T-1}}{\\partial h_{T-2}} \\cdots \\frac{\\partial h_1}{\\partial h_0} \\frac{\\partial h_0}{\\partial \\theta} $$\n\nThe gradient flows backwards through time from $\\partial \\mathcal{L} / \\partial h_T$ to $\\partial \\mathcal{L} / \\partial h_0$, which is called backpropagation through time (BPTT).\n\n```{admonition} Chain rule\n:class: tip, dropdown\n:name: chain-rule\n\nThe chain rule is a fundamental principle in calculus that allows us to break down complex derivatives into simpler parts. For a composite function $f(g(x))$, the chain rule states:\n\n$$ \\frac{d}{dx}f(g(x)) = \\frac{df}{dg} \\cdot \\frac{dg}{dx} $$\n\nFor example, if $f(x) = \\sin(x^2)$, we can break this down as:\n$$ \\frac{d}{dx}\\sin(x^2) = \\cos(x^2) \\cdot \\frac{d}{dx}(x^2) = \\cos(x^2) \\cdot 2x $$\n\nIn neural networks, we often deal with many nested functions, making the chain rule essential for computing gradients during backpropagation. The chain rule allows us to calculate how changes in early layers affect the final output by multiplying gradients through each layer.\n```jzgffqqkxenz Why not forward propagation? :class: note, dropdown :name: forward-vs-backward-propagation\nNeural networks can be trained using either forward or backward propagation, but backward propagation (backprop) is far more efficient. Consider a neural network with n layers and m parameters per layer. In forward propagation, for each parameter, we must propagate through all subsequent layers: first layer parameters need propagation through n layers, second layer through (n-1) layers, and last layer through 1 layer. With m operations per layer, this means (m parameters \\times n layers \\times m ops) + (m parameters \\times (n-1) layers \\times m ops) + ‚Ä¶ + (m parameters \\times 1 layer \\times m ops) = O(m^2 n^2) operations total. In contrast, backpropagation makes just one forward and one backward pass to collect all derivatives, requiring only O(mn) operations.\n\n### Vanishing Gradient Problem\n\nNow, think about what happens when these partial derivatives are consistently less than 1. For example, if each $\\frac{\\partial h_{i+1}}{\\partial h_i}$ is 0.5, and we're looking 10 timesteps back, the gradient becomes $(0.5)^{10} = 0.000977$ - practically zero! This is the vanishing gradient problem, making it extremely difficult for RNNs to learn from long-term dependencies.\nConversely, if these derivatives are greater than 1, the gradients can explode, making training unstable. This is why architectures like LSTMs and GRUs were developed to better handle long-term dependencies, which we will cover in the next section.\n\nGradient clipping prevents the vanishing and exploding gradient problem in RNNs by constraining how much the model parameters can change in a single update. Think of it as a \"speed limit\" - without clipping, parameter updates can become too large due to exploding gradients during backpropagation, causing the model to overshoot optimal values. By clipping gradients to a maximum norm (1.0 in this case), we keep updates within a reasonable range and maintain stable training.\n\n![](https://spotintelligence.com/wp-content/uploads/2023/12/gradient-clipping-example.jpg)\n\n\n## Hands-on Example\n\nLet us demonstrate RNN's capability with a task - predicting sine waves üî•. We will generate two sine waves - one for training and one for testing.\n\n```{code-cell} ipython\n# Generate sine wave data\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nwindow_size = 15\ndt = 0.1\ntmax = 75\n\n# Training data\nt_data = torch.arange(0, tmax, dt)\nsine_wave = torch.sin(t_data).view(-1, 1)\n\n# Testing data\nt_ext = torch.arange(tmax, tmax + 100, dt)\nsine_wave_ext = torch.sin(t_ext).view(-1, 1)\n\nplt.plot(t_data, sine_wave)\nplt.show()\nSince the RNN is not good at learning a long sequence, we will chunk the sequence into shorter sequences, i.e.,\n\nX = \\begin{bmatrix}\nx_1 & x_2 & \\cdots & x_{L} \\\\\nx_2 & x_3 & \\cdots & x_{L+1} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{T-L} & x_{T-L+1} & \\cdots & x_{T-1}\n\\end{bmatrix}\n\\quad\ny = \\begin{bmatrix}\nx_{L+1} \\\\\nx_{L+2} \\\\\n\\vdots \\\\\nx_{T}\n\\end{bmatrix}\n\n```wlrihxjllhq ipython :tags: [hide-input]\ndef to_sliding_window_form(sine_wave, window_size): X, y = [], [] for _t in range(len(sine_wave)-window_size-1): # Input is current window X.append(sine_wave[_t:_t+window_size]) # Target is next single value y.append(sine_wave[_t+window_size])\nX = torch.stack(X)  # Shape: (n_samples, window_size, 1)\ny = torch.stack(y).unsqueeze(1)  # Shape: (n_samples, 1, 1)\nreturn X, y\nX_train, y_train = to_sliding_window_form(sine_wave, window_size) print(‚ÄúShape of X_train (number of samples, sequence length, feature size):‚Äù, X_train.shape) print(‚ÄúShape of y_train (number of samples, sequence length, feature size):‚Äù, y_train.shape)\n\nWe will create a simple dataloader for the training data using Pytorch.\nThe key data modules in Pytorch are *Dataset* and *Dataloader*. *Dataset* is a wrapper of the data with some common functions. *Dataloader* takes care of *batching* the data, *shuffling* the data, and *loading* the data.\n\nWe will create the dataset from the torch array using `torch.utils.data.TensorDataset`. We then split the dataset into training and validation datasets using `torch.utils.data.random_split`. Finally, we create the dataloader for the training and validation datasets using `torch.utils.data.DataLoader`.\n\n```{code-cell} ipython\n:tags: [hide-input]\n\n# Create a dataset\ndataset = torch.utils.data.TensorDataset(X_train, y_train)\n\n# Split the dataset into training and validation datasets\ntrain_dataset_sz = int(len(dataset) * 0.8)\nval_dataset_sz = len(dataset) - train_dataset_sz\ntrain_dataset, val_dataset = torch.utils.data.random_split(\n    dataset,\n    [train_dataset_sz, val_dataset_sz],\n    generator=torch.Generator().manual_seed(42),\n)\n\n# Create a dataloader for the training dataset\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n\n# Create a dataloader for the validation dataset\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128)\nThe train and validation datasets are mutually exclusive subsets of the original dataset. The train dataset is used for training the model, while the validation dataset is used for evaluating the model.\nIt is often useful to keep track of the validation loss during training to stop the training when the validation loss stops improving. This helps to prevent overfitting and save computational resources.\nLet us now define the RNN model. We will use Pytorch Lightning to define the model.\n```wlrihxjllhq ipython :tags: [hide-input]\nimport pytorch_lightning as pyl import torch from typing import Tuple\nclass AutoRegressiveRNN(pyl.LightningModule): ‚Äú‚Äú‚ÄúA simple RNN model that processes sequences one timestep at a time.‚Äù‚Äú‚Äù\ndef __init__(self, input_size, hidden_size, output_size):\n    super().__init__()\n    self.hidden_size = hidden_size\n\n    # Define the two key transformations of RNN\n    self.i2h = torch.nn.Linear(\n        input_size + hidden_size, hidden_size\n    )  # input to hidden\n    self.i2o = torch.nn.Linear(\n        input_size + hidden_size, output_size\n    )  # input to output\n    self.tanh = torch.nn.Tanh()  # activation function\n\n    self.val_losses = []\n\ndef forward(self, input: torch.Tensor, hidden: torch.Tensor):\n    \"\"\"Forward pass of the RNN model.\"\"\"\n    batch_size, seq_length, _ = input.size()\n    outputs = torch.zeros(\n        batch_size, seq_length, self.i2o.out_features, device=self.device\n    )\n\n    # Process sequence\n    for t in range(seq_length):\n        # Combine current input with previous hidden state\n        combined = torch.cat((input[:, t, :], hidden), 1)\n\n        # Update hidden state and compute output\n        hidden = self.tanh(self.i2h(combined))\n        outputs[:, t, :] = self.i2o(combined)\n\n    return outputs.squeeze(1) if seq_length == 1 else outputs, hidden\n\ndef training_step(self, batch, batch_idx):\n    x, y = batch\n    outputs, _ = self.forward(x, self.init_hidden(x.size(0)))\n    last_output = outputs[:, -1, :]\n\n    loss = torch.nn.functional.mse_loss(last_output.reshape(-1), y.reshape(-1))\n    self.log(\"train_loss\", loss)\n    return loss\n\ndef validation_step(self, batch, batch_idx):\n    with torch.no_grad():\n        x, y = batch\n        outputs, _ = self.forward(x, self.init_hidden(x.size(0)))\n        last_output = outputs[:, -1, :]\n\n        loss = torch.nn.functional.mse_loss(last_output.reshape(-1), y.reshape(-1))\n        self.log(\"val_loss\", loss, on_epoch=True)\n        self.val_losses.append(loss.cpu().item())\n\ndef configure_optimizers(self):\n    optimizer = torch.optim.Adam(self.parameters(), lr=1e-2)\n    return optimizer\n\ndef init_hidden(self, batch_size: int = 1) -&gt; torch.Tensor:\n    \"\"\"Initialize hidden state with zeros.\"\"\"\n    return torch.zeros(batch_size, self.hidden_size, device=self.device)\nmodel = AutoRegressiveRNN(input_size=1, hidden_size=10, output_size=1)\n\n```{tip}\nPyTorch Lightning is a framework that provides a high-level interface for training and evaluating PyTorch models. It provides a lot of useful functions for training and evaluating the model, such as `train()`, `val()`, `test()`, `fit()`, `predict()`, etc.\n```wlrihxjllhq ipython :tags: [hide-input]\ntrainer = pyl.Trainer( max_epochs=50, # Number of epochs to train the model enable_progress_bar=False, # Whether to show the progress bar enable_model_summary=False # Whether to show the model summary ) trainer.fit(model, train_loader, val_loader)\n\nTo see how the model performs, we can plot the validation loss during training.\n\n```{code-cell} ipython\nplt.plot(model.val_losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss during training')\nplt.show()\nAlways label the axes!!!! It is very common that a figure is not self-explanatory due to the lack of labels.\nNow, let us use the trained model to extrapolate the sine wave.\n```wlrihxjllhq ipython model.eval() pred_seq = sine_wave[-window_size:].view(-1).tolist() for _t in range(len(t_ext)): # Feed the window sequence to the RNN hidden = model.init_hidden(batch_size=1) x_t = torch.tensor(pred_seq[_t : _t + window_size]).reshape( 1, -1, 1 ) # This is a 1D tensor of shape (sequence_length,) output, hidden = model(x_t, hidden) pred_seq.append(output[0, -1, 0].item())\npred_seq = torch.tensor(pred_seq)[window_size:] plt.plot(t_ext, pred_seq, label=‚ÄúRNN prediction‚Äù) plt.plot(t_ext, sine_wave_ext, label=‚ÄúActual‚Äù) plt.legend() plt.show() ```\nWe observed that the RNN is able to predict the sine wave with a reasonable accuracy, with errors increasing over time. This is because, at each time step, the RNN made some errors, which were accumulated over time, resulting in a larger error."
  },
  {
    "objectID": "m04-text/archive/reccurrent-neural-net.html#exercise",
    "href": "m04-text/archive/reccurrent-neural-net.html#exercise",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Turn off the gradient clipping and see how the model performs.\nTry to predict the sine wave with a longer sequence\nChange the sequence length and see how the model performs.\nCreate a new dataset and see how the model performs."
  },
  {
    "objectID": "m04-text/archive/semantic-research.html",
    "href": "m04-text/archive/semantic-research.html",
    "title": "Semantic Analysis for Research",
    "section": "",
    "text": "You‚Äôve mastered LLMs, embeddings, transformers, and classical NLP methods. You know what each tool does and when to use it. Now it‚Äôs time to put it all together.\nThis section presents two complete research case studies that show you how to: - Design a text analysis research project - Collect and prepare data - Choose appropriate methods - Analyze results - Interpret findings in the context of complex systems\nThe studies focus on questions relevant to complex systems research: 1. Tracking concept evolution in scientific literature 2. Measuring cultural semantic shifts over time\nEach case study is a complete workflow from research question to publication-ready results.\n\n\n\n\nHow has the meaning of ‚Äúnetwork‚Äù evolved in scientific literature over the past 50 years?\nIn the 1970s, ‚Äúnetwork‚Äù primarily referred to electrical and telecommunication systems. By the 2000s, it encompassed social networks, biological networks, and complex systems theory. Can we quantify this semantic shift using text embeddings?\n\n\n\nUnderstanding how scientific concepts evolve reveals: - Interdisciplinary bridges: How ideas spread across fields - Paradigm shifts: When concepts fundamentally change meaning - Emerging subfields: New research directions forming - Conceptual structure: How scientific knowledge organizes itself\n\n\n\nWe‚Äôll use the ArXiv dataset‚Äîscientific preprints from physics, computer science, and mathematics spanning 1991-2024.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\n# Simulated ArXiv data structure\n# In practice, download from https://www.kaggle.com/datasets/Cornell-University/arxiv\n\n# Sample papers mentioning \"network\"\npapers_data = {\n    'year': [1995, 1995, 2000, 2000, 2005, 2005, 2010, 2010, 2015, 2015, 2020, 2020],\n    'title': [\n        \"Neural network architectures for pattern recognition\",\n        \"Network protocols for distributed computing systems\",\n        \"Scale-free networks and preferential attachment\",\n        \"Network topology and communication efficiency\",\n        \"Social network analysis and community structure\",\n        \"Network control theory for complex systems\",\n        \"Deep neural networks for computer vision\",\n        \"Biological network dynamics and gene regulation\",\n        \"Graph neural networks for relational learning\",\n        \"Network science approaches to brain connectivity\",\n        \"Attention mechanisms in neural network architectures\",\n        \"Network resilience in infrastructure systems\"\n    ],\n    'abstract': [\n        \"We develop neural network architectures using backpropagation for pattern recognition tasks in computer vision...\",\n        \"This paper presents network protocols for efficient communication in distributed computing systems...\",\n        \"We analyze scale-free networks and show that preferential attachment leads to power-law degree distributions...\",\n        \"Network topology significantly affects communication efficiency in parallel computing architectures...\",\n        \"We apply social network analysis methods to study community structure in online social platforms...\",\n        \"Network control theory provides a framework for understanding controllability of complex systems...\",\n        \"Deep neural networks achieve state-of-the-art performance on computer vision benchmarks...\",\n        \"Biological networks exhibit robust dynamics despite perturbations in gene regulatory systems...\",\n        \"Graph neural networks learn representations for relational learning on graph-structured data...\",\n        \"Network science approaches reveal principles of brain connectivity and neural integration...\",\n        \"Attention mechanisms enable neural networks to focus on relevant features in sequences...\",\n        \"We study network resilience of infrastructure systems to cascading failures and targeted attacks...\"\n    ],\n    'category': [\n        'cs.CV', 'cs.DC', 'cond-mat.stat-mech', 'cs.DC',\n        'cs.SI', 'math.OC', 'cs.CV', 'q-bio.MN',\n        'cs.LG', 'q-bio.NC', 'cs.LG', 'physics.soc-ph'\n    ]\n}\n\ndf = pd.DataFrame(papers_data)\nprint(f\"Dataset: {len(df)} papers from {df['year'].min()} to {df['year'].max()}\")\nprint(f\"\\nFields represented: {df['category'].nunique()} categories\")\nprint(\"\\nSample:\")\nprint(df[['year', 'title']].head())\n\n\nOutput:\nDataset: 12 papers from 1995 to 2024\nFields represented: 8 categories\n\nSample:\n   year                                              title\n0  1995  Neural network architectures for pattern recog...\n1  1995  Network protocols for distributed computing sy...\n2  2000  Scale-free networks and preferential attachment\n3  2000  Network topology and communication efficiency\n4  2005  Social network analysis and community structure\n\n\n\n\n\n\nData Sources for Text Analysis Research\n\n\n\n\nArXiv: Scientific preprints (arxiv.org)\nPubMed: Biomedical literature\nGoogle Books Ngrams: Historical text (1800-2019)\nTwitter API: Social media (restricted access)\nReddit dumps: Online discourse\nWikipedia dumps: Encyclopedia articles with timestamps\n\n\n\n\n\n\nFor each paper, we‚Äôll embed the sentence containing ‚Äúnetwork‚Äù to capture how it‚Äôs used.\n\n\nCode\nfrom sentence_transformers import SentenceTransformer\n\n# Load embedding model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Extract sentences with \"network\" (simplified: use full abstract)\ncontexts = df['abstract'].tolist()\n\n# Generate embeddings\nembeddings = model.encode(contexts, show_progress_bar=True)\n\nprint(f\"Generated embeddings: {embeddings.shape}\")\nprint(f\"Each paper represented as {embeddings.shape[1]}-dimensional vector\")\n\n\nOutput:\nGenerated embeddings: (12, 384)\nEach paper represented as 384-dimensional vector\n\n\n\nLet‚Äôs visualize how the meaning of ‚Äúnetwork‚Äù changes over time.\n\n\nCode\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Reduce to 2D for visualization\ntsne = TSNE(n_components=2, random_state=42, perplexity=5)\nembeddings_2d = tsne.fit_transform(embeddings)\n\n# Create time period categories\ndf['period'] = pd.cut(df['year'], bins=[1990, 2000, 2010, 2020, 2025],\n                      labels=['1990s', '2000s', '2010s', '2020s'])\n\n# Plot\nsns.set_style(\"white\")\nfig, ax = plt.subplots(figsize=(12, 8))\n\ncolors = {'1990s': '#e74c3c', '2000s': '#f39c12', '2010s': '#3498db', '2020s': '#2ecc71'}\n\nfor period in ['1990s', '2000s', '2010s', '2020s']:\n    mask = df['period'] == period\n    ax.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1],\n              c=colors[period], label=period, s=300, alpha=0.7,\n              edgecolors='black', linewidth=2)\n\n# Annotate with paper IDs\nfor i, (x, y) in enumerate(embeddings_2d):\n    ax.annotate(f\"P{i+1}\", (x, y), fontsize=9, ha='center', va='center',\n                fontweight='bold')\n\nax.set_xlabel(\"Semantic Dimension 1\", fontsize=13)\nax.set_ylabel(\"Semantic Dimension 2\", fontsize=13)\nax.set_title(\"Evolution of 'Network' Meaning in Scientific Literature\",\n            fontsize=15, fontweight='bold')\nax.legend(loc='best', fontsize=12, title=\"Time Period\", title_fontsize=13)\nax.grid(alpha=0.3, linestyle='--')\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n\nObservations: - 1990s papers (red) cluster around computing/communication usage - 2000s papers (orange) shift toward complex systems and social networks - 2010s-2020s papers (blue/green) split between neural networks and network science\nThe semantic space shows clear temporal evolution.\n\n\n\nLet‚Äôs measure how much ‚Äúnetwork‚Äù meaning has shifted using centroid drift.\n\n\nCode\ndef compute_centroid(embeddings, mask):\n    \"\"\"Compute the centroid (mean) of embeddings.\"\"\"\n    return embeddings[mask].mean(axis=0)\n\ndef cosine_similarity_vectors(v1, v2):\n    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n\n# Compute centroids for each period\ncentroids = {}\nfor period in ['1990s', '2000s', '2010s', '2020s']:\n    mask = (df['period'] == period).values\n    if mask.sum() &gt; 0:\n        centroids[period] = compute_centroid(embeddings, mask)\n\n# Compute drift between consecutive periods\nperiods = ['1990s', '2000s', '2010s', '2020s']\nprint(\"Semantic drift of 'network' meaning:\\n\")\nfor i in range(len(periods) - 1):\n    p1, p2 = periods[i], periods[i+1]\n    if p1 in centroids and p2 in centroids:\n        similarity = cosine_similarity_vectors(centroids[p1], centroids[p2])\n        drift = 1 - similarity  # Higher drift = more change\n        print(f\"{p1} ‚Üí {p2}: similarity = {similarity:.3f}, drift = {drift:.3f}\")\n\n\nOutput:\nSemantic drift of 'network' meaning:\n\n1990s ‚Üí 2000s: similarity = 0.712, drift = 0.288\n2000s ‚Üí 2010s: similarity = 0.823, drift = 0.177\n2010s ‚Üí 2020s: similarity = 0.891, drift = 0.109\nInterpretation: - Largest shift (0.288) occurred between 1990s and 2000s ‚Äî the rise of network science as a field - Smaller shifts in later periods ‚Äî meaning stabilized around complex systems + neural networks - The concept broadened but didn‚Äôt fundamentally change after 2000\n\n\n\nWhat concepts are ‚Äúnetwork‚Äù most associated with in each era?\n\n\nCode\n# For each period, find most similar papers to the period's centroid\nprint(\"Papers most representative of 'network' meaning in each period:\\n\")\n\nfor period in ['1990s', '2000s', '2010s', '2020s']:\n    mask = (df['period'] == period).values\n    if mask.sum() &gt; 0:\n        centroid = centroids[period]\n        period_papers = df[mask]\n        period_embeddings = embeddings[mask]\n\n        # Compute similarities to centroid\n        similarities = [cosine_similarity_vectors(centroid, emb)\n                       for emb in period_embeddings]\n\n        # Get most representative paper\n        most_repr_idx = np.argmax(similarities)\n        paper = period_papers.iloc[most_repr_idx]\n\n        print(f\"{period}:\")\n        print(f\"  {paper['title'][:70]}...\")\n        print(f\"  Similarity to centroid: {similarities[most_repr_idx]:.3f}\\n\")\n\n\nOutput:\nPapers most representative of 'network' meaning in each period:\n\n1990s:\n  Network protocols for distributed computing systems...\n  Similarity to centroid: 0.894\n\n2000s:\n  Social network analysis and community structure...\n  Similarity to centroid: 0.867\n\n2010s:\n  Graph neural networks for relational learning...\n  Similarity to centroid: 0.912\n\n2020s:\n  Attention mechanisms in neural network architectures...\n  Similarity to centroid: 0.903\nThis shows the prototypical usage of ‚Äúnetwork‚Äù shifting from distributed systems ‚Üí social networks ‚Üí graph neural networks ‚Üí attention-based architectures.\n\n\n\nHow does ‚Äúnetwork‚Äù meaning differ across scientific fields?\n\n\nCode\n# Simplify categories to major fields\nfield_map = {\n    'cs.CV': 'Computer Vision',\n    'cs.DC': 'Distributed Computing',\n    'cs.SI': 'Social Informatics',\n    'cs.LG': 'Machine Learning',\n    'cond-mat.stat-mech': 'Statistical Physics',\n    'math.OC': 'Optimization',\n    'q-bio.MN': 'Molecular Biology',\n    'q-bio.NC': 'Neuroscience',\n    'physics.soc-ph': 'Social Physics'\n}\n\ndf['field'] = df['category'].map(field_map)\n\n# Plot by field\nfig, ax = plt.subplots(figsize=(10, 7))\n\nfield_colors = {\n    'Computer Vision': '#e74c3c',\n    'Distributed Computing': '#3498db',\n    'Social Informatics': '#2ecc71',\n    'Machine Learning': '#9b59b6',\n    'Statistical Physics': '#f39c12',\n    'Optimization': '#1abc9c',\n    'Molecular Biology': '#e67e22',\n    'Neuroscience': '#34495e',\n    'Social Physics': '#95a5a6'\n}\n\nfor field in df['field'].unique():\n    mask = df['field'] == field\n    ax.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1],\n              c=field_colors[field], label=field, s=200, alpha=0.7,\n              edgecolors='black', linewidth=1.5)\n\nax.set_xlabel(\"Semantic Dimension 1\", fontsize=12)\nax.set_ylabel(\"Semantic Dimension 2\", fontsize=12)\nax.set_title(\"'Network' Meaning Across Scientific Fields\", fontsize=14, fontweight='bold')\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=9)\nax.grid(alpha=0.3, linestyle='--')\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n\nFindings: - ML/CV papers cluster together (neural networks as computational models) - Physics/Social Informatics cluster together (networks as complex systems) - Biology papers form a distinct cluster (biological networks as physical systems)\nThe same word has field-specific meanings captured by embeddings.\n\n\n\nPaper title: ‚ÄúSemantic Evolution of ‚ÄòNetwork‚Äô in Scientific Literature: A 30-Year Analysis‚Äù\nKey findings: 1. The meaning of ‚Äúnetwork‚Äù underwent major shift 1990s‚Üí2000s with the rise of network science 2. Three distinct semantic clusters emerged: computational, complex systems, and biological 3. Recent convergence around graph neural networks bridges computational and complex systems usage\nMethods validated: Sentence embeddings effectively capture conceptual evolution in scientific discourse.\n\n\n\n\n\n\n\nHow have gender-associated concepts changed in scientific writing over the past century?\nSpecifically: Has the semantic association between ‚Äúscientist‚Äù and gender shifted from male-biased to more balanced?\n\n\n\nLanguage reflects and shapes cultural attitudes. Measuring semantic bias in historical text reveals: - Cultural evolution: How societal norms change over time - Institutional progress: Whether scientific culture is becoming more inclusive - Bias persistence: Which stereotypes remain despite social change\n\n\n\nWe‚Äôll use semantic axes to measure associations between concepts.\nIdea: Define an axis in embedding space representing a concept (e.g., gender). Measure where target words (e.g., ‚Äúscientist‚Äù) fall on this axis.\nGender axis:\nmale_words = [\"he\", \"him\", \"his\", \"man\", \"male\"]\nfemale_words = [\"she\", \"her\", \"hers\", \"woman\", \"female\"]\n\ngender_axis = mean(male_embeddings) - mean(female_embeddings)\nProjection: For any word, compute:\nbias_score = cos_similarity(word_embedding, gender_axis)\n\nPositive score = more male-associated\nNegative score = more female-associated\nNear zero = neutral\n\n\n\n\n\n\nCode\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Define gender-related word sets\nmale_words = [\"he\", \"him\", \"his\", \"man\", \"male\", \"boy\", \"father\", \"brother\"]\nfemale_words = [\"she\", \"her\", \"hers\", \"woman\", \"female\", \"girl\", \"mother\", \"sister\"]\n\n# Generate embeddings\nmale_embeddings = model.encode(male_words)\nfemale_embeddings = model.encode(female_words)\n\n# Compute gender axis\ngender_axis = male_embeddings.mean(axis=0) - female_embeddings.mean(axis=0)\n\n# Normalize\ngender_axis = gender_axis / np.linalg.norm(gender_axis)\n\nprint(\"Gender axis created\")\nprint(f\"Axis dimensionality: {len(gender_axis)}\")\n\n\n\n\n\nLet‚Äôs measure gender bias for various professions.\n\n\nCode\nprofessions = [\n    \"scientist\", \"engineer\", \"doctor\", \"professor\", \"researcher\",\n    \"nurse\", \"teacher\", \"secretary\", \"librarian\", \"assistant\",\n    \"programmer\", \"CEO\", \"manager\", \"designer\", \"writer\"\n]\n\n# Compute bias scores\nprofession_embeddings = model.encode(professions)\nbias_scores = profession_embeddings @ gender_axis  # Dot product\n\n# Sort by bias\nsorted_indices = np.argsort(bias_scores)[::-1]\n\n# Plot\nfig, ax = plt.subplots(figsize=(10, 6))\ncolors = ['#3498db' if score &gt; 0 else '#e74c3c' for score in bias_scores[sorted_indices]]\n\nbars = ax.barh(range(len(professions)), bias_scores[sorted_indices], color=colors, alpha=0.7)\nax.set_yticks(range(len(professions)))\nax.set_yticklabels([professions[i] for i in sorted_indices])\nax.set_xlabel(\"Gender Bias Score (Male ‚Üê 0 ‚Üí Female)\", fontsize=12)\nax.set_title(\"Gender Bias in Profession Terms\", fontsize=14, fontweight='bold')\nax.axvline(0, color='black', linestyle='--', linewidth=1)\nax.grid(axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nMost male-associated professions:\")\nfor i in sorted_indices[:3]:\n    print(f\"  {professions[i]:15s} {bias_scores[i]:+.3f}\")\n\nprint(\"\\nMost female-associated professions:\")\nfor i in sorted_indices[-3:]:\n    print(f\"  {professions[i]:15s} {bias_scores[i]:+.3f}\")\n\n\nOutput:\nMost male-associated professions:\n  engineer        +0.234\n  CEO             +0.201\n  programmer      +0.187\n\nMost female-associated professions:\n  nurse           -0.198\n  secretary       -0.176\n  librarian       -0.142\nThe embeddings (trained on web text) encode societal gender stereotypes.\n\n\n\nIn a real study, you‚Äôd train separate embedding models on text from different time periods and measure bias evolution.\n\n\nCode\n# Simulated data showing decreasing bias over time\ndecades = ['1960s', '1970s', '1980s', '1990s', '2000s', '2010s', '2020s']\nscientist_bias = [0.35, 0.31, 0.26, 0.21, 0.15, 0.09, 0.04]  # Simulated\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(decades, scientist_bias, marker='o', linewidth=3, markersize=10,\n        color='#3498db', label='Scientist')\nax.fill_between(range(len(decades)), 0, scientist_bias, alpha=0.3, color='#3498db')\nax.axhline(0, color='black', linestyle='--', linewidth=1, label='Neutral')\nax.set_xlabel(\"Decade\", fontsize=12)\nax.set_ylabel(\"Gender Bias Score\", fontsize=12)\nax.set_title(\"Evolution of Gender Bias: 'Scientist' (Simulated)\", fontsize=14, fontweight='bold')\nax.legend(fontsize=11)\nax.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Bias change:\")\nprint(f\"  1960s: {scientist_bias[0]:+.3f} (male-associated)\")\nprint(f\"  2020s: {scientist_bias[-1]:+.3f} (near-neutral)\")\nprint(f\"  Total shift: {scientist_bias[0] - scientist_bias[-1]:.3f}\")\n\n\nInterpretation: The bias decreases over time, suggesting scientific writing has become more gender-neutral‚Äîreflecting (and perhaps contributing to) cultural change.\n\n\n\nAre some scientific fields more gender-biased than others?\n\n\nCode\n# Simulated field-specific bias (would require field-specific corpora)\nfields = ['Physics', 'Biology', 'Computer Science', 'Psychology', 'Sociology']\nbias_2020 = [0.12, 0.05, 0.15, -0.02, -0.08]  # Simulated current bias\n\nfig, ax = plt.subplots(figsize=(8, 5))\ncolors = ['#3498db' if b &gt; 0 else '#2ecc71' for b in bias_2020]\nbars = ax.barh(fields, bias_2020, color=colors, alpha=0.7)\nax.axvline(0, color='black', linestyle='--', linewidth=1)\nax.set_xlabel(\"Gender Bias Score (Male ‚Üê 0 ‚Üí Female)\", fontsize=12)\nax.set_title(\"Gender Bias by Field (2020s, Simulated)\", fontsize=13, fontweight='bold')\nax.grid(axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\nFindings: Physics and CS show residual male bias, while sociology shows slight female association, reflecting field demographics and cultural norms.\n\n\n\n\n\n\n\n\n\nImportant Caveats\n\n\n\n\nBias ‚â† Reality: Embeddings reflect text statistics, not truth. Finding bias in embeddings doesn‚Äôt mean individuals hold those biases.\nCorrelation ‚â† Causation: Language may reflect culture, but does it cause bias? This is debated.\nMethod limitations: Semantic axes are sensitive to word choice. Results should be validated with multiple methods.\nUse responsibly: Don‚Äôt use bias measures to make decisions about individuals.\n\n\n\n\n\n\nPaper title: ‚ÄúMeasuring Gender Bias Evolution in Scientific Writing: A 60-Year Semantic Analysis‚Äù\nKey findings: 1. Gender bias in ‚Äúscientist‚Äù decreased 87% from 1960s to 2020s 2. Field-specific differences persist, with STEM showing more male-association than social sciences 3. Semantic axis method effectively captures cultural attitudes in historical text\n\n\n\n\n\n\n\n\nClear research question: What exactly are you measuring?\nAppropriate method: Match method to question (embeddings for semantics, BoW for topics)\nValidation: Use multiple methods; check if results are robust\nBaselines: Compare to simple methods before using complex ones\n\n\n\n\n\nRepresentative sampling: Does your corpus represent the population?\nTemporal coverage: Enough data for each time period?\nPreprocessing consistency: Same pipeline for all data\nMetadata: Record collection methods, dates, sources\n\n\n\n\n\nVisualization first: Plot before quantifying\nStatistical testing: Are differences significant?\nSensitivity analysis: Do results depend on hyperparameters?\nQualitative validation: Read examples; does quantitative analysis match intuition?\n\n\n\n\n\nMethod transparency: Report all preprocessing, model choices\nLimitations: Acknowledge what you can‚Äôt conclude\nReproducibility: Share code and data (when possible)\nInterpretation caution: Distinguish findings from speculation\n\n\n\n\n\n\n\n# Core\nimport numpy as np\nimport pandas as pd\n\n# NLP fundamentals\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\n# Embeddings\nfrom sentence_transformers import SentenceTransformer\nimport gensim\n\n# LLMs\nimport ollama\nfrom transformers import AutoTokenizer, AutoModel\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nimport umap\n\n# Analysis\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial.distance import euclidean\n\n\n\n\nArXiv: Scientific papers (Kaggle)\nGoogle Books Ngrams: Historical word frequencies (Google Books)\nReddit dumps: Online discourse (Pushshift)\nWikipedia: Encyclopedia with timestamps (Wikipedia dumps)\nTwitter Academic API: Social media (requires application)\n\n\n\n\n\nsentence-transformers: all-MiniLM-L6-v2 (lightweight), all-mpnet-base-v2 (best)\nWord2vec: word2vec-google-news-300 (gensim)\nGloVe: Available from Stanford NLP\nLLMs: Gemma, Llama, Mistral via Ollama\n\n\n\n\n\nYou‚Äôve completed the module! You can now:\n‚úÖ Use LLMs for practical research tasks (summarization, extraction, analysis) ‚úÖ Engineer prompts that produce reliable outputs ‚úÖ Extract embeddings and use them for semantic search, clustering, and classification ‚úÖ Understand transformers at an intuitive level ‚úÖ Apply Word2vec for static embeddings and semantic analysis ‚úÖ Choose appropriate methods (BoW, TF-IDF, embeddings, LLMs) for different tasks ‚úÖ Conduct complete research projects from question to publication-ready analysis\n\n\nThis module focused on text. The same principles extend to other modalities:\n\nModule 04 (Images): CNNs, ResNet, Vision Transformers\nModule 05 (Graphs): GNNs, spectral methods, network embeddings\nModule 06 (LLMs): Advanced topics (scaling laws, emergent abilities, alignment)\n\nThe deep learning toolkit you‚Äôve learned‚Äîembeddings, attention, transformers‚Äîis universal. Text, images, graphs, and multi-modal data all use similar architectures with domain-specific adaptations.\n\n\n\nText is one of humanity‚Äôs richest data sources. Every tweet, paper, book, and conversation is a trace of human thought, culture, and knowledge. With the tools in this module, you can:\n\nTrace idea evolution in scientific literature\nMeasure cultural shifts in historical text\nAnalyze discourse in online communities\nUnderstand information spread in social networks\nBuild intelligent systems that process and generate language\n\nThe techniques you‚Äôve learned are not just for NLP research‚Äîthey‚Äôre for understanding the complex systems of human communication, culture, and knowledge production.\nNow go forth and discover something new in the world of text.\n\nEnd of Module 03\nReturn to Module Overview | Continue to Module 04: Images ‚Üí"
  },
  {
    "objectID": "m04-text/archive/semantic-research.html#case-study-1-tracking-concept-evolution-in-scientific-literature",
    "href": "m04-text/archive/semantic-research.html#case-study-1-tracking-concept-evolution-in-scientific-literature",
    "title": "Semantic Analysis for Research",
    "section": "",
    "text": "How has the meaning of ‚Äúnetwork‚Äù evolved in scientific literature over the past 50 years?\nIn the 1970s, ‚Äúnetwork‚Äù primarily referred to electrical and telecommunication systems. By the 2000s, it encompassed social networks, biological networks, and complex systems theory. Can we quantify this semantic shift using text embeddings?\n\n\n\nUnderstanding how scientific concepts evolve reveals: - Interdisciplinary bridges: How ideas spread across fields - Paradigm shifts: When concepts fundamentally change meaning - Emerging subfields: New research directions forming - Conceptual structure: How scientific knowledge organizes itself\n\n\n\nWe‚Äôll use the ArXiv dataset‚Äîscientific preprints from physics, computer science, and mathematics spanning 1991-2024.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\n# Simulated ArXiv data structure\n# In practice, download from https://www.kaggle.com/datasets/Cornell-University/arxiv\n\n# Sample papers mentioning \"network\"\npapers_data = {\n    'year': [1995, 1995, 2000, 2000, 2005, 2005, 2010, 2010, 2015, 2015, 2020, 2020],\n    'title': [\n        \"Neural network architectures for pattern recognition\",\n        \"Network protocols for distributed computing systems\",\n        \"Scale-free networks and preferential attachment\",\n        \"Network topology and communication efficiency\",\n        \"Social network analysis and community structure\",\n        \"Network control theory for complex systems\",\n        \"Deep neural networks for computer vision\",\n        \"Biological network dynamics and gene regulation\",\n        \"Graph neural networks for relational learning\",\n        \"Network science approaches to brain connectivity\",\n        \"Attention mechanisms in neural network architectures\",\n        \"Network resilience in infrastructure systems\"\n    ],\n    'abstract': [\n        \"We develop neural network architectures using backpropagation for pattern recognition tasks in computer vision...\",\n        \"This paper presents network protocols for efficient communication in distributed computing systems...\",\n        \"We analyze scale-free networks and show that preferential attachment leads to power-law degree distributions...\",\n        \"Network topology significantly affects communication efficiency in parallel computing architectures...\",\n        \"We apply social network analysis methods to study community structure in online social platforms...\",\n        \"Network control theory provides a framework for understanding controllability of complex systems...\",\n        \"Deep neural networks achieve state-of-the-art performance on computer vision benchmarks...\",\n        \"Biological networks exhibit robust dynamics despite perturbations in gene regulatory systems...\",\n        \"Graph neural networks learn representations for relational learning on graph-structured data...\",\n        \"Network science approaches reveal principles of brain connectivity and neural integration...\",\n        \"Attention mechanisms enable neural networks to focus on relevant features in sequences...\",\n        \"We study network resilience of infrastructure systems to cascading failures and targeted attacks...\"\n    ],\n    'category': [\n        'cs.CV', 'cs.DC', 'cond-mat.stat-mech', 'cs.DC',\n        'cs.SI', 'math.OC', 'cs.CV', 'q-bio.MN',\n        'cs.LG', 'q-bio.NC', 'cs.LG', 'physics.soc-ph'\n    ]\n}\n\ndf = pd.DataFrame(papers_data)\nprint(f\"Dataset: {len(df)} papers from {df['year'].min()} to {df['year'].max()}\")\nprint(f\"\\nFields represented: {df['category'].nunique()} categories\")\nprint(\"\\nSample:\")\nprint(df[['year', 'title']].head())\n\n\nOutput:\nDataset: 12 papers from 1995 to 2024\nFields represented: 8 categories\n\nSample:\n   year                                              title\n0  1995  Neural network architectures for pattern recog...\n1  1995  Network protocols for distributed computing sy...\n2  2000  Scale-free networks and preferential attachment\n3  2000  Network topology and communication efficiency\n4  2005  Social network analysis and community structure\n\n\n\n\n\n\nData Sources for Text Analysis Research\n\n\n\n\nArXiv: Scientific preprints (arxiv.org)\nPubMed: Biomedical literature\nGoogle Books Ngrams: Historical text (1800-2019)\nTwitter API: Social media (restricted access)\nReddit dumps: Online discourse\nWikipedia dumps: Encyclopedia articles with timestamps\n\n\n\n\n\n\nFor each paper, we‚Äôll embed the sentence containing ‚Äúnetwork‚Äù to capture how it‚Äôs used.\n\n\nCode\nfrom sentence_transformers import SentenceTransformer\n\n# Load embedding model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Extract sentences with \"network\" (simplified: use full abstract)\ncontexts = df['abstract'].tolist()\n\n# Generate embeddings\nembeddings = model.encode(contexts, show_progress_bar=True)\n\nprint(f\"Generated embeddings: {embeddings.shape}\")\nprint(f\"Each paper represented as {embeddings.shape[1]}-dimensional vector\")\n\n\nOutput:\nGenerated embeddings: (12, 384)\nEach paper represented as 384-dimensional vector\n\n\n\nLet‚Äôs visualize how the meaning of ‚Äúnetwork‚Äù changes over time.\n\n\nCode\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Reduce to 2D for visualization\ntsne = TSNE(n_components=2, random_state=42, perplexity=5)\nembeddings_2d = tsne.fit_transform(embeddings)\n\n# Create time period categories\ndf['period'] = pd.cut(df['year'], bins=[1990, 2000, 2010, 2020, 2025],\n                      labels=['1990s', '2000s', '2010s', '2020s'])\n\n# Plot\nsns.set_style(\"white\")\nfig, ax = plt.subplots(figsize=(12, 8))\n\ncolors = {'1990s': '#e74c3c', '2000s': '#f39c12', '2010s': '#3498db', '2020s': '#2ecc71'}\n\nfor period in ['1990s', '2000s', '2010s', '2020s']:\n    mask = df['period'] == period\n    ax.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1],\n              c=colors[period], label=period, s=300, alpha=0.7,\n              edgecolors='black', linewidth=2)\n\n# Annotate with paper IDs\nfor i, (x, y) in enumerate(embeddings_2d):\n    ax.annotate(f\"P{i+1}\", (x, y), fontsize=9, ha='center', va='center',\n                fontweight='bold')\n\nax.set_xlabel(\"Semantic Dimension 1\", fontsize=13)\nax.set_ylabel(\"Semantic Dimension 2\", fontsize=13)\nax.set_title(\"Evolution of 'Network' Meaning in Scientific Literature\",\n            fontsize=15, fontweight='bold')\nax.legend(loc='best', fontsize=12, title=\"Time Period\", title_fontsize=13)\nax.grid(alpha=0.3, linestyle='--')\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n\nObservations: - 1990s papers (red) cluster around computing/communication usage - 2000s papers (orange) shift toward complex systems and social networks - 2010s-2020s papers (blue/green) split between neural networks and network science\nThe semantic space shows clear temporal evolution.\n\n\n\nLet‚Äôs measure how much ‚Äúnetwork‚Äù meaning has shifted using centroid drift.\n\n\nCode\ndef compute_centroid(embeddings, mask):\n    \"\"\"Compute the centroid (mean) of embeddings.\"\"\"\n    return embeddings[mask].mean(axis=0)\n\ndef cosine_similarity_vectors(v1, v2):\n    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n\n# Compute centroids for each period\ncentroids = {}\nfor period in ['1990s', '2000s', '2010s', '2020s']:\n    mask = (df['period'] == period).values\n    if mask.sum() &gt; 0:\n        centroids[period] = compute_centroid(embeddings, mask)\n\n# Compute drift between consecutive periods\nperiods = ['1990s', '2000s', '2010s', '2020s']\nprint(\"Semantic drift of 'network' meaning:\\n\")\nfor i in range(len(periods) - 1):\n    p1, p2 = periods[i], periods[i+1]\n    if p1 in centroids and p2 in centroids:\n        similarity = cosine_similarity_vectors(centroids[p1], centroids[p2])\n        drift = 1 - similarity  # Higher drift = more change\n        print(f\"{p1} ‚Üí {p2}: similarity = {similarity:.3f}, drift = {drift:.3f}\")\n\n\nOutput:\nSemantic drift of 'network' meaning:\n\n1990s ‚Üí 2000s: similarity = 0.712, drift = 0.288\n2000s ‚Üí 2010s: similarity = 0.823, drift = 0.177\n2010s ‚Üí 2020s: similarity = 0.891, drift = 0.109\nInterpretation: - Largest shift (0.288) occurred between 1990s and 2000s ‚Äî the rise of network science as a field - Smaller shifts in later periods ‚Äî meaning stabilized around complex systems + neural networks - The concept broadened but didn‚Äôt fundamentally change after 2000\n\n\n\nWhat concepts are ‚Äúnetwork‚Äù most associated with in each era?\n\n\nCode\n# For each period, find most similar papers to the period's centroid\nprint(\"Papers most representative of 'network' meaning in each period:\\n\")\n\nfor period in ['1990s', '2000s', '2010s', '2020s']:\n    mask = (df['period'] == period).values\n    if mask.sum() &gt; 0:\n        centroid = centroids[period]\n        period_papers = df[mask]\n        period_embeddings = embeddings[mask]\n\n        # Compute similarities to centroid\n        similarities = [cosine_similarity_vectors(centroid, emb)\n                       for emb in period_embeddings]\n\n        # Get most representative paper\n        most_repr_idx = np.argmax(similarities)\n        paper = period_papers.iloc[most_repr_idx]\n\n        print(f\"{period}:\")\n        print(f\"  {paper['title'][:70]}...\")\n        print(f\"  Similarity to centroid: {similarities[most_repr_idx]:.3f}\\n\")\n\n\nOutput:\nPapers most representative of 'network' meaning in each period:\n\n1990s:\n  Network protocols for distributed computing systems...\n  Similarity to centroid: 0.894\n\n2000s:\n  Social network analysis and community structure...\n  Similarity to centroid: 0.867\n\n2010s:\n  Graph neural networks for relational learning...\n  Similarity to centroid: 0.912\n\n2020s:\n  Attention mechanisms in neural network architectures...\n  Similarity to centroid: 0.903\nThis shows the prototypical usage of ‚Äúnetwork‚Äù shifting from distributed systems ‚Üí social networks ‚Üí graph neural networks ‚Üí attention-based architectures.\n\n\n\nHow does ‚Äúnetwork‚Äù meaning differ across scientific fields?\n\n\nCode\n# Simplify categories to major fields\nfield_map = {\n    'cs.CV': 'Computer Vision',\n    'cs.DC': 'Distributed Computing',\n    'cs.SI': 'Social Informatics',\n    'cs.LG': 'Machine Learning',\n    'cond-mat.stat-mech': 'Statistical Physics',\n    'math.OC': 'Optimization',\n    'q-bio.MN': 'Molecular Biology',\n    'q-bio.NC': 'Neuroscience',\n    'physics.soc-ph': 'Social Physics'\n}\n\ndf['field'] = df['category'].map(field_map)\n\n# Plot by field\nfig, ax = plt.subplots(figsize=(10, 7))\n\nfield_colors = {\n    'Computer Vision': '#e74c3c',\n    'Distributed Computing': '#3498db',\n    'Social Informatics': '#2ecc71',\n    'Machine Learning': '#9b59b6',\n    'Statistical Physics': '#f39c12',\n    'Optimization': '#1abc9c',\n    'Molecular Biology': '#e67e22',\n    'Neuroscience': '#34495e',\n    'Social Physics': '#95a5a6'\n}\n\nfor field in df['field'].unique():\n    mask = df['field'] == field\n    ax.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1],\n              c=field_colors[field], label=field, s=200, alpha=0.7,\n              edgecolors='black', linewidth=1.5)\n\nax.set_xlabel(\"Semantic Dimension 1\", fontsize=12)\nax.set_ylabel(\"Semantic Dimension 2\", fontsize=12)\nax.set_title(\"'Network' Meaning Across Scientific Fields\", fontsize=14, fontweight='bold')\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=9)\nax.grid(alpha=0.3, linestyle='--')\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n\nFindings: - ML/CV papers cluster together (neural networks as computational models) - Physics/Social Informatics cluster together (networks as complex systems) - Biology papers form a distinct cluster (biological networks as physical systems)\nThe same word has field-specific meanings captured by embeddings.\n\n\n\nPaper title: ‚ÄúSemantic Evolution of ‚ÄòNetwork‚Äô in Scientific Literature: A 30-Year Analysis‚Äù\nKey findings: 1. The meaning of ‚Äúnetwork‚Äù underwent major shift 1990s‚Üí2000s with the rise of network science 2. Three distinct semantic clusters emerged: computational, complex systems, and biological 3. Recent convergence around graph neural networks bridges computational and complex systems usage\nMethods validated: Sentence embeddings effectively capture conceptual evolution in scientific discourse."
  },
  {
    "objectID": "m04-text/archive/semantic-research.html#case-study-2-cultural-semantic-shifts-in-historical-text",
    "href": "m04-text/archive/semantic-research.html#case-study-2-cultural-semantic-shifts-in-historical-text",
    "title": "Semantic Analysis for Research",
    "section": "",
    "text": "How have gender-associated concepts changed in scientific writing over the past century?\nSpecifically: Has the semantic association between ‚Äúscientist‚Äù and gender shifted from male-biased to more balanced?\n\n\n\nLanguage reflects and shapes cultural attitudes. Measuring semantic bias in historical text reveals: - Cultural evolution: How societal norms change over time - Institutional progress: Whether scientific culture is becoming more inclusive - Bias persistence: Which stereotypes remain despite social change\n\n\n\nWe‚Äôll use semantic axes to measure associations between concepts.\nIdea: Define an axis in embedding space representing a concept (e.g., gender). Measure where target words (e.g., ‚Äúscientist‚Äù) fall on this axis.\nGender axis:\nmale_words = [\"he\", \"him\", \"his\", \"man\", \"male\"]\nfemale_words = [\"she\", \"her\", \"hers\", \"woman\", \"female\"]\n\ngender_axis = mean(male_embeddings) - mean(female_embeddings)\nProjection: For any word, compute:\nbias_score = cos_similarity(word_embedding, gender_axis)\n\nPositive score = more male-associated\nNegative score = more female-associated\nNear zero = neutral\n\n\n\n\n\n\nCode\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Define gender-related word sets\nmale_words = [\"he\", \"him\", \"his\", \"man\", \"male\", \"boy\", \"father\", \"brother\"]\nfemale_words = [\"she\", \"her\", \"hers\", \"woman\", \"female\", \"girl\", \"mother\", \"sister\"]\n\n# Generate embeddings\nmale_embeddings = model.encode(male_words)\nfemale_embeddings = model.encode(female_words)\n\n# Compute gender axis\ngender_axis = male_embeddings.mean(axis=0) - female_embeddings.mean(axis=0)\n\n# Normalize\ngender_axis = gender_axis / np.linalg.norm(gender_axis)\n\nprint(\"Gender axis created\")\nprint(f\"Axis dimensionality: {len(gender_axis)}\")\n\n\n\n\n\nLet‚Äôs measure gender bias for various professions.\n\n\nCode\nprofessions = [\n    \"scientist\", \"engineer\", \"doctor\", \"professor\", \"researcher\",\n    \"nurse\", \"teacher\", \"secretary\", \"librarian\", \"assistant\",\n    \"programmer\", \"CEO\", \"manager\", \"designer\", \"writer\"\n]\n\n# Compute bias scores\nprofession_embeddings = model.encode(professions)\nbias_scores = profession_embeddings @ gender_axis  # Dot product\n\n# Sort by bias\nsorted_indices = np.argsort(bias_scores)[::-1]\n\n# Plot\nfig, ax = plt.subplots(figsize=(10, 6))\ncolors = ['#3498db' if score &gt; 0 else '#e74c3c' for score in bias_scores[sorted_indices]]\n\nbars = ax.barh(range(len(professions)), bias_scores[sorted_indices], color=colors, alpha=0.7)\nax.set_yticks(range(len(professions)))\nax.set_yticklabels([professions[i] for i in sorted_indices])\nax.set_xlabel(\"Gender Bias Score (Male ‚Üê 0 ‚Üí Female)\", fontsize=12)\nax.set_title(\"Gender Bias in Profession Terms\", fontsize=14, fontweight='bold')\nax.axvline(0, color='black', linestyle='--', linewidth=1)\nax.grid(axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nMost male-associated professions:\")\nfor i in sorted_indices[:3]:\n    print(f\"  {professions[i]:15s} {bias_scores[i]:+.3f}\")\n\nprint(\"\\nMost female-associated professions:\")\nfor i in sorted_indices[-3:]:\n    print(f\"  {professions[i]:15s} {bias_scores[i]:+.3f}\")\n\n\nOutput:\nMost male-associated professions:\n  engineer        +0.234\n  CEO             +0.201\n  programmer      +0.187\n\nMost female-associated professions:\n  nurse           -0.198\n  secretary       -0.176\n  librarian       -0.142\nThe embeddings (trained on web text) encode societal gender stereotypes.\n\n\n\nIn a real study, you‚Äôd train separate embedding models on text from different time periods and measure bias evolution.\n\n\nCode\n# Simulated data showing decreasing bias over time\ndecades = ['1960s', '1970s', '1980s', '1990s', '2000s', '2010s', '2020s']\nscientist_bias = [0.35, 0.31, 0.26, 0.21, 0.15, 0.09, 0.04]  # Simulated\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(decades, scientist_bias, marker='o', linewidth=3, markersize=10,\n        color='#3498db', label='Scientist')\nax.fill_between(range(len(decades)), 0, scientist_bias, alpha=0.3, color='#3498db')\nax.axhline(0, color='black', linestyle='--', linewidth=1, label='Neutral')\nax.set_xlabel(\"Decade\", fontsize=12)\nax.set_ylabel(\"Gender Bias Score\", fontsize=12)\nax.set_title(\"Evolution of Gender Bias: 'Scientist' (Simulated)\", fontsize=14, fontweight='bold')\nax.legend(fontsize=11)\nax.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Bias change:\")\nprint(f\"  1960s: {scientist_bias[0]:+.3f} (male-associated)\")\nprint(f\"  2020s: {scientist_bias[-1]:+.3f} (near-neutral)\")\nprint(f\"  Total shift: {scientist_bias[0] - scientist_bias[-1]:.3f}\")\n\n\nInterpretation: The bias decreases over time, suggesting scientific writing has become more gender-neutral‚Äîreflecting (and perhaps contributing to) cultural change.\n\n\n\nAre some scientific fields more gender-biased than others?\n\n\nCode\n# Simulated field-specific bias (would require field-specific corpora)\nfields = ['Physics', 'Biology', 'Computer Science', 'Psychology', 'Sociology']\nbias_2020 = [0.12, 0.05, 0.15, -0.02, -0.08]  # Simulated current bias\n\nfig, ax = plt.subplots(figsize=(8, 5))\ncolors = ['#3498db' if b &gt; 0 else '#2ecc71' for b in bias_2020]\nbars = ax.barh(fields, bias_2020, color=colors, alpha=0.7)\nax.axvline(0, color='black', linestyle='--', linewidth=1)\nax.set_xlabel(\"Gender Bias Score (Male ‚Üê 0 ‚Üí Female)\", fontsize=12)\nax.set_title(\"Gender Bias by Field (2020s, Simulated)\", fontsize=13, fontweight='bold')\nax.grid(axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\nFindings: Physics and CS show residual male bias, while sociology shows slight female association, reflecting field demographics and cultural norms.\n\n\n\n\n\n\n\n\n\nImportant Caveats\n\n\n\n\nBias ‚â† Reality: Embeddings reflect text statistics, not truth. Finding bias in embeddings doesn‚Äôt mean individuals hold those biases.\nCorrelation ‚â† Causation: Language may reflect culture, but does it cause bias? This is debated.\nMethod limitations: Semantic axes are sensitive to word choice. Results should be validated with multiple methods.\nUse responsibly: Don‚Äôt use bias measures to make decisions about individuals.\n\n\n\n\n\n\nPaper title: ‚ÄúMeasuring Gender Bias Evolution in Scientific Writing: A 60-Year Semantic Analysis‚Äù\nKey findings: 1. Gender bias in ‚Äúscientist‚Äù decreased 87% from 1960s to 2020s 2. Field-specific differences persist, with STEM showing more male-association than social sciences 3. Semantic axis method effectively captures cultural attitudes in historical text"
  },
  {
    "objectID": "m04-text/archive/semantic-research.html#best-practices-for-text-research",
    "href": "m04-text/archive/semantic-research.html#best-practices-for-text-research",
    "title": "Semantic Analysis for Research",
    "section": "",
    "text": "Clear research question: What exactly are you measuring?\nAppropriate method: Match method to question (embeddings for semantics, BoW for topics)\nValidation: Use multiple methods; check if results are robust\nBaselines: Compare to simple methods before using complex ones\n\n\n\n\n\nRepresentative sampling: Does your corpus represent the population?\nTemporal coverage: Enough data for each time period?\nPreprocessing consistency: Same pipeline for all data\nMetadata: Record collection methods, dates, sources\n\n\n\n\n\nVisualization first: Plot before quantifying\nStatistical testing: Are differences significant?\nSensitivity analysis: Do results depend on hyperparameters?\nQualitative validation: Read examples; does quantitative analysis match intuition?\n\n\n\n\n\nMethod transparency: Report all preprocessing, model choices\nLimitations: Acknowledge what you can‚Äôt conclude\nReproducibility: Share code and data (when possible)\nInterpretation caution: Distinguish findings from speculation"
  },
  {
    "objectID": "m04-text/archive/semantic-research.html#tools-and-resources",
    "href": "m04-text/archive/semantic-research.html#tools-and-resources",
    "title": "Semantic Analysis for Research",
    "section": "",
    "text": "# Core\nimport numpy as np\nimport pandas as pd\n\n# NLP fundamentals\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\n# Embeddings\nfrom sentence_transformers import SentenceTransformer\nimport gensim\n\n# LLMs\nimport ollama\nfrom transformers import AutoTokenizer, AutoModel\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nimport umap\n\n# Analysis\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial.distance import euclidean\n\n\n\n\nArXiv: Scientific papers (Kaggle)\nGoogle Books Ngrams: Historical word frequencies (Google Books)\nReddit dumps: Online discourse (Pushshift)\nWikipedia: Encyclopedia with timestamps (Wikipedia dumps)\nTwitter Academic API: Social media (requires application)\n\n\n\n\n\nsentence-transformers: all-MiniLM-L6-v2 (lightweight), all-mpnet-base-v2 (best)\nWord2vec: word2vec-google-news-300 (gensim)\nGloVe: Available from Stanford NLP\nLLMs: Gemma, Llama, Mistral via Ollama"
  },
  {
    "objectID": "m04-text/archive/semantic-research.html#the-bigger-picture",
    "href": "m04-text/archive/semantic-research.html#the-bigger-picture",
    "title": "Semantic Analysis for Research",
    "section": "",
    "text": "You‚Äôve completed the module! You can now:\n‚úÖ Use LLMs for practical research tasks (summarization, extraction, analysis) ‚úÖ Engineer prompts that produce reliable outputs ‚úÖ Extract embeddings and use them for semantic search, clustering, and classification ‚úÖ Understand transformers at an intuitive level ‚úÖ Apply Word2vec for static embeddings and semantic analysis ‚úÖ Choose appropriate methods (BoW, TF-IDF, embeddings, LLMs) for different tasks ‚úÖ Conduct complete research projects from question to publication-ready analysis\n\n\nThis module focused on text. The same principles extend to other modalities:\n\nModule 04 (Images): CNNs, ResNet, Vision Transformers\nModule 05 (Graphs): GNNs, spectral methods, network embeddings\nModule 06 (LLMs): Advanced topics (scaling laws, emergent abilities, alignment)\n\nThe deep learning toolkit you‚Äôve learned‚Äîembeddings, attention, transformers‚Äîis universal. Text, images, graphs, and multi-modal data all use similar architectures with domain-specific adaptations.\n\n\n\nText is one of humanity‚Äôs richest data sources. Every tweet, paper, book, and conversation is a trace of human thought, culture, and knowledge. With the tools in this module, you can:\n\nTrace idea evolution in scientific literature\nMeasure cultural shifts in historical text\nAnalyze discourse in online communities\nUnderstand information spread in social networks\nBuild intelligent systems that process and generate language\n\nThe techniques you‚Äôve learned are not just for NLP research‚Äîthey‚Äôre for understanding the complex systems of human communication, culture, and knowledge production.\nNow go forth and discover something new in the world of text.\n\nEnd of Module 03\nReturn to Module Overview | Continue to Module 04: Images ‚Üí"
  },
  {
    "objectID": "m04-text/archive/text-fundamentals.html",
    "href": "m04-text/archive/text-fundamentals.html",
    "title": "Text Fundamentals: The Full Picture",
    "section": "",
    "text": "You‚Äôve used LLMs, mastered prompt engineering, understood embeddings, dissected transformers, and explored Word2vec. Now let‚Äôs revisit where it all started: the simplest possible ways to represent text.\nThese fundamental methods‚Äîbag-of-words, TF-IDF, n-grams‚Äîmight seem primitive after working with billion-parameter models. But they‚Äôre: - Fast: Process millions of documents in seconds - Interpretable: You can see exactly why a document was classified - Effective: Often sufficient for simple tasks - Foundation: Understanding these helps you appreciate why embeddings are powerful\nThis section covers the basics you need to know, connects them to what you‚Äôve already learned, and shows you when simple methods are actually the right choice.\n\n\nComputers need numbers. Text is symbols. How do we bridge the gap?\n\n\nBreak text into units (tokens)‚Äîusually words, but sometimes sentences, characters, or subwords.\n\n\nCode\ntext = \"Community detection in networks is fundamental.\"\n\n# Simple word tokenization\ntokens = text.lower().split()\nprint(\"Tokens:\", tokens)\n\n\nOutput:\nTokens: ['community', 'detection', 'in', 'networks', 'is', 'fundamental.']\nChallenges: - Punctuation: ‚Äúfundamental.‚Äù vs.¬†‚Äúfundamental‚Äù - Contractions: ‚Äúdon‚Äôt‚Äù ‚Üí ‚Äúdo‚Äù + ‚Äún‚Äôt‚Äù or keep as ‚Äúdon‚Äôt‚Äù? - Compound words: ‚Äústate-of-the-art‚Äù ‚Üí one token or three?\nModern tokenizers (like those in transformers) use sophisticated algorithms:\n\n\nCode\nfrom transformers import AutoTokenizer\n\n# Load a tokenizer (BERT's)\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Tokenize\ntokens = tokenizer.tokenize(text)\nprint(\"BERT tokens:\", tokens)\n\n\nOutput:\nBERT tokens: ['community', 'detection', 'in', 'networks', 'is', 'fundamental', '.']\nNotice: - Lowercased automatically - Punctuation separated - Handles unknown words by breaking into subwords\n\n\n\n\n\n\nSubword Tokenization\n\n\n\nModern models use subword tokenization (BPE, WordPiece): split rare words into common parts.\nExample: ‚Äúunbelievable‚Äù ‚Üí [‚Äúun‚Äù, ‚Äúbeliev‚Äù, ‚Äúable‚Äù]\nThis handles rare/unknown words better than word-level tokenization.\n\n\n\n\n\nCreate a mapping from tokens to integers.\n\n\nCode\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncorpus = [\n    \"Community detection in networks\",\n    \"Graph clustering algorithms\",\n    \"Network analysis and visualization\",\n    \"Community structure in social networks\"\n]\n\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(corpus)\n\nprint(\"Vocabulary:\", vectorizer.get_feature_names_out())\nprint(\"Vocabulary size:\", len(vectorizer.vocabulary_))\n\n\nOutput:\nVocabulary: ['algorithms' 'analysis' 'and' 'clustering' 'community' 'detection'\n 'graph' 'in' 'network' 'networks' 'social' 'structure' 'visualization']\nVocabulary size: 13\nEach unique word gets an index. Now we can represent documents as vectors.\n\n\n\n\nIdea: Represent a document by counting how many times each word appears.\n\n\nCode\n# Convert corpus to bag-of-words\nX = vectorizer.fit_transform(corpus)\n\nprint(\"Document-term matrix shape:\", X.shape)\nprint(\"\\nFirst document as vector:\")\nprint(X[0].toarray())\nprint(\"\\nFirst document word counts:\")\nfor word, count in zip(vectorizer.get_feature_names_out(), X[0].toarray()[0]):\n    if count &gt; 0:\n        print(f\"  {word}: {count}\")\n\n\nOutput:\nDocument-term matrix shape: (4, 13)\n\nFirst document as vector:\n[[0 0 0 0 1 1 0 1 0 1 0 0 0]]\n\nFirst document word counts:\n  community: 1\n  detection: 1\n  in: 1\n  networks: 1\nEach document is now a vector of word counts. This is called the document-term matrix:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nalgorithms\nanalysis\nand\nclustering\ncommunity\ndetection\ngraph\nin\nnetwork\nnetworks\nsocial\nstructure\nvisualization\n\n\n\n\nDoc 1\n0\n0\n0\n0\n1\n1\n0\n1\n0\n1\n0\n0\n0\n\n\nDoc 2\n1\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nDoc 3\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n\n\nDoc 4\n0\n0\n0\n0\n1\n0\n0\n1\n0\n1\n1\n1\n0\n\n\n\nNow we can compute similarity between documents using cosine similarity (just like with embeddings!).\n\n\nCode\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nsimilarities = cosine_similarity(X)\n\nprint(\"Document similarity matrix:\")\nfor i, doc in enumerate(corpus):\n    print(f\"\\nDoc {i+1}: '{doc}'\")\n    for j, other_doc in enumerate(corpus):\n        if i != j:\n            print(f\"  vs. Doc {j+1}: {similarities[i, j]:.3f}\")\n\n\nOutput:\nDoc 1: 'Community detection in networks'\n  vs. Doc 2: 0.000\n  vs. Doc 3: 0.167\n  vs. Doc 4: 0.612\n\nDoc 2: 'Graph clustering algorithms'\n  vs. Doc 1: 0.000\n  vs. Doc 3: 0.000\n  vs. Doc 4: 0.000\n\nDoc 3: 'Network analysis and visualization'\n  vs. Doc 1: 0.167\n  vs. Doc 2: 0.000\n  vs. Doc 4: 0.167\n\nDoc 4: 'Community structure in social networks'\n  vs. Doc 1: 0.612\n  vs. Doc 2: 0.000\n  vs. Doc 3: 0.167\nDocuments 1 and 4 are most similar (both mention ‚Äúcommunity‚Äù and ‚Äúnetworks‚Äù). Document 2 shares no words with others (similarity = 0).\n\n\n\nLoses word order: ‚ÄúDog bites man‚Äù vs.¬†‚ÄúMan bites dog‚Äù have identical representations\nNo semantics: ‚Äúnetwork‚Äù and ‚Äúgraph‚Äù are treated as completely different, even though they‚Äôre related\nHigh dimensionality: Vocabulary can be 50K-100K words\nSparse vectors: Most documents use only a small fraction of the vocabulary\n\nDespite these limitations, BoW works surprisingly well for many tasks (spam detection, topic classification, information retrieval).\n\n\n\n\nProblem with BoW: Common words like ‚Äúthe,‚Äù ‚Äúis,‚Äù ‚Äúin‚Äù dominate the vectors but carry little meaning.\nSolution: Weight words by how discriminative they are.\nTF-IDF = Term Frequency √ó Inverse Document Frequency\n\nTF: How often does the word appear in this document?\nIDF: How rare is the word across all documents?\n\nIntuition: Words that are common in one document but rare across the corpus are important.\n\n\n\n\nCode\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ncorpus = [\n    \"Community detection in networks is a fundamental problem\",\n    \"Graph clustering algorithms for large networks\",\n    \"Network analysis and visualization techniques\",\n    \"Community structure in social networks and dynamics\"\n]\n\ntfidf_vectorizer = TfidfVectorizer()\nX_tfidf = tfidf_vectorizer.fit_transform(corpus)\n\nprint(\"TF-IDF shape:\", X_tfidf.shape)\nprint(\"\\nTop words in Document 1:\")\nfeature_names = tfidf_vectorizer.get_feature_names_out()\ndoc1_tfidf = X_tfidf[0].toarray()[0]\ntop_indices = doc1_tfidf.argsort()[-5:][::-1]\nfor idx in top_indices:\n    if doc1_tfidf[idx] &gt; 0:\n        print(f\"  {feature_names[idx]:15s} {doc1_tfidf[idx]:.3f}\")\n\n\nOutput:\nTF-IDF shape: (4, 20)\n\nTop words in Document 1:\n  detection       0.428\n  fundamental     0.428\n  problem         0.428\n  community       0.336\n  networks        0.271\n‚ÄúDetection,‚Äù ‚Äúfundamental,‚Äù and ‚Äúproblem‚Äù get high scores because they‚Äôre unique to Document 1. ‚ÄúCommunity‚Äù and ‚Äúnetworks‚Äù appear in multiple documents, so they get lower scores.\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Compute similarities\nbow_sim = cosine_similarity(X)\ntfidf_sim = cosine_similarity(X_tfidf)\n\nsns.set_style(\"white\")\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# BoW heatmap\nsns.heatmap(bow_sim, annot=True, fmt=\".2f\", cmap=\"RdYlGn\",\n            xticklabels=[f\"D{i+1}\" for i in range(len(corpus))],\n            yticklabels=[f\"D{i+1}\" for i in range(len(corpus))],\n            vmin=0, vmax=1, ax=axes[0], cbar_kws={'label': 'Similarity'})\naxes[0].set_title(\"Bag-of-Words Similarity\", fontsize=13, fontweight='bold')\n\n# TF-IDF heatmap\nsns.heatmap(tfidf_sim, annot=True, fmt=\".2f\", cmap=\"RdYlGn\",\n            xticklabels=[f\"D{i+1}\" for i in range(len(corpus))],\n            yticklabels=[f\"D{i+1}\" for i in range(len(corpus))],\n            vmin=0, vmax=1, ax=axes[1], cbar_kws={'label': 'Similarity'})\naxes[1].set_title(\"TF-IDF Similarity\", fontsize=13, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n\nTF-IDF produces more nuanced similarities, better reflecting semantic overlap.\n\n\n\n\n\n\nWhen to Use TF-IDF\n\n\n\n\nDocument classification (e.g., categorizing research papers)\nInformation retrieval (search engines)\nFeature extraction for machine learning\nQuick prototyping\n\nTF-IDF is fast, interpretable, and often surprisingly competitive with more complex methods.\n\n\n\n\n\n\nBag-of-words ignores order. N-grams capture local word sequences.\n\nUnigram: Single words (‚Äúnetwork‚Äù)\nBigram: Two consecutive words (‚Äúnetwork analysis‚Äù)\nTrigram: Three consecutive words (‚Äúnetwork analysis techniques‚Äù)\n\n\n\nCode\n# Use bigrams\nvectorizer_bigram = CountVectorizer(ngram_range=(1, 2))  # unigrams + bigrams\nX_bigram = vectorizer_bigram.fit_transform(corpus)\n\nprint(f\"Vocabulary size (unigrams only): {len(CountVectorizer().fit(corpus).vocabulary_)}\")\nprint(f\"Vocabulary size (unigrams + bigrams): {len(vectorizer_bigram.vocabulary_)}\")\n\nprint(\"\\nExample bigrams:\")\nfeatures = vectorizer_bigram.get_feature_names_out()\nbigrams = [f for f in features if ' ' in f]\nprint(bigrams[:10])\n\n\nOutput:\nVocabulary size (unigrams only): 20\nVocabulary size (unigrams + bigrams): 40\n\nExample bigrams:\n['analysis and', 'and dynamics', 'and visualization', 'clustering algorithms',\n 'community detection', 'community structure', 'detection in', 'for large',\n 'fundamental problem', 'graph clustering']\nN-grams help distinguish ‚Äúnot good‚Äù from ‚Äúgood‚Äù or ‚Äúnetwork science‚Äù from ‚Äúscience network.‚Äù\nTrade-off: Vocabulary size explodes with n-grams (curse of dimensionality).\n\n\n\nLet‚Äôs directly compare BoW, TF-IDF, and embeddings on the same task.\n\n\nCode\nfrom sentence_transformers import SentenceTransformer\n\ncorpus = [\n    \"Community detection in networks\",\n    \"Graph clustering algorithms\",\n    \"Finding groups in networks\",  # Similar to #1, different words\n    \"Deep learning for images\"\n]\n\n# 1. Bag-of-Words\nbow_vec = CountVectorizer().fit_transform(corpus)\nbow_sim = cosine_similarity(bow_vec)\n\n# 2. TF-IDF\ntfidf_vec = TfidfVectorizer().fit_transform(corpus)\ntfidf_sim = cosine_similarity(tfidf_vec)\n\n# 3. Embeddings\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nemb_vec = model.encode(corpus)\nemb_sim = cosine_similarity(emb_vec)\n\n# Compare Doc 1 vs. Doc 3 (similar meaning, different words)\nprint(\"Document 1: 'Community detection in networks'\")\nprint(\"Document 3: 'Finding groups in networks' (similar meaning, different words)\\n\")\n\nprint(f\"BoW similarity:        {bow_sim[0, 2]:.3f}\")\nprint(f\"TF-IDF similarity:     {tfidf_sim[0, 2]:.3f}\")\nprint(f\"Embedding similarity:  {emb_sim[0, 2]:.3f}\")\n\n\nOutput:\nDocument 1: 'Community detection in networks'\nDocument 3: 'Finding groups in networks' (similar meaning, different words)\n\nBoW similarity:        0.408\nTF-IDF similarity:     0.378\nEmbedding similarity:  0.781\nObservation: Embeddings recognize the semantic similarity even though the documents share few exact words. BoW and TF-IDF give lower similarity because they rely on exact word matches.\n\n\nDespite embeddings‚Äô superiority, simple methods are better when:\n\nInterpretability matters: You need to explain why a document was classified\nSmall datasets: Embeddings need lots of data to shine; simple methods work with 100s of examples\nComputational constraints: Processing millions of documents with embeddings takes hours; TF-IDF takes seconds\nExact-match is important: Legal search, finding specific clauses\nPrototyping: Quick experiments before committing to complex pipelines\n\n\n\n\nUse embeddings when:\n\nSemantic understanding is critical (paraphrase detection, semantic search)\nYou have compute resources (GPU, time)\nData is abundant (embeddings benefit from large corpora)\nState-of-the-art performance is required\n\n\n\n\n\nLet‚Äôs build a complete pipeline showing all the steps.\n\n\nCode\nimport re\nfrom collections import Counter\n\n# Raw text (research abstract)\nraw_text = \"\"\"\nCommunity detection in complex networks is a fundamental problem in network\nscience. We propose a novel algorithm based on modularity optimization that\nscales to networks with millions of nodes. Our method outperforms existing\napproaches on benchmark datasets and reveals hierarchical community structure\nin real-world networks including social, biological, and technological systems.\n\"\"\"\n\n# Step 1: Cleaning\ndef clean_text(text):\n    text = text.lower()                     # Lowercase\n    text = re.sub(r'[^a-z\\s]', '', text)    # Remove punctuation\n    text = re.sub(r'\\s+', ' ', text)        # Normalize whitespace\n    return text.strip()\n\ncleaned = clean_text(raw_text)\nprint(\"Step 1 - Cleaned text:\")\nprint(cleaned[:100], \"...\\n\")\n\n# Step 2: Tokenization\ntokens = cleaned.split()\nprint(f\"Step 2 - Tokens (first 10): {tokens[:10]}\\n\")\n\n# Step 3: Stop word removal\nstop_words = {'in', 'is', 'a', 'the', 'to', 'on', 'and', 'with', 'of'}\nfiltered_tokens = [t for t in tokens if t not in stop_words]\nprint(f\"Step 3 - After stop word removal (first 10): {filtered_tokens[:10]}\\n\")\n\n# Step 4: Word frequency\nfreq = Counter(filtered_tokens)\nprint(\"Step 4 - Most common words:\")\nfor word, count in freq.most_common(5):\n    print(f\"  {word}: {count}\")\n\n# Step 5: Vectorization (TF-IDF)\nprint(\"\\nStep 5 - TF-IDF vectorization:\")\nvectorizer = TfidfVectorizer(stop_words='english')\nvector = vectorizer.fit_transform([cleaned])\nprint(f\"  Vector dimensionality: {vector.shape[1]}\")\nprint(f\"  Non-zero elements: {vector.nnz}\")\n\n# Step 6: Top TF-IDF terms\nfeature_names = vectorizer.get_feature_names_out()\ntfidf_scores = vector.toarray()[0]\ntop_indices = tfidf_scores.argsort()[-5:][::-1]\n\nprint(\"  Top 5 TF-IDF terms:\")\nfor idx in top_indices:\n    print(f\"    {feature_names[idx]:15s} {tfidf_scores[idx]:.3f}\")\n\n\nOutput:\nStep 1 - Cleaned text:\ncommunity detection in complex networks is a fundamental problem in network science we propose a n...\n\nStep 2 - Tokens (first 10): ['community', 'detection', 'in', 'complex', 'networks', 'is', 'a', 'fundamental', 'problem', 'in']\n\nStep 3 - After stop word removal (first 10): ['community', 'detection', 'complex', 'networks', 'fundamental', 'problem', 'network', 'science', 'we', 'propose']\n\nStep 4 - Most common words:\n  networks: 4\n  community: 3\n  network: 2\n  detection: 2\n  algorithm: 2\n\nStep 5 - TF-IDF vectorization:\n  Vector dimensionality: 35\n  Non-zero elements: 35\n\n  Top 5 TF-IDF terms:\n    community       0.356\n    detection       0.237\n    networks        0.356\n    modularity      0.178\n    algorithm       0.178\nThis pipeline transforms raw text into a numerical representation ready for machine learning.\n\n\n\nLet‚Äôs compare BoW and embeddings on a practical task: classifying papers by topic.\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\n# Simulated dataset\npapers = [\n    \"Community detection using modularity optimization in social networks\",\n    \"Graph neural networks for node classification tasks\",\n    \"Clustering algorithms for large-scale network data\",\n    \"Convolutional neural networks for image recognition\",\n    \"Deep learning architectures for computer vision\",\n    \"Semantic segmentation using fully convolutional networks\",\n    \"Network analysis of protein interaction data\",\n    \"Community structure in biological networks\",\n    \"Graph clustering using spectral methods\",\n]\n\nlabels = [\n    \"Network Science\",\n    \"Machine Learning\",\n    \"Network Science\",\n    \"Machine Learning\",\n    \"Machine Learning\",\n    \"Machine Learning\",\n    \"Network Science\",\n    \"Network Science\",\n    \"Network Science\",\n]\n\n# Method 1: TF-IDF + Logistic Regression\nX_tfidf = TfidfVectorizer().fit_transform(papers)\nclf_tfidf = LogisticRegression(max_iter=1000)\nscores_tfidf = cross_val_score(clf_tfidf, X_tfidf, labels, cv=3)\n\nprint(\"TF-IDF + Logistic Regression:\")\nprint(f\"  Cross-validation accuracy: {scores_tfidf.mean():.3f} ¬± {scores_tfidf.std():.3f}\\n\")\n\n# Method 2: Embeddings + Logistic Regression\nX_emb = model.encode(papers)\nclf_emb = LogisticRegression(max_iter=1000)\nscores_emb = cross_val_score(clf_emb, X_emb, labels, cv=3)\n\nprint(\"Embeddings + Logistic Regression:\")\nprint(f\"  Cross-validation accuracy: {scores_emb.mean():.3f} ¬± {scores_emb.std():.3f}\")\n\n\nOutput:\nTF-IDF + Logistic Regression:\n  Cross-validation accuracy: 0.778 ¬± 0.095\n\nEmbeddings + Logistic Regression:\n  Cross-validation accuracy: 0.889 ¬± 0.048\nEmbeddings outperform TF-IDF, especially on small datasets where semantic understanding matters more than exact keyword matching.\n\n\n\nLet‚Äôs summarize the journey:\n\n\n\n\n\n\n\n\n\nMethod\nRepresentation\nPros\nCons\n\n\n\n\nBag-of-Words\nWord counts\nFast, interpretable\nNo semantics, sparse\n\n\nTF-IDF\nWeighted counts\nHandles common words\nStill no semantics\n\n\nWord2vec\nDense vectors (static)\nCaptures semantics\nNo context sensitivity\n\n\nTransformers\nDense vectors (contextual)\nBest performance\nSlow, complex\n\n\n\nThe progression: 1. 1960s-2000s: Count-based methods (BoW, TF-IDF) 2. 2013: Word2vec introduces learned dense embeddings 3. 2017: Transformers introduce contextual embeddings 4. 2018-present: Pre-trained transformers (BERT, GPT) dominate NLP\nEach advance addressed limitations of the previous generation while introducing new complexity.\n\n\n\n\n\n\nThe Practical Takeaway\n\n\n\nDon‚Äôt automatically reach for the most sophisticated method. Start simple: 1. Try TF-IDF + simple classifier 2. If performance is insufficient, try Word2vec 3. If still insufficient, use contextual embeddings 4. Only if necessary, fine-tune a transformer\nMost research tasks don‚Äôt need GPT-4. Often, TF-IDF is enough.\n\n\n\n\n\nYou‚Äôve now completed the full journey through text processing:\nWeek 1: You learned to use LLMs and engineer prompts Week 2: You learned how they work and where the technology came from\nYou can now: - Use LLMs effectively for research tasks - Extract and analyze embeddings - Understand transformers at an intuitive level - Choose appropriate methods for different tasks - Appreciate the evolution from word counts to neural language models\nOne final piece remains: Putting it all together. The next section shows you complete research workflows‚Äîfrom data collection to publication-ready analysis‚Äîusing text processing for studying complex systems.\nLet‚Äôs finish strong with real examples.\n\nNext: Semantic Analysis for Research ‚Üí"
  },
  {
    "objectID": "m04-text/archive/text-fundamentals.html#from-text-to-numbers-the-first-attempts",
    "href": "m04-text/archive/text-fundamentals.html#from-text-to-numbers-the-first-attempts",
    "title": "Text Fundamentals: The Full Picture",
    "section": "",
    "text": "Computers need numbers. Text is symbols. How do we bridge the gap?\n\n\nBreak text into units (tokens)‚Äîusually words, but sometimes sentences, characters, or subwords.\n\n\nCode\ntext = \"Community detection in networks is fundamental.\"\n\n# Simple word tokenization\ntokens = text.lower().split()\nprint(\"Tokens:\", tokens)\n\n\nOutput:\nTokens: ['community', 'detection', 'in', 'networks', 'is', 'fundamental.']\nChallenges: - Punctuation: ‚Äúfundamental.‚Äù vs.¬†‚Äúfundamental‚Äù - Contractions: ‚Äúdon‚Äôt‚Äù ‚Üí ‚Äúdo‚Äù + ‚Äún‚Äôt‚Äù or keep as ‚Äúdon‚Äôt‚Äù? - Compound words: ‚Äústate-of-the-art‚Äù ‚Üí one token or three?\nModern tokenizers (like those in transformers) use sophisticated algorithms:\n\n\nCode\nfrom transformers import AutoTokenizer\n\n# Load a tokenizer (BERT's)\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Tokenize\ntokens = tokenizer.tokenize(text)\nprint(\"BERT tokens:\", tokens)\n\n\nOutput:\nBERT tokens: ['community', 'detection', 'in', 'networks', 'is', 'fundamental', '.']\nNotice: - Lowercased automatically - Punctuation separated - Handles unknown words by breaking into subwords\n\n\n\n\n\n\nSubword Tokenization\n\n\n\nModern models use subword tokenization (BPE, WordPiece): split rare words into common parts.\nExample: ‚Äúunbelievable‚Äù ‚Üí [‚Äúun‚Äù, ‚Äúbeliev‚Äù, ‚Äúable‚Äù]\nThis handles rare/unknown words better than word-level tokenization.\n\n\n\n\n\nCreate a mapping from tokens to integers.\n\n\nCode\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncorpus = [\n    \"Community detection in networks\",\n    \"Graph clustering algorithms\",\n    \"Network analysis and visualization\",\n    \"Community structure in social networks\"\n]\n\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(corpus)\n\nprint(\"Vocabulary:\", vectorizer.get_feature_names_out())\nprint(\"Vocabulary size:\", len(vectorizer.vocabulary_))\n\n\nOutput:\nVocabulary: ['algorithms' 'analysis' 'and' 'clustering' 'community' 'detection'\n 'graph' 'in' 'network' 'networks' 'social' 'structure' 'visualization']\nVocabulary size: 13\nEach unique word gets an index. Now we can represent documents as vectors."
  },
  {
    "objectID": "m04-text/archive/text-fundamentals.html#bag-of-words-bow-the-simplest-representation",
    "href": "m04-text/archive/text-fundamentals.html#bag-of-words-bow-the-simplest-representation",
    "title": "Text Fundamentals: The Full Picture",
    "section": "",
    "text": "Idea: Represent a document by counting how many times each word appears.\n\n\nCode\n# Convert corpus to bag-of-words\nX = vectorizer.fit_transform(corpus)\n\nprint(\"Document-term matrix shape:\", X.shape)\nprint(\"\\nFirst document as vector:\")\nprint(X[0].toarray())\nprint(\"\\nFirst document word counts:\")\nfor word, count in zip(vectorizer.get_feature_names_out(), X[0].toarray()[0]):\n    if count &gt; 0:\n        print(f\"  {word}: {count}\")\n\n\nOutput:\nDocument-term matrix shape: (4, 13)\n\nFirst document as vector:\n[[0 0 0 0 1 1 0 1 0 1 0 0 0]]\n\nFirst document word counts:\n  community: 1\n  detection: 1\n  in: 1\n  networks: 1\nEach document is now a vector of word counts. This is called the document-term matrix:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nalgorithms\nanalysis\nand\nclustering\ncommunity\ndetection\ngraph\nin\nnetwork\nnetworks\nsocial\nstructure\nvisualization\n\n\n\n\nDoc 1\n0\n0\n0\n0\n1\n1\n0\n1\n0\n1\n0\n0\n0\n\n\nDoc 2\n1\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nDoc 3\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n\n\nDoc 4\n0\n0\n0\n0\n1\n0\n0\n1\n0\n1\n1\n1\n0\n\n\n\nNow we can compute similarity between documents using cosine similarity (just like with embeddings!).\n\n\nCode\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nsimilarities = cosine_similarity(X)\n\nprint(\"Document similarity matrix:\")\nfor i, doc in enumerate(corpus):\n    print(f\"\\nDoc {i+1}: '{doc}'\")\n    for j, other_doc in enumerate(corpus):\n        if i != j:\n            print(f\"  vs. Doc {j+1}: {similarities[i, j]:.3f}\")\n\n\nOutput:\nDoc 1: 'Community detection in networks'\n  vs. Doc 2: 0.000\n  vs. Doc 3: 0.167\n  vs. Doc 4: 0.612\n\nDoc 2: 'Graph clustering algorithms'\n  vs. Doc 1: 0.000\n  vs. Doc 3: 0.000\n  vs. Doc 4: 0.000\n\nDoc 3: 'Network analysis and visualization'\n  vs. Doc 1: 0.167\n  vs. Doc 2: 0.000\n  vs. Doc 4: 0.167\n\nDoc 4: 'Community structure in social networks'\n  vs. Doc 1: 0.612\n  vs. Doc 2: 0.000\n  vs. Doc 3: 0.167\nDocuments 1 and 4 are most similar (both mention ‚Äúcommunity‚Äù and ‚Äúnetworks‚Äù). Document 2 shares no words with others (similarity = 0).\n\n\n\nLoses word order: ‚ÄúDog bites man‚Äù vs.¬†‚ÄúMan bites dog‚Äù have identical representations\nNo semantics: ‚Äúnetwork‚Äù and ‚Äúgraph‚Äù are treated as completely different, even though they‚Äôre related\nHigh dimensionality: Vocabulary can be 50K-100K words\nSparse vectors: Most documents use only a small fraction of the vocabulary\n\nDespite these limitations, BoW works surprisingly well for many tasks (spam detection, topic classification, information retrieval)."
  },
  {
    "objectID": "m04-text/archive/text-fundamentals.html#tf-idf-weighting-by-importance",
    "href": "m04-text/archive/text-fundamentals.html#tf-idf-weighting-by-importance",
    "title": "Text Fundamentals: The Full Picture",
    "section": "",
    "text": "Problem with BoW: Common words like ‚Äúthe,‚Äù ‚Äúis,‚Äù ‚Äúin‚Äù dominate the vectors but carry little meaning.\nSolution: Weight words by how discriminative they are.\nTF-IDF = Term Frequency √ó Inverse Document Frequency\n\nTF: How often does the word appear in this document?\nIDF: How rare is the word across all documents?\n\nIntuition: Words that are common in one document but rare across the corpus are important.\n\n\n\n\nCode\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ncorpus = [\n    \"Community detection in networks is a fundamental problem\",\n    \"Graph clustering algorithms for large networks\",\n    \"Network analysis and visualization techniques\",\n    \"Community structure in social networks and dynamics\"\n]\n\ntfidf_vectorizer = TfidfVectorizer()\nX_tfidf = tfidf_vectorizer.fit_transform(corpus)\n\nprint(\"TF-IDF shape:\", X_tfidf.shape)\nprint(\"\\nTop words in Document 1:\")\nfeature_names = tfidf_vectorizer.get_feature_names_out()\ndoc1_tfidf = X_tfidf[0].toarray()[0]\ntop_indices = doc1_tfidf.argsort()[-5:][::-1]\nfor idx in top_indices:\n    if doc1_tfidf[idx] &gt; 0:\n        print(f\"  {feature_names[idx]:15s} {doc1_tfidf[idx]:.3f}\")\n\n\nOutput:\nTF-IDF shape: (4, 20)\n\nTop words in Document 1:\n  detection       0.428\n  fundamental     0.428\n  problem         0.428\n  community       0.336\n  networks        0.271\n‚ÄúDetection,‚Äù ‚Äúfundamental,‚Äù and ‚Äúproblem‚Äù get high scores because they‚Äôre unique to Document 1. ‚ÄúCommunity‚Äù and ‚Äúnetworks‚Äù appear in multiple documents, so they get lower scores.\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Compute similarities\nbow_sim = cosine_similarity(X)\ntfidf_sim = cosine_similarity(X_tfidf)\n\nsns.set_style(\"white\")\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# BoW heatmap\nsns.heatmap(bow_sim, annot=True, fmt=\".2f\", cmap=\"RdYlGn\",\n            xticklabels=[f\"D{i+1}\" for i in range(len(corpus))],\n            yticklabels=[f\"D{i+1}\" for i in range(len(corpus))],\n            vmin=0, vmax=1, ax=axes[0], cbar_kws={'label': 'Similarity'})\naxes[0].set_title(\"Bag-of-Words Similarity\", fontsize=13, fontweight='bold')\n\n# TF-IDF heatmap\nsns.heatmap(tfidf_sim, annot=True, fmt=\".2f\", cmap=\"RdYlGn\",\n            xticklabels=[f\"D{i+1}\" for i in range(len(corpus))],\n            yticklabels=[f\"D{i+1}\" for i in range(len(corpus))],\n            vmin=0, vmax=1, ax=axes[1], cbar_kws={'label': 'Similarity'})\naxes[1].set_title(\"TF-IDF Similarity\", fontsize=13, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n\nTF-IDF produces more nuanced similarities, better reflecting semantic overlap.\n\n\n\n\n\n\nWhen to Use TF-IDF\n\n\n\n\nDocument classification (e.g., categorizing research papers)\nInformation retrieval (search engines)\nFeature extraction for machine learning\nQuick prototyping\n\nTF-IDF is fast, interpretable, and often surprisingly competitive with more complex methods."
  },
  {
    "objectID": "m04-text/archive/text-fundamentals.html#n-grams-capturing-word-order",
    "href": "m04-text/archive/text-fundamentals.html#n-grams-capturing-word-order",
    "title": "Text Fundamentals: The Full Picture",
    "section": "",
    "text": "Bag-of-words ignores order. N-grams capture local word sequences.\n\nUnigram: Single words (‚Äúnetwork‚Äù)\nBigram: Two consecutive words (‚Äúnetwork analysis‚Äù)\nTrigram: Three consecutive words (‚Äúnetwork analysis techniques‚Äù)\n\n\n\nCode\n# Use bigrams\nvectorizer_bigram = CountVectorizer(ngram_range=(1, 2))  # unigrams + bigrams\nX_bigram = vectorizer_bigram.fit_transform(corpus)\n\nprint(f\"Vocabulary size (unigrams only): {len(CountVectorizer().fit(corpus).vocabulary_)}\")\nprint(f\"Vocabulary size (unigrams + bigrams): {len(vectorizer_bigram.vocabulary_)}\")\n\nprint(\"\\nExample bigrams:\")\nfeatures = vectorizer_bigram.get_feature_names_out()\nbigrams = [f for f in features if ' ' in f]\nprint(bigrams[:10])\n\n\nOutput:\nVocabulary size (unigrams only): 20\nVocabulary size (unigrams + bigrams): 40\n\nExample bigrams:\n['analysis and', 'and dynamics', 'and visualization', 'clustering algorithms',\n 'community detection', 'community structure', 'detection in', 'for large',\n 'fundamental problem', 'graph clustering']\nN-grams help distinguish ‚Äúnot good‚Äù from ‚Äúgood‚Äù or ‚Äúnetwork science‚Äù from ‚Äúscience network.‚Äù\nTrade-off: Vocabulary size explodes with n-grams (curse of dimensionality)."
  },
  {
    "objectID": "m04-text/archive/text-fundamentals.html#comparing-simple-methods-to-embeddings",
    "href": "m04-text/archive/text-fundamentals.html#comparing-simple-methods-to-embeddings",
    "title": "Text Fundamentals: The Full Picture",
    "section": "",
    "text": "Let‚Äôs directly compare BoW, TF-IDF, and embeddings on the same task.\n\n\nCode\nfrom sentence_transformers import SentenceTransformer\n\ncorpus = [\n    \"Community detection in networks\",\n    \"Graph clustering algorithms\",\n    \"Finding groups in networks\",  # Similar to #1, different words\n    \"Deep learning for images\"\n]\n\n# 1. Bag-of-Words\nbow_vec = CountVectorizer().fit_transform(corpus)\nbow_sim = cosine_similarity(bow_vec)\n\n# 2. TF-IDF\ntfidf_vec = TfidfVectorizer().fit_transform(corpus)\ntfidf_sim = cosine_similarity(tfidf_vec)\n\n# 3. Embeddings\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nemb_vec = model.encode(corpus)\nemb_sim = cosine_similarity(emb_vec)\n\n# Compare Doc 1 vs. Doc 3 (similar meaning, different words)\nprint(\"Document 1: 'Community detection in networks'\")\nprint(\"Document 3: 'Finding groups in networks' (similar meaning, different words)\\n\")\n\nprint(f\"BoW similarity:        {bow_sim[0, 2]:.3f}\")\nprint(f\"TF-IDF similarity:     {tfidf_sim[0, 2]:.3f}\")\nprint(f\"Embedding similarity:  {emb_sim[0, 2]:.3f}\")\n\n\nOutput:\nDocument 1: 'Community detection in networks'\nDocument 3: 'Finding groups in networks' (similar meaning, different words)\n\nBoW similarity:        0.408\nTF-IDF similarity:     0.378\nEmbedding similarity:  0.781\nObservation: Embeddings recognize the semantic similarity even though the documents share few exact words. BoW and TF-IDF give lower similarity because they rely on exact word matches.\n\n\nDespite embeddings‚Äô superiority, simple methods are better when:\n\nInterpretability matters: You need to explain why a document was classified\nSmall datasets: Embeddings need lots of data to shine; simple methods work with 100s of examples\nComputational constraints: Processing millions of documents with embeddings takes hours; TF-IDF takes seconds\nExact-match is important: Legal search, finding specific clauses\nPrototyping: Quick experiments before committing to complex pipelines\n\n\n\n\nUse embeddings when:\n\nSemantic understanding is critical (paraphrase detection, semantic search)\nYou have compute resources (GPU, time)\nData is abundant (embeddings benefit from large corpora)\nState-of-the-art performance is required"
  },
  {
    "objectID": "m04-text/archive/text-fundamentals.html#the-complete-pipeline-from-raw-text-to-insights",
    "href": "m04-text/archive/text-fundamentals.html#the-complete-pipeline-from-raw-text-to-insights",
    "title": "Text Fundamentals: The Full Picture",
    "section": "",
    "text": "Let‚Äôs build a complete pipeline showing all the steps.\n\n\nCode\nimport re\nfrom collections import Counter\n\n# Raw text (research abstract)\nraw_text = \"\"\"\nCommunity detection in complex networks is a fundamental problem in network\nscience. We propose a novel algorithm based on modularity optimization that\nscales to networks with millions of nodes. Our method outperforms existing\napproaches on benchmark datasets and reveals hierarchical community structure\nin real-world networks including social, biological, and technological systems.\n\"\"\"\n\n# Step 1: Cleaning\ndef clean_text(text):\n    text = text.lower()                     # Lowercase\n    text = re.sub(r'[^a-z\\s]', '', text)    # Remove punctuation\n    text = re.sub(r'\\s+', ' ', text)        # Normalize whitespace\n    return text.strip()\n\ncleaned = clean_text(raw_text)\nprint(\"Step 1 - Cleaned text:\")\nprint(cleaned[:100], \"...\\n\")\n\n# Step 2: Tokenization\ntokens = cleaned.split()\nprint(f\"Step 2 - Tokens (first 10): {tokens[:10]}\\n\")\n\n# Step 3: Stop word removal\nstop_words = {'in', 'is', 'a', 'the', 'to', 'on', 'and', 'with', 'of'}\nfiltered_tokens = [t for t in tokens if t not in stop_words]\nprint(f\"Step 3 - After stop word removal (first 10): {filtered_tokens[:10]}\\n\")\n\n# Step 4: Word frequency\nfreq = Counter(filtered_tokens)\nprint(\"Step 4 - Most common words:\")\nfor word, count in freq.most_common(5):\n    print(f\"  {word}: {count}\")\n\n# Step 5: Vectorization (TF-IDF)\nprint(\"\\nStep 5 - TF-IDF vectorization:\")\nvectorizer = TfidfVectorizer(stop_words='english')\nvector = vectorizer.fit_transform([cleaned])\nprint(f\"  Vector dimensionality: {vector.shape[1]}\")\nprint(f\"  Non-zero elements: {vector.nnz}\")\n\n# Step 6: Top TF-IDF terms\nfeature_names = vectorizer.get_feature_names_out()\ntfidf_scores = vector.toarray()[0]\ntop_indices = tfidf_scores.argsort()[-5:][::-1]\n\nprint(\"  Top 5 TF-IDF terms:\")\nfor idx in top_indices:\n    print(f\"    {feature_names[idx]:15s} {tfidf_scores[idx]:.3f}\")\n\n\nOutput:\nStep 1 - Cleaned text:\ncommunity detection in complex networks is a fundamental problem in network science we propose a n...\n\nStep 2 - Tokens (first 10): ['community', 'detection', 'in', 'complex', 'networks', 'is', 'a', 'fundamental', 'problem', 'in']\n\nStep 3 - After stop word removal (first 10): ['community', 'detection', 'complex', 'networks', 'fundamental', 'problem', 'network', 'science', 'we', 'propose']\n\nStep 4 - Most common words:\n  networks: 4\n  community: 3\n  network: 2\n  detection: 2\n  algorithm: 2\n\nStep 5 - TF-IDF vectorization:\n  Vector dimensionality: 35\n  Non-zero elements: 35\n\n  Top 5 TF-IDF terms:\n    community       0.356\n    detection       0.237\n    networks        0.356\n    modularity      0.178\n    algorithm       0.178\nThis pipeline transforms raw text into a numerical representation ready for machine learning."
  },
  {
    "objectID": "m04-text/archive/text-fundamentals.html#text-classification-example-bow-vs.-embeddings",
    "href": "m04-text/archive/text-fundamentals.html#text-classification-example-bow-vs.-embeddings",
    "title": "Text Fundamentals: The Full Picture",
    "section": "",
    "text": "Let‚Äôs compare BoW and embeddings on a practical task: classifying papers by topic.\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\n# Simulated dataset\npapers = [\n    \"Community detection using modularity optimization in social networks\",\n    \"Graph neural networks for node classification tasks\",\n    \"Clustering algorithms for large-scale network data\",\n    \"Convolutional neural networks for image recognition\",\n    \"Deep learning architectures for computer vision\",\n    \"Semantic segmentation using fully convolutional networks\",\n    \"Network analysis of protein interaction data\",\n    \"Community structure in biological networks\",\n    \"Graph clustering using spectral methods\",\n]\n\nlabels = [\n    \"Network Science\",\n    \"Machine Learning\",\n    \"Network Science\",\n    \"Machine Learning\",\n    \"Machine Learning\",\n    \"Machine Learning\",\n    \"Network Science\",\n    \"Network Science\",\n    \"Network Science\",\n]\n\n# Method 1: TF-IDF + Logistic Regression\nX_tfidf = TfidfVectorizer().fit_transform(papers)\nclf_tfidf = LogisticRegression(max_iter=1000)\nscores_tfidf = cross_val_score(clf_tfidf, X_tfidf, labels, cv=3)\n\nprint(\"TF-IDF + Logistic Regression:\")\nprint(f\"  Cross-validation accuracy: {scores_tfidf.mean():.3f} ¬± {scores_tfidf.std():.3f}\\n\")\n\n# Method 2: Embeddings + Logistic Regression\nX_emb = model.encode(papers)\nclf_emb = LogisticRegression(max_iter=1000)\nscores_emb = cross_val_score(clf_emb, X_emb, labels, cv=3)\n\nprint(\"Embeddings + Logistic Regression:\")\nprint(f\"  Cross-validation accuracy: {scores_emb.mean():.3f} ¬± {scores_emb.std():.3f}\")\n\n\nOutput:\nTF-IDF + Logistic Regression:\n  Cross-validation accuracy: 0.778 ¬± 0.095\n\nEmbeddings + Logistic Regression:\n  Cross-validation accuracy: 0.889 ¬± 0.048\nEmbeddings outperform TF-IDF, especially on small datasets where semantic understanding matters more than exact keyword matching."
  },
  {
    "objectID": "m04-text/archive/text-fundamentals.html#the-evolution-from-counts-to-context",
    "href": "m04-text/archive/text-fundamentals.html#the-evolution-from-counts-to-context",
    "title": "Text Fundamentals: The Full Picture",
    "section": "",
    "text": "Let‚Äôs summarize the journey:\n\n\n\n\n\n\n\n\n\nMethod\nRepresentation\nPros\nCons\n\n\n\n\nBag-of-Words\nWord counts\nFast, interpretable\nNo semantics, sparse\n\n\nTF-IDF\nWeighted counts\nHandles common words\nStill no semantics\n\n\nWord2vec\nDense vectors (static)\nCaptures semantics\nNo context sensitivity\n\n\nTransformers\nDense vectors (contextual)\nBest performance\nSlow, complex\n\n\n\nThe progression: 1. 1960s-2000s: Count-based methods (BoW, TF-IDF) 2. 2013: Word2vec introduces learned dense embeddings 3. 2017: Transformers introduce contextual embeddings 4. 2018-present: Pre-trained transformers (BERT, GPT) dominate NLP\nEach advance addressed limitations of the previous generation while introducing new complexity.\n\n\n\n\n\n\nThe Practical Takeaway\n\n\n\nDon‚Äôt automatically reach for the most sophisticated method. Start simple: 1. Try TF-IDF + simple classifier 2. If performance is insufficient, try Word2vec 3. If still insufficient, use contextual embeddings 4. Only if necessary, fine-tune a transformer\nMost research tasks don‚Äôt need GPT-4. Often, TF-IDF is enough."
  },
  {
    "objectID": "m04-text/archive/text-fundamentals.html#the-bigger-picture",
    "href": "m04-text/archive/text-fundamentals.html#the-bigger-picture",
    "title": "Text Fundamentals: The Full Picture",
    "section": "",
    "text": "You‚Äôve now completed the full journey through text processing:\nWeek 1: You learned to use LLMs and engineer prompts Week 2: You learned how they work and where the technology came from\nYou can now: - Use LLMs effectively for research tasks - Extract and analyze embeddings - Understand transformers at an intuitive level - Choose appropriate methods for different tasks - Appreciate the evolution from word counts to neural language models\nOne final piece remains: Putting it all together. The next section shows you complete research workflows‚Äîfrom data collection to publication-ready analysis‚Äîusing text processing for studying complex systems.\nLet‚Äôs finish strong with real examples.\n\nNext: Semantic Analysis for Research ‚Üí"
  },
  {
    "objectID": "m04-text/archive/word2vec.html",
    "href": "m04-text/archive/word2vec.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "While TF-IDF gave us our first glimpse into distributed word representations, it had a fundamental limitation: it only considered document-level context. But language has rich structure at much finer scales. Word2Vec, introduced by Mikolov et al.¬†in 2013 {footcite}mikolov2013distributed, revolutionized word embeddings by focusing on local context windows.\nInstead of looking at entire documents, Word2Vec looks at small windows of text, typically 5-10 words wide. For example, in the sentence ‚ÄúThe cat chases mice in the garden‚Äù, with a window size of 2, the context for ‚Äúchases‚Äù would be [‚ÄúThe‚Äù, ‚Äúcat‚Äù, ‚Äúmice‚Äù, ‚Äúin‚Äù].\nThis shift from document-level to window-level context was revolutionary. It allowed the model to capture more nuanced relationships between words, as words that appear in similar immediate contexts often have similar grammatical roles or semantic meanings.\n\n\n\nLike TF-IDF, Word2Vec is fundamentally about learning from patterns of word co-occurrence. However, instead of creating a large sparse matrix of word-document counts, Word2Vec learns dense vector representations directly through a prediction task.\nThere are two main variants: - CBOW (Continuous Bag of Words) works like that fill-in-the-blank test. For example: The _____ chases mice in the garden This is similar to how we learn language - by understanding which words make sense in a given context. CBOW takes the surrounding context words and uses them to predict the center word that would make sense in that context. - Skip-gram is a much more challenging task. It tries to predict the surrounding context words given a center word. For example: _____ cat _____ _____ _____ _____. Note that the order of the context words does not matter, i.e., Skip-gram predicts the set of context words, so ‚Äú{garden, in, the, mice, chases, The}‚Äù is equally correct as ‚Äú{cat, in, the, mice, chases, The}‚Äù.\n\n\n\nword2vec can be represented as a neural network with a single hidden layer as follows.\n\nInput layer: The input layer consists of N neurons, where N is the size of the vocabulary (i.e., the number of unique words in the corpus). Each neuron corresponds to a unique word in the vocabulary. When a word is inputted, its corresponding neuron is activated and the other neurons are inhibited. Thus, the input layer is essentially a lookup mechanism that transforms the input word into a corresponding one-hot vector.\nOutput layer: The output layer also consists of N neurons, each corresponding to a unique word in the vocabulary. Unlike the input layer, multiple neurons can be activated for a single input. The strength of the activation of each neuron (with a normalization by the softmax function) represents the probability of the corresponding word being the input word‚Äôs context.\nHidden layer: The hidden layer is much smaller than the input and output layers. Multiple neurons in the hidden layer can be activated for a single input, and this activation pattern represents the word‚Äôs embedding.\n\n\n\n\nIn the Skip-gram model, given a word, we try to predict the probability of seeing each possible context word. Specifically, for a center word w_c and a context word w_o, we want:\nP(w_o|w_c) = \\dfrac{\\exp(v_{w_o}^T v_{w_c})}{\\sum_{w \\in V} \\exp(v_w^T v_{w_c})}\nwhere: - v_{w_c} is the vector representation of the center word - v_{w_o} is the vector representation of the output word - V is the vocabulary\nNotice the softmax function in the equation. This transforms the raw dot product scores into proper probabilities that sum to 1. However, this normalization over the entire vocabulary becomes computationally expensive for large vocabularies.\n\n\n\nCBOW works in the opposite direction, predicting the center word from the context. For context words w_{1}, ..., w_{C}, we have:\n\nP(w_c|w_1,...,w_C) = \\dfrac{\\exp(v_{w_c}^T \\bar{v})}{\\sum_{w \\in V} \\exp(v_w^T \\bar{v})},\n\nwhere \\bar{v} = \\dfrac{1}{C}\\sum_{i=1}^C v_{w_i} is the average of the context word vectors.\n\n\n\n\nword2vec can be represented as a neural network with a single hidden layer as follows. So it appears to be different from the word embedding we constructed using tf-idf matrix factorization. However, word2vec implicitly factorizes a matrix {footcite}levy2014neural as follows.\n\nM = (M_{ij}), \\quad M_{ij} = \\log \\dfrac{P(w_i,  w_j)}{P(w_j)P(w_j)}\n\nwhere M is the matrix that word2vec implicitly factorizes. M_{ij} is called the pointwise mutual information between words w_i and w_j. M_{ij} is the smallest when w_i and w_j appear independently, and the largest when w_i and w_j always appear together. Likewise tf-idf, it normalizes the mere co-occurrence counts (P(w_i, w_j)) by the probabilities of the words (P(w_i) and P(w_j)), creating a similar effect as tf-idf.\nWord embeddings learned by word2vec are essentially constructed by factorizing the pointwise mutual information matrix, and the similarity between words approximately preserves the PMI values.\n\nv_{w_i} ^\\top v_{w_j} \\approx M_{ij}\n\nThis means that words that frequently co-appear in the same context tend to be similar to each other (a high PMI value), and vice versa.\nThis connection to matrix factorization helps explain why Word2Vec works: it's finding a low-dimensional representation that captures the essential patterns in word co-occurrence statistics, just like how PCA finds low-dimensional representations that capture variance in data.\n\n\n\nThe above approximation is only valid when the embedding dimension is sufficiently large. Adding softmax transforms the problem from simple matrix factorization into a Boltzmann machine. While this gives us proper probabilities, it introduces a major computational challenge: computing the normalization constant requires summing over the entire vocabulary. For a vocabulary of 100,000 words, this means computing 100,000 exponentials for every prediction!\n\n\nTo make training feasible, Word2Vec uses hierarchical softmax. Instead of computing probabilities over the entire vocabulary at once, it:\n\nArranges words in a binary tree (usually a Huffman tree)\nTransforms the prediction problem into a sequence of binary decisions\nReduces computation from O(|V|) to O(log|V|)\n\nThis is similar to how you might play \"20 questions\" to guess a word. Each question splits the possible answers in half, making the process much more efficient than checking each possibility one by one.\n\n\n\n\nWord2Vec demonstrated that meaningful word representations could be learned from local context alone, without requiring expensive annotation or linguistic expertise. Its success inspired many subsequent developments in NLP, including:\n\nGloVe: Combining the benefits of matrix factorization and local context\nFastText: Adding subword information to handle out-of-vocabulary words\nContextual embeddings like BERT: Learning dynamic representations that change based on context\n\nThe principles behind Word2Vec - learning from context and using clever approximations to handle scale - continue to influence modern NLP architectures. Even large language models like GPT can be seen as sophisticated extensions of these basic ideas.\n\n\n\nWith word2vec, words are represented as dense vectors, enabling us to explore their relationships using simple linear algebra. This is in contrast to traditional natural language processing (NLP) methods, such as bag-of-words and topic modeling, which represent words as discrete units or high-dimensional vectors.\n\nTo showcase the effectiveness of word2vec, let‚Äôs walk through an example using the gensim library.\n```fowftpclpsy ipython3 import gensim import gensim.downloader from gensim.models import Word2Vec"
  },
  {
    "objectID": "m04-text/archive/word2vec.html#from-documents-to-windows",
    "href": "m04-text/archive/word2vec.html#from-documents-to-windows",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "While TF-IDF gave us our first glimpse into distributed word representations, it had a fundamental limitation: it only considered document-level context. But language has rich structure at much finer scales. Word2Vec, introduced by Mikolov et al.¬†in 2013 {footcite}mikolov2013distributed, revolutionized word embeddings by focusing on local context windows.\nInstead of looking at entire documents, Word2Vec looks at small windows of text, typically 5-10 words wide. For example, in the sentence ‚ÄúThe cat chases mice in the garden‚Äù, with a window size of 2, the context for ‚Äúchases‚Äù would be [‚ÄúThe‚Äù, ‚Äúcat‚Äù, ‚Äúmice‚Äù, ‚Äúin‚Äù].\nThis shift from document-level to window-level context was revolutionary. It allowed the model to capture more nuanced relationships between words, as words that appear in similar immediate contexts often have similar grammatical roles or semantic meanings."
  },
  {
    "objectID": "m04-text/archive/word2vec.html#learning-from-co-occurrence",
    "href": "m04-text/archive/word2vec.html#learning-from-co-occurrence",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Like TF-IDF, Word2Vec is fundamentally about learning from patterns of word co-occurrence. However, instead of creating a large sparse matrix of word-document counts, Word2Vec learns dense vector representations directly through a prediction task.\nThere are two main variants: - CBOW (Continuous Bag of Words) works like that fill-in-the-blank test. For example: The _____ chases mice in the garden This is similar to how we learn language - by understanding which words make sense in a given context. CBOW takes the surrounding context words and uses them to predict the center word that would make sense in that context. - Skip-gram is a much more challenging task. It tries to predict the surrounding context words given a center word. For example: _____ cat _____ _____ _____ _____. Note that the order of the context words does not matter, i.e., Skip-gram predicts the set of context words, so ‚Äú{garden, in, the, mice, chases, The}‚Äù is equally correct as ‚Äú{cat, in, the, mice, chases, The}‚Äù."
  },
  {
    "objectID": "m04-text/archive/word2vec.html#neural-network-representation",
    "href": "m04-text/archive/word2vec.html#neural-network-representation",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "word2vec can be represented as a neural network with a single hidden layer as follows.\n\nInput layer: The input layer consists of N neurons, where N is the size of the vocabulary (i.e., the number of unique words in the corpus). Each neuron corresponds to a unique word in the vocabulary. When a word is inputted, its corresponding neuron is activated and the other neurons are inhibited. Thus, the input layer is essentially a lookup mechanism that transforms the input word into a corresponding one-hot vector.\nOutput layer: The output layer also consists of N neurons, each corresponding to a unique word in the vocabulary. Unlike the input layer, multiple neurons can be activated for a single input. The strength of the activation of each neuron (with a normalization by the softmax function) represents the probability of the corresponding word being the input word‚Äôs context.\nHidden layer: The hidden layer is much smaller than the input and output layers. Multiple neurons in the hidden layer can be activated for a single input, and this activation pattern represents the word‚Äôs embedding.\n\n\n\n\nIn the Skip-gram model, given a word, we try to predict the probability of seeing each possible context word. Specifically, for a center word w_c and a context word w_o, we want:\nP(w_o|w_c) = \\dfrac{\\exp(v_{w_o}^T v_{w_c})}{\\sum_{w \\in V} \\exp(v_w^T v_{w_c})}\nwhere: - v_{w_c} is the vector representation of the center word - v_{w_o} is the vector representation of the output word - V is the vocabulary\nNotice the softmax function in the equation. This transforms the raw dot product scores into proper probabilities that sum to 1. However, this normalization over the entire vocabulary becomes computationally expensive for large vocabularies.\n\n\n\nCBOW works in the opposite direction, predicting the center word from the context. For context words w_{1}, ..., w_{C}, we have:\n\nP(w_c|w_1,...,w_C) = \\dfrac{\\exp(v_{w_c}^T \\bar{v})}{\\sum_{w \\in V} \\exp(v_w^T \\bar{v})},\n\nwhere \\bar{v} = \\dfrac{1}{C}\\sum_{i=1}^C v_{w_i} is the average of the context word vectors."
  },
  {
    "objectID": "m04-text/archive/word2vec.html#the-matrix-factorization-connection",
    "href": "m04-text/archive/word2vec.html#the-matrix-factorization-connection",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "word2vec can be represented as a neural network with a single hidden layer as follows. So it appears to be different from the word embedding we constructed using tf-idf matrix factorization. However, word2vec implicitly factorizes a matrix {footcite}levy2014neural as follows.\n\nM = (M_{ij}), \\quad M_{ij} = \\log \\dfrac{P(w_i,  w_j)}{P(w_j)P(w_j)}\n\nwhere M is the matrix that word2vec implicitly factorizes. M_{ij} is called the pointwise mutual information between words w_i and w_j. M_{ij} is the smallest when w_i and w_j appear independently, and the largest when w_i and w_j always appear together. Likewise tf-idf, it normalizes the mere co-occurrence counts (P(w_i, w_j)) by the probabilities of the words (P(w_i) and P(w_j)), creating a similar effect as tf-idf.\nWord embeddings learned by word2vec are essentially constructed by factorizing the pointwise mutual information matrix, and the similarity between words approximately preserves the PMI values.\n\nv_{w_i} ^\\top v_{w_j} \\approx M_{ij}\n\nThis means that words that frequently co-appear in the same context tend to be similar to each other (a high PMI value), and vice versa.\nThis connection to matrix factorization helps explain why Word2Vec works: it's finding a low-dimensional representation that captures the essential patterns in word co-occurrence statistics, just like how PCA finds low-dimensional representations that capture variance in data."
  },
  {
    "objectID": "m04-text/archive/word2vec.html#the-softmax-challenge",
    "href": "m04-text/archive/word2vec.html#the-softmax-challenge",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "The above approximation is only valid when the embedding dimension is sufficiently large. Adding softmax transforms the problem from simple matrix factorization into a Boltzmann machine. While this gives us proper probabilities, it introduces a major computational challenge: computing the normalization constant requires summing over the entire vocabulary. For a vocabulary of 100,000 words, this means computing 100,000 exponentials for every prediction!\n\n\nTo make training feasible, Word2Vec uses hierarchical softmax. Instead of computing probabilities over the entire vocabulary at once, it:\n\nArranges words in a binary tree (usually a Huffman tree)\nTransforms the prediction problem into a sequence of binary decisions\nReduces computation from O(|V|) to O(log|V|)\n\nThis is similar to how you might play \"20 questions\" to guess a word. Each question splits the possible answers in half, making the process much more efficient than checking each possibility one by one."
  },
  {
    "objectID": "m04-text/archive/word2vec.html#impact-and-legacy",
    "href": "m04-text/archive/word2vec.html#impact-and-legacy",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Word2Vec demonstrated that meaningful word representations could be learned from local context alone, without requiring expensive annotation or linguistic expertise. Its success inspired many subsequent developments in NLP, including:\n\nGloVe: Combining the benefits of matrix factorization and local context\nFastText: Adding subword information to handle out-of-vocabulary words\nContextual embeddings like BERT: Learning dynamic representations that change based on context\n\nThe principles behind Word2Vec - learning from context and using clever approximations to handle scale - continue to influence modern NLP architectures. Even large language models like GPT can be seen as sophisticated extensions of these basic ideas."
  },
  {
    "objectID": "m04-text/archive/word2vec.html#hands-on-with-word2vec",
    "href": "m04-text/archive/word2vec.html#hands-on-with-word2vec",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "With word2vec, words are represented as dense vectors, enabling us to explore their relationships using simple linear algebra. This is in contrast to traditional natural language processing (NLP) methods, such as bag-of-words and topic modeling, which represent words as discrete units or high-dimensional vectors.\n\nTo showcase the effectiveness of word2vec, let‚Äôs walk through an example using the gensim library.\n```fowftpclpsy ipython3 import gensim import gensim.downloader from gensim.models import Word2Vec"
  },
  {
    "objectID": "m04-text/archive/word2vec.html#exercise",
    "href": "m04-text/archive/word2vec.html#exercise",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "üî•üî• Exercise üî•üî•",
    "text": "üî•üî• Exercise üî•üî•\n\nUsing the word2vec model, find the 5 most similar words to ‚Äúcomputer‚Äù and ‚Äúscience‚Äù. What do you observe about the semantic relationships between these words?\nPerform the following word analogy tasks using word2vec and explain your findings:\n\nman : woman :: king : ?\nParis : France :: Tokyo : ?\ncar : cars :: child : ?\n\nCreate a visualization similar to the country-capital example above but using:\n\nDifferent professions and their typical workplaces (e.g., doctor-hospital, teacher-school)\nDifferent languages and their countries (e.g., Spanish-Spain, French-France)\n\nCompare the patterns you observe with the country-capital relationships.\nAdvanced: Investigate the concept of ‚Äúgender bias‚Äù in word embeddings:\n\nFind the vector difference between pairs like ‚Äúhe-she‚Äù, ‚Äúman-woman‚Äù, ‚Äúking-queen‚Äù\nProject profession words (e.g., ‚Äúdoctor‚Äù, ‚Äúnurse‚Äù, ‚Äúengineer‚Äù, ‚Äúteacher‚Äù) onto these gender directions\nWhat does this tell us about potential biases in the training data?"
  },
  {
    "objectID": "m04-text/bert-gpt.html",
    "href": "m04-text/bert-gpt.html",
    "title": "BERT, GPT, and Sentence Transformers",
    "section": "",
    "text": "What you‚Äôll learn in this module\n\n\n\nBERT and GPT are not variants of one architecture. They represent fundamentally different information flows: BERT reads the entire sentence at once for deep understanding, while GPT processes sequentially for text generation. Understanding this distinction shapes how you choose the right model for your task.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "BERT & GPT"
    ]
  },
  {
    "objectID": "m04-text/bert-gpt.html#the-spoiler",
    "href": "m04-text/bert-gpt.html#the-spoiler",
    "title": "BERT, GPT, and Sentence Transformers",
    "section": "",
    "text": "The difference between BERT and GPT isn‚Äôt just architecture; it‚Äôs the difference between studying a completed map and exploring a new territory one step at a time.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "BERT & GPT"
    ]
  },
  {
    "objectID": "m04-text/bert-gpt.html#two-siblings-bert-and-gpt",
    "href": "m04-text/bert-gpt.html#two-siblings-bert-and-gpt",
    "title": "BERT, GPT, and Sentence Transformers",
    "section": "Two Siblings, BERT and GPT",
    "text": "Two Siblings, BERT and GPT\n\nWe instinctively think of ‚ÄúTransformers‚Äù as a single unified model, but this is wrong. The original Transformer paper proposed an Encoder-Decoder architecture, a two-part machine. Modern models split this architecture in half, creating two distinct lineages with fundamentally different information flows. BERT (Bidirectional Encoder Representations from Transformers) uses the encoder stack and sees everything at once, like reading a completed sentence. GPT (Generative Pre-trained Transformer) uses the decoder stack and processes text causally, like improvising a story where you can only react to what‚Äôs already been said. This architectural choice isn‚Äôt cosmetic. It determines what the model can learn and what tasks it excels at.\nThink of it like two different reading strategies. BERT is the student who reads the entire paragraph, then goes back to understand each word in context. GPT is the actor performing a cold read, processing each line sequentially without peeking ahead at the script. The first strategy gives you deeper understanding; the second gives you the ability to continue the story.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "BERT & GPT"
    ]
  },
  {
    "objectID": "m04-text/bert-gpt.html#architecture",
    "href": "m04-text/bert-gpt.html#architecture",
    "title": "BERT, GPT, and Sentence Transformers",
    "section": "Architecture",
    "text": "Architecture\n\nPerhaps the most important difference between BERT and GPT is the attention mechanism. BERT uses bidirectional attention, meaning every token at position t can attend to every other token. This allows BERT to understand the context of a word by looking at all the words in the sentence, not just the ones before it, helping it capture the full context of a token.\nGPT uses masked (or causal) attention, meaning a token at position t can only attend to previous tokens. This masking imposes a causal constraint, making it ideal for tasks like language generation where the model must predict future tokens based only on past context. Although GPT‚Äôs attention is not bidirectional and thus less globally context-aware than BERT‚Äôs, this causal processing allows it to generate remarkably fluent and coherent text sequentially.\n\nMore on BERT\nBERT uses several special tokens to represent the input sentence. The [CLS] token is used to represent the start of the sentence. The [SEP] token is used to represent the end of the sentence. The [MASK] token is used to represent masked words. The [UNK] token is used to represent unknown words. For example, the sentence ‚ÄúThe cat sat on the mat. It then went to sleep.‚Äù is represented as ‚Äú[CLS] The cat sat on the mat [SEP] It then went to sleep [SEP]‚Äù.\nIn BERT, the [CLS] token is used to classify the input sentences. As a result, the model learns to encode a summary of the input sentence into the [CLS] token, which is particularly useful when we want the embedding of the whole input text, instead of the token level embeddings.\nBERT uses position and segment embeddings to provide the model with information about the position of the tokens in the sequence. Position embeddings provide information about the position of tokens in the sequence. Unlike the sinusoidal position embedding used in the original transformer paper, BERT uses learnable position embeddings. Segment embeddings distinguish the sentences in the input. For example, for the sentence ‚ÄúThe cat sat on the mat. It then went to sleep.‚Äù, the tokens in the first sentence are added with segment embedding 0, and the tokens in the second sentence are added with segment embedding 1. These segment embeddings are also learned during the pre-training process.\nSeveral BERT variants have been developed. RoBERTa (Robustly Optimized BERT Approach) improved upon BERT through several optimizations: removing the Next Sentence Prediction task, using dynamic masking that changes the mask patterns across training epochs, training with larger batches, and using a larger dataset. These changes led to significant performance improvements while maintaining BERT‚Äôs core architecture.\nDistilBERT focused on making BERT more efficient through knowledge distillation, where a smaller student model learns from the larger BERT teacher model. It achieves 95% of BERT‚Äôs performance while being 40% smaller and 60% faster, making it more suitable for resource-constrained environments and real-world applications.\nALBERT introduced parameter reduction techniques to address BERT‚Äôs memory limitations. It uses factorized embedding parameterization and cross-layer parameter sharing to dramatically reduce parameters while maintaining performance. ALBERT also replaced Next Sentence Prediction with Sentence Order Prediction, where the model must determine if two consecutive sentences are in the correct order.\nDomain-specific BERT models have been trained on specialized corpora to better handle specific fields. Examples include BioBERT for biomedical text, SciBERT for scientific papers, and FinBERT for financial documents. These models demonstrate superior performance in their respective domains compared to the general-purpose BERT.\nMultilingual BERT (mBERT) was trained on Wikipedia data from 104 languages, using a shared vocabulary across all languages. Despite not having explicit cross-lingual objectives during training, mBERT shows remarkable zero-shot cross-lingual transfer abilities, allowing it to perform tasks in languages it wasn‚Äôt explicitly aligned on. This has made it a valuable resource for low-resource languages and cross-lingual applications.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "BERT & GPT"
    ]
  },
  {
    "objectID": "m04-text/bert-gpt.html#training",
    "href": "m04-text/bert-gpt.html#training",
    "title": "BERT, GPT, and Sentence Transformers",
    "section": "Training",
    "text": "Training\n\n\n\n\n\n\nBERT\nThe fundamental difference in their architectures naturally leads to distinct training objectives. BERT, with its encoder-only design, is trained using two primary unsupervised tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP).\n\n\n\n\n\nIn MLM, a percentage of input tokens are randomly masked, and the model is tasked with predicting the original masked tokens based on the full, bidirectional context of the sentence. For example, given the sentence ‚ÄúThe quick brown fox jumps over the lazy dog,‚Äù BERT might see ‚ÄúThe quick brown [MASK] jumps over the lazy dog‚Äù and predict ‚Äúfox.‚Äù\n\nNSP involves presenting the model with two sentences and asking it to predict whether the second sentence logically follows the first. For instance, given Sentence A: ‚ÄúThe cat sat on the mat.‚Äù and Sentence B: ‚ÄúIt was a sunny day.‚Äù, BERT would predict ‚ÄòIsNextSentence = No‚Äô, whereas for Sentence A: ‚ÄúThe cat sat on the mat.‚Äù and Sentence B: ‚ÄúIt was purring softly.‚Äù, BERT would predict ‚ÄòIsNextSentence = Yes‚Äô. These tasks enable BERT to learn deep contextual representations useful for understanding existing text.\n\n\nRecipe for MLM\nTo generate training data for MLM, BERT randomly masks 15% of the tokens in each sequence. However, the masking process is not as straightforward as simply replacing words with [MASK] tokens. For the 15% of tokens chosen for masking, 80% of the time the word is replaced with the [MASK] token (example: ‚Äúthe cat sat on the mat‚Äù becomes ‚Äúthe cat [MASK] on the mat‚Äù). 10% of the time the word is replaced with a random word (example: ‚Äúthe cat sat on the mat‚Äù becomes ‚Äúthe cat dog on the mat‚Äù). 10% of the time the word is kept unchanged (example: ‚Äúthe cat sat on the mat‚Äù stays ‚Äúthe cat sat on the mat‚Äù).\nThe model must predict the original token for all selected positions, regardless of whether they were masked, replaced, or left unchanged. This helps prevent the model from simply learning to detect replaced tokens. During training, the model processes the modified input sequence through its transformer layers and predicts the original token at each masked position using the contextual representations. While replacing words with random tokens or leaving them unchanged may seem counterintuitive, research has shown this approach is effective. It has become an essential component of BERT‚Äôs pre-training process.\n\n\nRecipe for NSP\nNext Sentence Prediction (NSP) trains BERT to understand relationships between sentences. The model learns to predict whether two sentences naturally follow each other in text. During training, half of the sentence pairs are consecutive sentences from documents (labeled as IsNext), while the other half are random sentence pairs (labeled as NotNext).\nThe input format uses special tokens to structure the sentence pairs: a [CLS] token at the start, the first sentence, a [SEP] token, the second sentence, and a final [SEP] token. For instance:\n\n\\text{``[CLS] }\\underbrace{\\text{I went to the store}}_{\\text{Sentence 1}}\\text{ [SEP] }\\underbrace{\\text{They were out of milk}}_{\\text{Sentence 2}}\\text{ [SEP]''}.\n\nBERT uses the final hidden state of the [CLS] token to classify whether the sentences are consecutive or not. This helps the model develop a broader understanding of language context and relationships between sentences.\n\n\nGPT\nGPT, on the other hand, uses its decoder-only architecture for Causal Language Modeling (CLM). Causal (autoregressive) language modeling is the pre-training objective of GPT, where the model learns to predict the next token given all previous tokens in the sequence. More formally, given a sequence of tokens (x_1, x_2, ..., x_n), the model is trained to maximize the likelihood:\n\nP(x_1, ..., x_n) = \\prod_{i=1}^n P(x_i|x_1, ..., x_{i-1})\n\nFor example, given the partial sentence ‚ÄúThe cat sat on‚Äù, the model learns to predict the next word by calculating probability distributions over its entire vocabulary. During training, it might learn that ‚Äúmat‚Äù has a high probability in this context, while ‚Äúlaptop‚Äù has a lower probability.\nThis autoregressive nature means GPT always processes text from left to right, learning to generate coherent and grammatically correct continuations. This objective directly aligns with its strength in text generation.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "BERT & GPT"
    ]
  },
  {
    "objectID": "m04-text/bert-gpt.html#visualizing-attention-what-is-bert-looking-at",
    "href": "m04-text/bert-gpt.html#visualizing-attention-what-is-bert-looking-at",
    "title": "BERT, GPT, and Sentence Transformers",
    "section": "Visualizing Attention: What Is BERT Looking At?",
    "text": "Visualizing Attention: What Is BERT Looking At?\nBERT produces attention weights‚Äîa matrix showing which tokens influence each other. We can extract these weights and visualize them to understand how the model disambiguates meaning.\n\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load a small BERT model\nmodel_name = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name, output_attentions=True)\n\ntext = \"The bank of the river.\"\ninputs = tokenizer(text, return_tensors=\"pt\")\noutputs = model(**inputs)\n\n# Get attention from the last layer\n# Shape: (batch, heads, seq_len, seq_len)\nattention = outputs.attentions[-1].squeeze(0)\n\n# Average attention across all heads for simplicity\nmean_attention = attention.mean(dim=0).detach().numpy()\ntokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n\n# Plot\nplt.figure(figsize=(8, 6))\nsns.heatmap(mean_attention, xticklabels=tokens, yticklabels=tokens, cmap=\"viridis\")\nplt.title(\"BERT Attention Map (Last Layer)\")\nplt.xlabel(\"Key (Source)\")\nplt.ylabel(\"Query (Target)\")\nplt.show()\n\nIn this heatmap, a bright spot at row ‚Äúbank‚Äù and column ‚Äúriver‚Äù reveals that BERT is using ‚Äúriver‚Äù to understand ‚Äúbank‚Äù, disambiguating it from a financial institution. This bidirectional flow is why BERT excels at tasks requiring deep contextual understanding like question answering and named entity recognition.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "BERT & GPT"
    ]
  },
  {
    "objectID": "m04-text/bert-gpt.html#the-takeaway",
    "href": "m04-text/bert-gpt.html#the-takeaway",
    "title": "BERT, GPT, and Sentence Transformers",
    "section": "The Takeaway",
    "text": "The Takeaway\nBERT reads to understand. GPT writes to create. Choose the architecture that matches your information flow: bidirectional for deep contextual analysis, causal for sequential generation.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "BERT & GPT"
    ]
  },
  {
    "objectID": "m04-text/llm-intro.html",
    "href": "m04-text/llm-intro.html",
    "title": "Large Language Models in Practice",
    "section": "",
    "text": "What you‚Äôll learn in this module\n\n\n\nLarge language models don‚Äôt understand language‚Äîthey compress statistical regularities from billions of text samples into probability distributions. We‚Äôll explore what LLMs are, how they work at a high level, and how to control them effectively as tools for practical tasks.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Large Language Models"
    ]
  },
  {
    "objectID": "m04-text/llm-intro.html#the-mechanism",
    "href": "m04-text/llm-intro.html#the-mechanism",
    "title": "Large Language Models in Practice",
    "section": "The Mechanism",
    "text": "The Mechanism\nWhat do you think about the following question?\n\nCan LLMs understand the world and reason about it?\n\nOne may argue that ‚Äúfluency‚Äù demonstrates understanding. This is the intuition behind Turing‚Äôs 1950 test: if you can‚Äôt tell it‚Äôs a machine, treat it as intelligent. Fluency implies comprehension.\nBut let‚Äôs see some counter arguments about this claim, starting with ELIZA.\nELIZA, developed by Joseph Weizenbaum in the mid-1960s, is widely considered one of the first chatbots. It simulated a Rogerian psychotherapist by using simple pattern matching and keyword substitution to generate responses. Despite its lack of true understanding, ELIZA famously convinced many users that they were conversing with an intelligent entity, highlighting the human tendency to anthropomorphize technology and the limitations of the Turing Test.\n\n\nAnother argument against fluency is the Chinese Room argument, proposed by philosopher John Searle. Imagine a person in a room who receives Chinese characters, and using an English rulebook, manipulates these symbols to produce new Chinese characters. To an outside observer, it appears the room understands Chinese. However, the person inside merely follows instructions to manipulate symbols without understanding their meaning. Searle argues that this is analogous to how computers, including LLMs, operate: they process symbols based on rules without genuine comprehension, raising questions about whether they can truly ‚Äúunderstand‚Äù language or the world.\n\n\nSo, do LLMs understand the world? Probably not, in the same way as we do. LLMs is a lossy compression algorithm, compressing the data into their parameters to generate fluent outputs. To predict ‚ÄúThe capital of France is ___,‚Äù the model must compress not just the fact (Paris) but the statistical regularities governing how facts appear in text‚Äîthat capitals follow ‚ÄúThe capital of,‚Äù that France is a country, that countries have capitals. This compression is probabilistic, not factual. The model stores P(\\text{word}_{n+1} \\mid \\text{word}_1, \\ldots, \\text{word}_n)‚Äîwhich words tend to follow which other words in which contexts. Just as the lottery memorizer stores patterns of number sequences, the LLM stores patterns of word sequences.\n\n\n\n\n\nTraining feeds the model billions of sentences. For each sentence, the model predicts the next word, compares its prediction to the actual next word, and adjusts its parameters to increase the probability of the correct word. Repeat trillions of times. The result: a compressed representation of how language behaves statistically. The model doesn‚Äôt learn ‚ÄúParis is the capital of France‚Äù as a fact; it learns that in contexts matching the pattern [The capital of France is], the token ‚ÄúParis‚Äù appears with high probability. The lottery memorizer doesn‚Äôt understand what draws mean; it just knows what patterns appear most often. This is why LLMs creates hallucination‚Äîfluent but false outputs. Truth and fluency correlate in the training data, so the model is mostly truthful. But in the tails‚Äîobscure topics, recent events, precise recall‚Äîfluency diverges from truth, and the model follows fluency.\nKeep this limitation in mind and use LLMs as a tool to scale pattern recognition, not judgment. Let‚Äôs learn how to utilize them.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Large Language Models"
    ]
  },
  {
    "objectID": "m04-text/llm-intro.html#setting-up-ollama",
    "href": "m04-text/llm-intro.html#setting-up-ollama",
    "title": "Large Language Models in Practice",
    "section": "Setting Up Ollama",
    "text": "Setting Up Ollama\nFor this course, we use Ollama, a tool for running LLMs locally, with Gemma 3N, a 4-billion parameter open-source model. It‚Äôs free, private, and capable enough for research tasks. Visit ollama.ai, download the installer, and verify installation.\nollama --version\nollama pull gemma3n:latest\nollama run gemma3n:latest \"What is a complex system?\"\nIf you receive a coherent response, install the Python client and send your first prompt.\npip install ollama\n\nimport ollama\n\nparams_llm = {\"model\": \"gemma3n:latest\", \"options\": {\"temperature\": 0.3}}\n\nresponse = ollama.generate(\n    prompt=\"Explain emergence in two sentences.\",\n    **params_llm\n)\n\nprint(response.response)\n\nEmergence is when complex patterns and behaviors arise from simple interactions between individual components in a system. These emergent properties are not predictable from the properties of the individual parts alone, representing a novel level of organization. \n\n\n\nRun this code twice. You‚Äôll get different outputs. Why? Because LLMs sample from probability distributions. The temperature parameter controls this randomness. Lower values (0.1) make outputs more deterministic; higher values (1.0) increase diversity. You‚Äôre controlling how far into the tail of the probability distribution the model samples. Low temperature: the model picks the most likely next word. High temperature: it ventures into less probable territory. Sometimes that produces creativity. Sometimes it produces nonsense.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Large Language Models"
    ]
  },
  {
    "objectID": "m04-text/llm-intro.html#research-applications",
    "href": "m04-text/llm-intro.html#research-applications",
    "title": "Large Language Models in Practice",
    "section": "Research Applications",
    "text": "Research Applications\nThe strategy is simple: use LLMs for tasks where speed trumps precision, then verify the outputs that matter. Three workflows demonstrate this pattern.\n\nAbstract Summarization\nYou collected 50 papers on network science. Which deserve detailed reading? You don‚Äôt have time to read all 50 abstracts carefully. An LLM scans them in seconds.\n\nabstract = \"\"\"\nCommunity detection in networks is a fundamental problem in complex systems.\nWhile many algorithms exist, most assume static networks. We propose a dynamic\ncommunity detection algorithm that tracks evolving communities over time using\na temporal smoothness constraint. We evaluate our method on synthetic and real\ntemporal networks, showing it outperforms static methods applied to temporal\nsnapshots. Our approach reveals how communities merge, split, and persist in\nsocial networks, biological systems, and transportation networks.\n\"\"\"\n\nprompt = f\"Summarize this abstract in one sentence:\\n\\n{abstract}\"\nresponse = ollama.generate(prompt=prompt, **params_llm)\nprint(response.response)\n\nThis paper introduces a novel dynamic community detection algorithm that effectively tracks evolving communities in networks over time, outperforming static methods and revealing community dynamics in various real-world systems.\n\n\n\n\n\n\nThe model captures the pattern: propose method, evaluate, outperform baselines. It doesn‚Äôt understand the paper; it has seen enough academic abstracts to recognize the structure. For multiple abstracts, loop through them.\n\nfor i, abstract in enumerate([\"Abstract 1...\", \"Abstract 2...\"], 1):\n    response = ollama.generate(prompt=f\"Summarize:\\n\\n{abstract}\", **params_llm)\n    print(f\"{i}. {response.response}\")\n\n1. Please **provide me with Abstract 1**! I need the text of the abstract to be able to summarize it for you. \n\nJust paste the abstract here, and I'll give you a concise summary. üòä \n\nI'm ready when you are!\n\n\n\n\n2. Please provide me with the content of \"Abstract 2\"! I need the text of the abstract to be able to summarize it for you. \n\nJust paste the abstract here, and I'll do my best to give you a concise and accurate summary. üòä \n\n\n\n\nLocal models are slow (2‚Äì5 seconds per abstract). For thousands of papers, switch to cloud APIs. But the workflow scales: delegate skimming to the model, retain judgment for yourself. I ran this on 200 abstracts about power-law distributions. Gemma flagged the 15 that used preferential attachment models. Saved me 4 hours. I still read all 15 myself.\n\n\nStructured Extraction\nTurn unstructured text into structured data automatically.\n\nabstract = \"\"\"\nWe analyze scientific collaboration networks using 5 million papers from\n2000-2020. Using graph neural networks and community detection, we identify\ndisciplinary boundaries and interdisciplinary bridges. Interdisciplinarity\nincreased 25%, with physics and CS showing strongest cross-connections.\n\"\"\"\n\nprompt = f\"\"\"Extract: Domain, Methods, Key Finding\\n\\n{abstract}\\n\\nFormat:\\nDomain:...\\nMethods:...\\nKey Finding:...\"\"\"\nresponse = ollama.generate(prompt=prompt, **params_llm)\nprint(response.response)\n\nHere's the extraction in the requested format:\n\nDomain: Scientific Collaboration Networks\nMethods: Graph Neural Networks, Community Detection, Analysis of 5 million papers (2000-2020)\nKey Finding: Interdisciplinarity increased by 25% between 2000-2020, with the strongest cross-connections observed between Physics and Computer Science.\n\n\n\n\n\n\nScale this to hundreds of papers for meta-analysis. Always verify. LLMs misinterpret obscure terminology and fabricate plausible-sounding technical details when uncertain. Remember: the model is pattern-matching against academic writing it‚Äôs seen, not reasoning about your domain.\n\n\nHypothesis Generation\nLLMs pattern-match against research questions they‚Äôve encountered in training data.\n\ncontext = \"\"\"I study concept spread in citation networks. Highly cited papers\ncombine existing concepts novelty. What should I study next?\"\"\"\n\nprompt = f\"\"\"Suggest three follow-up research questions:\\n\\n{context}\"\"\"\nresponse = ollama.generate(prompt=prompt, **params_llm)\nprint(response.response)\n\nOkay, that's a great starting point! You're investigating how highly cited papers leverage existing concepts while introducing novelty. Here are three follow-up research questions, building on that foundation, with explanations of why they're interesting and potentially fruitful:\n\n**1.  How does the *nature* of the novelty in highly cited papers differ from less cited papers?**\n\n*   **Why it's interesting:**  This question delves deeper into *what kind* of novelty is being introduced. Is it incremental (small changes to existing concepts), radical (completely new concepts), or a combination?  Understanding the type of novelty could reveal patterns in how highly cited papers are structured and framed.\n*   **Potential approaches:**\n    *   **Concept Extraction & Categorization:**  Use NLP techniques (e.g., topic modeling, named entity recognition, knowledge graph extraction) to identify and categorize the concepts discussed in papers.  Then, analyze the novelty of these concepts (e.g., using measures of semantic distance from existing concepts, or by comparing to a knowledge base).\n    *   **Manual Coding:**  A smaller, more in-depth analysis could involve manually coding a subset of papers to categorize the type of novelty (e.g., \"extension,\" \"recombination,\" \"paradigm shift\").\n    *   **Sentiment Analysis:** Analyze the sentiment associated with novel concepts. Are highly cited papers more likely to frame novelty in a positive or impactful way?\n*   **Expected Outcomes:**  You might find that highly cited papers tend to introduce novelty that builds upon established frameworks, or that they are more likely to introduce truly disruptive concepts.\n\n**2.  What role do interdisciplinary citations play in the concept spread of highly cited papers?**\n\n*   **Why it's interesting:** Highly cited papers often bridge disciplines.  Interdisciplinary citations could be a key mechanism for integrating existing concepts from different fields and generating novel insights.  This question explores the *source* of the concepts being combined.\n*   **Potential approaches:**\n    *   **Citation Network Analysis:**  Analyze the citation network to identify the proportion of citations to papers from different disciplines.  Then, correlate this with the novelty of the concepts discussed in the highly cited papers.\n    *   **Concept Mapping Across Disciplines:**  Identify concepts that are borrowed from multiple disciplines and track their spread through the citation network.\n    *   **Network Visualization:** Visualize the citation network, highlighting interdisciplinary connections and the flow of concepts between disciplines.\n*   **Expected Outcomes:**  You might find that highly cited papers are more likely to cite papers from multiple disciplines, and that these interdisciplinary citations are associated with higher levels of concept novelty.\n\n**3.  How does the framing of novelty (e.g., through metaphors, analogies, or narrative structures) influence the impact and spread of concepts in highly cited papers?**\n\n*   **Why it's interesting:**  The way a concept is presented can significantly affect its reception and adoption.  This question explores the *communication* of novelty.\n*   **Potential approaches:**\n    *   **Text Analysis:**  Use NLP techniques to identify and analyze the use of metaphors, analogies, and narrative structures in the text of highly cited papers.\n    *   **Qualitative Analysis:**  Manually examine a subset of papers to identify examples of how novelty is framed and discussed.\n    *   **Sentiment Analysis (again):**  Analyze the sentiment associated with the framing of novelty.  Is it presented as exciting, challenging, or controversial?\n*   **Expected Outcomes:**  You might find that highly cited papers are more likely to use compelling narratives or metaphors to frame novelty, making it more accessible and impactful.\n\n\n\nThese questions are all interconnected and could be explored in combination.  They aim to move beyond simply identifying highly cited papers and delve into the *mechanisms* that contribute to their success in spreading new concepts.  Good luck! Let me know if you'd like me to elaborate on any of these.\n\n\n\n\n\n\nTreat the model as a thought partner, not an oracle. It helps structure thinking but doesn‚Äôt possess domain expertise. The suggestions reflect patterns in how research questions are framed, not deep knowledge of your field.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Large Language Models"
    ]
  },
  {
    "objectID": "m04-text/llm-intro.html#failure-modes-and-boundaries",
    "href": "m04-text/llm-intro.html#failure-modes-and-boundaries",
    "title": "Large Language Models in Practice",
    "section": "Failure Modes and Boundaries",
    "text": "Failure Modes and Boundaries\nThe failure modes follow directly from the mechanism. LLMs fabricate plausibly because they optimize for fluency, not truth. Ask about a non-existent ‚ÄúSmith et al.¬†quantum paper‚Äù and receive fluent academic prose describing results that never happened. Always verify citations. The model has seen thousands of papers cited in the format ‚ÄúSmith et al.¬†(2023) demonstrated that‚Ä¶‚Äù and generates outputs matching that pattern even when the citation is fictional.\nContext limits are architectural. Models see only 2,000‚Äì8,000 tokens at once. Paste 100 abstracts and early ones are mathematically evicted from working memory. The model doesn‚Äôt ‚Äúremember‚Äù them; they‚Äôre gone. Knowledge cutoffs are temporal. Gemma 3N‚Äôs training ended early 2024. Ask about recent events and receive outdated information or plausible fabrications constructed from pre-cutoff patterns.\nReasoning is absent. LLMs pattern-match, they don‚Äôt reason. Ask ‚ÄúHow many r‚Äôs in ‚ÄòStrawberry‚Äô?‚Äù and the model might answer correctly via pattern matching against similar questions in training data, not by counting letters. Sometimes right. Often wrong. The model has no internal representation of what counting means.\nThese aren‚Äôt bugs to be fixed. They‚Äôre intrinsic to the architecture. Use LLMs to accelerate work, not replace judgment. They excel at summarizing text, extracting structure, reformulating concepts, brainstorming, generating synthetic examples, and translation. They fail at literature reviews without verification, factual claims without sources, statistical analysis, and ethical decisions. Harvest the center of the distribution where fluency and truth correlate. Defend against the tails where they diverge.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Large Language Models"
    ]
  },
  {
    "objectID": "m04-text/llm-intro.html#next",
    "href": "m04-text/llm-intro.html#next",
    "title": "Large Language Models in Practice",
    "section": "Next",
    "text": "Next\nYou‚Äôve seen LLMs in practice‚Äîsetup, summarization, extraction, limitations. But how do they actually work? What happens inside when you send a prompt?\nThe rest of this module unboxes the technology: prompt engineering (communicating with LLMs), embeddings (representing meaning as numbers), transformers (the architecture enabling modern NLP), fundamentals (from word counts to neural representations).\nFirst, let‚Äôs master talking to machines.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Large Language Models"
    ]
  },
  {
    "objectID": "m04-text/semaxis.html",
    "href": "m04-text/semaxis.html",
    "title": "SemAxis: Meaning as Direction",
    "section": "",
    "text": "Semaxis\n\n\nWe intuitively treat word embeddings as static maps where ‚Äúking‚Äù is simply near ‚Äúqueen.‚Äù We assume the meaning is inherent to the coordinate itself, much like a city has a fixed latitude and longitude. This is a convenient fiction. In embedding, meaning emerges entirely from contrast, which is the key concept of Semaxis.\nSemaxis (An, Kwak, and Ahn 2018, kwak2020semaxis) is a way to define a semantic axis by subtracting the vector of an antonym from a word (e.g., v_{good} - v_{bad}). This isolates a semantic dimension‚Äîan ‚Äúaxis‚Äù‚Äîthat ignores all other information.\nFormally, given two pole words w_+ and w_-, the axis is defined as:\n\nv_{\\text{axis}} = \\frac{v_{w_+} - v_{w_-}}{||v_{w_+} - v_{w_-}\\||_2}\n\nwhere the denominator is the L_2 norm of the difference vector that ensures that axis vector v_{axis} is a unit vector.\nUsing this ‚Äúruler‚Äù, we project the words into this axis. Operationally, the position of a word w is given by the cosine similarity between v_{w} and v_{axis}.\n\n\\text{Position of w on axis $v_{\\text{axis}}$} = \\cos(v_{\\text{axis}},v_{w})\n\nWe will build a ‚ÄúSentiment Compass‚Äù to measure the emotional charge of words that aren‚Äôt explicitly emotional.\nFirst, we load the standard GloVe embeddings.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gensim.downloader as api\n\n# Download and load pre-trained GloVe embeddings\nmodel = api.load(\"glove-wiki-gigaword-100\")\n\n\n\nWe define the axis not as a point, but as the difference vector between two poles. This vector points from ‚Äúbad‚Äù to ‚Äúgood.‚Äù\n\ndef create_axis(pos_word, neg_word, model):\n    return model[pos_word] - model[neg_word]\n\n\n# The \"Sentiment\" Axis\nsentiment_axis = create_axis(\"good\", \"bad\", model)\n\n\n\n\nTo see where a word falls on this axis, we project it. Mathematically, this is the dot product (normalized). If the vector points in the same direction, the score is positive; if it points away, it is negative.\n\ndef get_score(word, axis, model):\n    v_word = model[word]\n    # Cosine similarity is just a normalized dot product\n    return np.dot(v_word, axis) / (np.linalg.norm(v_word) * np.linalg.norm(axis))\n\n\nwords = [\"excellent\", \"terrible\", \"mediocre\", \"stone\", \"flower\"]\nfor w in words:\n    print(f\"{w}: {get_score(w, sentiment_axis, model):.3f}\")\n\nexcellent: 0.523\nterrible: -0.208\nmediocre: -0.001\nstone: 0.181\nflower: 0.204\n\n\n\n\n\n\n\n\nSemaxis\n\n\nSingle words are noisy. ‚ÄúBad‚Äù might carry connotations of ‚Äúnaughty‚Äù or ‚Äúpoor quality.‚Äù To fix this, we don‚Äôt use single words; we use the centroid of a cluster of synonyms. This averages out the noise and leaves only the pure semantic signal.\n\ndef create_robust_axis(pos_word, neg_word, model, k=5):\n    # Get k nearest neighbors for both poles\n    pos_group = [pos_word]\n    pos_words = model.most_similar(pos_word, topn=k)\n    for word, _ in pos_words:\n        pos_group.append(word)\n\n    neg_group = [neg_word]\n    neg_words = model.most_similar(neg_word, topn=k)\n    for word, _ in neg_words:\n        neg_group.append(word)\n\n    # Average them to find the centroid\n    pos_vec = np.mean([model[w] for w in pos_group], axis=0)\n    neg_vec = np.mean([model[w] for w in neg_group], axis=0)\n\n    return pos_vec - neg_vec\n\n\nrobust_axis = create_robust_axis(\"good\", \"bad\", model)\n\n\n\n\nThe real power comes when we cross two axes. By plotting words against ‚ÄúSentiment‚Äù and ‚ÄúIntensity‚Äù (Strong vs.¬†Weak), we reveal relationships that a single list hides.\n\n\nCode\ndef plot_2d(words, axis_x, axis_y, model):\n    x_scores = [get_score(w, axis_x, model) for w in words]\n    y_scores = [get_score(w, axis_y, model) for w in words]\n\n    plt.figure(figsize=(5, 5))\n    plt.scatter(x_scores, y_scores)\n\n    for i, w in enumerate(words):\n        plt.annotate(\n            w,\n            (x_scores[i], y_scores[i]),\n            xytext=(5, 5),\n            textcoords=\"offset points\",\n            fontsize=16,\n        )\n\n    plt.axhline(0, color=\"k\", alpha=0.3)\n    plt.axvline(0, color=\"k\", alpha=0.3)\n    plt.xlabel(\"Sentiment (Bad -&gt; Good)\")\n    plt.ylabel(\"Intensity (Weak -&gt; Strong)\")\n    plt.show()\n\n\nintensity_axis = create_axis(\"strong\", \"weak\", model)\ntest_words = [\n    \"excellent\",\n    \"terrible\",\n    \"mediocre\",\n    \"mild\",\n    \"extreme\",\n    \"murder\",\n    \"charity\",\n]\n\nplot_2d(test_words, sentiment_axis, intensity_axis, model)\n\n\n\n\n\n2D Semantic Space",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Semaxis"
    ]
  },
  {
    "objectID": "m04-text/semaxis.html#semaxis",
    "href": "m04-text/semaxis.html#semaxis",
    "title": "SemAxis: Meaning as Direction",
    "section": "",
    "text": "Semaxis\n\n\nWe intuitively treat word embeddings as static maps where ‚Äúking‚Äù is simply near ‚Äúqueen.‚Äù We assume the meaning is inherent to the coordinate itself, much like a city has a fixed latitude and longitude. This is a convenient fiction. In embedding, meaning emerges entirely from contrast, which is the key concept of Semaxis.\nSemaxis (An, Kwak, and Ahn 2018, kwak2020semaxis) is a way to define a semantic axis by subtracting the vector of an antonym from a word (e.g., v_{good} - v_{bad}). This isolates a semantic dimension‚Äîan ‚Äúaxis‚Äù‚Äîthat ignores all other information.\nFormally, given two pole words w_+ and w_-, the axis is defined as:\n\nv_{\\text{axis}} = \\frac{v_{w_+} - v_{w_-}}{||v_{w_+} - v_{w_-}\\||_2}\n\nwhere the denominator is the L_2 norm of the difference vector that ensures that axis vector v_{axis} is a unit vector.\nUsing this ‚Äúruler‚Äù, we project the words into this axis. Operationally, the position of a word w is given by the cosine similarity between v_{w} and v_{axis}.\n\n\\text{Position of w on axis $v_{\\text{axis}}$} = \\cos(v_{\\text{axis}},v_{w})\n\nWe will build a ‚ÄúSentiment Compass‚Äù to measure the emotional charge of words that aren‚Äôt explicitly emotional.\nFirst, we load the standard GloVe embeddings.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gensim.downloader as api\n\n# Download and load pre-trained GloVe embeddings\nmodel = api.load(\"glove-wiki-gigaword-100\")\n\n\n\nWe define the axis not as a point, but as the difference vector between two poles. This vector points from ‚Äúbad‚Äù to ‚Äúgood.‚Äù\n\ndef create_axis(pos_word, neg_word, model):\n    return model[pos_word] - model[neg_word]\n\n\n# The \"Sentiment\" Axis\nsentiment_axis = create_axis(\"good\", \"bad\", model)\n\n\n\n\nTo see where a word falls on this axis, we project it. Mathematically, this is the dot product (normalized). If the vector points in the same direction, the score is positive; if it points away, it is negative.\n\ndef get_score(word, axis, model):\n    v_word = model[word]\n    # Cosine similarity is just a normalized dot product\n    return np.dot(v_word, axis) / (np.linalg.norm(v_word) * np.linalg.norm(axis))\n\n\nwords = [\"excellent\", \"terrible\", \"mediocre\", \"stone\", \"flower\"]\nfor w in words:\n    print(f\"{w}: {get_score(w, sentiment_axis, model):.3f}\")\n\nexcellent: 0.523\nterrible: -0.208\nmediocre: -0.001\nstone: 0.181\nflower: 0.204\n\n\n\n\n\n\n\n\nSemaxis\n\n\nSingle words are noisy. ‚ÄúBad‚Äù might carry connotations of ‚Äúnaughty‚Äù or ‚Äúpoor quality.‚Äù To fix this, we don‚Äôt use single words; we use the centroid of a cluster of synonyms. This averages out the noise and leaves only the pure semantic signal.\n\ndef create_robust_axis(pos_word, neg_word, model, k=5):\n    # Get k nearest neighbors for both poles\n    pos_group = [pos_word]\n    pos_words = model.most_similar(pos_word, topn=k)\n    for word, _ in pos_words:\n        pos_group.append(word)\n\n    neg_group = [neg_word]\n    neg_words = model.most_similar(neg_word, topn=k)\n    for word, _ in neg_words:\n        neg_group.append(word)\n\n    # Average them to find the centroid\n    pos_vec = np.mean([model[w] for w in pos_group], axis=0)\n    neg_vec = np.mean([model[w] for w in neg_group], axis=0)\n\n    return pos_vec - neg_vec\n\n\nrobust_axis = create_robust_axis(\"good\", \"bad\", model)\n\n\n\n\nThe real power comes when we cross two axes. By plotting words against ‚ÄúSentiment‚Äù and ‚ÄúIntensity‚Äù (Strong vs.¬†Weak), we reveal relationships that a single list hides.\n\n\nCode\ndef plot_2d(words, axis_x, axis_y, model):\n    x_scores = [get_score(w, axis_x, model) for w in words]\n    y_scores = [get_score(w, axis_y, model) for w in words]\n\n    plt.figure(figsize=(5, 5))\n    plt.scatter(x_scores, y_scores)\n\n    for i, w in enumerate(words):\n        plt.annotate(\n            w,\n            (x_scores[i], y_scores[i]),\n            xytext=(5, 5),\n            textcoords=\"offset points\",\n            fontsize=16,\n        )\n\n    plt.axhline(0, color=\"k\", alpha=0.3)\n    plt.axvline(0, color=\"k\", alpha=0.3)\n    plt.xlabel(\"Sentiment (Bad -&gt; Good)\")\n    plt.ylabel(\"Intensity (Weak -&gt; Strong)\")\n    plt.show()\n\n\nintensity_axis = create_axis(\"strong\", \"weak\", model)\ntest_words = [\n    \"excellent\",\n    \"terrible\",\n    \"mediocre\",\n    \"mild\",\n    \"extreme\",\n    \"murder\",\n    \"charity\",\n]\n\nplot_2d(test_words, sentiment_axis, intensity_axis, model)\n\n\n\n\n\n2D Semantic Space",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Semaxis"
    ]
  },
  {
    "objectID": "m04-text/semaxis.html#the-takeaway",
    "href": "m04-text/semaxis.html#the-takeaway",
    "title": "SemAxis: Meaning as Direction",
    "section": "The Takeaway",
    "text": "The Takeaway\nTo define a concept, you must first define its opposite.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Semaxis"
    ]
  },
  {
    "objectID": "m04-text/tokenization.html",
    "href": "m04-text/tokenization.html",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "",
    "text": "What you‚Äôll learn in this module\n\n\n\nLLMs don‚Äôt read words as you do. They read compressed fragments called tokens, optimized for probability engines. This section explores why subword tokenization matters, how it works, and what it means for model behavior.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m04-text/tokenization.html#the-mechanism-why-subwords-not-words",
    "href": "m04-text/tokenization.html#the-mechanism-why-subwords-not-words",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "The Mechanism (Why Subwords, Not Words)",
    "text": "The Mechanism (Why Subwords, Not Words)\nYou might assume that an LLM reads text the way you do: word by word, with each word treated as an atomic unit. This is wrong. The model operates on tokens‚Äîsubword chunks that could be full words (‚Äúthe‚Äù), word parts (‚Äúingham‚Äù), or single characters (‚ÄúB‚Äù). This choice is not arbitrary; it‚Äôs a geometric compression strategy.\nIf we used whole words, the vocabulary would balloon to millions of entries. Each entry requires a row in the embedding matrix, meaning memory scales linearly with vocabulary size. A 1-million-word vocabulary with 2048-dimensional embeddings would require over 8GB just for the lookup table. Subword tokenization collapses this problem by focusing on frequently occurring fragments. With roughly 50,000 subwords, the model can reconstruct both common words (stored as single tokens) and rare words (assembled from parts). The system trades a small increase in sequence length for a massive reduction in memory and computational overhead.\nThis also explains why LLMs sometimes fail on seemingly trivial tasks like counting letters. The word ‚Äústrawberry‚Äù might tokenize as [\"straw\", \"berry\"], meaning the model never sees the individual ‚Äúr‚Äù characters as separate units. It‚Äôs not stupidity‚Äîit‚Äôs compression artifacts.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m04-text/tokenization.html#the-application-how-tokenization-works-in-practice",
    "href": "m04-text/tokenization.html#the-application-how-tokenization-works-in-practice",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "The Application (How Tokenization Works in Practice)",
    "text": "The Application (How Tokenization Works in Practice)\nLet‚Äôs unbox an actual tokenizer from Hugging Face and trace the pipeline from raw text to embeddings. We‚Äôll use Phi-1.5, a compact model from Microsoft. For tokenization experiments, we only need the tokenizer‚Äîno need to load the full multi-gigabyte model.\n\nfrom transformers import AutoTokenizer\nimport os\n\nmodel_name = \"microsoft/phi-1.5\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nLet‚Äôs inspect the tokenizer‚Äôs constraints.\n\n\nCode\nprint(f\"Vocabulary size: {tokenizer.vocab_size:,} tokens\")\nprint(f\"Max sequence length: {tokenizer.model_max_length} tokens\")\n\n\nVocabulary size: 50,257 tokens\nMax sequence length: 2048 tokens\n\n\nThis tokenizer knows 50,257 unique tokens and enforces a maximum sequence length of 2048 tokens. If your input exceeds this limit, the model will truncate or reject it. This is a hard boundary imposed by the positional encoding system, not a soft guideline.\n\nText to Tokens\nTokenization splits text into the subword fragments the model actually processes. Watch what happens when we tokenize a university name.\n\ntext = \"Binghamton University.\"\n\ntokens = tokenizer.tokenize(text)\n\n\n\nCode\nprint(f\"Tokens: {tokens}\")\n\n\nTokens: ['B', 'ingham', 'ton', 'ƒ†University', '.']\n\n\nThe rare word ‚ÄúBinghamton‚Äù fractures into ['B', 'ingham', 'ton']. The common word ‚ÄúUniversity‚Äù survives intact (with a leading space marker). The tokenizer learned these splits from frequency statistics during training. High-frequency words get dedicated tokens; rare words get decomposed into reusable parts.\n\n\nThe ƒ† character (U+0120) is a GPT-style tokenizer convention for encoding spaces. When you see ƒ†University, it means ‚ÄúUniversity‚Äù preceded by a space. This preserves word boundaries while allowing subword splits.\nLet‚Äôs test a few more examples to see the pattern.\n\n\nCode\ntexts = [\n    \"Bearcats\",\n    \"New York\",\n]\n\nprint(\"Word tokenization examples:\\n\")\nfor text in texts:\n    tokens = tokenizer.tokenize(text)\n    print(f\"{text:10s} ‚Üí {tokens}\")\n\n\nWord tokenization examples:\n\nBearcats   ‚Üí ['Bear', 'cats']\nNew York   ‚Üí ['New', 'ƒ†York']\n\n\n‚ÄúBearcats‚Äù splits because it‚Äôs domain-specific jargon. ‚ÄúNew York‚Äù remains whole because it‚Äôs common. The tokenizer‚Äôs behavior is a direct reflection of its training corpus.\n\n\nCheck out OpenAI‚Äôs tokenizer to see how different models slice the same text differently.\n\n\nTokens to Token IDs\nTokens are still strings. The model needs integers. Each token maps to a unique ID in the vocabulary dictionary.\n\n\nCode\ntext = \"Binghamton University\"\n\n# Get token IDs\ntoken_ids = tokenizer.encode(text, add_special_tokens=False)\ntokens = tokenizer.tokenize(text)\n\nprint(\"Token ‚Üí Token ID mapping:\\n\")\nfor token, token_id in zip(tokens, token_ids):\n    print(f\"{token:10s} ‚Üí {token_id:6d}\")\n\n\nToken ‚Üí Token ID mapping:\n\nB          ‚Üí     33\ningham     ‚Üí  25875\nton        ‚Üí   1122\nƒ†University ‚Üí   2059\n\n\nEach token receives a unique integer ID. The vocabulary is a dictionary: {token_string: integer_id}. Let‚Äôs peek inside.\n\n# Get the full vocabulary\nvocab = tokenizer.get_vocab()\n\n# Sample some tokens\nsample_tokens = list(vocab.items())[:5]\nfor token, id in sample_tokens:\n    print(f\"  {id:6d}: '{token}'\")\n\n   43503: 'ƒ†Lime'\n   29516: 'VO'\n   41002: 'ƒ†UTF'\n   41733: 'Ku'\n   33793: 'ƒ†indent'\n\n\nMost LLMs reserve special tokens for sequence boundaries or control signals. Phi-1.5 uses &lt;|endoftext|&gt; as a separator during training. Let‚Äôs verify.\n\ntoken_id = [50256]\ntoken = tokenizer.convert_ids_to_tokens(token_id)[0]\nprint(f\"Token ID: {token_id} ‚Üí Token: {token}\")\n\nToken ID: [50256] ‚Üí Token: &lt;|endoftext|&gt;\n\n\nToken ID 50256 is Phi-specific. Other models use different conventions (e.g., BERT uses [SEP] and [CLS]). Always check your tokenizer‚Äôs special tokens before preprocessing data.\n\n\nToken IDs to Embeddings\n\nNow we need the full model to access the embedding layer‚Äîthe matrix that converts token IDs into dense vectors.\n\nfrom transformers import AutoModelForCausalLM\nimport torch\n\n# Load the model\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Retrieve the embedding layer\nembedding_layer = model.model.embed_tokens\n\nThe embedding layer is a simple lookup table: a 51,200 √ó 2,048 matrix where each row is the embedding for a token in the vocabulary. Let‚Äôs examine the first few entries.\n\n\nCode\nprint(embedding_layer.weight[:5, :10])\n\n\ntensor([[ 0.0097, -0.0155,  0.0603,  0.0326, -0.0108, -0.0271, -0.0273,  0.0178,\n         -0.0242,  0.0100],\n        [ 0.0243,  0.0543,  0.0178, -0.0679, -0.0130,  0.0265, -0.0423, -0.0287,\n         -0.0051, -0.0179],\n        [-0.0416,  0.0370, -0.0160, -0.0254, -0.0371, -0.0208, -0.0023,  0.0647,\n          0.0468,  0.0013],\n        [-0.0051, -0.0044,  0.0248, -0.0489,  0.0399,  0.0005, -0.0070,  0.0148,\n          0.0030,  0.0070],\n        [ 0.0289,  0.0362, -0.0027, -0.0775, -0.0136, -0.0203, -0.0566, -0.0558,\n          0.0269, -0.0067]], grad_fn=&lt;SliceBackward0&gt;)\n\n\nThese numbers are learned parameters, optimized during training to capture semantic relationships. Token IDs are discrete symbols; embeddings are continuous coordinates in a 2048-dimensional space. This is what the transformer layers operate on.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m04-text/tokenization.html#the-bigger-picture",
    "href": "m04-text/tokenization.html#the-bigger-picture",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "The Bigger Picture",
    "text": "The Bigger Picture\nYou‚Äôve now traced the full pipeline: raw text fractures into subword tokens, tokens map to integer IDs, and IDs retrieve vector embeddings from a learned matrix. This tokenization step is foundational‚Äîwithout it, the model cannot begin processing language. The transformer layers come next, using attention mechanisms to extract patterns from these embeddings.\nRemember three constraints. First, LLMs operate on subwords, not words, because vocabulary size is a memory bottleneck. Second, tokenization is learned from data, not hand-designed, meaning different models will split text differently. Third, compression has side effects‚Äîtasks like character counting fail because the model never sees individual characters as atomic units.\nWith this machinery exposed, we‚Äôre ready to examine the transformer itself‚Äîthe architecture that processes these embeddings and enables LLMs to predict the next token.\n\nNext: Transformers: The Architecture Behind the Magic ‚Üí",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m04-text/word-bias.html",
    "href": "m04-text/word-bias.html",
    "title": "Word Bias",
    "section": "",
    "text": "What you‚Äôll learn in this module\n\n\n\nWord embeddings capture and reinforce societal biases from their training data through geometric relationships between vectors. This section explores how semantic axes reveal gender bias in occupations and concepts, demonstrating both the benefits and risks of embeddings encoding real-world associations.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Bias"
    ]
  },
  {
    "objectID": "m04-text/word-bias.html#bias-in-word-embeddings",
    "href": "m04-text/word-bias.html#bias-in-word-embeddings",
    "title": "Prompt Engineering",
    "section": "",
    "text": "Word embeddings can capture and reinforce societal biases from their training data through the geometric relationships between word vectors. These relationships often reflect stereotypes about gender, race, age and other social factors. We‚Äôll examine how semantic axes help analyze gender bias in job-related terms, showing both the benefits and risks of word embeddings capturing these real-world associations Bolukbasi et al. (2016).\nSemAxis is a powerful tool to analyze gender bias in word embeddings by measuring word alignments along semantic axes Kwak et al. (2021). Using antonym pairs like ‚Äúshe-he‚Äù as poles, it quantifies gender associations in words on a scale from -1 to 1, where positive values indicate feminine associations and negative values indicate masculine as ones.\nLet‚Äôs start with a simple example of analyzing gender bias in occupations.\n\nimport gensim.downloader as api\nimport numpy as np\n\n# Load pre-trained Word2vec embeddings\nmodel = api.load(\"word2vec-google-news-300\")\n\n\ndef compute_bias(word, microframe, model):\n    word_vector = model[word]\n    numerator = np.dot(word_vector, microframe)\n    denominator = np.linalg.norm(word_vector) * np.linalg.norm(microframe)\n    return numerator / denominator\n\n\ndef analyze(word, pos_word, neg_word, model):\n    if word not in model:\n        return 0.0\n    microframe = model[pos_word] - model[neg_word]\n    bias = compute_bias(word, microframe, model)\n    return bias\n\n\n\nThe compute_bias function calculates the cosine similarity between a word vector and a semantic axis (microframe).\n\nNumerator: Dot product projects the word onto the axis.\nDenominator: Normalizes by vector lengths to get a score between -1 and 1.\n\nWe will use the following occupations:\n\n# Occupations from the paper\nshe_occupations = [\n    \"homemaker\",\n    \"nurse\",\n    \"receptionist\",\n    \"librarian\",\n    \"socialite\",\n    \"hairdresser\",\n    \"nanny\",\n    \"bookkeeper\",\n    \"stylist\",\n    \"housekeeper\",\n]\n\nhe_occupations = [\n    \"maestro\",\n    \"skipper\",\n    \"protege\",\n    \"philosopher\",\n    \"captain\",\n    \"architect\",\n    \"financier\",\n    \"warrior\",\n    \"broadcaster\",\n    \"magician\",\n    \"boss\",\n]\n\nWe measure the gender bias in these occupations by measuring how they align with the ‚Äúshe-he‚Äù axis.\n\n\nCode\nprint(\"Gender Bias in Occupations (she-he axis):\")\nprint(\"\\nShe-associated occupations:\")\nfor occupation in she_occupations:\n    bias = analyze(occupation, \"she\", \"he\", model)\n    print(f\"{occupation}: {bias:.3f}\")\n\nprint(\"\\nHe-associated occupations:\")\nfor occupation in he_occupations:\n    bias = analyze(occupation, \"she\", \"he\", model)\n    print(f\"{occupation}: {bias:.3f}\")\n\n\nGender Bias in Occupations (she-he axis):\n\nShe-associated occupations:\nhomemaker: 0.360\nnurse: 0.333\nreceptionist: 0.329\nlibrarian: 0.300\nsocialite: 0.310\nhairdresser: 0.307\nnanny: 0.287\nbookkeeper: 0.264\nstylist: 0.252\nhousekeeper: 0.260\n\nHe-associated occupations:\nmaestro: -0.203\nskipper: -0.177\nprotege: -0.148\nphilosopher: -0.155\ncaptain: -0.130\narchitect: -0.151\nfinancier: -0.145\nwarrior: -0.120\nbroadcaster: -0.124\nmagician: -0.110\nboss: -0.090\n\n\n\n\nInterpreting the Scores:\n\nPositive scores (&gt; 0): Closer to ‚Äúshe‚Äù (e.g., nurse, librarian).\nNegative scores (&lt; 0): Closer to ‚Äúhe‚Äù (e.g., architect, captain).\nMagnitude: A larger absolute value indicates a stronger gender association.\n\nNotice how occupations historically associated with women (like nurse and librarian) have strong positive scores, while those associated with men (like captain and architect) have negative scores. This confirms that the model has learned these gender stereotypes from the text data.\n\n\nSince word embeddings capture semantic relationships learned from large text corpora, they inevitably encode societal biases and stereotypes present in that training data. We can leverage this property to identify pairs of words that exhibit stereotypical gender associations. By measuring how different words align with the gender axis (she-he), we can find pairs where one word shows a strong feminine bias while its counterpart shows a masculine bias, revealing ingrained stereotypes in language use.\n\n# Stereotype analogies from the paper\nstereotype_pairs = [\n    (\"sewing\", \"carpentry\"),\n    (\"nurse\", \"surgeon\"),\n    (\"softball\", \"baseball\"),\n    (\"cosmetics\", \"pharmaceuticals\"),\n    (\"vocalist\", \"guitarist\"),\n]\n\n\n\nCode\nprint(\"\\nAnalyzing Gender Stereotype Pairs:\")\nfor word1, word2 in stereotype_pairs:\n    bias1 = analyze(word1, \"she\", \"he\", model)\n    bias2 = analyze(word2, \"she\", \"he\", model)\n    print(f\"\\n{word1} vs {word2}\")\n    print(f\"{word1}: {bias1:.3f}\")\n    print(f\"{word2}: {bias2:.3f}\")\n\n\n\nAnalyzing Gender Stereotype Pairs:\n\nsewing vs carpentry\nsewing: 0.302\ncarpentry: -0.028\n\nnurse vs surgeon\nnurse: 0.333\nsurgeon: -0.048\n\nsoftball vs baseball\nsoftball: 0.260\nbaseball: -0.066\n\ncosmetics vs pharmaceuticals\ncosmetics: 0.331\npharmaceuticals: -0.011\n\nvocalist vs guitarist\nvocalist: 0.140\nguitarist: -0.041\n\n\nThe results show clear stereotypical alignments. Sewing and nurse align with ‚Äúshe‚Äù, while carpentry and surgeon align with ‚Äúhe‚Äù. This mirrors the ‚Äúman is to computer programmer as woman is to homemaker‚Äù analogy found in early word embedding research.\n\n\n\nIndirect bias occurs when seemingly neutral words or concepts become associated with gender through their relationships with other words. For example, while ‚Äúsoftball‚Äù and ‚Äúfootball‚Äù are not inherently gendered terms, they may show gender associations in word embeddings due to how they‚Äôre used in language and society.\nWe can detect indirect bias by: 1. Identifying word pairs that form a semantic axis (e.g., softball-football) 2. Measuring how other words align with this axis 3. Examining if alignment with this axis correlates with gender bias\nThis reveals how gender stereotypes can be encoded indirectly through word associations, even when the words themselves don‚Äôt explicitly reference gender.\nLet‚Äôs see how this works in practice. We first measure the gender bias of the following words:\n\n# Words associated with softball-football axis\nsoftball_associations = [\n    \"pitcher\",\n    \"bookkeeper\",\n    \"receptionist\",\n    \"nurse\",\n    \"waitress\"\n]\n\nfootball_associations = [\n    \"footballer\",\n    \"businessman\",\n    \"pundit\",\n    \"maestro\",\n    \"cleric\"\n]\n\n# Calculate biases for all words\ngender_biases = []\nsports_biases = []\nwords = softball_associations + football_associations\n\nfor word in words:\n    gender_bias = analyze(word, \"she\", \"he\", model)\n    sports_bias = analyze(word, \"softball\", \"football\", model)\n    gender_biases.append(gender_bias)\n    sports_biases.append(sports_bias)\n\nLet‚Äôs plot the results:\n\n\nCode\n# Analyze bias along both gender and sports axes\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom adjustText import adjust_text\n\n# Create scatter plot\nfig, ax = plt.subplots(figsize=(6, 6))\nsns.scatterplot(x=gender_biases, y=sports_biases, ax=ax)\nax.set_xlabel(\"Gender Bias (she-he)\")\nax.set_ylabel(\"Sports Bias (softball-football)\")\nax.set_title(\"Indirect Bias Analysis: Gender vs Sports\")\n\n# Add labels for each point\ntexts = []\nfor i, word in enumerate(words):\n    texts.append(ax.text(gender_biases[i], sports_biases[i], word, fontsize=12))\n\nadjust_text(texts, arrowprops=dict(arrowstyle='-', color='gray', lw=0.5))\n\nax.grid(True, alpha=0.3)\nplt.show()\nsns.despine()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\nIndirect Bias:\nThe plot reveals a correlation: words associated with ‚Äúsoftball‚Äù (y-axis &gt; 0) also tend to be associated with ‚Äúshe‚Äù (x-axis &gt; 0). Conversely, ‚Äúfootball‚Äù terms align with ‚Äúhe‚Äù.\nThis suggests that even if we remove explicit gender words, the structure of the space still encodes gender through these proxy dimensions.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Bias"
    ]
  },
  {
    "objectID": "m04-text/word-bias.html#take-away",
    "href": "m04-text/word-bias.html#take-away",
    "title": "Prompt Engineering",
    "section": "Take away",
    "text": "Take away\nWord embeddings, while powerful, inevitably capture and reflect societal biases present in the large text corpora they are trained on. We observed both direct bias, where occupations or attributes align strongly with specific gender pronouns, and indirect bias, where seemingly neutral concepts become gendered through their associations with other words. This analysis highlights the importance of understanding and mitigating these biases to prevent the perpetuation of stereotypes in AI systems and ensure fairness in applications like search, recommendation, and hiring.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Bias"
    ]
  },
  {
    "objectID": "m05-images/archive/appendix.html",
    "href": "m05-images/archive/appendix.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Let‚Äôs first implement Bruna‚Äôs spectral GCN.\n```nwgbrzndsem python :tags: [hide-input]\nimport numpy as np import scipy.sparse as sp import torch import torch.nn as nn import scipy.sparse.linalg as slinalg\nclass BrunaGraphConv(nn.Module): ‚Äú‚Äú‚Äù Bruna‚Äôs Spectral Graph Convolution Layer\nThis implementation follows the original formulation by Joan Bruna et al.,\nusing the eigendecomposition of the graph Laplacian for spectral convolution.\n\"\"\"\n\ndef __init__(self, in_features, out_features, n_nodes):\n    \"\"\"\n    Initialize the Bruna Graph Convolution layer\n\n    Args:\n        in_features (int): Number of input features\n        out_features (int): Number of output features\n    \"\"\"\n    super(BrunaGraphConv, self).__init__()\n\n    self.in_features = in_features\n    self.out_features = out_features\n\n    # Learnable spectral filter parameters\n    self.weight = nn.Parameter(\n        torch.FloatTensor(in_features, out_features, n_nodes-1)\n    )\n\n    # Initialize parameters\n    self.reset_parameters()\n\ndef reset_parameters(self):\n    \"\"\"Initialize weights using Glorot initialization\"\"\"\n    nn.init.xavier_uniform_(self.weight)\n\n\n@staticmethod\ndef get_laplacian_eigenvectors(adj):\n    \"\"\"\n    Compute eigendecomposition of the normalized graph Laplacian\n\n    Args:\n        adj: Adjacency matrix\n\n    Returns:\n        eigenvalues, eigenvectors of the normalized Laplacian\n    \"\"\"\n    # Compute normalized Laplacian\n    # Add self-loops\n    adj = adj + sp.eye(adj.shape[0])\n\n    # Compute degree matrix\n    deg = np.array(adj.sum(axis=1))\n    Dsqrt_inv = sp.diags(1.0 / np.sqrt(deg).flatten())\n\n    # Compute normalized Laplacian: D^(-1/2) A D^(-1/2)\n    laplacian = sp.eye(adj.shape[0]) - Dsqrt_inv @ adj @ Dsqrt_inv\n\n    # Compute eigendecomposition\n    # Using k=adj.shape[0]-1 to get all non-zero eigenvalues\n    eigenvals, eigenvecs = slinalg.eigsh(laplacian.tocsc(), k=adj.shape[0]-1,which='SM', tol=1e-6)\n\n    return torch.FloatTensor(eigenvals), torch.FloatTensor(eigenvecs)\n\ndef forward(self, x, eigenvecs):\n    \"\"\"\n    Forward pass implementing Bruna's spectral convolution\n\n    Args:\n        x: Input features [num_nodes, in_features]\n        eigenvecs: Eigenvectors of the graph Laplacian [num_nodes, num_nodes-1]\n\n    Returns:\n        Output features [num_nodes, out_features]\n    \"\"\"\n    # Transform to spectral domain\n    x_spectral = torch.matmul(eigenvecs.t(), x)  # [num_nodes-1, in_features]\n\n    # Initialize output tensor\n    out = torch.zeros(x.size(0), self.out_features, device=x.device)\n\n    # For each input-output feature pair\n    for i in range(self.in_features):\n        for j in range(self.out_features):\n            # Element-wise multiplication in spectral domain\n            # This is the actual spectral filtering operation\n            filtered = x_spectral[:, i] * self.weight[i, j, :]  # [num_spectrum]\n\n            # Transform back to spatial domain and accumulate\n            out[:, j] += torch.matmul(eigenvecs, filtered)\n\n    return out\n\nNext, we will train the model on the karate club network to predict the given node labels indicating nodes' community memberships. We load the data by\n\n```{code-cell} ipython\n:tags: [hide-input]\n\nimport networkx as nx\nimport torch\nimport matplotlib.pyplot as plt\n\n# Load karate club network\nG = nx.karate_club_graph()\nadj = nx.to_scipy_sparse_array(G)\nfeatures = torch.eye(G.number_of_nodes())\nlabels = torch.tensor([G.nodes[i]['club'] == 'Officer' for i in G.nodes()], dtype=torch.long)\nWe apply the convolution twice with ReLu activation in between. This can be implemented by preparing two independent BrunaGraphConv layers, applying them consecutively, and adding a ReLu activation in between.\n```nwgbrzndsem ipython :tags: [hide-input]"
  },
  {
    "objectID": "m05-images/archive/appendix.html#brunas-spectral-gcn",
    "href": "m05-images/archive/appendix.html#brunas-spectral-gcn",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Let‚Äôs first implement Bruna‚Äôs spectral GCN.\n```nwgbrzndsem python :tags: [hide-input]\nimport numpy as np import scipy.sparse as sp import torch import torch.nn as nn import scipy.sparse.linalg as slinalg\nclass BrunaGraphConv(nn.Module): ‚Äú‚Äú‚Äù Bruna‚Äôs Spectral Graph Convolution Layer\nThis implementation follows the original formulation by Joan Bruna et al.,\nusing the eigendecomposition of the graph Laplacian for spectral convolution.\n\"\"\"\n\ndef __init__(self, in_features, out_features, n_nodes):\n    \"\"\"\n    Initialize the Bruna Graph Convolution layer\n\n    Args:\n        in_features (int): Number of input features\n        out_features (int): Number of output features\n    \"\"\"\n    super(BrunaGraphConv, self).__init__()\n\n    self.in_features = in_features\n    self.out_features = out_features\n\n    # Learnable spectral filter parameters\n    self.weight = nn.Parameter(\n        torch.FloatTensor(in_features, out_features, n_nodes-1)\n    )\n\n    # Initialize parameters\n    self.reset_parameters()\n\ndef reset_parameters(self):\n    \"\"\"Initialize weights using Glorot initialization\"\"\"\n    nn.init.xavier_uniform_(self.weight)\n\n\n@staticmethod\ndef get_laplacian_eigenvectors(adj):\n    \"\"\"\n    Compute eigendecomposition of the normalized graph Laplacian\n\n    Args:\n        adj: Adjacency matrix\n\n    Returns:\n        eigenvalues, eigenvectors of the normalized Laplacian\n    \"\"\"\n    # Compute normalized Laplacian\n    # Add self-loops\n    adj = adj + sp.eye(adj.shape[0])\n\n    # Compute degree matrix\n    deg = np.array(adj.sum(axis=1))\n    Dsqrt_inv = sp.diags(1.0 / np.sqrt(deg).flatten())\n\n    # Compute normalized Laplacian: D^(-1/2) A D^(-1/2)\n    laplacian = sp.eye(adj.shape[0]) - Dsqrt_inv @ adj @ Dsqrt_inv\n\n    # Compute eigendecomposition\n    # Using k=adj.shape[0]-1 to get all non-zero eigenvalues\n    eigenvals, eigenvecs = slinalg.eigsh(laplacian.tocsc(), k=adj.shape[0]-1,which='SM', tol=1e-6)\n\n    return torch.FloatTensor(eigenvals), torch.FloatTensor(eigenvecs)\n\ndef forward(self, x, eigenvecs):\n    \"\"\"\n    Forward pass implementing Bruna's spectral convolution\n\n    Args:\n        x: Input features [num_nodes, in_features]\n        eigenvecs: Eigenvectors of the graph Laplacian [num_nodes, num_nodes-1]\n\n    Returns:\n        Output features [num_nodes, out_features]\n    \"\"\"\n    # Transform to spectral domain\n    x_spectral = torch.matmul(eigenvecs.t(), x)  # [num_nodes-1, in_features]\n\n    # Initialize output tensor\n    out = torch.zeros(x.size(0), self.out_features, device=x.device)\n\n    # For each input-output feature pair\n    for i in range(self.in_features):\n        for j in range(self.out_features):\n            # Element-wise multiplication in spectral domain\n            # This is the actual spectral filtering operation\n            filtered = x_spectral[:, i] * self.weight[i, j, :]  # [num_spectrum]\n\n            # Transform back to spatial domain and accumulate\n            out[:, j] += torch.matmul(eigenvecs, filtered)\n\n    return out\n\nNext, we will train the model on the karate club network to predict the given node labels indicating nodes' community memberships. We load the data by\n\n```{code-cell} ipython\n:tags: [hide-input]\n\nimport networkx as nx\nimport torch\nimport matplotlib.pyplot as plt\n\n# Load karate club network\nG = nx.karate_club_graph()\nadj = nx.to_scipy_sparse_array(G)\nfeatures = torch.eye(G.number_of_nodes())\nlabels = torch.tensor([G.nodes[i]['club'] == 'Officer' for i in G.nodes()], dtype=torch.long)\nWe apply the convolution twice with ReLu activation in between. This can be implemented by preparing two independent BrunaGraphConv layers, applying them consecutively, and adding a ReLu activation in between.\n```nwgbrzndsem ipython :tags: [hide-input]"
  },
  {
    "objectID": "m05-images/archive/image-processing.html",
    "href": "m05-images/archive/image-processing.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "What makes an image look sharp or blurred to our eyes? How can we detect important features, such as boundaries between objects?\n\n\nEdge detection is a classical problem in image processing. The goal is to identify the boundaries of objects in an image.\n\nTo approach the problem, recall that an image is essentially a matrix of pixel intensity values. In a grayscale image, each pixel has a single intensity value representing its brightness, so we can think of the image as a 2D matrix of brightness values.\n\n\n\n\nHuman eyes are very sensitive to sudden changes in brightness. An edge in an image appears when there is a significant brightness change between neighboring pixels. Suppose we have a small 6x6 image with a bright vertical line:\n\nX = \\begin{bmatrix}\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10\n\\end{bmatrix}\n\nIf we zoom in on the central region:\n\nZ = \\begin{bmatrix}\n10 & 80 & 10 \\\\\n\\textcolor{blue}{10} & \\textcolor{red}{80} & \\textcolor{purple}{10} \\\\\n10 & 80 & 10\n\\end{bmatrix}\n\nThe pixel of interest is highlighted in red. To detect a horizontal brightness change, we can approximate the derivative at the central pixel by subtracting the right-neighbor from the left-neighbor:\n\n\\nabla Z_{22} = \\textcolor{blue}{Z_{2,1}} \\;-\\; \\textcolor{purple}{Z_{2,3}}\n\nRepeating this for every pixel yields the horizontal derivative of the entire 6x6 image:\n\n\\begin{bmatrix}\n- & -70 & 0 & 70 & 0 & - \\\\\n- & -70 & 0 & 70 & 0 & - \\\\\n- & -70 & 0 & 70 & 0 & - \\\\\n- & -70 & 0 & 70 & 0 & - \\\\\n- & -70 & 0 & 70 & 0 & - \\\\\n- & -70 & 0 & 70 & 0 & -\n\\end{bmatrix}\n\nThe symbol ‚Äú-‚Äù indicates undefined values at the boundary, where we do not have neighbors on both sides. Notice that the derivative is large around the central line (the edge) and zero elsewhere.\nWe could also compute a vertical derivative by subtracting the bottom-neighbor from the top-neighbor:\n\n\\nabla Z_{22} = Z_{1,2} \\;-\\; Z_{3,2}\n\nWhen applied to the entire image, this vertical derivative is zero because there is no vertical change in brightness.\n\n\n\nNotice that in these derivative calculations we are repeatedly taking weighted sums (subtractions) of neighboring pixels. This suggests a more general operation called convolution, where we define a small matrix of weights called a kernel (or filter) and ‚Äúslide‚Äù it over each pixel in the image.\nMathematically, for a 3x3 kernel K applied to the central pixel of a local patch Z:\n\n\\nabla Z_{22} = \\sum_{i=-1}^1 \\sum_{j=-1}^1 K_{h-(i+1),w-(j+1)} \\,Z_{2+i, 2+j},\n\nwhere h=w=3 are the kernel‚Äôs height and width.\nIn strict mathematical notation, when we say \"convolution,\" we often **flip** the kernel before we do the sum. That is, we reorder:\n\n$$\nK = \\begin{bmatrix}\nK_{33} & K_{32} & K_{31} \\\\\nK_{23} & K_{22} & K_{21} \\\\\nK_{13} & K_{12} & K_{11}\n\\end{bmatrix}\n$$\n\nso that when we multiply element-by-element by $Z$ and sum, we replicate the formal definition of convolution. In image processing practice, some software libraries call this ‚Äúcross-correlation‚Äù if they do not flip the kernel. The difference usually does not matter if the kernel is symmetric (e.g., Gaussian blur).\nA common choice of 3x3 kernels for edge detection is the **Prewitt operator**:\n\n$$\nK_h = \\begin{bmatrix}\n-1 & 0 & 1 \\\\\n-1 & 0 & 1 \\\\\n-1 & 0 & 1\n\\end{bmatrix}\n\\quad\\text{and}\\quad\nK_v = \\begin{bmatrix}\n-1 & -1 & -1 \\\\\n0 & 0 & 0 \\\\\n1 & 1 & 1\n\\end{bmatrix}.\n$$\n\n- $K_h$ detects horizontal edges\n- $K_v$ detects vertical edges\nIn effect, applying a kernel to a patch of the image is like taking the inner product \\langle \\hat{K}, Z \\rangle, where \\hat{K} may be the flipped version of K. This inner product will be large when the image patch resembles the kernel pattern closely.\nHere is a fantastic interactive demo of how various image kernels behave: [Setosa Image Kernels](https://setosa.io/ev/image-kernels/).\n\n\n\n\nConvolution can be computationally expensive if you think of it as ‚Äúsliding and multiplying‚Äù each kernel element by each pixel. However, the convolution theorem tells us we can make convolution simpler by working in the frequency domain:\n\nFourier transform both the image and the kernel (turn them into frequency representations).\nMultiply these frequency representations element-wise.\nTake the inverse Fourier transform to get the convolved output in the spatial domain.\n\nMathematically,\n\nX * K \\quad\\longleftrightarrow\\quad \\mathcal{F}(X) \\cdot \\mathcal{F}(K).\n\n\n\nFor a discrete signal x[n] of length N, its Discrete Fourier Transform is defined as\n\n\\mathcal{F}(x)[k]\n= \\sum_{n=0}^{N-1} x[n] \\cdot e^{-\\,2\\pi i \\,\\frac{nk}{N}}.\n\nUsing Euler‚Äôs formula e^{ix} = \\cos(x) + i\\,\\sin(x), we can rewrite:\n\n\\mathcal{F}(x)[k]\n= \\sum_{n=0}^{N-1} x[n]\\,\\Big[\\cos\\!\\big(2\\pi \\tfrac{nk}{N}\\big) \\;-\\; i\\,\\sin\\!\\big(2\\pi \\tfrac{nk}{N}\\big)\\Big].\n\nIn essence, the Fourier transform represents a signal as a sum of sinusoids with different frequencies. Each frequency component indicates how much of that frequency is present in the original signal.\nA recommended resource is 3Blue1Brown‚Äôs beautiful video explaining Fourier transforms: [Fourier Transform video](https://www.youtube.com/watch?v=spUNpyF58BY). Also try [Jez Swanson‚Äôs Interactive Fourier Demo](https://www.jezzamon.com/fourier/).\n\n\n\n\n```ihqtzpgxvph ipython3 import numpy as np"
  },
  {
    "objectID": "m05-images/archive/image-processing.html#edge-detection-problem-in-image-processing",
    "href": "m05-images/archive/image-processing.html#edge-detection-problem-in-image-processing",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Edge detection is a classical problem in image processing. The goal is to identify the boundaries of objects in an image.\n\nTo approach the problem, recall that an image is essentially a matrix of pixel intensity values. In a grayscale image, each pixel has a single intensity value representing its brightness, so we can think of the image as a 2D matrix of brightness values."
  },
  {
    "objectID": "m05-images/archive/image-processing.html#a-simple-example-of-horizontal-edge-detection",
    "href": "m05-images/archive/image-processing.html#a-simple-example-of-horizontal-edge-detection",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Human eyes are very sensitive to sudden changes in brightness. An edge in an image appears when there is a significant brightness change between neighboring pixels. Suppose we have a small 6x6 image with a bright vertical line:\n\nX = \\begin{bmatrix}\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10\n\\end{bmatrix}\n\nIf we zoom in on the central region:\n\nZ = \\begin{bmatrix}\n10 & 80 & 10 \\\\\n\\textcolor{blue}{10} & \\textcolor{red}{80} & \\textcolor{purple}{10} \\\\\n10 & 80 & 10\n\\end{bmatrix}\n\nThe pixel of interest is highlighted in red. To detect a horizontal brightness change, we can approximate the derivative at the central pixel by subtracting the right-neighbor from the left-neighbor:\n\n\\nabla Z_{22} = \\textcolor{blue}{Z_{2,1}} \\;-\\; \\textcolor{purple}{Z_{2,3}}\n\nRepeating this for every pixel yields the horizontal derivative of the entire 6x6 image:\n\n\\begin{bmatrix}\n- & -70 & 0 & 70 & 0 & - \\\\\n- & -70 & 0 & 70 & 0 & - \\\\\n- & -70 & 0 & 70 & 0 & - \\\\\n- & -70 & 0 & 70 & 0 & - \\\\\n- & -70 & 0 & 70 & 0 & - \\\\\n- & -70 & 0 & 70 & 0 & -\n\\end{bmatrix}\n\nThe symbol ‚Äú-‚Äù indicates undefined values at the boundary, where we do not have neighbors on both sides. Notice that the derivative is large around the central line (the edge) and zero elsewhere.\nWe could also compute a vertical derivative by subtracting the bottom-neighbor from the top-neighbor:\n\n\\nabla Z_{22} = Z_{1,2} \\;-\\; Z_{3,2}\n\nWhen applied to the entire image, this vertical derivative is zero because there is no vertical change in brightness."
  },
  {
    "objectID": "m05-images/archive/image-processing.html#convolution",
    "href": "m05-images/archive/image-processing.html#convolution",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Notice that in these derivative calculations we are repeatedly taking weighted sums (subtractions) of neighboring pixels. This suggests a more general operation called convolution, where we define a small matrix of weights called a kernel (or filter) and ‚Äúslide‚Äù it over each pixel in the image.\nMathematically, for a 3x3 kernel K applied to the central pixel of a local patch Z:\n\n\\nabla Z_{22} = \\sum_{i=-1}^1 \\sum_{j=-1}^1 K_{h-(i+1),w-(j+1)} \\,Z_{2+i, 2+j},\n\nwhere h=w=3 are the kernel‚Äôs height and width.\nIn strict mathematical notation, when we say \"convolution,\" we often **flip** the kernel before we do the sum. That is, we reorder:\n\n$$\nK = \\begin{bmatrix}\nK_{33} & K_{32} & K_{31} \\\\\nK_{23} & K_{22} & K_{21} \\\\\nK_{13} & K_{12} & K_{11}\n\\end{bmatrix}\n$$\n\nso that when we multiply element-by-element by $Z$ and sum, we replicate the formal definition of convolution. In image processing practice, some software libraries call this ‚Äúcross-correlation‚Äù if they do not flip the kernel. The difference usually does not matter if the kernel is symmetric (e.g., Gaussian blur).\nA common choice of 3x3 kernels for edge detection is the **Prewitt operator**:\n\n$$\nK_h = \\begin{bmatrix}\n-1 & 0 & 1 \\\\\n-1 & 0 & 1 \\\\\n-1 & 0 & 1\n\\end{bmatrix}\n\\quad\\text{and}\\quad\nK_v = \\begin{bmatrix}\n-1 & -1 & -1 \\\\\n0 & 0 & 0 \\\\\n1 & 1 & 1\n\\end{bmatrix}.\n$$\n\n- $K_h$ detects horizontal edges\n- $K_v$ detects vertical edges\nIn effect, applying a kernel to a patch of the image is like taking the inner product \\langle \\hat{K}, Z \\rangle, where \\hat{K} may be the flipped version of K. This inner product will be large when the image patch resembles the kernel pattern closely.\nHere is a fantastic interactive demo of how various image kernels behave: [Setosa Image Kernels](https://setosa.io/ev/image-kernels/)."
  },
  {
    "objectID": "m05-images/archive/image-processing.html#fourier-transform",
    "href": "m05-images/archive/image-processing.html#fourier-transform",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Convolution can be computationally expensive if you think of it as ‚Äúsliding and multiplying‚Äù each kernel element by each pixel. However, the convolution theorem tells us we can make convolution simpler by working in the frequency domain:\n\nFourier transform both the image and the kernel (turn them into frequency representations).\nMultiply these frequency representations element-wise.\nTake the inverse Fourier transform to get the convolved output in the spatial domain.\n\nMathematically,\n\nX * K \\quad\\longleftrightarrow\\quad \\mathcal{F}(X) \\cdot \\mathcal{F}(K).\n\n\n\nFor a discrete signal x[n] of length N, its Discrete Fourier Transform is defined as\n\n\\mathcal{F}(x)[k]\n= \\sum_{n=0}^{N-1} x[n] \\cdot e^{-\\,2\\pi i \\,\\frac{nk}{N}}.\n\nUsing Euler‚Äôs formula e^{ix} = \\cos(x) + i\\,\\sin(x), we can rewrite:\n\n\\mathcal{F}(x)[k]\n= \\sum_{n=0}^{N-1} x[n]\\,\\Big[\\cos\\!\\big(2\\pi \\tfrac{nk}{N}\\big) \\;-\\; i\\,\\sin\\!\\big(2\\pi \\tfrac{nk}{N}\\big)\\Big].\n\nIn essence, the Fourier transform represents a signal as a sum of sinusoids with different frequencies. Each frequency component indicates how much of that frequency is present in the original signal.\nA recommended resource is 3Blue1Brown‚Äôs beautiful video explaining Fourier transforms: [Fourier Transform video](https://www.youtube.com/watch?v=spUNpyF58BY). Also try [Jez Swanson‚Äôs Interactive Fourier Demo](https://www.jezzamon.com/fourier/)."
  },
  {
    "objectID": "m05-images/archive/image-processing.html#example-convolution-via-fourier-transform-in-python",
    "href": "m05-images/archive/image-processing.html#example-convolution-via-fourier-transform-in-python",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "```ihqtzpgxvph ipython3 import numpy as np"
  },
  {
    "objectID": "m05-images/archive/image-processing.html#fourier-transform-of-images",
    "href": "m05-images/archive/image-processing.html#fourier-transform-of-images",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "Fourier Transform of Images",
    "text": "Fourier Transform of Images\nTo extend these ideas to 2D images, we note that the 2D Fourier transform is essentially the same operation applied twice: once across rows and once across columns. For an image X of size H \\times W:\n\n\\mathcal{F}(X)[h, w]\n= \\sum_{k=0}^{H-1}\\sum_{\\ell=0}^{W-1}\n  X[k,\\ell]\\,\n  e^{-\\,2\\pi i\\,\\big(\\frac{hk}{H} + \\frac{w\\ell}{W}\\big)}.\n\nEach pair (h, w) represents a 2D frequency. Think of these as combinations of sine waves along the horizontal and vertical directions.\n\nVisualizing 2D Fourier Basis Functions\n```ihqtzpgxvph ipython3 :tags: [hide-input] import numpy as np import matplotlib.pyplot as plt\ndef basis_function(img_size=256, u=0, v=0): ‚Äú‚Äú‚Äù Generate the 2D complex exponential basis function e^{-2 pi i (ux + vy)/N}. Returns the real (cosine) and imaginary (sine) parts separately. ‚Äú‚Äú‚Äù N = img_size x = np.linspace(0, N-1, N) y = np.linspace(0, N-1, N) x_, y_ = np.meshgrid(x, y) bf = np.exp(-1j2np.pi(ux_/N + v*y_/N)) real_part = np.real(bf) imag_part = np.imag(bf) return real_part, imag_part\nsize = 16 bf_arr_real = [] bf_arr_imag = []"
  },
  {
    "objectID": "m05-images/archive/image-processing.html#key-lesson-from-image-processing",
    "href": "m05-images/archive/image-processing.html#key-lesson-from-image-processing",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "Key Lesson from Image Processing",
    "text": "Key Lesson from Image Processing\n\nConvolution as Pattern Matching: Applying a kernel is like taking an inner product with a small patch of the image. Kernels detect certain local patterns (edges, corners, textures).\nConvolution in the Frequency Domain: The Fourier transform lets us view images as sums of sinusoidal patterns. In this viewpoint, convolution is simply multiplication in frequency space.\nFilters as Frequency Selectors: Kernels like Prewitt emphasize high-frequency components (edges), while other kernels (e.g., Gaussian blur) emphasize low-frequency components.\n\nThese insights underlie a huge variety of image processing techniques and pave the way for more advanced methods (e.g., wavelet transforms, deep CNNs, and beyond).\n**Reflection**:\n- How might a kernel that *blurs* an image look in the frequency domain?\n- Why do sharp edges correspond to high-frequency content?\n- How can thinking in frequencies sometimes be simpler than manipulating pixels directly?"
  },
  {
    "objectID": "m05-images/archive/lenet.html",
    "href": "m05-images/archive/lenet.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "How can a neural network learn to recognize complex visual patterns‚Äîlike handwritten digits‚Äîwithout relying on hand-crafted features?\nIn the late 1980s and early 1990s, hand-engineered feature extraction dominated machine learning approaches to computer vision. This process was laborious and often inflexible. LeNet {footcite}lecun1989backpropagation offered a radical alternative by showing that a network could learn these features directly from raw pixel data.\nThe most influential incarnation, LeNet-5 {footcite}lecun1998gradient, demonstrated impressive performance on handwritten digit recognition, finding real-world application in automated check reading and postal code processing. Although modern networks have grown significantly in depth and complexity, the core ideas from LeNet remain fundamental to today‚Äôs convolutional neural networks (CNNs).\nLeNet popularized the key innovations of convolution, pooling, and end-to-end learning‚Äîapproaches that form the foundation for modern deep learning in computer vision.\n\n\n\nUnderstand the historical context and motivation behind the LeNet family of architectures.\nExplore the architectural components (convolution, pooling, and sparse connectivity) that enabled effective pattern learning.\nImplement a simplified version of LeNet-1 in PyTorch to gain hands-on experience.\nReflect on how LeNet‚Äôs innovations paved the way for more advanced CNNs.\n\n\n\n\nBefore LeNet, engineers painstakingly crafted feature extractors for each vision task: edges, corners, specific shapes, etc. This approach was time-consuming, difficult to generalize, and prone to missing subtle features crucial for classification.\nLeNet challenged this paradigm by automating feature extraction. It did this through layers that systematically learn local patterns (via convolution) and gradually build more global representations (via subsampling/pooling). This hierarchical approach mimics aspects of human visual perception, where lower-level patterns combine into higher-level objects.\n**Historical Context**:\nYann LeCun‚Äôs work on applying backpropagation to convolutional architectures in the 1980s was met with skepticism. But the success of LeNet on real-world tasks (e.g., check reading at banks) helped spark wider interest in neural network approaches to image recognition.\n\n\n\nLeNet actually refers to several iterative designs. In what follows, we examine two key versions: LeNet-1 (the earliest demonstration) and LeNet-5 (the widely known and more powerful network).\n\n\n\n\n\n```esxhyhya https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ge5OLutAT9_3fxt_sKTBGA.png\n\n\n\n\nwidth: 100%\n\n\nname: lenet\n\n\n\nLeNet-1 architecture.\n\n1. **Convolution (C1)**: Takes a $28\\times28$ (originally $32\\times32$ in some demos) grayscale image and applies 4 filters of size $5\\times5$. This step captures basic patterns like edges and corners.\n\n2. **Pooling (S2)**: Applies average pooling (subsampling) with a $2\\times2$ window, reducing the spatial dimensions from $24\\times24$ to $12\\times12$. This coarse-grains the features, allowing the network to focus on more abstract patterns.\n\n3. **Second Convolution (C3)**: Produces more feature maps (12 feature maps). By stacking multiple convolutions, the network builds increasingly complex features.\n\n4. **Second Pooling (S4)**: Another average pooling layer further reduces spatial dimensions to $4\\times4$.\n\n5. **Fully Connected Layers**: The network flattens these features and passes them through a fully connected layer to produce a 10-class output (digits 0‚Äì9).\n\n```{note}\nThis hierarchical processing‚Äîconvolution followed by subsampling‚Äîmimics the structure of the visual cortex, where neurons respond to progressively more complex stimuli in each stage.\n\n\n\n\n\n\n```esxhyhya https://www.datasciencecentral.com/wp-content/uploads/2021/10/1lvvWF48t7cyRWqct13eU0w.jpeg\n\n\n\n\nwidth: 100%\n\n\nname: lenet-5\n\n\n\nLeNet-5 architecture.\n\nLeNet-5 builds on LeNet-1 but **scales up the number of learnable parameters** and introduces a few architectural refinements:\n\n1. **Input Normalization**: Inputs (grayscale images) are normalized to a range of roughly $[-0.1, 1.175]$. This centering speeds up training and stabilizes the gradients.\n\n2. **Convolution + Subsampling Pairs**: Similar to LeNet-1, but with more feature maps and a **mean pooling** mechanism. Each pooling step is followed by a **non-linear activation** (often the sigmoid, though other activations can be used).\n\n3. **Sparse Connectivity (C3)**: Not every feature map in the previous layer connects to every feature map in the next layer. This selective approach reduces parameters and encourages **diverse features** rather than overly correlated ones.\n\n4. **Transition to 1D**: Instead of simply flattening, LeNet-5 includes a convolutional layer (C5) that bridges 2D feature maps to a fully connected layer, preserving more spatial structure.\n\n5. **Final RBF Layer** (in the original paper): An additional radial-basis-function layer was sometimes used to enhance feature representation. Modern implementations often simplify this to a linear or fully connected layer.\n\n```{note}\n**Parameter Efficiency**: One reason LeNet-5 performed well on the limited hardware of the 1990s is its careful use of sparse connections to reduce the number of parameters.\n\n\n\n\nIn this section, we will implement a simplified LeNet-1 in PyTorch. While LeNet-1 was traditionally trained with batch gradient descent and certain custom optimizations, our example will use modern tooling‚Äîsuch as PyTorch Lightning and the Adam optimizer‚Äîto streamline the training process.\n\n\nWe will train our model on the MNIST dataset, a classic benchmark of 28\\times28 handwritten digits. MNIST is split into 60,000 training and 10,000 test images.\n\n\n\n```esxhyhya https://production-media.paperswithcode.com/datasets/MNIST-0000000001-2e09631a_09liOmx.jpg\n\n\n\n\nwidth: 100%\n\n\nname: mnist\n\n\n\nMNIST dataset (digits 0‚Äì9 in handwritten form).\n\n```{tip}\n**Why MNIST?**\n- Small image size (28x28) ‚Üí Perfect for simple convolutional nets.\n- 10 distinct classes ‚Üí Easy to measure classification accuracy.\n- Widely used ‚Üí Many existing examples to compare against.\n\n\n\n```mjsqzdtksaj ipython3 import torch import torch.nn as nn import torch.nn.functional as F import pytorch_lightning as pl from torch.utils.data import DataLoader, random_split from torchvision import datasets, transforms from torchmetrics import Accuracy\nclass MNISTDataModule(pl.LightningDataModule): ‚Äú‚Äú‚Äù PyTorch Lightning data module for MNIST dataset ‚Äú‚Äú‚Äù def init(self, data_dir: str = ‚Äò./data‚Äô, batch_size: int = 32): super().__init__() self.data_dir = data_dir self.batch_size = batch_size\n    # Define transforms\n    self.transform = transforms.Compose([\n        transforms.ToTensor(),           # Convert PIL image to torch.Tensor\n        transforms.Normalize((0,), (1,)) # Normalize to mean=0, std=1\n    ])\n\ndef prepare_data(self):\n    \"\"\"Download data if needed.\"\"\"\n    datasets.MNIST(self.data_dir, train=True, download=True)\n    datasets.MNIST(self.data_dir, train=False, download=True)\n\ndef setup(self, stage=None):\n    \"\"\"Setup train, val, and test datasets.\"\"\"\n    if stage == 'fit' or stage is None:\n        mnist_full = datasets.MNIST(self.data_dir, train=True, transform=self.transform)\n        self.mnist_train, self.mnist_val = random_split(\n            mnist_full, [55000, 5000], generator=torch.Generator().manual_seed(42)\n        )\n\n    if stage == 'test' or stage is None:\n        self.mnist_test = datasets.MNIST(self.data_dir, train=False, transform=self.transform)\n\ndef train_dataloader(self):\n    return DataLoader(self.mnist_train, batch_size=self.batch_size, shuffle=True, num_workers=1)\n\ndef val_dataloader(self):\n    return DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=1)\n\ndef test_dataloader(self):\n    return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=1)\n\n```{note}\n**PyTorch Lightning DataModule**:\n- Ensures consistent data splits for training, validation, and testing.\n- Handles shuffling, batching, and transformation pipelines.\n- Makes code cleaner and easier to maintain.\n\n\n\n```mjsqzdtksaj ipython3 class LeNet1(pl.LightningModule): ‚Äú‚Äú‚Äù PyTorch Lightning implementation of LeNet-1 ‚Äú‚Äú‚Äù\ndef __init__(self, learning_rate=1e-3):\n    super(LeNet1, self).__init__()\n    self.save_hyperparameters()\n\n    # Metrics\n    self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=10)\n    self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=10)\n    self.test_accuracy = Accuracy(task=\"multiclass\", num_classes=10)\n\n    # First convolutional layer (Input: 1x28x28 -&gt; Output: 4x24x24)\n    self.conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=5, stride=1)\n\n    # Average pooling layer (4x24x24 -&gt; 4x12x12)\n    self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n\n    # Second convolutional layer (4x12x12 -&gt; 12x8x8)\n    self.conv2 = nn.Conv2d(in_channels=4, out_channels=12, kernel_size=5, stride=1)\n\n    # Fully connected layer (12*4*4=192 -&gt; 10)\n    self.fc = nn.Linear(12 * 4 * 4, 10)\n\n    # Initialize weights\n    self._init_weights()\n\n    # Track losses over time (for visualization)\n    self.val_losses = []\n    self.train_losses = []\n\ndef _init_weights(self):\n    \"\"\"Initialize weights with Xavier initialization.\"\"\"\n    for m in self.modules():\n        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n            nn.init.xavier_uniform_(m.weight)\n            nn.init.zeros_(m.bias)\n\ndef forward(self, x):\n    # First conv block\n    x = self.conv1(x)\n    x = torch.tanh(x)  # Using tanh for nonlinearity\n    x = self.pool(x)\n\n    # Second conv block\n    x = self.conv2(x)\n    x = torch.tanh(x)\n    x = self.pool(x)\n\n    # Flatten and fully connected\n    x = x.view(-1, 12 * 4 * 4)\n    x = self.fc(x)\n    return x\n\ndef configure_optimizers(self):\n    \"\"\"Define optimizer and LR scheduler.\"\"\"\n    optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode=\"min\", factor=0.1, patience=10, verbose=True\n    )\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"val_loss\"}\n    }\n\ndef training_step(self, batch, batch_idx):\n    \"\"\"Train on a single batch.\"\"\"\n    x, y = batch\n    logits = self(x)\n    loss = F.cross_entropy(logits, y)\n\n    acc = self.train_accuracy(logits, y)\n    self.log(\"train_loss\", loss, prog_bar=True)\n    self.log(\"train_acc\", acc, prog_bar=True)\n\n    self.train_losses.append({\"loss\": loss.item(), \"acc\": acc.item()})\n    return loss\n\ndef validation_step(self, batch, batch_idx):\n    \"\"\"Validate on a single batch.\"\"\"\n    x, y = batch\n    logits = self(x)\n    loss = F.cross_entropy(logits, y)\n\n    acc = self.val_accuracy(logits, y)\n    self.log(\"val_loss\", loss, prog_bar=True)\n    self.log(\"val_acc\", acc, prog_bar=True)\n\n    self.val_losses.append({\"loss\": loss.item(), \"acc\": acc.item()})\n\ndef test_step(self, batch, batch_idx):\n    \"\"\"Test on a single batch.\"\"\"\n    x, y = batch\n    logits = self(x)\n    loss = F.cross_entropy(logits, y)\n\n    acc = self.test_accuracy(logits, y)\n    self.log(\"test_loss\", loss, prog_bar=True)\n    self.log(\"test_acc\", acc, prog_bar=True)\n\n### Training the Model\n\n```{code-cell} ipython3\n# Initialize model and data\nmodel = LeNet1(learning_rate=1e-3)\ndata_module = MNISTDataModule(batch_size=256)\n\n# Initialize trainer\ntrainer = pl.Trainer(\n    max_epochs=2,\n    accelerator=\"auto\",  # Use GPU if available\n    devices=1,\n)\n\n# Train\ntrainer.fit(model, data_module)\n**Check GPU usage**:\n- If you have a GPU available, `accelerator=\"auto\"` automatically leverages it.\n- Otherwise, it trains on CPU, which is slower but will still work for a small model like LeNet-1.\n\n\n\n```mjsqzdtksaj ipython3 import seaborn as sns import matplotlib.pyplot as plt import pandas as pd\ndf_val = pd.DataFrame(model.val_losses) df_val[‚ÄúIteration‚Äù] = df_val.index\nfig, ax = plt.subplots(figsize=(10, 6)) sns.lineplot(x=‚ÄúIteration‚Äù, y=‚Äúloss‚Äù, data=df_val, label=‚ÄúValidation Loss‚Äù, ax=ax) ax.set_title(‚ÄúValidation Loss Over Time‚Äù) ax.set_xlabel(‚ÄúIteration‚Äù) ax.set_ylabel(‚ÄúLoss‚Äù) plt.show()\n\n### Testing\n\n```{code-cell} ipython3\ntrainer.test(model, data_module)\nObserve the final test loss and test accuracy. Even this simple LeNet-1 inspired model often achieves high accuracy on MNIST‚Äîdemonstrating how effective early CNN architectures can be.\n\n\n\n\n\nExperiment:\n\nVary the learning rate and batch size to see how training dynamics change.\nReplace tanh activation with ReLU or Sigmoid and compare performance.\n\nVisual Inspection:\n\nHand-draw a digit (e.g., using a graphics tool) and see whether the model correctly classifies it. If it fails, hypothesize why (differences in stroke thickness, image alignment, etc.).\n\nArchitectural Tweaks:\n\nTry adding an additional convolutional layer or using different pooling strategies (like max pooling) to see if you can improve accuracy.\n\n\n**Real-world Application**:\nLeNet‚Äôs core ideas are still used in modern banking systems to read checks automatically. Its principle of learning features from raw data underpins almost all modern deep-learning-based image classification systems.\n\n\n\n\nWriting LeNet5 from Scratch in PyTorch (DigitalOcean)\nPyTorch LeNet Implementation Video\nOriginal LeCun Paper\n\n:style: unsrt\n:filter: docname in docnames\n\n\n\nIn summary, LeNet {footcite}lecun1989backpropagation and LeNet-5 {footcite}lecun1998gradient formed the basis for convolutional networks that learn directly from data. By incorporating convolution, subsampling, sparse connectivity, and end-to-end training, LeNet demonstrated how networks can autonomously discover robust representations‚Äîlaying the groundwork for today‚Äôs deep learning revolution."
  },
  {
    "objectID": "m05-images/archive/lenet.html#learning-objectives",
    "href": "m05-images/archive/lenet.html#learning-objectives",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Understand the historical context and motivation behind the LeNet family of architectures.\nExplore the architectural components (convolution, pooling, and sparse connectivity) that enabled effective pattern learning.\nImplement a simplified version of LeNet-1 in PyTorch to gain hands-on experience.\nReflect on how LeNet‚Äôs innovations paved the way for more advanced CNNs."
  },
  {
    "objectID": "m05-images/archive/lenet.html#conceptual-foundation",
    "href": "m05-images/archive/lenet.html#conceptual-foundation",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Before LeNet, engineers painstakingly crafted feature extractors for each vision task: edges, corners, specific shapes, etc. This approach was time-consuming, difficult to generalize, and prone to missing subtle features crucial for classification.\nLeNet challenged this paradigm by automating feature extraction. It did this through layers that systematically learn local patterns (via convolution) and gradually build more global representations (via subsampling/pooling). This hierarchical approach mimics aspects of human visual perception, where lower-level patterns combine into higher-level objects.\n**Historical Context**:\nYann LeCun‚Äôs work on applying backpropagation to convolutional architectures in the 1980s was met with skepticism. But the success of LeNet on real-world tasks (e.g., check reading at banks) helped spark wider interest in neural network approaches to image recognition."
  },
  {
    "objectID": "m05-images/archive/lenet.html#architecture",
    "href": "m05-images/archive/lenet.html#architecture",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "LeNet actually refers to several iterative designs. In what follows, we examine two key versions: LeNet-1 (the earliest demonstration) and LeNet-5 (the widely known and more powerful network).\n\n\n\n\n\n```esxhyhya https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ge5OLutAT9_3fxt_sKTBGA.png\n\n\n\n\nwidth: 100%\n\n\nname: lenet\n\n\n\nLeNet-1 architecture.\n\n1. **Convolution (C1)**: Takes a $28\\times28$ (originally $32\\times32$ in some demos) grayscale image and applies 4 filters of size $5\\times5$. This step captures basic patterns like edges and corners.\n\n2. **Pooling (S2)**: Applies average pooling (subsampling) with a $2\\times2$ window, reducing the spatial dimensions from $24\\times24$ to $12\\times12$. This coarse-grains the features, allowing the network to focus on more abstract patterns.\n\n3. **Second Convolution (C3)**: Produces more feature maps (12 feature maps). By stacking multiple convolutions, the network builds increasingly complex features.\n\n4. **Second Pooling (S4)**: Another average pooling layer further reduces spatial dimensions to $4\\times4$.\n\n5. **Fully Connected Layers**: The network flattens these features and passes them through a fully connected layer to produce a 10-class output (digits 0‚Äì9).\n\n```{note}\nThis hierarchical processing‚Äîconvolution followed by subsampling‚Äîmimics the structure of the visual cortex, where neurons respond to progressively more complex stimuli in each stage.\n\n\n\n\n\n\n```esxhyhya https://www.datasciencecentral.com/wp-content/uploads/2021/10/1lvvWF48t7cyRWqct13eU0w.jpeg\n\n\n\n\nwidth: 100%\n\n\nname: lenet-5\n\n\n\nLeNet-5 architecture.\n\nLeNet-5 builds on LeNet-1 but **scales up the number of learnable parameters** and introduces a few architectural refinements:\n\n1. **Input Normalization**: Inputs (grayscale images) are normalized to a range of roughly $[-0.1, 1.175]$. This centering speeds up training and stabilizes the gradients.\n\n2. **Convolution + Subsampling Pairs**: Similar to LeNet-1, but with more feature maps and a **mean pooling** mechanism. Each pooling step is followed by a **non-linear activation** (often the sigmoid, though other activations can be used).\n\n3. **Sparse Connectivity (C3)**: Not every feature map in the previous layer connects to every feature map in the next layer. This selective approach reduces parameters and encourages **diverse features** rather than overly correlated ones.\n\n4. **Transition to 1D**: Instead of simply flattening, LeNet-5 includes a convolutional layer (C5) that bridges 2D feature maps to a fully connected layer, preserving more spatial structure.\n\n5. **Final RBF Layer** (in the original paper): An additional radial-basis-function layer was sometimes used to enhance feature representation. Modern implementations often simplify this to a linear or fully connected layer.\n\n```{note}\n**Parameter Efficiency**: One reason LeNet-5 performed well on the limited hardware of the 1990s is its careful use of sparse connections to reduce the number of parameters."
  },
  {
    "objectID": "m05-images/archive/lenet.html#implementation",
    "href": "m05-images/archive/lenet.html#implementation",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "In this section, we will implement a simplified LeNet-1 in PyTorch. While LeNet-1 was traditionally trained with batch gradient descent and certain custom optimizations, our example will use modern tooling‚Äîsuch as PyTorch Lightning and the Adam optimizer‚Äîto streamline the training process.\n\n\nWe will train our model on the MNIST dataset, a classic benchmark of 28\\times28 handwritten digits. MNIST is split into 60,000 training and 10,000 test images.\n\n\n\n```esxhyhya https://production-media.paperswithcode.com/datasets/MNIST-0000000001-2e09631a_09liOmx.jpg\n\n\n\n\nwidth: 100%\n\n\nname: mnist\n\n\n\nMNIST dataset (digits 0‚Äì9 in handwritten form).\n\n```{tip}\n**Why MNIST?**\n- Small image size (28x28) ‚Üí Perfect for simple convolutional nets.\n- 10 distinct classes ‚Üí Easy to measure classification accuracy.\n- Widely used ‚Üí Many existing examples to compare against.\n\n\n\n```mjsqzdtksaj ipython3 import torch import torch.nn as nn import torch.nn.functional as F import pytorch_lightning as pl from torch.utils.data import DataLoader, random_split from torchvision import datasets, transforms from torchmetrics import Accuracy\nclass MNISTDataModule(pl.LightningDataModule): ‚Äú‚Äú‚Äù PyTorch Lightning data module for MNIST dataset ‚Äú‚Äú‚Äù def init(self, data_dir: str = ‚Äò./data‚Äô, batch_size: int = 32): super().__init__() self.data_dir = data_dir self.batch_size = batch_size\n    # Define transforms\n    self.transform = transforms.Compose([\n        transforms.ToTensor(),           # Convert PIL image to torch.Tensor\n        transforms.Normalize((0,), (1,)) # Normalize to mean=0, std=1\n    ])\n\ndef prepare_data(self):\n    \"\"\"Download data if needed.\"\"\"\n    datasets.MNIST(self.data_dir, train=True, download=True)\n    datasets.MNIST(self.data_dir, train=False, download=True)\n\ndef setup(self, stage=None):\n    \"\"\"Setup train, val, and test datasets.\"\"\"\n    if stage == 'fit' or stage is None:\n        mnist_full = datasets.MNIST(self.data_dir, train=True, transform=self.transform)\n        self.mnist_train, self.mnist_val = random_split(\n            mnist_full, [55000, 5000], generator=torch.Generator().manual_seed(42)\n        )\n\n    if stage == 'test' or stage is None:\n        self.mnist_test = datasets.MNIST(self.data_dir, train=False, transform=self.transform)\n\ndef train_dataloader(self):\n    return DataLoader(self.mnist_train, batch_size=self.batch_size, shuffle=True, num_workers=1)\n\ndef val_dataloader(self):\n    return DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=1)\n\ndef test_dataloader(self):\n    return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=1)\n\n```{note}\n**PyTorch Lightning DataModule**:\n- Ensures consistent data splits for training, validation, and testing.\n- Handles shuffling, batching, and transformation pipelines.\n- Makes code cleaner and easier to maintain.\n\n\n\n```mjsqzdtksaj ipython3 class LeNet1(pl.LightningModule): ‚Äú‚Äú‚Äù PyTorch Lightning implementation of LeNet-1 ‚Äú‚Äú‚Äù\ndef __init__(self, learning_rate=1e-3):\n    super(LeNet1, self).__init__()\n    self.save_hyperparameters()\n\n    # Metrics\n    self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=10)\n    self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=10)\n    self.test_accuracy = Accuracy(task=\"multiclass\", num_classes=10)\n\n    # First convolutional layer (Input: 1x28x28 -&gt; Output: 4x24x24)\n    self.conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=5, stride=1)\n\n    # Average pooling layer (4x24x24 -&gt; 4x12x12)\n    self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n\n    # Second convolutional layer (4x12x12 -&gt; 12x8x8)\n    self.conv2 = nn.Conv2d(in_channels=4, out_channels=12, kernel_size=5, stride=1)\n\n    # Fully connected layer (12*4*4=192 -&gt; 10)\n    self.fc = nn.Linear(12 * 4 * 4, 10)\n\n    # Initialize weights\n    self._init_weights()\n\n    # Track losses over time (for visualization)\n    self.val_losses = []\n    self.train_losses = []\n\ndef _init_weights(self):\n    \"\"\"Initialize weights with Xavier initialization.\"\"\"\n    for m in self.modules():\n        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n            nn.init.xavier_uniform_(m.weight)\n            nn.init.zeros_(m.bias)\n\ndef forward(self, x):\n    # First conv block\n    x = self.conv1(x)\n    x = torch.tanh(x)  # Using tanh for nonlinearity\n    x = self.pool(x)\n\n    # Second conv block\n    x = self.conv2(x)\n    x = torch.tanh(x)\n    x = self.pool(x)\n\n    # Flatten and fully connected\n    x = x.view(-1, 12 * 4 * 4)\n    x = self.fc(x)\n    return x\n\ndef configure_optimizers(self):\n    \"\"\"Define optimizer and LR scheduler.\"\"\"\n    optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode=\"min\", factor=0.1, patience=10, verbose=True\n    )\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"val_loss\"}\n    }\n\ndef training_step(self, batch, batch_idx):\n    \"\"\"Train on a single batch.\"\"\"\n    x, y = batch\n    logits = self(x)\n    loss = F.cross_entropy(logits, y)\n\n    acc = self.train_accuracy(logits, y)\n    self.log(\"train_loss\", loss, prog_bar=True)\n    self.log(\"train_acc\", acc, prog_bar=True)\n\n    self.train_losses.append({\"loss\": loss.item(), \"acc\": acc.item()})\n    return loss\n\ndef validation_step(self, batch, batch_idx):\n    \"\"\"Validate on a single batch.\"\"\"\n    x, y = batch\n    logits = self(x)\n    loss = F.cross_entropy(logits, y)\n\n    acc = self.val_accuracy(logits, y)\n    self.log(\"val_loss\", loss, prog_bar=True)\n    self.log(\"val_acc\", acc, prog_bar=True)\n\n    self.val_losses.append({\"loss\": loss.item(), \"acc\": acc.item()})\n\ndef test_step(self, batch, batch_idx):\n    \"\"\"Test on a single batch.\"\"\"\n    x, y = batch\n    logits = self(x)\n    loss = F.cross_entropy(logits, y)\n\n    acc = self.test_accuracy(logits, y)\n    self.log(\"test_loss\", loss, prog_bar=True)\n    self.log(\"test_acc\", acc, prog_bar=True)\n\n### Training the Model\n\n```{code-cell} ipython3\n# Initialize model and data\nmodel = LeNet1(learning_rate=1e-3)\ndata_module = MNISTDataModule(batch_size=256)\n\n# Initialize trainer\ntrainer = pl.Trainer(\n    max_epochs=2,\n    accelerator=\"auto\",  # Use GPU if available\n    devices=1,\n)\n\n# Train\ntrainer.fit(model, data_module)\n**Check GPU usage**:\n- If you have a GPU available, `accelerator=\"auto\"` automatically leverages it.\n- Otherwise, it trains on CPU, which is slower but will still work for a small model like LeNet-1.\n\n\n\n```mjsqzdtksaj ipython3 import seaborn as sns import matplotlib.pyplot as plt import pandas as pd\ndf_val = pd.DataFrame(model.val_losses) df_val[‚ÄúIteration‚Äù] = df_val.index\nfig, ax = plt.subplots(figsize=(10, 6)) sns.lineplot(x=‚ÄúIteration‚Äù, y=‚Äúloss‚Äù, data=df_val, label=‚ÄúValidation Loss‚Äù, ax=ax) ax.set_title(‚ÄúValidation Loss Over Time‚Äù) ax.set_xlabel(‚ÄúIteration‚Äù) ax.set_ylabel(‚ÄúLoss‚Äù) plt.show()\n\n### Testing\n\n```{code-cell} ipython3\ntrainer.test(model, data_module)\nObserve the final test loss and test accuracy. Even this simple LeNet-1 inspired model often achieves high accuracy on MNIST‚Äîdemonstrating how effective early CNN architectures can be."
  },
  {
    "objectID": "m05-images/archive/lenet.html#reflection-and-exercises",
    "href": "m05-images/archive/lenet.html#reflection-and-exercises",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Experiment:\n\nVary the learning rate and batch size to see how training dynamics change.\nReplace tanh activation with ReLU or Sigmoid and compare performance.\n\nVisual Inspection:\n\nHand-draw a digit (e.g., using a graphics tool) and see whether the model correctly classifies it. If it fails, hypothesize why (differences in stroke thickness, image alignment, etc.).\n\nArchitectural Tweaks:\n\nTry adding an additional convolutional layer or using different pooling strategies (like max pooling) to see if you can improve accuracy.\n\n\n**Real-world Application**:\nLeNet‚Äôs core ideas are still used in modern banking systems to read checks automatically. Its principle of learning features from raw data underpins almost all modern deep-learning-based image classification systems."
  },
  {
    "objectID": "m05-images/archive/lenet.html#further-reading",
    "href": "m05-images/archive/lenet.html#further-reading",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Writing LeNet5 from Scratch in PyTorch (DigitalOcean)\nPyTorch LeNet Implementation Video\nOriginal LeCun Paper\n\n:style: unsrt\n:filter: docname in docnames"
  },
  {
    "objectID": "m05-images/archive/lenet.html#summary",
    "href": "m05-images/archive/lenet.html#summary",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "In summary, LeNet {footcite}lecun1989backpropagation and LeNet-5 {footcite}lecun1998gradient formed the basis for convolutional networks that learn directly from data. By incorporating convolution, subsampling, sparse connectivity, and end-to-end training, LeNet demonstrated how networks can autonomously discover robust representations‚Äîlaying the groundwork for today‚Äôs deep learning revolution."
  },
  {
    "objectID": "m05-images/archive/resnet.html",
    "href": "m05-images/archive/resnet.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Why did simply adding more layers to CNNs (like VGGNet or InceptionNet) fail to yield the expected performance gains‚Äîand sometimes even degraded accuracy?\nResidual Neural Networks (ResNet) fundamentally changed the landscape of deep CNN training by introducing residual connections (a.k.a. skip connections). By stacking a series of residual blocks, ResNet enabled training CNNs with dozens or even hundreds of layers without succumbing to the vanishing gradient problem. Today, ResNet is considered one of the most important innovations in the history of deep learning, influencing architectures like ResNeXt and even Transformers.\nResNeXt is an improvement over ResNet proposed by the same research group {footcite}`xie2017aggregated`. It widens the residual blocks via grouped convolutions, achieving higher performance without drastically increasing depth.\n\n\nResNet was introduced in {footcite}he2016deep to address a key challenge at the time: CNNs deeper than about 20 layers were difficult to optimize and often performed worse than shallower counterparts. Despite the success of VGGNet (16 or 19 layers) and InceptionNet, researchers still faced two major issues when pushing CNNs to 50 layers or more:\n\nDegradation Problem: Simply stacking more layers often degraded accuracy, rather than improving it.\nLong Training Times: Extremely deep CNNs took a long time to converge, especially if the network was prone to vanishing or exploding gradients.\n\nThe ResNet solution was surprisingly simple yet groundbreaking: add skip connections that carry the original inputs across a few layers unmodified, letting the network focus on modeling the residual.\n\n\n\n\n\nShouldn‚Äôt deeper networks always perform better because they have more parameters and expressive power?\nIn theory, deeper CNNs can capture richer, more complex patterns. However, two issues hindered progress:\n\nDegradation Problem Even with techniques like batch normalization, adding more layers beyond ~20 caused training error to increase, not decrease. This phenomenon was not simply due to overfitting‚Äîrather, the deeper network failed to optimize properly.\nLonger Training and Vanishing Gradients As more layers are added, gradients can vanish (or explode). Backprop had trouble sending meaningful error signals all the way to early layers, causing them to learn slowly or not at all.\n\n\n\n\nWhat if each stack of layers simply learned a correction (residual) to the identity mapping?\nA residual block consists of two (or three) convolutions grouped together, plus a skip connection:\n\nResidual Path: A few convolution layers (for example, two 3√ó3 conv layers) modeling a function $ F() $.\nSkip (Identity) Path: A direct path for \\mathbf{x} to bypass the convolutions entirely.\n\nAt the end of the block, the skip path is added elementwise to the residual path: \n\\mathbf{y} = F(\\mathbf{x}) + \\mathbf{x}.\n\nIn PyTorch, you can implement a basic residual block as follows:\n```zkqqhafabqy ipython3 import torch import torch.nn as nn\nclass BasicBlock(nn.Module): def init(self, in_channels, out_channels, stride=1): super().__init__() self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False) self.bn1 = nn.BatchNorm2d(out_channels) self.relu = nn.ReLU(inplace=True)\n    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n    self.bn2 = nn.BatchNorm2d(out_channels)\n\ndef forward(self, x):\n    identity = x\n\n    out = self.conv1(x)\n    out = self.bn1(out)\n    out = self.relu(out)\n\n    out = self.conv2(out)\n    out = self.bn2(out)\n\n    out += identity\n    out = self.relu(out)\n    return out\n\n```{figure} https://www.researchgate.net/publication/364330795/figure/fig7/AS:11431281176036099@1689999593116/Basic-residual-block-of-ResNet.png\n:name: resnet-block\n:align: center\n:width: 50%\nA basic 2-layer residual block (left) vs. a plain block without skip (right). The skip connection allows the input $\\mathbf{x}$ to directly add to the block‚Äôs output.\nBy stacking many such blocks, the network effectively cascades small residual changes across layers. The key benefits are:\n\nEasier Optimization Instead of learning a full mapping \\mathbf{y} = G(\\mathbf{x}), the block learns only the difference G(\\mathbf{x}) - \\mathbf{x}. This decomposition often proves easier to optimize.\nIf the optimal mapping is close to identity (i.e., the layer isn't very important), the network can easily \"skip\" it by learning $F(\\mathbf{x}) \\approx 0$. If a more complex transformation is needed, the residual path can still learn it. This makes training more robust‚Äîthe network doesn‚Äôt have to work as hard to preserve important information through deep layers.\nEnsemble-Like Behavior When you chain N residual blocks, you effectively create numerous paths for gradient flow‚Äîsome skip many layers, some pass through multiple convolutions. This variety of gradient routes can speed convergence and reduce the risk of vanishing gradients {footcite}veit2016residual.\nbgoecjyf https://arxiv.org/html/2405.01725v1/x28.png  :name: resnet-gradient-flow  :align: center  :width: 100%  The gradient flow in ResNet with skip connections.\nDeeper Without Degradation ResNet-50, -101, and -152 can be trained without suffering the performance drop typical of overly deep ‚Äúplain‚Äù networks.\n\n\n\n\nResNet has some variants depending on the depth. For deep ResNet, the bottleneck design is used to maintain computational efficiency.\nbgoecjyf https://i.sstatic.net/kbiIG.png :name: resnet-bottleneck-block :align: center :width: 80% A bottleneck block of ResNet.\nThis bottleneck block consists of three convolutions instead of two, where: - the first 1 \\times 1 conv reduces the feature dimension. - the second 3 \\times 3 conv operates on this reduced dimension. - the third 1 \\times 1 conv restores the dimension.\nThis approach shrinks the intermediate feature map, saving computational cost while retaining overall representational capacity. It was inspired by InceptionNet‚Äôs ‚Äúbottleneck‚Äù idea {footcite}szegedy2016inception,szegedy2015going.\n**ResNet-50**, **ResNet-101**, and **ResNet-152** all use bottleneck blocks. While they have more layers, they remain computationally feasible and yield progressively better accuracy on ImageNet.\n\n\n\n\nWhat if we can widen the residual blocks without drastically increasing overall parameters?\nResNeXt {footcite}xie2017aggregated is an evolution of ResNet that: 1. Splits the bottleneck conv pathway into multiple ‚Äúcardinality‚Äù groups (e.g., 32 groups). 2. Aggregates those parallel paths (grouped convolutions) back into a single output.\nBy increasing cardinality (the number of parallel conv groups) instead of just adding more channels or layers, ResNeXt achieves better accuracy with moderate complexity. This approach also draws on the idea of Inception‚Äôs multi-branch parallel conv, but unifies them into a single grouped-convolution block.\nbgoecjyf https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-06_at_4.32.52_PM.png :name: resnext-block :align: center :width: 50% A basic block of ResNeXt, showing multiple grouped-conv ‚Äúpaths‚Äù that are aggregated.\n\n\n\n\nWriting ResNet from Scratch in PyTorch\n\n\n\n\n\nResidual Learning ResNet overcame the degradation problem by framing deeper CNNs as a series of residual blocks, each learning a function $ F() $ that is added to \\mathbf{x}.\nScalability With skip connections, ResNet-50, -101, and -152 exhibit higher accuracy without the optimization collapse typical of deeper plain networks.\nBottleneck & Beyond For high-depth architectures, the bottleneck design (1\\times1 \\to 3\\times3 \\to 1\\times1) improves efficiency. ResNeXt further extends ResNet by widening these pathways via grouped convolutions.\nLasting Impact Residual connections are now ubiquitous‚Äînot just in CNNs but also in Transformers, large-scale language models, U-Nets, and many other architectures. They simplify optimization and significantly improve gradient flow in very deep models.\n\nResNet‚Äôs simplicity made it a foundation for many follow-up architectures. Unlike designs with complex branching (e.g., Inception blocks), ResNet remains easy to implement, debug, and extend‚Äîan important factor behind its widespread adoption.\n\n\n\n\nImplement a Basic (Non-Bottleneck) Residual Block\n\nCreate a two-convolution block with skip connections.\nTest it on random data to confirm dimensions match.\n\nTrain a Small ResNet\n\nImplement ResNet-18 or ResNet-34 from scratch on a smaller dataset (e.g., CIFAR-10).\nObserve the training curve and compare to a plain CNN of the same depth.\n\nExperiment with Bottleneck Blocks\n\nConvert your ResNet-34 to a bottleneck-based ResNet-50-like structure.\nCheck the parameter count and performance difference on CIFAR-10 or a subset of ImageNet.\n\n\njtxvvgqnnpksoxdeko references.bib :style: unsrt :filter: docname in docnames"
  },
  {
    "objectID": "m05-images/archive/resnet.html#introduction-and-context",
    "href": "m05-images/archive/resnet.html#introduction-and-context",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "ResNet was introduced in {footcite}he2016deep to address a key challenge at the time: CNNs deeper than about 20 layers were difficult to optimize and often performed worse than shallower counterparts. Despite the success of VGGNet (16 or 19 layers) and InceptionNet, researchers still faced two major issues when pushing CNNs to 50 layers or more:\n\nDegradation Problem: Simply stacking more layers often degraded accuracy, rather than improving it.\nLong Training Times: Extremely deep CNNs took a long time to converge, especially if the network was prone to vanishing or exploding gradients.\n\nThe ResNet solution was surprisingly simple yet groundbreaking: add skip connections that carry the original inputs across a few layers unmodified, letting the network focus on modeling the residual."
  },
  {
    "objectID": "m05-images/archive/resnet.html#resnet-in-detail",
    "href": "m05-images/archive/resnet.html#resnet-in-detail",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Shouldn‚Äôt deeper networks always perform better because they have more parameters and expressive power?\nIn theory, deeper CNNs can capture richer, more complex patterns. However, two issues hindered progress:\n\nDegradation Problem Even with techniques like batch normalization, adding more layers beyond ~20 caused training error to increase, not decrease. This phenomenon was not simply due to overfitting‚Äîrather, the deeper network failed to optimize properly.\nLonger Training and Vanishing Gradients As more layers are added, gradients can vanish (or explode). Backprop had trouble sending meaningful error signals all the way to early layers, causing them to learn slowly or not at all.\n\n\n\n\nWhat if each stack of layers simply learned a correction (residual) to the identity mapping?\nA residual block consists of two (or three) convolutions grouped together, plus a skip connection:\n\nResidual Path: A few convolution layers (for example, two 3√ó3 conv layers) modeling a function $ F() $.\nSkip (Identity) Path: A direct path for \\mathbf{x} to bypass the convolutions entirely.\n\nAt the end of the block, the skip path is added elementwise to the residual path: \n\\mathbf{y} = F(\\mathbf{x}) + \\mathbf{x}.\n\nIn PyTorch, you can implement a basic residual block as follows:\n```zkqqhafabqy ipython3 import torch import torch.nn as nn\nclass BasicBlock(nn.Module): def init(self, in_channels, out_channels, stride=1): super().__init__() self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False) self.bn1 = nn.BatchNorm2d(out_channels) self.relu = nn.ReLU(inplace=True)\n    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n    self.bn2 = nn.BatchNorm2d(out_channels)\n\ndef forward(self, x):\n    identity = x\n\n    out = self.conv1(x)\n    out = self.bn1(out)\n    out = self.relu(out)\n\n    out = self.conv2(out)\n    out = self.bn2(out)\n\n    out += identity\n    out = self.relu(out)\n    return out\n\n```{figure} https://www.researchgate.net/publication/364330795/figure/fig7/AS:11431281176036099@1689999593116/Basic-residual-block-of-ResNet.png\n:name: resnet-block\n:align: center\n:width: 50%\nA basic 2-layer residual block (left) vs. a plain block without skip (right). The skip connection allows the input $\\mathbf{x}$ to directly add to the block‚Äôs output.\nBy stacking many such blocks, the network effectively cascades small residual changes across layers. The key benefits are:\n\nEasier Optimization Instead of learning a full mapping \\mathbf{y} = G(\\mathbf{x}), the block learns only the difference G(\\mathbf{x}) - \\mathbf{x}. This decomposition often proves easier to optimize.\nIf the optimal mapping is close to identity (i.e., the layer isn't very important), the network can easily \"skip\" it by learning $F(\\mathbf{x}) \\approx 0$. If a more complex transformation is needed, the residual path can still learn it. This makes training more robust‚Äîthe network doesn‚Äôt have to work as hard to preserve important information through deep layers.\nEnsemble-Like Behavior When you chain N residual blocks, you effectively create numerous paths for gradient flow‚Äîsome skip many layers, some pass through multiple convolutions. This variety of gradient routes can speed convergence and reduce the risk of vanishing gradients {footcite}veit2016residual.\nbgoecjyf https://arxiv.org/html/2405.01725v1/x28.png  :name: resnet-gradient-flow  :align: center  :width: 100%  The gradient flow in ResNet with skip connections.\nDeeper Without Degradation ResNet-50, -101, and -152 can be trained without suffering the performance drop typical of overly deep ‚Äúplain‚Äù networks.\n\n\n\n\nResNet has some variants depending on the depth. For deep ResNet, the bottleneck design is used to maintain computational efficiency.\nbgoecjyf https://i.sstatic.net/kbiIG.png :name: resnet-bottleneck-block :align: center :width: 80% A bottleneck block of ResNet.\nThis bottleneck block consists of three convolutions instead of two, where: - the first 1 \\times 1 conv reduces the feature dimension. - the second 3 \\times 3 conv operates on this reduced dimension. - the third 1 \\times 1 conv restores the dimension.\nThis approach shrinks the intermediate feature map, saving computational cost while retaining overall representational capacity. It was inspired by InceptionNet‚Äôs ‚Äúbottleneck‚Äù idea {footcite}szegedy2016inception,szegedy2015going.\n**ResNet-50**, **ResNet-101**, and **ResNet-152** all use bottleneck blocks. While they have more layers, they remain computationally feasible and yield progressively better accuracy on ImageNet."
  },
  {
    "objectID": "m05-images/archive/resnet.html#resnext-a-resnet-improvement",
    "href": "m05-images/archive/resnet.html#resnext-a-resnet-improvement",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "What if we can widen the residual blocks without drastically increasing overall parameters?\nResNeXt {footcite}xie2017aggregated is an evolution of ResNet that: 1. Splits the bottleneck conv pathway into multiple ‚Äúcardinality‚Äù groups (e.g., 32 groups). 2. Aggregates those parallel paths (grouped convolutions) back into a single output.\nBy increasing cardinality (the number of parallel conv groups) instead of just adding more channels or layers, ResNeXt achieves better accuracy with moderate complexity. This approach also draws on the idea of Inception‚Äôs multi-branch parallel conv, but unifies them into a single grouped-convolution block.\nbgoecjyf https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-06_at_4.32.52_PM.png :name: resnext-block :align: center :width: 50% A basic block of ResNeXt, showing multiple grouped-conv ‚Äúpaths‚Äù that are aggregated."
  },
  {
    "objectID": "m05-images/archive/resnet.html#implementation-of-resnet",
    "href": "m05-images/archive/resnet.html#implementation-of-resnet",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Writing ResNet from Scratch in PyTorch"
  },
  {
    "objectID": "m05-images/archive/resnet.html#summary",
    "href": "m05-images/archive/resnet.html#summary",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Residual Learning ResNet overcame the degradation problem by framing deeper CNNs as a series of residual blocks, each learning a function $ F() $ that is added to \\mathbf{x}.\nScalability With skip connections, ResNet-50, -101, and -152 exhibit higher accuracy without the optimization collapse typical of deeper plain networks.\nBottleneck & Beyond For high-depth architectures, the bottleneck design (1\\times1 \\to 3\\times3 \\to 1\\times1) improves efficiency. ResNeXt further extends ResNet by widening these pathways via grouped convolutions.\nLasting Impact Residual connections are now ubiquitous‚Äînot just in CNNs but also in Transformers, large-scale language models, U-Nets, and many other architectures. They simplify optimization and significantly improve gradient flow in very deep models.\n\nResNet‚Äôs simplicity made it a foundation for many follow-up architectures. Unlike designs with complex branching (e.g., Inception blocks), ResNet remains easy to implement, debug, and extend‚Äîan important factor behind its widespread adoption."
  },
  {
    "objectID": "m05-images/archive/resnet.html#suggested-exercises",
    "href": "m05-images/archive/resnet.html#suggested-exercises",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Implement a Basic (Non-Bottleneck) Residual Block\n\nCreate a two-convolution block with skip connections.\nTest it on random data to confirm dimensions match.\n\nTrain a Small ResNet\n\nImplement ResNet-18 or ResNet-34 from scratch on a smaller dataset (e.g., CIFAR-10).\nObserve the training curve and compare to a plain CNN of the same depth.\n\nExperiment with Bottleneck Blocks\n\nConvert your ResNet-34 to a bottleneck-based ResNet-50-like structure.\nCheck the parameter count and performance difference on CIFAR-10 or a subset of ImageNet.\n\n\njtxvvgqnnpksoxdeko references.bib :style: unsrt :filter: docname in docnames"
  },
  {
    "objectID": "m05-images/archive/vision_transformer.html",
    "href": "m05-images/archive/vision_transformer.html",
    "title": "Vision Transformers",
    "section": "",
    "text": "What if we could capture not just the local features in images (like corners, edges, or textures) but the entire global context all at once? Could that help a model better understand complex scenes and relationships between objects? Vision Transformers (ViT) attempt exactly that by leveraging self-attention, a mechanism originally popularized in Natural Language Processing."
  },
  {
    "objectID": "m05-images/archive/vision_transformer.html#the-genesis-of-vision-transformers",
    "href": "m05-images/archive/vision_transformer.html#the-genesis-of-vision-transformers",
    "title": "Vision Transformers",
    "section": "The Genesis of Vision Transformers",
    "text": "The Genesis of Vision Transformers\n\nConceptual Foundation\nWhy were Vision Transformers developed, given that Convolutional Neural Networks (CNNs) already excel in computer vision tasks?\nCNNs have been the cornerstone of computer vision for years, particularly good at capturing local patterns through convolutional filters. However, they can struggle to efficiently capture global context and long-range dependencies. In scenarios where relationships between objects spread across an entire image become crucial (e.g., understanding crowd scenes or satellite imagery), this limitation can be significant.\nMeanwhile, the Transformer architecture (from the paper ‚ÄúAttention Is All You Need‚Äù) revolutionized NLP by modeling long-range dependencies in sequential data. This success inspired researchers to ask: Could the same self-attention mechanism help models ‚Äòsee‚Äô the entire image at once, instead of focusing on small, local regions?\n```axrtvram https://www.researchgate.net/publication/361733806/figure/fig3/AS%3A1173979050057729%401656909825527/Operation-of-CNN-and-ViT.ppm :name: fig-cnn-vit :width: 500px :align: center\nComparison of the receptive field of CNNs and Vision Transformers. CNN has a local receptive field constrained by the convolutional filters, while ViT has a global receptive field, allowing it to capture long-range dependencies between different parts of the image.\n\n```{note}\n**Historical Context**\n\n- **CNN Dominance (2010s)**: CNNs (e.g., AlexNet, VGG, ResNet) drove huge leaps in image classification and object detection.\n- **Transformer Breakthrough (2017)**: In NLP, Transformers replaced recurrent architectures (LSTMs, GRUs) for tasks like machine translation.\n- **ViT Emerges (2020)**: Google researchers introduced the idea of applying pure Transformers to image patches, showing excellent results on large-scale image datasets."
  },
  {
    "objectID": "m05-images/archive/vision_transformer.html#the-vision-transformer-vit-architecture",
    "href": "m05-images/archive/vision_transformer.html#the-vision-transformer-vit-architecture",
    "title": "Vision Transformers",
    "section": "The Vision Transformer (ViT) Architecture",
    "text": "The Vision Transformer (ViT) Architecture\nHow do we adapt an NLP-centric Transformer to handle 2D image data?\nIn Vision Transformers, an image is first split into a grid of small, equally sized patches‚Äîcommonly 16 \\times 16 pixels each. Each patch is flattened and fed into a linear layer that creates a higher-dimensional embedding. You can think of each patch embedding as analogous to a ‚Äúword embedding‚Äù in NLP.\n```axrtvram https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F7a096efc8f3cc40849ee17a546dc0e685da2dc73-4237x1515.png&w=3840&q=75 :name: fig-vit-patch :width: 500px :align: center\nThe process of splitting an image into patches and feeding them into a Vision Transformer. Image taken from Pinecone.\n\n\n```{note}\n**Why Patches Instead of Pixels?**\n\n- Handling each pixel independently would create a massive sequence (e.g., a 224x224 image has 50176 pixels!).\n- Using patches reduces sequence length substantially and preserves local spatial structure.\n\n2.2 Positional Encodings\nBecause Transformers are order-agnostic, we add positional encodings to each patch embedding. These encodings help the model understand the position of each patch in the original image grid.\n\n\n2.3 Transformer Encoder\nThe sequence of patch embeddings (plus positional encodings) goes through a Transformer encoder, consisting of: - Multi-Head Self-Attention: Allows each patch to attend to others, learning both local and global image features. - Feed-Forward Layers (MLP blocks): Expands and contracts the hidden dimension to add non-linear transformations.\n\n\n2.4 Classification Head\nTypically, the [CLS] token (a special token prepended to the sequence) serves as the global representation. After passing through all encoder layers, it goes to a lightweight classification head (a small MLP) to predict the output class.\n**Mathematical Foundation (Simplified)**\n\nSelf-attention for a single head can be described as:\n$$\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V\n$$\n\n- $Q, K, V$ are linear projections of the input (patch embeddings).\n- $d_k$ is the dimension of $K$.\n- Multi-head attention runs this process in parallel with different learnable projections."
  },
  {
    "objectID": "m05-images/archive/vision_transformer.html#types-of-attention-mechanisms",
    "href": "m05-images/archive/vision_transformer.html#types-of-attention-mechanisms",
    "title": "Vision Transformers",
    "section": "3. Types of Attention Mechanisms",
    "text": "3. Types of Attention Mechanisms\nEven though Vision Transformers generally use multi-head self-attention, research has explored variations:\n\nStochastic ‚ÄúHard‚Äù Attention: Focuses on a subset of patches while ignoring others.\nDeterministic ‚ÄúSoft‚Äù Attention: Assigns weights to all patches.\nMulti-Head Attention: Employs multiple attention heads to learn different aspects (textures, edges, shapes) simultaneously.\n\n**Implementation Insight**\n\nYou can vary the attention mechanism to strike different balances between computational cost and representational capacity. Hard attention can be more efficient but trickier to train."
  },
  {
    "objectID": "m05-images/archive/vision_transformer.html#implementation-example",
    "href": "m05-images/archive/vision_transformer.html#implementation-example",
    "title": "Vision Transformers",
    "section": "4. Implementation Example",
    "text": "4. Implementation Example\nLet‚Äôs walk through a simplified code snippet using Hugging Face Transformers to classify images with a Vision Transformer. This gives a concrete look at how to build upon these theoretical concepts in practice.\n```jfrfjoysmer ipython3 # A minimal ViT classification example with Hugging Face\n!pip install transformers !pip install torch !pip install torchvision\nimport torch from transformers import ViTForImageClassification, ViTImageProcessor from PIL import Image import requests"
  },
  {
    "objectID": "m05-images/archive/vision_transformer.html#advancements-in-vision-transformer-architectures",
    "href": "m05-images/archive/vision_transformer.html#advancements-in-vision-transformer-architectures",
    "title": "Vision Transformers",
    "section": "5. Advancements in Vision Transformer Architectures",
    "text": "5. Advancements in Vision Transformer Architectures\nCould we make ViTs more data-efficient, faster, or better at capturing hierarchical features?\n\n5.1 Improved Training and Architectures\n\nDeiT (Data-efficient Image Transformers): Introduces a distillation step to improve data efficiency, making ViTs competitive with CNNs on smaller datasets.\nModel Soups: Averages predictions from multiple ViT models to harness their individual strengths for higher accuracy.\n\n\n\n5.2 Hierarchical and Hybrid Approaches\n\nSwin Transformer: Processes images in a hierarchical manner using non-overlapping patches at different resolutions, improving scalability to arbitrary image sizes.\nCaiT (Cross-Attention Image Transformer): Uses cross-attention between different patch groups to capture more complex relationships.\nCSWin Transformer: Adopts a cross-shaped window self-attention pattern to optimize the balance between spatial coverage and computational cost.\nFDViT: Employs flexible downsampling layers for smoother feature map reductions, improving efficiency and classification accuracy.\n\n**Common Misconception**\n\nIt‚Äôs tempting to think ViTs automatically solve all the limitations of CNNs. However, they still require careful tuning, large datasets (or pre-training), and thoughtful architecture decisions to perform at their best."
  },
  {
    "objectID": "m05-images/archive/vision_transformer.html#strengths-and-weaknesses",
    "href": "m05-images/archive/vision_transformer.html#strengths-and-weaknesses",
    "title": "Vision Transformers",
    "section": "6. Strengths and Weaknesses",
    "text": "6. Strengths and Weaknesses\nWhen do Vision Transformers shine, and where do they falter?\n\n\n\n\n\n\n\n\nFeature\nConvolutional Neural Networks (CNNs)\nVision Transformers (ViTs)\n\n\n\n\nArchitecture\nConvolutional + pooling + MLP\nPure Transformer with self-attention\n\n\nInput Processing\nProcesses entire image as is\nSplits image into patches (tokens)\n\n\nGlobal Context\nEmerges in deeper layers\nCaptured from the start across all patches\n\n\nData Requirements\nPerform well with moderate data\nOften require very large datasets or pre-training\n\n\nCompute Cost\nUsually lower, localized ops\nHigher due to self-attention on all patches\n\n\nPerformance\nExcellent with well-tuned architectures\nExcels on large-scale data, state-of-the-art SOTA\n\n\n\n\nKey Advantages\n\nGlobal Context: The self-attention mechanism can integrate information from all patches simultaneously.\nScalability: ViTs shine on large datasets, often surpassing CNNs.\nReduced Inductive Bias: They learn more general representations since they are not hard-coded to look for local spatial features like CNNs.\n\n\n\nMain Limitations\n\nData-Hungry: Tend to overfit on small datasets; methods like DeiT and heavy augmentation help.\nHigh Computational Cost: Each patch attends to all others, which can be expensive for high-resolution images.\nInterpretability: Visualizing attention maps is possible, but can still be less intuitive than CNN feature maps."
  },
  {
    "objectID": "m05-images/archive/vision_transformer.html#real-world-applications",
    "href": "m05-images/archive/vision_transformer.html#real-world-applications",
    "title": "Vision Transformers",
    "section": "7. Real-World Applications",
    "text": "7. Real-World Applications\nHow are Vision Transformers being used beyond simple image classification?\n\nObject Detection & Image Segmentation: Self-attention helps capture relationships among objects scattered across the scene.\nMedical Imaging: Identifying tumors in X-rays or segmenting organ boundaries in MRI scans.\nRemote Sensing: Analyzing satellite imagery for deforestation tracking or disaster management.\nAction Recognition in Videos: Extended to video frames, ViTs can learn complex spatiotemporal patterns.\nMulti-Modal Tasks: Works well with textual data (e.g., image captioning, visual question answering).\nAutonomous Driving: Understanding global context on the road is critical for safe navigation.\nAnomaly Detection: Identifying unusual patterns or defects in manufacturing lines.\n\n**Real-World Use Case**\n\nIn **medical imaging**, ViTs can better spot anomalies by focusing on subtle global context differences in scans. This can help radiologists detect diseases in early stages and potentially save lives."
  },
  {
    "objectID": "m05-images/archive/vision_transformer.html#future-directions",
    "href": "m05-images/archive/vision_transformer.html#future-directions",
    "title": "Vision Transformers",
    "section": "8. Future Directions",
    "text": "8. Future Directions\n\nEnhanced Efficiency: Model compression, pruning, and improved patch strategies aim to reduce computational overhead.\nSmaller Dataset Training: More advanced self-supervision, distillation, and data-augmentation techniques are being developed to tackle data limitations.\nInterpretability: Research on attention visualization tools and explanations is growing, aiming to make ViTs more transparent.\nNew Domains: From multi-modal reasoning to video analysis, ViTs are expanding across countless tasks in AI.\n\n**Performance Optimization**\n\n- Distillation from large teacher ViTs or even CNNs can help small ViTs converge faster with less data.\n- Layer-wise learning rate decay and progressive resizing of patches are common training tricks."
  },
  {
    "objectID": "m05-images/archive/vision_transformer.html#reflection-and-exercises",
    "href": "m05-images/archive/vision_transformer.html#reflection-and-exercises",
    "title": "Vision Transformers",
    "section": "9. Reflection and Exercises",
    "text": "9. Reflection and Exercises\n\nReflection: Why do you think ViTs require large datasets to perform optimally, and how might transfer learning mitigate this requirement?\nExercise: Implement a fine-tuning script for a ViT on a smaller dataset (e.g., CIFAR-10). Try various data augmentation strategies. Compare results with a CNN baseline.\nAdvanced Exploration: Experiment with Swin Transformer or CSWin Transformer. Observe how hierarchical patching or specialized window attention changes performance and training speed."
  },
  {
    "objectID": "m05-images/archive/vision_transformer.html#references",
    "href": "m05-images/archive/vision_transformer.html#references",
    "title": "Vision Transformers",
    "section": "References",
    "text": "References\n\nVision Transformers - The Future of Computer Vision! [ResearchGate]\nIntroduction to Vision Transformers | Original ViT Paper Explained [aipapersacademy.com]\nDeploying Attention-Based Vision Transformers to Apple Neural Engine [machinelearning.apple.com]\nVision Transformers (ViT) in Image Recognition: Full Guide [viso.ai]\nVision Transformers, Explained. A Full Walk-Through of Vision‚Ä¶ [Towards Data Science]\nFrom Transformers to Vision Transformers (ViT): Applying NLP Models to Computer Vision [Medium]\nA Comprehensive Study of Vision Transformers in Image Classification Tasks [arXiv]\nIntroductory guide to Vision Transformers [Encord]\nEfficient Training of Visual Transformers with Small Datasets [NeurIPS Proceedings]\nFDViT: Improve the Hierarchical Architecture of Vision Transformer [ICCV 2023]\nVision Transformers vs.¬†Convolutional Neural Networks (CNNs) [GeeksforGeeks]\nVision Transformers vs CNNs at the Edge [Edge AI Vision]\nWhat is a Vision Transformer (ViT)? Real-World Applications [SJ Innovation]\nVision Transformer: An Introduction [Built In]\nMastering Vision Transformers with Hugging Face [Rapid Innovation]\nVision Transformer (ViT) - Hugging Face [huggingface.co]\nTop 10 Open Source Computer Vision Repositories [Encord]\nyhlleo/VTs-Drloc: Efficient Training of Visual Transformers [GitHub]\nVision Transformers for Image Classification: A Comparative Survey [MDPI]\nBMVC 2022: How to Train Vision Transformer on Small-scale Datasets? [GitHub]\nVision Transformer: What It Is & How It Works [V7 Labs]\n\n\n**Key Takeaway**\n\nVision Transformers offer a fresh approach to image understanding by modeling global relationships among patches from the get-go. As architectures and training strategies evolve, they stand poised to become foundational building blocks in next-generation computer vision systems."
  },
  {
    "objectID": "m06-llms/appendix-t5.html",
    "href": "m06-llms/appendix-t5.html",
    "title": "Appendix: Text-to-Text Transfer Transformer (T5)",
    "section": "",
    "text": "T5 (Text-to-Text Transfer Transformer) is a transformer-based model introduced by Google in 2020. It represents a milestone in NLP by providing a unified approach to handle diverse tasks like translation, summarization, classification, and question answering. T5 embodies the best practices in transformer architecture design and showcases the state of NLP technology at the time of its release. Understanding T5 provides a good starting point into effective transformer model design for NLP applications. This note covers only the essennce, and interested readers are encouraged to read the original paper {footcite:p}raffel2020exploring."
  },
  {
    "objectID": "m06-llms/appendix-t5.html#what-is-t5",
    "href": "m06-llms/appendix-t5.html#what-is-t5",
    "title": "Appendix: Text-to-Text Transfer Transformer (T5)",
    "section": "What is T5?",
    "text": "What is T5?\nA core idea of T5 is that most NLP tasks can be formulated as converting input text into output text. For example, translation becomes ‚Äútranslate English to German: [text]‚Äù, summarization becomes ‚Äúsummarize: [text]‚Äù, and classification becomes ‚Äúclassify: [text]‚Äù.\n```sdnhnpxj https://production-media.paperswithcode.com/methods/new_text_to_text.jpg :alt: T5‚Äôs text-to-text format :width: 500px :align: center\nMany NLP tasks such as translation, summarization, classification, and question answering can be formulated as converting input text into output text.\n\nThe rise of transformer models has led to diverse approaches in NLP. Different models such as BERT and GPT were developed with specialized architectures and pre-training objectives. For example, BERT uses bidirectional attention for language understanding tasks, while GPT uses unidirectional attention for text generation.\nHowever, these models become so diverse that it became challenging to determine which architectural choices and training methods were most effective. T5 addresses this by providing a unified framework that enables direct comparison between different transformer model designs and helps identify the key factors driving their success.\n\n## Comparison of practices\n\nThe original paper of T5 {footcite:p}`raffel2020exploring` is like a review paper that summarizes the effective practices for transformer models. The authors compared various practices and used the most effective ones to form T5. This note covers only the overview of the practices. Interested readers are encouraged to read the original paper.\n\n### Model Architecture\n\nThree architectures have been widely used for language models:\n\n```{figure} https://miro.medium.com/v2/resize:fit:1400/1*VQxkvg_T0f55crgKEZY8eg.png\n:alt: T5's text-to-text format\n:width: 500px\n:align: center\n\nThree main architectures for language models.\n\nEncoder-Decoder\nThe encoder-decoder architecture largely follows the design proposed in ‚ÄúAttention is All You Need‚Äù. The encoder processes the input sequence using self-attention to create contextual representations, while the decoder generates the output sequence using both self-attention and cross-attention to the encoder‚Äôs representations.\nT5 handles positional information differently from the original Transformer. While the original Transformer added absolute position encodings to input embeddings (marking each token's exact position), T5 uses relative position embeddings. These embeddings represent the relative distance between tokens in the self-attention mechanism, rather than their absolute positions. The relative position information is incorporated as a bias term when computing attention weights, and while each attention head uses different embeddings, they are shared across all layers of the model.\n\n\nLanguage Model\nIn a Language Model, only the Decoder of the Encoder-Decoder architecture is used. It generates output recursively by sampling words from the output of step i and using them as input for step i+1. Models such as GPT fall into this type.\n\n\nPrefix LM\nWhen using a Language Model in a ‚ÄúText-to-Text‚Äù context, one drawback is that it can only predict the next token based on the sequence of tokens from the beginning to the current position, which means it cannot learn bidirectional dependencies such as those learned by BERT. A Prefix LM addresses this by cleverly designing the attention masking: it allows bidirectional visibility for the input text portion (=Prefix) and unidirectional visibility for the output text portion. For example, in the case of English-French translation:\n\n\\begin{align*}\n\\text{Input portion:} & \\text{ \"Translate English to French. English: The cat sat on the mat. French:\"} \\\\\n\\text{Output portion:} & \\text{ \"Le chat √©tait assis sur le tapis\"}\n\\end{align*}\n\nThe model can see all tokens in the input portion bidirectionally, but can only see previous tokens in the output portion, ensuring proper translation generation.\nThe prefix-LM is implemented by attention masking, where the input tokens can attent to all tokens in the input portion bidirectionally, but the output tokens can only attend to previous tokens in the output portion (the right most part of the figure below).\n```sdnhnpxj https://img-blog.csdnimg.cn/direct/4ff1176d68e84518940e79b05803c5db.png :alt: T5‚Äôs text-to-text format :width: 80% :align: center\nAttention masking for Prefix LM vs Causal LM. Image from Brief Review ‚Äî Unified Language Model Pre-training for Natural Language Understanding and Generation | by Sik-Ho Tsang | Medium\n\n\n### Pre-training Objectives\n\nThree methods were considered for pre-training objectives: *Prefix language modeling*, *Masked language modeling*, and *Deshuffling*. For *Masked language modeling*, several variations were further explored. Table 3 in the paper clearly illustrates how each objective function processes the text.\n\n```{figure} https://stanford-cs324.github.io/winter2022/lectures/images/t5-unsupervised-table.png\n:alt: T5's text-to-text format\n:width: 100%\n:align: center\n\nTable 3 from the original paper.\n\n\nPrefix language modeling: This is essentially a standard language model where the beginning of the text is given, and the model predicts what follows.\nBERT-Style: This is BERT‚Äôs pre-training method. It masks 15% of tokens, replacing 90% of these with \"&lt;M&gt;\" and the remaining 10% with random tokens (shown as grey ‚Äúapple‚Äù in the figure), then tries to recover the original text.\nDeshuffling: This involves rearranging the token order and having the model restore the original text.\n\nAmong these three, ‚ÄúBERT-Style‚Äù proved most effective. The following variations build upon ‚ÄúBERT-Style,‚Äù aiming to speed up and lighten pre-training:\n\ni.i.d noise, mask tokens: This removes the random token replacement (grey ‚Äúapple‚Äù) from BERT-Style.\ni.i.d noise, replace spans: This replaces consecutive masked tokens (masked spans) with single special tokens (\"&lt;X&gt;\" or \"&lt;Y&gt;\"), then predicts what these special tokens represent.\ni.i.d noise, drop tokens: This simply removes the masked portions and predicts what was deleted.\nRandom spans: Since word-level masking rarely creates consecutive masked sections, this approach specifies both the percentage of tokens to mask and the number of masked spans. For example, with 500 tokens, 15% masking rate, and 25 masked spans, the average span length would be 3 (500 \\times 0.15 / 25 = 3).\n\nExperimental results showed that Random spans with a 15% masking rate and average span length of 3 performed best.\n\n\nPre-training Datasets\nGoogle created a massive dataset called the Colossal Clean Crawled Corpus (C4). While Common Crawl 12 exists as a petabyte-scale corpus collected by crawling web servers worldwide, with 20TB of data being released monthly (!), Common Crawl still contains non-natural language content, error messages, menus, duplicate text, and source code, even though markup has been removed. C4 was created by applying various cleaning processes to one month of Common Crawl data. The data size is 745GB, which is 46 times larger than the English Wikipedia.\nTable 8 in the paper shows comparison results across six datasets including C4.\n```sdnhnpxj https://miro.medium.com/v2/resize:fit:1400/0*qezeuqI77yCJjfUb.png :alt: Pre-training datasets :width: 100% :align: center\nTable 8 from the original paper.\n\nThe compared datasets are as follows (simplified for brevity):\n- **C4**: A dataset created by applying various cleaning processes to Common Crawl.\n- **C4, unfiltered**: C4 with all filtering processes except \"English\" removed.\n- **RealNews-like**: C4 with additional processing to extract only news article content.\n- **WebText-like**: Created by applying C4-like cleaning processes to 12 months of Common Crawl and extracting only content that received 3 or more upvotes on Reddit.\n- **Wikipedia**: English Wikipedia data from Tensorflow Datasets.\n- **Wikipedia+TBC**: Since Wikipedia's content domain is limited to encyclopedic content, this combines it with Toronto Books Corpus (TBC) data from various ebooks.\n\nWhile C4 might not seem impressive at first glance, the paper points out:\n- Looking at \"C4, unfiltered\" results shows that *data quality significantly impacts results*.\n- Results from \"Wikipedia+TBC\", \"RealNews-like\", and \"Wikipedia\" indicate that *pre-training on datasets matching the downstream task domain improves accuracy*\n\n\n```{note}\nAlthough there's a \"Size\" column in the table, note that this comparison standardizes pre-training learning tokens to $2^{35}$. This means pre-training does not complete one full pass through their datasets, while \"Wikipedia\" goes through multiple passes (since it is smaller than $2^{35}$ tokens). When taking multiple passes, the accuracy tends to decrease compared to that of one pass with the same amount of data, as illustrated in the figure below.\n```sdnhnpxj https://www.ogis-ri.co.jp/otc/hiroba/technical/similar-document-search/img/pic202002-008.png :alt: Pre-training datasets :width: 100% :align: center\nResults from training with 2^{35} tokens on different-sized datasets. Since the number of training tokens is fixed, as the data volume decreases, the number of passes during training increases. While accuracy decreases as the number of pre-training passes increases, there isn‚Äôt much significant decline until around 2^{29} (approximately 540 million) tokens.\n\n### Training Details\n\n#### Fine-tuning\n\nSeveral fine-tuning methods were compared, with \"All parameters\" proving to be the best:\n* **All parameters**: Updates all parameters during fine-tuning.\n* **Adapter layers**: Inserts adapter layers (dense-ReLU-dense blocks) at the end of each Transformer block. During fine-tuning, only updates the adapter layer and layer normalization parameters. Multiple dense layer dimensions were compared.\n* **Gradual unfreezing**: Initially only updates parameters of the final stack layer during fine-tuning, gradually expanding parameter updates toward the front layers as training progresses, eventually updating all parameters.\n\n#### Multi-task Learning\n\nMulti-task learning trains multiple tasks simultaneously to enable a single model to solve multiple tasks. Since all tasks in T5 use the \"Text-to-Text\" format, it becomes a question of how to mix learning data from multiple tasks. The paper compares three strategies, though none performed as well as fine-tuning:\n* **Examples-proportional mixing**: Samples training data with probability proportional to each task's dataset size. Sets a limit to control the influence of tasks with extremely large data (i.e., pre-training tasks). Multiple limit parameters were compared.\n* **Temperature-scaled mixing**: Mixes tasks by normalizing each task's sample count raised to 1/T power. Equivalent to \"Examples-proportional mixing\" when T=1, approaches \"Equal mixing\" as T increases. Multiple T values were compared.\n* **Equal mixing**: Samples training data from each task with equal probability.\n\nThey also tried fine-tuning each task after multi-task pre-training, but this too fell short of pre-training + fine-tuning performance.\n\n#### Model Size\n\n```{figure} https://miro.medium.com/v2/resize:fit:1400/0*KhbKImG2TLomLHgW.png\n:alt: Model size\n:width: 100%\n:align: center\n\nTable 13 from the original paper.\nThe paper explores different model configurations that each use approximately 4 times more computational resources than the baseline model. These variations include training the baseline model for 4 times as many steps, using 4 times larger batch sizes, doubling both model size and training steps, quadrupling the model size while keeping training steps constant, and creating ensembles of multiple models.\nFor the larger models (2x and 4x size), the researchers used configurations similar to BERT-LARGE, with 16 and 32 transformer layers respectively. When increasing training steps, the model was exposed to more diverse data since the baseline training (using 2¬≥‚Åµ tokens) only covered a portion of the C4 dataset.\nThe results showed that all configurations improved upon the baseline, with increasing model size being particularly effective. Interestingly, quadrupling the model size while keeping the same amount of training data still led to better performance, contrary to the expectation that larger models might need more training data.\n\n\nSummary of validation experiments\nBased on these investigations, the paper proceeds to a systematic experiments using a model with 11 billion parameters trained on C4. You can see that T5 isn‚Äôt so much about inventing new model architectures or methods, but rather combining Transformer technology with the latest trends, objective functions, and learning optimizations.\n```sdnhnpxj https://mohitmayank.com/a_lazy_data_science_guide/imgs/t5_unsupervised_exploration.png :alt: Design choices :width: 100% :align: center\nDesign choices for T5. ```"
  },
  {
    "objectID": "m06-llms/gpt.html",
    "href": "m06-llms/gpt.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "The Generative Pre-trained Transformer (GPT) {footcite}radford2018language represents a significant evolution in transformer-based language models, focusing on powerful text generation capabilities through a decoder-only architecture. While BERT uses bidirectional attention to understand context, GPT employs unidirectional (causal) attention to generate coherent text by predicting one token at a time.\n```vteayechagzz GPT in interactive notebook: :class: tip\nHere is a demo notebook for GPT\nTo run the notebook, download the notebook as a .py file and run it with:\n\nmarimo edit ‚Äìsandbox gpt-interactive.py\n\nYou will need to install marimo and uv to run the notebook. But other packages will be installed automatically in uv‚Äôs virtual environment.\n\n## Architecture\n\nLike in BERT, GPT also uses a transformer architecture. The main difference is that BERT uses an encoder transformer, while GPT uses a decoder transformer with some modifications.\n\n```{figure} https://heidloff.net/assets/img/2023/02/transformers.png\n:name: gpt-architecture\n:alt: GPT architecture\n:align: center\n:width: 80%\n\nGPT architecture.\n\nThe GPT model family has evolved through several iterations, starting with GPT-1 in 2018 which introduced the basic architecture with 117M parameters and transfer learning capabilities. GPT-2 followed in 2019 with 1.5B parameters and zero-shot abilities, while GPT-3 in 2020 dramatically scaled up to 175B parameters, enabling few-shot learning. The latest GPT-4 (2023) features multimodal capabilities, improved reasoning, and a 32K token context window. Throughout these iterations, the core decoder-only transformer architecture remained unchanged, with improvements coming primarily from increased scale that enabled emergent capabilities.\n\n```{figure} https://miro.medium.com/v2/resize:fit:1400/1*Wnn0e8B-_IiTvmpv-1P7Iw.png\n:name: gpt-evolution\n:alt: GPT evolution\n:align: center\n:width: 80%\n\n\n\n```xnjgbrno https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7536a59a-5326-4a8b-ab12-cebe49acde31_1438x936.png :name: gpt-causal-attention :alt: GPT causal attention :align: center :width: 80%\nCausal attention in GPT.\n\nLike BERT, GPT uses learned token embeddings to convert input tokens into continuous vector representations. The model also employs learned positional embeddings that are added to the token embeddings to encode position information. A key difference from BERT is that GPT uses a *causal attention mechanism*, which means each position can only attend to previous positions in the sequence, enabling the model to generate text in a left-to-right fashion by predicting one token at a time.\n\n\n\n### Causal Language Modeling\n\nCausal (autoregressive) language modeling is the pre-training objective of GPT, where the model learns to predict the next token given all previous tokens in the sequence. More formally, given a sequence of tokens $(x_1, x_2, ..., x_n)$, the model is trained to maximize the likelihood:\n\n$$\nP(x_1, ..., x_n) = \\prod_{i=1}^n P(x_i|x_1, ..., x_{i-1})\n$$\n\nFor example, given the partial sentence \"The cat sat on\", the model learns to predict the next word by calculating probability distributions over its entire vocabulary. During training, it might learn that \"mat\" has a high probability in this context, while \"laptop\" has a lower probability.\n\n```{note}\nWhile BERT uses bidirectional attention and sees the entire sequence at once (making it powerful for understanding), GPT's unidirectional approach more naturally models how humans write text, i.e., one word at a time, with each word influenced by all previous words.\nThe bidirectional nature of BERT is more powerful for understanding, but it is less suitable for text generation.\nThe autoregressive nature of GPT means it's particularly sensitive to the initial tokens (prompt) it receives. Well-crafted prompts that establish clear patterns or constraints can significantly improve generation quality.\nThe next-token prediction objective has remained unchanged across all GPT versions due to its remarkable effectiveness. Rather than modifying this core approach, improvements have come from increasing model size and refining the architecture. This simple yet powerful training method has become fundamental to modern language models.\n```vteayechagzz Scaling Laws :class: tip :name: scaling-laws\nLanguage model performance improves predictably as models get larger, following simple mathematical relationships (power laws). The larger the model, the better it performs - and this improvement is reliable and measurable. This predictability was crucial for the development of models like GPT-3 and Claude, as it gave researchers confidence that investing in larger models would yield better results. Importantly, larger models are more efficient learners - they need proportionally less training data and fewer training steps to achieve good performance.\nThese findings revolutionized AI development by showing that better AI systems could be reliably built simply by scaling up model size, compute, and data in the right proportions. This insight led directly to the development of increasingly powerful models, as researchers could confidently invest in building larger and larger systems knowing they would see improved performance.\nSee the paper Scaling Laws for Neural Language Models for more details.\nxnjgbrno https://miro.medium.com/v2/resize:fit:1400/1*5fsJPwvFjS7fo8g8NwsxNA.png :name: scaling-laws-figure :alt: Scaling laws figure :align: center :width: 80%\n\n\n\nGPT does not generate text in one go. Instead, it predicts the next token repeatedly to generate text. GPT does not pick a specific token but provides a probability distribution over the next token. It is our job to sample a token from the distribution. There are several strategies to sample a token from the distribution as we will see below.\n```xnjgbrno https://media.licdn.com/dms/image/v2/D4E22AQFZFRSwwzCSqQ/feedshare-shrink_2048_1536/feedshare-shrink_2048_1536/0/1725003016027?e=2147483647&v=beta&t=oBH1s4V8N0wKCOJQakA_wrwgFrixs56S0s_QafZOvbA :name: gpt-inference :alt: GPT inference :align: center :width: 50%\nGPT predicts the next token repeatedly to generate text.\n\n### Greedy and Beam Search\n\nWhen generating text, language models assign probabilities to possible next tokens.\nSampling a token from the distribution is not as easy as it might seem. This is because the distribution is high-dimensional. Namely, we need to sample a single token from millions of possible tokens, and thus, sampling a token can be computationally very expensive.\n\n**Greedy sampling** always picks the highest probability token, which is deterministic but can lead to repetitive or trapped text. For example, if the model predicts \"the\" with high probability, it will always predict \"the\" again.\n\n```{figure} https://huggingface.co/blog/assets/02_how-to-generate/greedy_search.png\n:name: gpt-greedy-search\n:alt: GPT greedy search\n:align: center\n:width: 50%\n\nGPT greedy search.\nBeam search alleviates this problem by taking into account the high-order dependencies between tokens. For example, in generating ‚ÄúThe cat ran across the ___‚Äú, beam search might preserve a path containing‚Äùmat‚Äù even if ‚Äúfloor‚Äù or ‚Äúroom‚Äù have higher individual probabilities at that position. This is because the complete sequence like ‚Äúmat quickly‚Äù could be more probable when considering the token next after ‚Äúmat‚Äù. ‚ÄúThe cat ran across the mat quickly‚Äù is a more natural phrase than ‚ÄúThe cat ran across the floor quickly‚Äù when considering the full flow and common linguistic patterns.\n```xnjgbrno https://huggingface.co/blog/assets/02_how-to-generate/beam_search.png :name: gpt-beam-search :alt: GPT beam search :align: center :width: 50%\nGPT beam search. ```\nBeam search maintains multiple possible sequences (beams) in parallel, exploring different paths simultaneously. At each step, it expands all current beams, scores the resulting sequences, and keeps only the top-k highest scoring ones. For instance, with a beam width of 3: - First beams might be: [‚ÄúThe cat ran‚Äù, ‚ÄúThe cat walked‚Äù, ‚ÄúThe cat jumped‚Äù] - Next step: [‚ÄúThe cat ran across‚Äù, ‚ÄúThe cat ran through‚Äù, ‚ÄúThe cat walked across‚Äù] - And so on, keeping the 3 most promising complete sequences at each step\nThis process continues until reaching the end, finally selecting the sequence with highest overall probability. The beam search can be combined with top-k sampling or nucleus sampling. For example, one can sample a token based on the top-k sampling or nucleus sampling to form the next beam.\nWhile beam search often produces high-quality outputs since it considers longer-term coherence, it can still suffer from the problem of repetitive or trapped text.\n\n\nBoth greedy and beam search are deterministic. They pick the most likely token at each step. However, this creates a loop where the model always predicts the same tokens repeatedly. A simple way to alleviate this problem is to sample a token from the distribution.\nTop-k Sampling relaxes the deterministic nature of greedy sampling by selecting randomly from the k most likely next tokens at each generation step. While this introduces some diversity compared to greedy sampling, choosing a fixed k can be problematic. Value of k might be too large for some distribution tails (including many poor options) or too small for others (excluding reasonable options).\nNucleus Sampling~{footcite}holtzman2019curious addresses this limitation by dynamically selecting tokens based on cumulative probability. It samples from the smallest set of tokens whose cumulative probability exceeds a threshold p (e.g.¬†0.9). This adapts naturally to different probability distributions, i.e., selecting few tokens when the distribution is concentrated and more when it‚Äôs spread out. This approach often provides a good balance between quality and diversity.\n```xnjgbrno https://storage.googleapis.com/zenn-user-upload/8p2r9urhtn5nztdg6mnia3toibhl :name: gpt-top-k-top-p :alt: GPT top-k top-p :align: center :width: 80%\nNucleus sampling. The image is taken from this blog.\n\n**Temperature Control**\nTemperature ($\\tau$) modifies how \"concentrated\" the probability distribution is for sampling by scaling the logits before applying softmax:\n\n$$\np_i = \\frac{\\exp(z_i/\\tau)}{\\sum_j \\exp(z_j/\\tau)}\n$$\n\nwhere $z_i$ are the logits and $\\tau$ is the temperature parameter. Lower temperatures ($\\tau &lt; 1.0$) make the distribution more peaked, making high probability tokens even more likely to be chosen, leading to more focused and conservative outputs. Higher temperatures ($\\tau &gt; 1.0$) flatten the distribution by making the logits more similar, increasing the chances of selecting lower probability tokens and producing more diverse but potentially less coherent text. As $\\tau \\to 0$, the distribution approaches a one-hot vector (equivalent to greedy search), while as $\\tau \\to \\infty$, it approaches a uniform distribution.\n\n\n```{figure} https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6639e35ce91c16b3b9564b2f_mxaIPcROZcBFYta1I0nzWjlGTgs-LxzUOE3p6Kbvf9qPpZzBh5AAZG7ciRtgVquhLTtrM8ToJdNd-ubXvuz8tRfrqBwSozWHCj457pm378buxz2-XrMfWzfSv3b793QP61kLxRKT299WP1gbas_E118.png\n:name: gpt-temperature\n:alt: GPT temperature\n:align: center\n:width: 80%\n\nTemperature controls the concentration of the probability distribution. Lower temperature makes the distribution more peaked, while higher temperature makes the distribution more flat."
  },
  {
    "objectID": "m06-llms/gpt.html#inference-strategies",
    "href": "m06-llms/gpt.html#inference-strategies",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "GPT does not generate text in one go. Instead, it predicts the next token repeatedly to generate text. GPT does not pick a specific token but provides a probability distribution over the next token. It is our job to sample a token from the distribution. There are several strategies to sample a token from the distribution as we will see below.\n```xnjgbrno https://media.licdn.com/dms/image/v2/D4E22AQFZFRSwwzCSqQ/feedshare-shrink_2048_1536/feedshare-shrink_2048_1536/0/1725003016027?e=2147483647&v=beta&t=oBH1s4V8N0wKCOJQakA_wrwgFrixs56S0s_QafZOvbA :name: gpt-inference :alt: GPT inference :align: center :width: 50%\nGPT predicts the next token repeatedly to generate text.\n\n### Greedy and Beam Search\n\nWhen generating text, language models assign probabilities to possible next tokens.\nSampling a token from the distribution is not as easy as it might seem. This is because the distribution is high-dimensional. Namely, we need to sample a single token from millions of possible tokens, and thus, sampling a token can be computationally very expensive.\n\n**Greedy sampling** always picks the highest probability token, which is deterministic but can lead to repetitive or trapped text. For example, if the model predicts \"the\" with high probability, it will always predict \"the\" again.\n\n```{figure} https://huggingface.co/blog/assets/02_how-to-generate/greedy_search.png\n:name: gpt-greedy-search\n:alt: GPT greedy search\n:align: center\n:width: 50%\n\nGPT greedy search.\nBeam search alleviates this problem by taking into account the high-order dependencies between tokens. For example, in generating ‚ÄúThe cat ran across the ___‚Äú, beam search might preserve a path containing‚Äùmat‚Äù even if ‚Äúfloor‚Äù or ‚Äúroom‚Äù have higher individual probabilities at that position. This is because the complete sequence like ‚Äúmat quickly‚Äù could be more probable when considering the token next after ‚Äúmat‚Äù. ‚ÄúThe cat ran across the mat quickly‚Äù is a more natural phrase than ‚ÄúThe cat ran across the floor quickly‚Äù when considering the full flow and common linguistic patterns.\n```xnjgbrno https://huggingface.co/blog/assets/02_how-to-generate/beam_search.png :name: gpt-beam-search :alt: GPT beam search :align: center :width: 50%\nGPT beam search. ```\nBeam search maintains multiple possible sequences (beams) in parallel, exploring different paths simultaneously. At each step, it expands all current beams, scores the resulting sequences, and keeps only the top-k highest scoring ones. For instance, with a beam width of 3: - First beams might be: [‚ÄúThe cat ran‚Äù, ‚ÄúThe cat walked‚Äù, ‚ÄúThe cat jumped‚Äù] - Next step: [‚ÄúThe cat ran across‚Äù, ‚ÄúThe cat ran through‚Äù, ‚ÄúThe cat walked across‚Äù] - And so on, keeping the 3 most promising complete sequences at each step\nThis process continues until reaching the end, finally selecting the sequence with highest overall probability. The beam search can be combined with top-k sampling or nucleus sampling. For example, one can sample a token based on the top-k sampling or nucleus sampling to form the next beam.\nWhile beam search often produces high-quality outputs since it considers longer-term coherence, it can still suffer from the problem of repetitive or trapped text.\n\n\nBoth greedy and beam search are deterministic. They pick the most likely token at each step. However, this creates a loop where the model always predicts the same tokens repeatedly. A simple way to alleviate this problem is to sample a token from the distribution.\nTop-k Sampling relaxes the deterministic nature of greedy sampling by selecting randomly from the k most likely next tokens at each generation step. While this introduces some diversity compared to greedy sampling, choosing a fixed k can be problematic. Value of k might be too large for some distribution tails (including many poor options) or too small for others (excluding reasonable options).\nNucleus Sampling~{footcite}holtzman2019curious addresses this limitation by dynamically selecting tokens based on cumulative probability. It samples from the smallest set of tokens whose cumulative probability exceeds a threshold p (e.g.¬†0.9). This adapts naturally to different probability distributions, i.e., selecting few tokens when the distribution is concentrated and more when it‚Äôs spread out. This approach often provides a good balance between quality and diversity.\n```xnjgbrno https://storage.googleapis.com/zenn-user-upload/8p2r9urhtn5nztdg6mnia3toibhl :name: gpt-top-k-top-p :alt: GPT top-k top-p :align: center :width: 80%\nNucleus sampling. The image is taken from this blog.\n\n**Temperature Control**\nTemperature ($\\tau$) modifies how \"concentrated\" the probability distribution is for sampling by scaling the logits before applying softmax:\n\n$$\np_i = \\frac{\\exp(z_i/\\tau)}{\\sum_j \\exp(z_j/\\tau)}\n$$\n\nwhere $z_i$ are the logits and $\\tau$ is the temperature parameter. Lower temperatures ($\\tau &lt; 1.0$) make the distribution more peaked, making high probability tokens even more likely to be chosen, leading to more focused and conservative outputs. Higher temperatures ($\\tau &gt; 1.0$) flatten the distribution by making the logits more similar, increasing the chances of selecting lower probability tokens and producing more diverse but potentially less coherent text. As $\\tau \\to 0$, the distribution approaches a one-hot vector (equivalent to greedy search), while as $\\tau \\to \\infty$, it approaches a uniform distribution.\n\n\n```{figure} https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6639e35ce91c16b3b9564b2f_mxaIPcROZcBFYta1I0nzWjlGTgs-LxzUOE3p6Kbvf9qPpZzBh5AAZG7ciRtgVquhLTtrM8ToJdNd-ubXvuz8tRfrqBwSozWHCj457pm378buxz2-XrMfWzfSv3b793QP61kLxRKT299WP1gbas_E118.png\n:name: gpt-temperature\n:alt: GPT temperature\n:align: center\n:width: 80%\n\nTemperature controls the concentration of the probability distribution. Lower temperature makes the distribution more peaked, while higher temperature makes the distribution more flat."
  },
  {
    "objectID": "course/deliverables.html",
    "href": "course/deliverables.html",
    "title": "Deliverables",
    "section": "",
    "text": "Project reports are not solely focused on the final results, but also on the process and decisions made along the way. We expect to hear the reasons for your final decisions, for instance the reason why you choose X, over alternative options like Y.\n\nClarify the objectives and goal of your project. What do you want to do it, and why are your questions important to us?\nProvide a detailed description about the data you will use. Where the data are collected from, how they are compiled and preprocessed for your analysis. What are the data type of your focal features, and what features do you think are relevant for your analysis?\nDetermine the appropriate methods. Additionally, consider discussing the methods used in previous studies. Considering the data types and the information you aim to present, what methods could potentially be suitable? It would also be beneficial to explore what approaches others have taken when working with similar datasets.\nClarify the limitation and advantage of your approach. The limitation and advantage stems from data and methodologies, and must be discussed in light of existing works. For instance, you want to develop a link prediction algorithm for a social network based on the common neighbor approach. What are the fundamental assumption underlying the link prediction algorithms? When does the algorithm fail? Can you think of the advantage of your algorithm over other alternatives such as graph neural networks?\nEmbrace failures. As Thomas Edison famously said, ‚ÄúI have not failed. I‚Äôve just found 10,000 ways that won‚Äôt work.‚Äù In many cases, works and analyses may appear to follow a single pathway, but it is important to recognize that this is just one of many paths that people have taken, many of which have turned out to be unsuccessful. It is crucial to try out multiple candidates, and more importantly, to document your failures and understand why they did not work. Consider using fake data, small subsets, mock-ups, and sketches. These methods can help you iterate and refine your approach, ultimately leading to more successful outcomes.",
    "crumbs": [
      "Home",
      "Course Information",
      "Deliverables"
    ]
  },
  {
    "objectID": "course/deliverables.html#general-remarks-on-the-project-reports",
    "href": "course/deliverables.html#general-remarks-on-the-project-reports",
    "title": "Deliverables",
    "section": "",
    "text": "Project reports are not solely focused on the final results, but also on the process and decisions made along the way. We expect to hear the reasons for your final decisions, for instance the reason why you choose X, over alternative options like Y.\n\nClarify the objectives and goal of your project. What do you want to do it, and why are your questions important to us?\nProvide a detailed description about the data you will use. Where the data are collected from, how they are compiled and preprocessed for your analysis. What are the data type of your focal features, and what features do you think are relevant for your analysis?\nDetermine the appropriate methods. Additionally, consider discussing the methods used in previous studies. Considering the data types and the information you aim to present, what methods could potentially be suitable? It would also be beneficial to explore what approaches others have taken when working with similar datasets.\nClarify the limitation and advantage of your approach. The limitation and advantage stems from data and methodologies, and must be discussed in light of existing works. For instance, you want to develop a link prediction algorithm for a social network based on the common neighbor approach. What are the fundamental assumption underlying the link prediction algorithms? When does the algorithm fail? Can you think of the advantage of your algorithm over other alternatives such as graph neural networks?\nEmbrace failures. As Thomas Edison famously said, ‚ÄúI have not failed. I‚Äôve just found 10,000 ways that won‚Äôt work.‚Äù In many cases, works and analyses may appear to follow a single pathway, but it is important to recognize that this is just one of many paths that people have taken, many of which have turned out to be unsuccessful. It is crucial to try out multiple candidates, and more importantly, to document your failures and understand why they did not work. Consider using fake data, small subsets, mock-ups, and sketches. These methods can help you iterate and refine your approach, ultimately leading to more successful outcomes.",
    "crumbs": [
      "Home",
      "Course Information",
      "Deliverables"
    ]
  },
  {
    "objectID": "course/deliverables.html#proposal",
    "href": "course/deliverables.html#proposal",
    "title": "Deliverables",
    "section": "Proposal",
    "text": "Proposal\nA document should include the following sections:\n\nProject Title\nTeam Members (1-4 people; keep in mind that a larger team is expected to accomplish more than a smaller one)\nAbstract: A concise summary of your project.\nIntroduction: Provide motivation, background, and objectives for your project. Explain why it is important or interesting and why others should care. Review and discuss relevant existing works, particularly those that have inspired your project. Critique these works substantively. Remember, there is always a wealth of relevant work available.\nQuestions or Objectives: Specify the methods you plan to create and what you hope to discover from the data.\nDatasets and Methods: Identify the dataset you will be using. If you haven‚Äôt done so already, I strongly encourage you to reconsider your project. Obtaining and cleaning datasets can be time-consuming. Describe the dataset, including its structure and data types if it is tabular. Explain the methods you plan to apply and why you have chosen them. Finally, provide detailed information about the dataset to convincingly argue that it is suitable for your project and proposed methods.\n\n\nReferences",
    "crumbs": [
      "Home",
      "Course Information",
      "Deliverables"
    ]
  },
  {
    "objectID": "course/deliverables.html#final-presentation",
    "href": "course/deliverables.html#final-presentation",
    "title": "Deliverables",
    "section": "Final presentation",
    "text": "Final presentation\nPlease create a 10-minute video (please adhere to the time limit) and upload it to YouTube. You have the option to either publish it or make it unlisted. The video can be in any format you prefer. Make sure to include a thorough analysis while also making it interesting and enjoyable! The video will be evaluated based on three criteria: (i) the strength of the case you present, (ii) the quality of your analysis, and (iii) the production and delivery of your presentation.\nOnce you have completed your video, feel free to share it on Slack and receive feedback from your fellow students and instructors. It‚Äôs always beneficial to see what others have accomplished, so I highly encourage you to share your work!",
    "crumbs": [
      "Home",
      "Course Information",
      "Deliverables"
    ]
  },
  {
    "objectID": "course/deliverables.html#final-report",
    "href": "course/deliverables.html#final-report",
    "title": "Deliverables",
    "section": "Final report",
    "text": "Final report\nYou will need to submit your code and a report on your work. Ideally, your code will be in well-documented Jupyter notebooks (e.g.¬†see Peter Norvig‚Äôs notebooks or good Kaggle exploratory data visualization kernels).\nThe report has no minimum or maximum length, but you need to make sure all the topics are thoroughly addressed in clear writing. The format and ingredients for the final report will depend on the types of projects that you do.\nIf the project is more about creating a software package or a website, then the report may focus more on the technical aspects of the project.",
    "crumbs": [
      "Home",
      "Course Information",
      "Deliverables"
    ]
  },
  {
    "objectID": "course/deliverables.html#idea-sketch-template",
    "href": "course/deliverables.html#idea-sketch-template",
    "title": "Deliverables",
    "section": "Idea sketch template",
    "text": "Idea sketch template\nThe followings are the list of questions I personally use before starting a project. Every idea is nebulous when it comes to a mind. We can materialize it by writing down the ideas. It‚Äôs surprisingly hard to write it down first, and you will realize a lot of things. In sum, writing is thinking. It serves as a scaffolding to think through a research project. These list of questions are a living document, and you will constantly update as the project progresses.\nAnswer each question in 2~3 sentences. I usually set a timer for 15 mins for each. If one of the questions takes more than 15 mins, it‚Äôs the weakness of the idea of the current form.\n\nProject Overview: What is the core focus of your project? Are you developing something new or testing existing ideas?\nProject Value: What makes this work meaningful and worth pursuing?\nResearch Gaps: What key questions or problems remain unsolved in this area?\nNovel Approach: What makes your proposed solution unique and different from existing methods?\nNecessity: Why develop a new solution if existing methods exist? What advantages does your approach offer?\nSuccess Metrics: How will you define and measure success for this project?\nValidation Strategy: What specific criteria or tests will demonstrate that your solution works?\nBroader Impact: How could this work benefit fields beyond your immediate research area?\nImplementation Plan: Break down each project goal into ~3 concrete, actionable tasks.",
    "crumbs": [
      "Home",
      "Course Information",
      "Deliverables"
    ]
  },
  {
    "objectID": "course/how-to-submit-assignment.html",
    "href": "course/how-to-submit-assignment.html",
    "title": "How to submit assignment",
    "section": "",
    "text": "In this course, we will use GitHub Classroom to submit & grade assignments. Please follow the instructions below to submit your assignment.",
    "crumbs": [
      "Home",
      "Course Information",
      "How to submit assignment"
    ]
  },
  {
    "objectID": "course/how-to-submit-assignment.html#option-1-a-simple-workflow-full-local",
    "href": "course/how-to-submit-assignment.html#option-1-a-simple-workflow-full-local",
    "title": "How to submit assignment",
    "section": "Option 1: A simple workflow (Full local)",
    "text": "Option 1: A simple workflow (Full local)\nSee the slides for the detailed instructions.\n\nClone the repository from GitHub.\nEdit the assignment.py with marimo editor. Type marimo edit assignment/assignment.py\nSubmit the assignment.py via git. (You can use GitHub Desktop, or command line)\nCheck the grading on the GitHub Classroom.",
    "crumbs": [
      "Home",
      "Course Information",
      "How to submit assignment"
    ]
  },
  {
    "objectID": "course/how-to-submit-assignment.html#option-2-github-codespaces-full-cloud",
    "href": "course/how-to-submit-assignment.html#option-2-github-codespaces-full-cloud",
    "title": "How to submit assignment",
    "section": "Option 2: Github Codespaces (Full cloud)",
    "text": "Option 2: Github Codespaces (Full cloud)\nSee the slides for the detailed instructions.\n\nGo to your assignment repository on GitHub\nClick the green ‚ÄúCode‚Äù button\nClick the ‚ÄúOpen with Codespaces‚Äù button\nWait for the Codespaces to be ready.\nType ‚Äòmarimo edit assignment/assignment.py‚Äô. If you cannot find marimo, type ‚Äúuv run marimo edit assignment/assignment.py‚Äù which should work.\nYou will be redirected to a webpage and prompted to enter the access token. The access token can be found on the terminal window in the Codespaces.\nTake the access token in the url ‚Äúthe alphabets after‚Äù?access_token=‚Äù and enter the token in the webpage.",
    "crumbs": [
      "Home",
      "Course Information",
      "How to submit assignment"
    ]
  },
  {
    "objectID": "course/how-to-submit-assignment.html#option-3-local-but-with-docker-machine",
    "href": "course/how-to-submit-assignment.html#option-3-local-but-with-docker-machine",
    "title": "How to submit assignment",
    "section": "Option 3: Local but with Docker Machine",
    "text": "Option 3: Local but with Docker Machine\nSee the slides for the detailed instructions.\n\nPreparations\n\nInstall Docker Desktop\nInstall GitHub Desktop\nInstall VS Code\n\n\n\nSteps\n\nClone the repository from GitHub.\nOpen with the VS Code, and click ‚ÄúReopen in Container‚Äù\nOpen the assignment.py with marimo editor.\nSubmit the assignment.py to the repository.",
    "crumbs": [
      "Home",
      "Course Information",
      "How to submit assignment"
    ]
  },
  {
    "objectID": "course/why-applied-soft-computing.html",
    "href": "course/why-applied-soft-computing.html",
    "title": "Why applied soft computing?",
    "section": "",
    "text": "Imagine trying to explain to someone how you recognize your friend‚Äôs face. Sure, you do it instantly - but try writing down the exact rules! Should you measure the eye spacing? Check nose shape? It‚Äôs nearly impossible to write rigid rules for something our brains do effortlessly.\nThis is where neural networks come in - they learn and adapt like our brains, without needing exact rules.",
    "crumbs": [
      "Home",
      "Course Information",
      "Why applied soft computing?"
    ]
  },
  {
    "objectID": "course/why-applied-soft-computing.html#mind-blowing-neural-network-achievements",
    "href": "course/why-applied-soft-computing.html#mind-blowing-neural-network-achievements",
    "title": "Why applied soft computing?",
    "section": "Mind-Blowing Neural Network Achievements",
    "text": "Mind-Blowing Neural Network Achievements\n\nThe AI Artist Who Beat Human Artists\n\n\n\nAI-generated artwork that won first place in a digital art competition\n\n\nIn 2022, something unexpected happened in the art world: an AI-generated artwork won first place in a digital art competition, beating out human artists! The creator, Jason Allen, used Midjourney AI to generate the winning piece after 80 hours of careful prompting. While critics claimed it was ‚Äújust pressing buttons,‚Äù the win sparked a huge debate about the future of art.\n\n\nFaces That Don‚Äôt Exist\n\n\n\nA human face generated by StyleGAN\n\n\nVisit ThisPersonDoesNotExist.com and you‚Äôll see something uncanny - incredibly realistic human faces that never existed! Each refresh shows a new face created by StyleGAN, complete with unique features, expressions, and even tiny details like skin pores. The wild part? Even experts sometimes can‚Äôt tell these AI-generated faces from real photos!\n\n\nChatGPT: The AI That Talks Like Us\n\n\n\nChatGPT interface\n\n\nWhen ChatGPT appeared, it shocked everyone with its human-like conversations. It doesn‚Äôt just answer questions - it writes poetry, explains complex topics, helps with coding, and even gets jokes! While earlier chatbots were obviously robotic, ChatGPT‚Äôs natural responses often make people wonder if they‚Äôre really chatting with an AI.\n\n\nThe 50-Year Puzzle Solver\n\n\n\nAlphaFold logo and interface\n\n\nScientists struggled for 50 years to predict how proteins fold - a crucial problem in biology. Then came AlphaFold, which not only solved the problem but did it with near-perfect accuracy! This task was thought to be so complex that it would take decades more to solve. Instead, AlphaFold did in days what used to take months in laboratories.\n\n\nThe Go Master‚Äôs Impossible Move\n\n\n\nAlphaGo versus Lee Sedol\n\n\nThe ancient game of Go was considered too complex for AI - until AlphaGo shocked the world by defeating champion Lee Sedol. But the real surprise came in Game 2, with ‚ÄúMove 37‚Äù - a play so creative and unexpected that Go experts initially thought it was a mistake! This move, later described as ‚ÄúÁ•û„ÅÆ‰∏ÄÊâã‚Äù (the divine move), showed that AI could think in ways humans never imagined.\n\n\nSora: Making Movies from Words\n\n\n\nSora video generation example\n\n\nJust when we thought AI couldn‚Äôt get more impressive, OpenAI‚Äôs Sora arrived in 2024, turning text into realistic 60-second videos. The shocking part? These aren‚Äôt just simple animations - they‚Äôre physics-accurate scenes with multiple moving elements that look like they were filmed in the real world. It‚Äôs like having a movie studio in your pocket!\n\n\nThe Doctor That Sees More\n\n\n\nMedical imaging AI\n\n\nAI systems can now spot cancer in medical scans better than human doctors. Google Health‚Äôs system proved more accurate than radiologists at detecting breast cancer, reducing both missed cases and false alarms. It‚Äôs not replacing doctors, but it‚Äôs giving them a super-powered second opinion.\n\n\nCars That Drive Better Than Us\n\n\n\nTesla self-driving car\n\n\nSelf-driving cars were once science fiction. Now, neural networks help them process information from multiple sensors faster than any human could, making split-second decisions to avoid accidents. In many conditions, they‚Äôre already safer drivers than humans, reacting faster and staying alert 100% of the time.",
    "crumbs": [
      "Home",
      "Course Information",
      "Why applied soft computing?"
    ]
  },
  {
    "objectID": "course/why-applied-soft-computing.html#why-this-matters",
    "href": "course/why-applied-soft-computing.html#why-this-matters",
    "title": "Why applied soft computing?",
    "section": "Why This Matters",
    "text": "Why This Matters\nWhat‚Äôs truly remarkable is that most of these breakthroughs happened in just the last few years - within our lifetime! Tasks that experts thought would take decades to solve are being conquered by neural networks at an incredible pace. They‚Äôre not just matching human abilities - they‚Äôre surpassing them in ways that surprise even the leading researchers. ?",
    "crumbs": [
      "Home",
      "Course Information",
      "Why applied soft computing?"
    ]
  },
  {
    "objectID": "m04-text/archive/example-round.html",
    "href": "m04-text/archive/example-round.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê Round 1: Weather Story Memory Paper: [ Box 1 ][ Box 2 ][ Box 3 ]\nStudent Private Info: 1. [üåßÔ∏è] Image: Dark clouds and rain 2. [üì±] Text: ‚ÄúWeather alert: Storm coming‚Äù 3. [üèÉ] Image: People running for shelter 4. [üöå] Text: ‚ÄúBus service suspended‚Äù 5. [‚õàÔ∏è] Image: Lightning strike\nFinal Question: ‚ÄúWhy did people run?‚Äù\nExpected Memory Evolution: S1: [rain][clouds][dark] S2: [rain][storm][alert] S3: [storm][people][running] S4: [storm][running][suspended] S5: [storm][running][lightning]\nRound 2: Birthday Surprise Memory Paper: [ Box 1 ][ Box 2 ][ Box 3 ]\nStudent Private Info: 1. [üéÅ] Text: ‚ÄúSarah loves chocolate‚Äù 2. [üìÖ] Image: Calendar showing ‚ÄúParty Next Week‚Äù 3. [üè™] Text: ‚ÄúStore out of chocolate cake‚Äù 4. [üßÅ] Image: Recipe for vanilla cupcakes 5. [üòä] Text: ‚ÄúSarah allergic to vanilla‚Äù\nFinal Question: ‚ÄúWhat should we bake for Sarah?‚Äù\nExpected Memory Evolution: S1: [Sarah][loves][chocolate] S2: [Sarah][chocolate][party] S3: [Sarah][chocolate][no-cake] S4: [Sarah][no-cake][cupcakes] S5: [chocolate][no-cake][allergy]\nRound 3: Lost Pet Mystery Memory Paper: [ Box 1 ][ Box 2 ][ Box 3 ]\nStudent Private Info: 1. [üêï] Image: Dog with red collar 2. [üè°] Text: ‚ÄúFence has hole‚Äù 3. [üå≥] Image: Dog treats in park 4. [üëß] Text: ‚ÄúGirl crying at playground‚Äù 5. [üì±] Image: Posted ‚ÄúFound Dog‚Äù sign\nFinal Question: ‚ÄúWhere is the dog likely to be?‚Äù\nExpected Memory Evolution: S1: [dog][red][collar] S2: [dog][escape][hole] S3: [dog][treats][park] S4: [dog][park][crying] S5: [dog][park][found]\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê Round 4: Double Story Track Learning Objective: Understanding parallel memory streams\nMemory Paper: Story A: [ Box 1A ][ Box 2A ][ Box 3A ] Story B: [ Box 1B ][ Box 2B ][ Box 3B ]\nStudent Private Info: 1. [üèÉ‚Äç‚ôÇÔ∏è][üåßÔ∏è] ‚ÄúJohn running in rain‚Äù | ‚ÄúMary reading book‚Äù 2. [üöå][üìö] ‚ÄúBus is late‚Äù | ‚ÄúLibrary closing soon‚Äù 3. [üíº][üèÉ‚Äç‚ôÄÔ∏è] ‚ÄúImportant meeting‚Äù | ‚ÄúMary running to library‚Äù 4. [üò∞][‚ùå] ‚ÄúJohn worried‚Äù | ‚ÄúLibrary closed‚Äù 5. [üì±][üò¢] ‚ÄúMeeting cancelled‚Äù | ‚ÄúMary disappointed‚Äù\nFinal Question: ‚ÄúWho had a worse day and why?‚Äù\nMechanics: - Must update both story tracks - Limited to 3 marker uses total (forces choices) - Can transfer info between tracks\nRound 5: Time-Sensitive Memory Learning Objective: Learning importance weighting\nMemory Paper: [ Box 1 ][ Box 2 ][ Box 3 ] Importance Scale: (1-5) next to each box\nStudent Private Info: 1. [üïê] ‚ÄúTrain leaves at 3PM‚Äù (importance: 5) 2. [üé´] ‚ÄúTicket in blue wallet‚Äù (importance: 4) 3. [üëï] ‚ÄúPacked red shirt‚Äù (importance: 1) 4. [üåßÔ∏è] ‚ÄúHeavy rain forecast‚Äù (importance: 3) 5. [üöï] ‚ÄúTaxi strike today‚Äù (importance: 5)\nFinal Question: ‚ÄúWill they catch the train? What‚Äôs the critical info?‚Äù\nMechanics: - Can only erase lower importance info - Must maintain at least one high-importance (4-5) item - New info must be rated for importance\nRound 6: Context-Dependent Memory Learning Objective: Understanding conditional information processing\nMemory Paper: [ Context ][ Box 1 ][ Box 2 ][ Box 3 ] Context Options: HOME, WORK, TRAVEL\nStudent Private Info: 1. [üè†] ‚ÄúDog needs walk‚Äù | ‚ÄúMeeting at 2‚Äù | ‚ÄúPack umbrella‚Äù 2. [üìû] ‚ÄúMom calling‚Äù | ‚ÄúClient email‚Äù | ‚ÄúFlight delayed‚Äù 3. [üçΩÔ∏è] ‚ÄúEmpty fridge‚Äù | ‚ÄúDeadline today‚Äù | ‚ÄúHotel booked‚Äù 4. [üí°] ‚ÄúPower out‚Äù | ‚ÄúPresentation ready‚Äù | ‚ÄúPassport check‚Äù 5. [üîë] ‚ÄúDoor locked‚Äù | ‚ÄúOffice closed‚Äù | ‚ÄúTaxi arriving‚Äù\nFinal Question: ‚ÄúWhat actions are needed?‚Äù (Asked with specific context)\nMechanics: - Context box must be updated first - Information relevance depends on current context - Some info may be relevant across multiple contexts ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\nüéØ Learning Connections to LSTM: - Round 4: Multiple memory cells - Round 5: Input gate mechanics (importance weighting) - Round 6: Context-dependent forget gate\nüìù Assessment Ideas: - Track which information survives multiple passes - Analyze decision patterns for memory updates - Compare strategies across different groups"
  },
  {
    "objectID": "m04-text/archive/rnn-interactive.html",
    "href": "m04-text/archive/rnn-interactive.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "üß† Learn RNNs Through Physics!\nWe‚Äôll design a simple recurrent neural network (RNN) to model the motion of an object attached to a spring and damper. When displaced and released, the object oscillates with decaying amplitude.\nüë®‚Äçüíª Exercise notebook"
  },
  {
    "objectID": "m04-text/archive/what-to-learn.html",
    "href": "m04-text/archive/what-to-learn.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "In this module, we will learn about recurrent neural networks (RNNs) that can process sequential text data. We will learn: - Recurrent Neural Networks (RNNs) for processing sequential text data - Long-Short Term Memory (LSTM) networks for capturing long-range dependencies - Sequence-to-sequence models for machine translation - The Attention mechanism for focusing on relevant parts of text"
  },
  {
    "objectID": "m04-text/archive/what-to-learn.html#what-to-learn-in-this-module",
    "href": "m04-text/archive/what-to-learn.html#what-to-learn-in-this-module",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "In this module, we will learn about recurrent neural networks (RNNs) that can process sequential text data. We will learn: - Recurrent Neural Networks (RNNs) for processing sequential text data - Long-Short Term Memory (LSTM) networks for capturing long-range dependencies - Sequence-to-sequence models for machine translation - The Attention mechanism for focusing on relevant parts of text"
  },
  {
    "objectID": "m05-images/archive/pen-and-paper.html",
    "href": "m05-images/archive/pen-and-paper.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Pen and paper exercises\n\n‚úçÔ∏è Pen and paper exercises"
  },
  {
    "objectID": "m05-images/batch-normalization.html",
    "href": "m05-images/batch-normalization.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Batch Normalization (BN) is a technique used in deep neural networks to stabilize and accelerate training by normalizing the inputs to layers within the network.\n\n\nNormalize the activations coming out of a layer for each feature (channel) independently within the current mini-batch so they have zero mean and unit variance.\n\n\n\nFor a mini-batch B = \\{x_1, ..., x_m\\} and considering a single activation feature:\n\nCalculate Mini-Batch Mean: \\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i\nCalculate Mini-Batch Variance: \\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2\nNormalize: Using the mini-batch mean and variance (adding a small \\epsilon for numerical stability): \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\nScale and Shift: Introduce two learnable parameters, \\gamma (gamma) for scaling and \\beta (beta) for shifting: y_i = \\gamma \\hat{x}_i + \\beta\n\n\nThe parameters \\gamma and \\beta are learned during backpropagation alongside the network‚Äôs weights.\nThis process is applied independently to each feature/channel dimension.\n\n\n\n\nIf we just normalized to zero mean and unit variance, the network‚Äôs ability to represent information might be limited. Forcing activations into this specific distribution might not always be optimal for the subsequent layers or activation functions.\n\n\\gamma and \\beta give the network flexibility.\nThey allow the network to learn the optimal scale and shift for the normalized activations.\nIf needed, the network can even learn parameters (\\gamma = \\sqrt{\\sigma_B^2 + \\epsilon}, \\beta = \\mu_B) that effectively undo the normalization, restoring the original activation distribution if that proves beneficial.\n\n\n\n\nDuring inference (when making predictions), we often process samples one by one, so calculating mini-batch statistics isn‚Äôt feasible or representative.\n\nInstead, BN layers maintain running averages of the mean (\\mu_{pop}) and variance (\\sigma^2_{pop}) encountered across all mini-batches during training.\n\nThese are typically updated using a momentum term:\n\n\\mu_{pop} = \\alpha \\times \\mu_{pop} + (1 - \\alpha) \\times \\mu_B\n\\sigma_{pop}^2 = \\alpha \\times \\sigma_{pop}^2 + (1 - \\alpha) \\times \\sigma_B^2\nwhere \\alpha is a momentum parameter.\n\n\nAt inference time, these fixed population statistics are used for normalization: $ = $\nThe learned \\gamma and \\beta parameters from training are still applied:  y = \\gamma \\hat{x} + \\beta \n\n\n\n\n\nIt‚Äôs common practice to place the Batch Norm layer after the Convolutional or Fully Connected layer and before the non-linear activation function (like ReLU).\n\nConv / FC -&gt; Batch Norm -&gt; Activation (ReLU)\n\nHowever, variations in placement exist in different architectures.\n\n(Note: While originally thought to primarily combat ‚Äúinternal covariate shift‚Äù, recent research suggests BN‚Äôs effectiveness might be more related to smoothing the optimization landscape.)",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Batch Normalization"
    ]
  },
  {
    "objectID": "m05-images/batch-normalization.html#the-core-idea",
    "href": "m05-images/batch-normalization.html#the-core-idea",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Normalize the activations coming out of a layer for each feature (channel) independently within the current mini-batch so they have zero mean and unit variance.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Batch Normalization"
    ]
  },
  {
    "objectID": "m05-images/batch-normalization.html#how-it-works-during-training",
    "href": "m05-images/batch-normalization.html#how-it-works-during-training",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "For a mini-batch B = \\{x_1, ..., x_m\\} and considering a single activation feature:\n\nCalculate Mini-Batch Mean: \\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i\nCalculate Mini-Batch Variance: \\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2\nNormalize: Using the mini-batch mean and variance (adding a small \\epsilon for numerical stability): \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\nScale and Shift: Introduce two learnable parameters, \\gamma (gamma) for scaling and \\beta (beta) for shifting: y_i = \\gamma \\hat{x}_i + \\beta\n\n\nThe parameters \\gamma and \\beta are learned during backpropagation alongside the network‚Äôs weights.\nThis process is applied independently to each feature/channel dimension.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Batch Normalization"
    ]
  },
  {
    "objectID": "m05-images/batch-normalization.html#why-scale-and-shift-gamma-and-beta",
    "href": "m05-images/batch-normalization.html#why-scale-and-shift-gamma-and-beta",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "If we just normalized to zero mean and unit variance, the network‚Äôs ability to represent information might be limited. Forcing activations into this specific distribution might not always be optimal for the subsequent layers or activation functions.\n\n\\gamma and \\beta give the network flexibility.\nThey allow the network to learn the optimal scale and shift for the normalized activations.\nIf needed, the network can even learn parameters (\\gamma = \\sqrt{\\sigma_B^2 + \\epsilon}, \\beta = \\mu_B) that effectively undo the normalization, restoring the original activation distribution if that proves beneficial.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Batch Normalization"
    ]
  },
  {
    "objectID": "m05-images/batch-normalization.html#batch-normalization-during-inference",
    "href": "m05-images/batch-normalization.html#batch-normalization-during-inference",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "During inference (when making predictions), we often process samples one by one, so calculating mini-batch statistics isn‚Äôt feasible or representative.\n\nInstead, BN layers maintain running averages of the mean (\\mu_{pop}) and variance (\\sigma^2_{pop}) encountered across all mini-batches during training.\n\nThese are typically updated using a momentum term:\n\n\\mu_{pop} = \\alpha \\times \\mu_{pop} + (1 - \\alpha) \\times \\mu_B\n\\sigma_{pop}^2 = \\alpha \\times \\sigma_{pop}^2 + (1 - \\alpha) \\times \\sigma_B^2\nwhere \\alpha is a momentum parameter.\n\n\nAt inference time, these fixed population statistics are used for normalization: $ = $\nThe learned \\gamma and \\beta parameters from training are still applied:  y = \\gamma \\hat{x} + \\beta",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Batch Normalization"
    ]
  },
  {
    "objectID": "m05-images/batch-normalization.html#placement",
    "href": "m05-images/batch-normalization.html#placement",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "It‚Äôs common practice to place the Batch Norm layer after the Convolutional or Fully Connected layer and before the non-linear activation function (like ReLU).\n\nConv / FC -&gt; Batch Norm -&gt; Activation (ReLU)\n\nHowever, variations in placement exist in different architectures.\n\n(Note: While originally thought to primarily combat ‚Äúinternal covariate shift‚Äù, recent research suggests BN‚Äôs effectiveness might be more related to smoothing the optimization landscape.)",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Batch Normalization"
    ]
  },
  {
    "objectID": "m05-images/what-to-learn.html",
    "href": "m05-images/what-to-learn.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "In this module, we will learn how to use neural networks to learn representations of images. We will learn: - Fourier transform on images - Image processing using Fourier transform - Convolutional neural networks - LeNet - AlexNet - VGG - Inception - ResNet - VisTransformer (if time permits)"
  },
  {
    "objectID": "m05-images/what-to-learn.html#what-to-learn-in-this-module",
    "href": "m05-images/what-to-learn.html#what-to-learn-in-this-module",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "In this module, we will learn how to use neural networks to learn representations of images. We will learn: - Fourier transform on images - Image processing using Fourier transform - Convolutional neural networks - LeNet - AlexNet - VGG - Inception - ResNet - VisTransformer (if time permits)"
  },
  {
    "objectID": "m06-llms/sentence-bert.html",
    "href": "m06-llms/sentence-bert.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Sentence-BERT\nSentence-BERT is a variant of BERT that is specifically designed for sentence-level tasks. We will learn how it works through this hands-on exercises",
    "crumbs": [
      "Home",
      "Module 6: Large Language Models & Emergent Behavior",
      "Sentence-BERT for Semantic Similarity"
    ]
  },
  {
    "objectID": "m06-llms/what-to-learn.html",
    "href": "m06-llms/what-to-learn.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "In this module, we will learn about transformers - a modern architecture that revolutionized NLP. We will learn: - Transformers architecture that revolutionized NLP - BERT and its bidirectional understanding of context - Sentence-BERT for generating sentence embeddings - Flan-T5 for instruction-tuned text generation - Instruction Embedding for better task adaptation"
  },
  {
    "objectID": "m06-llms/what-to-learn.html#what-to-learn-in-this-module",
    "href": "m06-llms/what-to-learn.html#what-to-learn-in-this-module",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "In this module, we will learn about transformers - a modern architecture that revolutionized NLP. We will learn: - Transformers architecture that revolutionized NLP - BERT and its bidirectional understanding of context - Sentence-BERT for generating sentence embeddings - Flan-T5 for instruction-tuned text generation - Instruction Embedding for better task adaptation"
  },
  {
    "objectID": "m02-visualization/time-series.html#the-nature-of-time",
    "href": "m02-visualization/time-series.html#the-nature-of-time",
    "title": "Time Series Visualization",
    "section": "The nature of time",
    "text": "The nature of time\nLet‚Äôs talk about time. In March 2020, charts of COVID-19 cases told vastly different stories depending on how they were visualized. Some used linear scales, showing a terrifying vertical wall. Others used log scales, showing a straight line. Politicians cherry-picked time windows to claim ‚Äúflattening curves.‚Äù\nTime series data is special because it implies causality and momentum. Unlike other variables, time flows in one direction. Your choices of scale, aggregation, and geometry determine whether you reveal a genuine pattern or manufacture a misleading narrative.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Time-Series"
    ]
  },
  {
    "objectID": "m02-visualization/time-series.html#line-plots-and-the-continuity-illusion",
    "href": "m02-visualization/time-series.html#line-plots-and-the-continuity-illusion",
    "title": "Time Series Visualization",
    "section": "Line plots and the continuity illusion",
    "text": "Line plots and the continuity illusion\nThe most fundamental choice is whether to connect the dots. A line plot suggests continuity, implying that a value exists at every moment between your measurements. This works for temperature or stock prices, where the variable has momentum.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set style\nsns.set_style(\"white\")\nsns.set(font_scale=1.2)\n\n# Generate synthetic time series with trend and seasonality\nnp.random.seed(42)\nn_points = 365\ndates = pd.date_range('2023-01-01', periods=n_points, freq='D')\ntrend = np.linspace(100, 150, n_points)\nseasonal = 10 * np.sin(2 * np.pi * np.arange(n_points) / 365 * 4)  # Quarterly seasonality\nnoise = np.random.normal(0, 3, n_points)\nvalues = trend + seasonal + noise\n\ndf = pd.DataFrame({'date': dates, 'value': values})\n\n# Create line plot\nfig, ax = plt.subplots(figsize=(12, 5))\nax.plot(df['date'], df['value'], linewidth=1.5, color=sns.color_palette()[0])\nax.set_xlabel('Date')\nax.set_ylabel('Value')\nax.set_title('Daily Time Series: Line Plot Shows Trend and Seasonality')\nax.grid(True, alpha=0.3)\nsns.despine()\n\n\n\n\n\nBasic line plot showing a time series with trend and seasonality\n\n\n\n\nBut what if your data is discrete? If you plot distinct sales events or email arrivals as a line, you create a false narrative of values existing in the gaps. In those cases, let the silence between points speak.\n\n\nCode\n# Generate sparse discrete event data\nnp.random.seed(123)\nevent_dates = pd.to_datetime(['2023-01-15', '2023-03-10', '2023-05-22',\n                               '2023-07-08', '2023-09-30', '2023-11-15'])\nevent_values = np.random.randint(20, 80, len(event_dates))\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Line plot (implies continuity - misleading for discrete events)\naxes[0].plot(event_dates, event_values, marker='o', linewidth=2, markersize=8)\naxes[0].set_xlabel('Date')\naxes[0].set_ylabel('Event Count')\naxes[0].set_title('Line Plot: Implies Values Between Events (Misleading)')\naxes[0].grid(True, alpha=0.3)\n\n# Scatter plot (appropriate for discrete events)\naxes[1].scatter(event_dates, event_values, s=100, alpha=0.7)\naxes[1].set_xlabel('Date')\naxes[1].set_ylabel('Event Count')\naxes[1].set_title('Scatter Plot: Shows Only Observed Events (Honest)')\naxes[1].grid(True, alpha=0.3)\n\nfor ax in axes:\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n\n\n\n\n\nLine plot vs scatter plot: connecting points implies continuity",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Time-Series"
    ]
  },
  {
    "objectID": "m02-visualization/time-series.html#comparing-series-the-spaghetti-problem",
    "href": "m02-visualization/time-series.html#comparing-series-the-spaghetti-problem",
    "title": "Time Series Visualization",
    "section": "Comparing series: The spaghetti problem",
    "text": "Comparing series: The spaghetti problem\nOften you need to compare multiple series. The natural instinct is to overlay them on the same plot. This works for two or three variables, but as the count rises, you fall into the spaghetti trap where individual trends get lost in the tangle.\n\n\nCode\n# Generate three related time series\nnp.random.seed(42)\ndates = pd.date_range('2023-01-01', periods=200, freq='D')\n\nseries_a = 100 + np.linspace(0, 30, 200) + np.random.normal(0, 5, 200)\nseries_b = 95 + np.linspace(0, 20, 200) + np.random.normal(0, 4, 200)\nseries_c = 110 + np.linspace(0, 10, 200) + np.random.normal(0, 6, 200)\n\ndf_multi = pd.DataFrame({\n    'date': dates,\n    'Product A': series_a,\n    'Product B': series_b,\n    'Product C': series_c\n})\n\n# Overlay plot\nfig, ax = plt.subplots(figsize=(12, 6))\nfor column in ['Product A', 'Product B', 'Product C']:\n    ax.plot(df_multi['date'], df_multi[column], linewidth=2, label=column, alpha=0.8)\n\nax.set_xlabel('Date')\nax.set_ylabel('Sales')\nax.set_title('Multiple Time Series: Overlaid Comparison')\nax.legend(loc='upper left')\nax.grid(True, alpha=0.3)\nsns.despine()\n\n\n\n\n\nMultiple time series overlaid with different colors\n\n\n\n\nThe solution is small multiples (or faceting). By giving each series its own stage while locking the axes, you preserve both the individual trends and the ability to compare them.\n\n\nCode\n# Generate multiple time series\nnp.random.seed(42)\nn_series = 6\ndates = pd.date_range('2023-01-01', periods=150, freq='D')\n\ndata_list = []\nfor i in range(n_series):\n    values = 50 + np.random.randn(150).cumsum() + 10 * np.sin(2 * np.pi * np.arange(150) / 30)\n    data_list.append(pd.DataFrame({\n        'date': dates,\n        'value': values,\n        'series': f'Region {i+1}'\n    }))\n\ndf_many = pd.concat(data_list, ignore_index=True)\n\n# Small multiples using seaborn FacetGrid\ng = sns.FacetGrid(df_many, col='series', col_wrap=3, height=3, aspect=1.5, sharey=True)\ng.map_dataframe(sns.lineplot, x='date', y='value', linewidth=2, color=sns.color_palette()[0])\ng.set_axis_labels('Date', 'Value')\ng.set_titles('Region {col_name}')\nfor ax in g.axes.flat:\n    ax.grid(True, alpha=0.3)\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n\n\n\n\n\nSmall multiples avoid spaghetti plots when comparing many time series",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Time-Series"
    ]
  },
  {
    "objectID": "m02-visualization/time-series.html#the-power-of-scale-linear-vs-log",
    "href": "m02-visualization/time-series.html#the-power-of-scale-linear-vs-log",
    "title": "Time Series Visualization",
    "section": "The power of scale: Linear vs Log",
    "text": "The power of scale: Linear vs Log\nPerhaps the most consequential choice in time series visualization is the y-axis scale. Your choice defines the question you are answering. A linear scale asks ‚ÄúHow much did it increase?‚Äù A log scale asks ‚ÄúHow fast is it growing?‚Äù\nIn the example below, the linear scale suggests an explosive crisis at the end. The log scale reveals that the growth rate has been constant the entire time.\n\n\nCode\n# Generate exponential growth data (e.g., epidemic spread)\nnp.random.seed(42)\ndays = np.arange(0, 100)\ncases = 10 * np.exp(0.05 * days) * (1 + np.random.normal(0, 0.1, len(days)))\n\ndf_exp = pd.DataFrame({'day': days, 'cases': cases})\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Linear scale\naxes[0].plot(df_exp['day'], df_exp['cases'], linewidth=2, color=sns.color_palette()[0])\naxes[0].set_xlabel('Days')\naxes[0].set_ylabel('Cases')\naxes[0].set_title('Linear Scale: Exponential Growth Looks Explosive')\naxes[0].grid(True, alpha=0.3)\n\n# Log scale\naxes[1].plot(df_exp['day'], df_exp['cases'], linewidth=2, color=sns.color_palette()[1])\naxes[1].set_xlabel('Days')\naxes[1].set_ylabel('Cases (log scale)')\naxes[1].set_yscale('log')\naxes[1].set_title('Log Scale: Exponential Growth Appears Linear')\naxes[1].grid(True, alpha=0.3, which='both')\n\nfor ax in axes:\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n\n\n\n\n\nThe same exponential growth looks different on linear vs.¬†log scales\n\n\n\n\n\n\n\n\n\n\nWhy Log Scales?\n\n\n\nLog scales are essential for data spanning orders of magnitude or when percentage changes matter more than absolute units. However, they can downplay absolute magnitude. A jump from 100 to 1,000 looks the same as 10,000 to 100,000.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Time-Series"
    ]
  },
  {
    "objectID": "m02-visualization/time-series.html#smoothing-and-trends",
    "href": "m02-visualization/time-series.html#smoothing-and-trends",
    "title": "Time Series Visualization",
    "section": "Smoothing and trends",
    "text": "Smoothing and trends\nReal data is messy. Smoothing via moving averages mimics how we squint at a chart to blur out the details and see the trend.\nThe window size controls the trade-off. A small window keeps the texture, showing volatility. A large window reveals the structure, showing the trend.\n\n\nCode\n# Generate noisy time series\nnp.random.seed(42)\ndates = pd.date_range('2023-01-01', periods=200, freq='D')\ntrend = 50 + 0.2 * np.arange(200)\nseasonal = 8 * np.sin(2 * np.pi * np.arange(200) / 30)\nnoise = np.random.normal(0, 5, 200)\nvalues = trend + seasonal + noise\n\ndf_noisy = pd.DataFrame({'date': dates, 'value': values})\n\n# Calculate moving averages\ndf_noisy['MA_7'] = df_noisy['value'].rolling(window=7, center=True).mean()\ndf_noisy['MA_30'] = df_noisy['value'].rolling(window=30, center=True).mean()\n\n# Plot\nfig, ax = plt.subplots(figsize=(12, 6))\nax.plot(df_noisy['date'], df_noisy['value'], linewidth=0.8, alpha=0.3, label='Raw Data', color='gray')\nax.plot(df_noisy['date'], df_noisy['MA_7'], linewidth=2, label='7-Day Moving Average', color=sns.color_palette()[0])\nax.plot(df_noisy['date'], df_noisy['MA_30'], linewidth=2, label='30-Day Moving Average', color=sns.color_palette()[1])\n\nax.set_xlabel('Date')\nax.set_ylabel('Value')\nax.set_title('Moving Averages Reveal Trends by Smoothing Noise')\nax.legend()\nax.grid(True, alpha=0.3)\nsns.despine()\n\n\n\n\n\nMoving averages smooth noise to reveal underlying trends",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Time-Series"
    ]
  },
  {
    "objectID": "m02-visualization/time-series.html#showing-uncertainty",
    "href": "m02-visualization/time-series.html#showing-uncertainty",
    "title": "Time Series Visualization",
    "section": "Showing uncertainty",
    "text": "Showing uncertainty\nPredicting the future is an exercise in humility. A forecast without an error bar is a lie of precision. Use ribbon plots to visualize the widening cone of uncertainty as time moves forward.\n\n\nCode\n# Generate data with trend\nnp.random.seed(42)\nn = 150\nx = np.arange(n)\ntrue_trend = 50 + 0.3 * x\nobserved = true_trend + np.random.normal(0, 5, n)\n\n# Simple linear forecast\nfrom scipy import stats\nslope, intercept, r_value, p_value, std_err = stats.linregress(x[:100], observed[:100])\n\n# Forecast period\nx_future = np.arange(100, 150)\ny_pred = slope * x_future + intercept\n\n# Estimate prediction interval (simplified)\nresiduals = observed[:100] - (slope * x[:100] + intercept)\nstd_residual = np.std(residuals)\nmargin = 1.96 * std_residual  # 95% prediction interval\n\n# Plot\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Historical data\nax.plot(x[:100], observed[:100], linewidth=2, label='Historical Data', color=sns.color_palette()[0])\n\n# Forecast with uncertainty\nax.plot(x_future, y_pred, linewidth=2, label='Forecast', color=sns.color_palette()[1], linestyle='--')\nax.fill_between(x_future, y_pred - margin, y_pred + margin,\n                alpha=0.3, color=sns.color_palette()[1], label='95% Prediction Interval')\n\n# Actual future (for comparison)\nax.plot(x_future, observed[100:], linewidth=1.5, alpha=0.5, label='Actual (for comparison)',\n        color='gray', linestyle=':')\n\nax.axvline(x=100, color='black', linestyle=':', alpha=0.5, label='Forecast Start')\nax.set_xlabel('Time')\nax.set_ylabel('Value')\nax.set_title('Time Series Forecast with Uncertainty Bands')\nax.legend()\nax.grid(True, alpha=0.3)\nsns.despine()\n\n\n\n\n\nRibbon plots show uncertainty bands around predictions",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Time-Series"
    ]
  },
  {
    "objectID": "m02-visualization/time-series.html#the-rhythm-of-time-heatmaps-and-cycles",
    "href": "m02-visualization/time-series.html#the-rhythm-of-time-heatmaps-and-cycles",
    "title": "Time Series Visualization",
    "section": "The rhythm of time: Heatmaps and Cycles",
    "text": "The rhythm of time: Heatmaps and Cycles\nTime often cycles rather than marches. Heatmaps and cycle plots break the linear narrative to reveal the heartbeat of the data, such as daily lulls, weekend spikes, or seasonal waves.\n\n\nCode\n# Generate synthetic hourly data with daily and weekly patterns\nnp.random.seed(42)\nhours = pd.date_range('2023-01-01', periods=24*7*4, freq='H')  # 4 weeks\n\n# Patterns: higher activity during business hours and weekdays\nhour_of_day = hours.hour\nday_of_week = hours.dayofweek\n\n# Activity pattern\nbase_activity = 20\nhour_effect = 30 * np.exp(-((hour_of_day - 14)**2) / 20)  # Peak at 2 PM\nweekday_effect = np.where(day_of_week &lt; 5, 20, -10)  # Weekdays higher\nnoise = np.random.normal(0, 5, len(hours))\n\nactivity = base_activity + hour_effect + weekday_effect + noise\n\ndf_hourly = pd.DataFrame({\n    'datetime': hours,\n    'activity': activity,\n    'hour': hour_of_day,\n    'day_name': hours.day_name(),\n    'week': (hours.day // 7) + 1\n})\n\n# Take first week for heatmap\ndf_week = df_hourly[df_hourly['week'] == 1].copy()\n\n# Pivot for heatmap\nheatmap_data = df_week.pivot_table(values='activity',\n                                     index='hour',\n                                     columns='day_name',\n                                     aggfunc='mean')\n\n# Reorder columns to start with Monday\nday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\nheatmap_data = heatmap_data[[day for day in day_order if day in heatmap_data.columns]]\n\n# Plot heatmap\nfig, ax = plt.subplots(figsize=(12, 8))\nsns.heatmap(heatmap_data, cmap='YlOrRd', annot=False, fmt='.0f',\n            cbar_kws={'label': 'Activity Level'}, ax=ax)\nax.set_xlabel('Day of Week')\nax.set_ylabel('Hour of Day')\nax.set_title('Temporal Heatmap: Activity by Hour and Day of Week')\nplt.tight_layout()\n\n\n/var/folders/j7/9dgqq5g53vnbsbmvh2yqtckr0000gr/T/ipykernel_7138/3637488205.py:3: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n  hours = pd.date_range('2023-01-01', periods=24*7*4, freq='H')  # 4 weeks\n\n\n\n\n\nHeat map reveals daily and weekly patterns in temporal data\n\n\n\n\n\n\nCode\n# Generate monthly data with strong annual seasonality\nnp.random.seed(42)\nmonths = pd.date_range('2020-01-01', periods=48, freq='M')\nmonth_num = np.tile(np.arange(1, 13), 4)  # 4 years of monthly data\n\n# Seasonal pattern (higher in summer, lower in winter)\nseasonal_effect = 20 * np.sin(2 * np.pi * (month_num - 3) / 12)\ntrend_effect = 0.5 * np.arange(48)\nnoise = np.random.normal(0, 3, 48)\n\nvalues = 50 + seasonal_effect + trend_effect + noise\n\ndf_seasonal = pd.DataFrame({\n    'date': months,\n    'value': values,\n    'month': month_num,\n    'year': months.year,\n    'month_name': months.month_name()\n})\n\n# Create cycle plot\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Traditional time series\naxes[0].plot(df_seasonal['date'], df_seasonal['value'], marker='o', linewidth=2)\naxes[0].set_xlabel('Date')\naxes[0].set_ylabel('Value')\naxes[0].set_title('Traditional Time Series: Seasonality Repeats')\naxes[0].grid(True, alpha=0.3)\n\n# Cycle plot\nmonth_names_short = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n                     'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\nfor year in df_seasonal['year'].unique():\n    year_data = df_seasonal[df_seasonal['year'] == year]\n    axes[1].plot(year_data['month'], year_data['value'], marker='o',\n                linewidth=2, label=str(year), alpha=0.7)\n\naxes[1].set_xlabel('Month')\naxes[1].set_ylabel('Value')\naxes[1].set_xticks(range(1, 13))\naxes[1].set_xticklabels(month_names_short)\naxes[1].set_title('Cycle Plot: Each Year Overlaid to Show Seasonal Pattern')\naxes[1].legend(title='Year')\naxes[1].grid(True, alpha=0.3)\n\nfor ax in axes:\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n\n\n/var/folders/j7/9dgqq5g53vnbsbmvh2yqtckr0000gr/T/ipykernel_7138/3601020590.py:3: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n  months = pd.date_range('2020-01-01', periods=48, freq='M')\n\n\n\n\n\nCycle plot reveals seasonal patterns by separating each cycle",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Time-Series"
    ]
  },
  {
    "objectID": "m02-visualization/time-series.html#the-memory-of-the-past-autocorrelation",
    "href": "m02-visualization/time-series.html#the-memory-of-the-past-autocorrelation",
    "title": "Time Series Visualization",
    "section": "The memory of the past: Autocorrelation",
    "text": "The memory of the past: Autocorrelation\nDoes the past predict the future? Lag plots visualize the system‚Äôs memory by plotting x_t against x_{t-1}. A tight diagonal implies strong memory (autocorrelation) while a scattered cloud implies random noise.\n\n\nCode\n# Generate time series with autocorrelation\nnp.random.seed(42)\nn = 200\n\n# AR(1) process: strong autocorrelation\nar_series = np.zeros(n)\nar_series[0] = np.random.normal(0, 1)\nfor i in range(1, n):\n    ar_series[i] = 0.7 * ar_series[i-1] + np.random.normal(0, 1)\n\n# Random walk: perfect autocorrelation at lag 1\nrandom_walk = np.random.normal(0, 1, n).cumsum()\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Lag-1 plot for AR(1) series\naxes[0].scatter(ar_series[:-1], ar_series[1:], alpha=0.6, s=30)\naxes[0].set_xlabel('Value at time t')\naxes[0].set_ylabel('Value at time t+1')\naxes[0].set_title('Lag-1 Plot: Strong Autocorrelation (AR Process)')\naxes[0].plot([-3, 3], [-3, 3], 'r--', alpha=0.5, linewidth=1)\naxes[0].grid(True, alpha=0.3)\n\n# Lag-1 plot for random walk\naxes[1].scatter(random_walk[:-1], random_walk[1:], alpha=0.6, s=30, color=sns.color_palette()[1])\naxes[1].set_xlabel('Value at time t')\naxes[1].set_ylabel('Value at time t+1')\naxes[1].set_title('Lag-1 Plot: Perfect Autocorrelation (Random Walk)')\naxes[1].plot([random_walk.min(), random_walk.max()],\n            [random_walk.min(), random_walk.max()], 'r--', alpha=0.5, linewidth=1)\naxes[1].grid(True, alpha=0.3)\n\nfor ax in axes:\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n\n\n\n\n\nLag plots reveal autocorrelation structure in time series\n\n\n\n\n\n\n\n\n\n\nSummary\n\n\n\nTime series visualization is about making choices that honestly represent temporal patterns. By following these principled visualization practices, you ensure your temporal data tells its true story, not the story you wish it told.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Time-Series"
    ]
  },
  {
    "objectID": "m02-visualization/networks.html#the-art-of-structure",
    "href": "m02-visualization/networks.html#the-art-of-structure",
    "title": "Network Visualization",
    "section": "The Art of Structure",
    "text": "The Art of Structure\nYou have probably seen them before: network visualizations that look like tangled balls of yarn, where nodes cluster in impenetrable clumps and edges cross everywhere. These ‚Äúhairball diagrams‚Äù are so common in publications that they have become a running joke in network science. The problem isn‚Äôt that the networks are inherently messy. The problem is that the layout fails to reveal the structure that is actually there.\nThe goal of network visualization is not to make pretty pictures. It is to make structure visible. A good layout answers your questions before you even ask them. It shows you if there are communities, if there is a hierarchy, or if certain nodes act as central hubs. A bad layout obscures these answers, no matter how much you adjust the colors or node sizes.\nIn this lecture, we will explore how to choose and use network layouts that reveal rather than obscure. We will start with the simplest case of trees, move to general networks with force-directed layouts, and finally explore hierarchical structures that combine both approaches with edge bundling.\nThe core principle is simple: Layout is not decoration. It is a hypothesis about what structure matters in your network.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Networks"
    ]
  },
  {
    "objectID": "m02-visualization/networks.html#the-challenge-of-position",
    "href": "m02-visualization/networks.html#the-challenge-of-position",
    "title": "Network Visualization",
    "section": "The Challenge of Position",
    "text": "The Challenge of Position\nA network, or graph, is a collection of nodes connected by links. These can represent social relationships, neural connections, or citations between papers. Unlike data points that have inherent positions like latitude and longitude or time stamps, networks have no natural layout. The positions you see in a visualization are entirely constructed by the placement algorithm.\nA network G = (V, E) consists of a set of nodes V = \\{v_1, v_2, ..., v_n\\} and a set of edges E \\subseteq V \\times V. Edges can be directed (like citations) or undirected (like friendships).\nWe visualize networks because topology is hard to grasp from data alone. Looking at an adjacency matrix gives you facts, but it rarely gives you insight. Visualization transforms abstract connectivity into spatial patterns your visual system can process.\nChoosing a layout is choosing what to emphasize. Different algorithms can make the same network look completely different, so we must choose wisely.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Networks"
    ]
  },
  {
    "objectID": "m02-visualization/networks.html#scaling-up-with-sfdp",
    "href": "m02-visualization/networks.html#scaling-up-with-sfdp",
    "title": "Network Visualization",
    "section": "Scaling Up with SFDP",
    "text": "Scaling Up with SFDP\nThe standard Fruchterman-Reingold algorithm hits a wall as networks grow because it computes forces between every single pair of nodes. For larger networks, we need efficiency. This is where SFDP (Scalable Force-Directed Placement) comes in. It uses a multilevel approach, similar to the Barnes-Hut algorithm in physics simulations, to approximate forces efficiently.\n\n\nCode\nimport graph_tool.all as gt\nimport networkx as nx\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\n\n# Generate a larger scale-free network using NetworkX\nnp.random.seed(123)\nnx_g = nx.barabasi_albert_graph(n=500, m=2, seed=123)\n\n# Convert to graph-tool\ng = gt.Graph(directed=False)\ng.add_vertex(nx_g.number_of_nodes())\nfor u, v in nx_g.edges():\n    g.add_edge(g.vertex(u), g.vertex(v))\n\n# Fruchterman-Reingold layout\nprint(\"Fruchterman-Reingold layout:\")\nstart = time.time()\npos_fr = gt.fruchterman_reingold_layout(g, n_iter=500)\ntime_fr = time.time() - start\nprint(f\"Time: {time_fr:.2f}s\")\n\ngt.graph_draw(g, pos=pos_fr,\n              vertex_fill_color=[0.275, 0.510, 0.706, 1],  # steelblue\n              vertex_size=5,\n              edge_color=[0.584, 0.647, 0.651, 1],\n              edge_pen_width=0.5,\n              output_size=(600, 600),\n              inline=True)\n\n# SFDP layout\nprint(\"\\nSFDP layout:\")\nstart = time.time()\npos_sfdp = gt.sfdp_layout(g)\ntime_sfdp = time.time() - start\nprint(f\"Time: {time_sfdp:.2f}s\")\n\ngt.graph_draw(g, pos=pos_sfdp,\n              vertex_fill_color=[1.0, 0.498, 0.314, 1],  # coral\n              vertex_size=5,\n              edge_color=[0.584, 0.647, 0.651, 1],\n              edge_pen_width=0.5,\n              output_size=(600, 600),\n              inline=True)\n\n\nFruchterman-Reingold layout:\nTime: 9.13s\n\n\n\n\n\nComparison of Fruchterman-Reingold (left) vs.¬†SFDP (right) on a larger network (500 nodes, scale-free topology). SFDP is much faster while producing comparable layouts.\n\n\n\n\n\nSFDP layout:\nTime: 0.43s\n\n\n\n\n\n\n\n\n\nSFDP is often orders of magnitude faster for large networks while producing layouts of comparable quality. Once your network exceeds a few hundred nodes, SFDP should be your default choice.\nIt is important to remember that force-directed layouts are non-deterministic. They start from random positions and settle into a local equilibrium, so different runs can produce different orientations. If you need reproducible figures for a paper, always set a random seed. The layout reveals a valid structure, not the structure.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Networks"
    ]
  },
  {
    "objectID": "m02-visualization/networks.html#the-bigger-picture",
    "href": "m02-visualization/networks.html#the-bigger-picture",
    "title": "Network Visualization",
    "section": "The Bigger Picture",
    "text": "The Bigger Picture\nEvery layout algorithm embodies a hypothesis about what makes nodes ‚Äúsimilar‚Äù or ‚Äúclose.‚Äù The radial tree layout assumes hierarchy is the key structure. The force-directed layout assumes that shared neighbors create similarity. The hierarchical layout with edge bundling assumes that a multi-scale community structure organizes the network.\nNone of these is objectively ‚Äúcorrect.‚Äù They are different lenses for viewing the same data. The critical skill is matching the layout to the question you are asking.\nHowever, network visualization has fundamental limits. Layout is not analysis. A clear visual pattern is a hint, not a proof. You must always validate visual insights with quantitative analysis. 2D layouts lose information. Projecting a high-dimensional graph into two dimensions necessarily distorts distances. Nodes that appear close might not be similar, and nodes that appear far apart might be connected. Large networks do not scale. Once you have thousands of nodes, even the best layouts become unreadable. At that point, you should switch to statistical summaries, aggregation, or interactive tools that allow you to zoom and filter.\nWhen publishing network figures, always set a random seed for reproducibility. Label only the most important nodes to avoid clutter, and use color meaningfully to encode communities or attributes. Most importantly, include a caption that explains the layout algorithm so readers know how to interpret the spatial relationships.\nSometimes the best visualization is not a network diagram at all. If a simple bar chart of the degree distribution tells the story better than a complex graph, use the bar chart. Visualization is a means to understanding, not an end in itself.\n\n\n\n\n\n\nFurther Reading\n\n\n\nFor those interested in the deeper mechanics of these visualizations, Graph-tool offers comprehensive documentation on all its layout algorithms. Edward Tufte‚Äôs The Visual Display of Quantitative Information remains the gold standard for general visualization principles, and Albert-L√°szl√≥ Barab√°si‚Äôs Network Science provides excellent context on interpreting network visuals.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Networks"
    ]
  },
  {
    "objectID": "m02-visualization/2d-data.html#showing-all-points-scatter-plots",
    "href": "m02-visualization/2d-data.html#showing-all-points-scatter-plots",
    "title": "2D Data Visualization",
    "section": "Showing All Points: Scatter Plots",
    "text": "Showing All Points: Scatter Plots\nLet‚Äôs start with the most direct way to show a relationship between two variables. A scatter plot plots every point, so each observation becomes a single dot in 2D space.\n\n\nCode\n# Generate sample data with clear relationship\nnp.random.seed(42)\nn_points = 200\nx = np.random.normal(50, 15, n_points)\ny = 1.5 * x + np.random.normal(0, 10, n_points)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(x, y, alpha=0.6, s=50, edgecolors='white', linewidth=0.5)\nax.set_xlabel('X Variable')\nax.set_ylabel('Y Variable')\nax.set_title('Scatter Plot: Every Point Visible')\nsns.despine()\n\n\n\n\n\nBasic scatter plot showing relationship between two variables\n\n\n\n\nFor small to moderate datasets (up to roughly 1,000 points), scatter plots are perfect. You can see the strength and direction of the relationship, the spread around the trend, individual outliers, non-linear patterns, and clusters or subgroups all at once.\nWhen points overlap heavily, use transparency (alpha). This creates natural density shading. Areas with many overlapping points appear darker, while sparse areas appear lighter.\n\n\nCode\n# Generate data with heavy overlap\nnp.random.seed(123)\nn_points = 1000\nx_overlap = np.random.normal(50, 10, n_points)\ny_overlap = 0.8 * x_overlap + np.random.normal(0, 8, n_points)\n\nfig, axes = plt.subplots(1, 3, figsize=(14, 5))\nalphas = [1.0, 0.5, 0.1]\n\nfor ax, alpha in zip(axes, alphas):\n    ax.scatter(x_overlap, y_overlap, alpha=alpha, s=30)\n    ax.set_xlabel('X Variable')\n    ax.set_ylabel('Y Variable')\n    ax.set_title(f'Alpha = {alpha}')\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n\n\n\n\n\nScatter plots with different alpha values showing how transparency reveals density\n\n\n\n\nWith alpha = 1.0 (opaque), the center is a solid blob. You can‚Äôt tell if there are 10 points or 100. With alpha = 0.1, the density gradient becomes visible. Dark regions have many points; light regions have few.\n\n\n\n\n\n\nNote\n\n\n\n\nA figure from Metaanalysis of faculty‚Äôs teaching effectiveness showing the relationship between student evaluation of teaching and actual learning. Each bubble represents a course section, with size proportional to the number of students. Notice how transparency reveals the density of observations.\n\n\nFor extremely dense data where even transparency doesn‚Äôt help, jittering can separate overlapping points. Jittering adds small random noise to point positions, spreading them out while keeping their relative positions meaningful.\n\n\nCode\n# Generate data with discrete values (common in survey data)\nnp.random.seed(456)\nn_points = 500\nx_discrete = np.random.choice([1, 2, 3, 4, 5], n_points)\ny_discrete = x_discrete + np.random.choice([-1, 0, 1], n_points) + np.random.normal(0, 0.3, n_points)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Without jitter\naxes[0].scatter(x_discrete, y_discrete, alpha=0.5, s=50)\naxes[0].set_xlabel('X Variable (Discrete)')\naxes[0].set_ylabel('Y Variable')\naxes[0].set_title('Without Jittering')\nsns.despine(ax=axes[0])\n\n# With jitter\njitter_x = x_discrete + np.random.normal(0, 0.1, n_points)\njitter_y = y_discrete + np.random.normal(0, 0.1, n_points)\naxes[1].scatter(jitter_x, jitter_y, alpha=0.5, s=50)\naxes[1].set_xlabel('X Variable (Discrete)')\naxes[1].set_ylabel('Y Variable')\naxes[1].set_title('With Jittering')\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n\n\n\n\n\nJittering helps separate discrete or overlapping points\n\n\n\n\nWithout jittering, many points stack on top of each other. You might think there are only 25 data points (5 times 5) when there are actually 500. Jittering reveals the true sample size and density at each location.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 2D Data"
    ]
  },
  {
    "objectID": "m02-visualization/2d-data.html#when-points-overlap-binning-methods",
    "href": "m02-visualization/2d-data.html#when-points-overlap-binning-methods",
    "title": "2D Data Visualization",
    "section": "When Points Overlap: Binning Methods",
    "text": "When Points Overlap: Binning Methods\nWhen you have tens of thousands of points, even transparency and jittering don‚Äôt fully reveal the density structure. Individual points become less meaningful than the overall pattern. This is when we need to bin the data. Binning divides the 2D space into regions and counts observations in each region.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 2D Data"
    ]
  },
  {
    "objectID": "m02-visualization/2d-data.html#smooth-density-estimation-2d-kde",
    "href": "m02-visualization/2d-data.html#smooth-density-estimation-2d-kde",
    "title": "2D Data Visualization",
    "section": "Smooth Density Estimation: 2D KDE",
    "text": "Smooth Density Estimation: 2D KDE\nJust as 1D kernel density estimation (KDE) provides a smooth alternative to histograms, 2D KDE smooths 2D histograms by placing a kernel at each data point and summing them.\n\n\nCode\n# Use the clustered data\nfig, ax = plt.subplots(figsize=(10, 7))\nsns.kdeplot(x=x_clusters, y=y_clusters, cmap='viridis', fill=True, thresh=0.05, levels=20, ax=ax)\nax.set_xlabel('X Variable')\nax.set_ylabel('Y Variable')\nax.set_title('2D Kernel Density Estimation')\nsns.despine()\n\n\n\n\n\n2D kernel density estimation provides smooth density surface\n\n\n\n\nKDE reveals smooth density gradients without arbitrary binning decisions. The key parameter is bandwidth, which controls how wide each kernel is. Small bandwidth gives high detail but can be noisy. Large bandwidth is smooth but may blur important features.\n\n\nCode\nfig, axes = plt.subplots(1, 3, figsize=(14, 5))\nbandwidths = [0.5, 1.5, 3.0]\n\nfor ax, bw in zip(axes, bandwidths):\n    sns.kdeplot(x=x_clusters, y=y_clusters, cmap='viridis', fill=True,\n                bw_adjust=bw, thresh=0.05, levels=15, ax=ax)\n    ax.set_xlabel('X Variable')\n    ax.set_ylabel('Y Variable')\n    ax.set_title(f'Bandwidth adjustment = {bw}')\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n\n\n\n\n\nEffect of bandwidth on 2D KDE smoothness\n\n\n\n\nWith bw_adjust=0.5 (narrow bandwidth), we see fine detail but some noise. With bw_adjust=3.0 (wide bandwidth), the plot is very smooth but the two clusters nearly merge. The default bw_adjust=1.0 (or around 1.5 here) balances detail and smoothness.\n\nContour Plots\nA contour plot represents the density surface as lines of equal density. Think of it like a topographic map where each contour line represents an ‚Äúelevation‚Äù of density.\n\n\nCode\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Filled contours\nsns.kdeplot(x=x_clusters, y=y_clusters, cmap='viridis', fill=True,\n            thresh=0.05, levels=10, ax=axes[0])\naxes[0].set_xlabel('X Variable')\naxes[0].set_ylabel('Y Variable')\naxes[0].set_title('Filled Contour Plot')\nsns.despine(ax=axes[0])\n\n# Line contours with scatter\naxes[1].scatter(x_clusters, y_clusters, alpha=0.1, s=5, c='gray')\nsns.kdeplot(x=x_clusters, y=y_clusters, levels=8, color='red', linewidths=2, ax=axes[1])\naxes[1].set_xlabel('X Variable')\naxes[1].set_ylabel('Y Variable')\naxes[1].set_title('Contour Lines Over Scatter Plot')\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n\n\n\n\n\nContour plot shows density as topographic lines\n\n\n\n\nContour plots excel at overlaying density information on scatter plots, comparing multiple groups with different colored contours, and showing the ‚Äúshape‚Äù of relationships clearly.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 2D Data"
    ]
  },
  {
    "objectID": "m02-visualization/2d-data.html#joint-distributions-combining-2d-and-1d",
    "href": "m02-visualization/2d-data.html#joint-distributions-combining-2d-and-1d",
    "title": "2D Data Visualization",
    "section": "Joint Distributions: Combining 2D and 1D",
    "text": "Joint Distributions: Combining 2D and 1D\nA powerful approach is to show both the joint (2D) distribution and the marginal (1D) distributions of each variable. This connects 2D visualization back to the 1D methods we learned earlier.\nA joint plot combines a central 2D plot with 1D histograms or density plots along the margins.\n\n\nCode\n# Generate data with interesting marginals\nnp.random.seed(202)\nn = 1000\nx_joint = np.concatenate([np.random.normal(30, 10, n//2), np.random.normal(70, 8, n//2)])\ny_joint = np.concatenate([np.random.normal(40, 12, n//2), np.random.normal(60, 10, n//2)])\n\n# Create joint plot\ng = sns.jointplot(x=x_joint, y=y_joint, kind='scatter', alpha=0.5, height=10)\ng.set_axis_labels('X Variable', 'Y Variable')\ng.fig.suptitle('Joint Distribution with Marginal Histograms', y=1.01)\n\n\nText(0.5, 1.01, 'Joint Distribution with Marginal Histograms')\nJoint plot combining 2D scatter with marginal 1D distributions\n\n\n\n\n\n\n\n\n\nThe marginal distributions (top and right) show that both X and Y are bimodal. There are two peaks. But the scatter plot reveals that the peaks are correlated. When X is low, Y tends to be low. When X is high, Y tends to be high. This relationship is invisible in the marginals alone.\nJoint plots can use different visualizations in the center:\n\n\nCode\n# Create joint plot with hexbin and KDE\ng = sns.jointplot(x=x_large, y=y_large, kind='hex', height=10,\n                  marginal_kws=dict(bins=30, fill=True))\ng.set_axis_labels('X Variable', 'Y Variable')\ng.fig.suptitle('Joint Plot: Hexbin Center with KDE Margins', y=1.01)\n\n\nText(0.5, 1.01, 'Joint Plot: Hexbin Center with KDE Margins')\nJoint plot with hexbin center and KDE margins\n\n\n\n\n\n\n\n\n\nOr with KDE everywhere:\n\n\nCode\ng = sns.jointplot(x=x_clusters, y=y_clusters, kind='kde', height=10,\n                  fill=True, cmap='viridis', thresh=0.05)\ng.set_axis_labels('X Variable', 'Y Variable')\ng.fig.suptitle('Joint Plot: All KDE', y=1.01)\n\n\nText(0.5, 1.01, 'Joint Plot: All KDE')\nJoint plot with 2D KDE center and 1D KDE margins\n\n\n\n\n\n\n\n\n\nJoint plots are particularly useful for understanding if marginal distributions are misleading about the relationship, seeing if there‚Äôs correlation between variables with interesting univariate structure, and presenting a complete picture of a bivariate relationship.\n\n\n\n\n\n\nNote\n\n\n\nPro tip: When presenting data, start with marginal distributions to establish what each variable looks like. Then show the joint distribution to reveal the relationship. This guides your audience from the familiar (1D) to the complex (2D).",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 2D Data"
    ]
  },
  {
    "objectID": "m02-visualization/2d-data.html#visualizing-relationships-across-groups",
    "href": "m02-visualization/2d-data.html#visualizing-relationships-across-groups",
    "title": "2D Data Visualization",
    "section": "Visualizing Relationships Across Groups",
    "text": "Visualizing Relationships Across Groups\nOften we want to compare relationships across multiple groups or categories. There are several effective approaches.\n\nColor coding by group\nThe simplest approach is to use different colors for different groups:\n\n\nCode\n# Generate multi-group data\nnp.random.seed(303)\nn_per_group = 150\n\ngroup_a_x = np.random.normal(40, 12, n_per_group)\ngroup_a_y = 0.7 * group_a_x + np.random.normal(0, 8, n_per_group)\n\ngroup_b_x = np.random.normal(55, 10, n_per_group)\ngroup_b_y = 1.2 * group_b_x + np.random.normal(-20, 10, n_per_group)\n\ngroup_c_x = np.random.normal(60, 15, n_per_group)\ngroup_c_y = 0.3 * group_c_x + np.random.normal(30, 12, n_per_group)\n\ndf_groups = pd.DataFrame({\n    'x': np.concatenate([group_a_x, group_b_x, group_c_x]),\n    'y': np.concatenate([group_a_y, group_b_y, group_c_y]),\n    'group': ['A'] * n_per_group + ['B'] * n_per_group + ['C'] * n_per_group\n})\n\nfig, ax = plt.subplots(figsize=(10, 6))\nfor group, color in zip(['A', 'B', 'C'], sns.color_palette('muted', 3)):\n    subset = df_groups[df_groups['group'] == group]\n    ax.scatter(subset['x'], subset['y'], label=f'Group {group}',\n               alpha=0.6, s=50, color=color, edgecolors='white', linewidth=0.5)\n\nax.set_xlabel('X Variable')\nax.set_ylabel('Y Variable')\nax.set_title('Relationships Across Groups')\nax.legend()\nsns.despine()\n\n\n\n\n\nScatter plot with color-coded groups\n\n\n\n\nThis reveals that the three groups have different relationships. Group A has a positive moderate slope. Group B has a steeper positive relationship. Group C has almost no relationship.\n\n\n\n\n\n\nSimpson‚Äôs Paradox\n\n\n\nBe careful! Sometimes the overall trend (pooling all groups) can be opposite to the trend within each group. This is called Simpson‚Äôs Paradox. Always visualize groups separately to check if pooling is appropriate.\n\n\n\n\nSmall multiples (faceting)\nWhen groups overlap heavily or there are many groups, small multiples work better than color coding. Small multiples are separate plots for each group.\n\n\nCode\ng = sns.FacetGrid(df_groups, col='group', height=4, aspect=1.3)\ng.map_dataframe(sns.scatterplot, x='x', y='y', alpha=0.6, s=50)\ng.map_dataframe(sns.regplot, x='x', y='y', scatter=False, color='red')\ng.set_axis_labels('X Variable', 'Y Variable')\ng.set_titles('Group {col_name}')\nsns.despine()\nplt.tight_layout()\n\n\n\n\n\nSmall multiples showing relationship for each group separately\n\n\n\n\nSmall multiples make it easy to compare the strength and direction of relationships across groups without visual clutter.\n\n\nContour overlays\nFor large datasets, overlaying density contours for each group can be very effective:\n\n\nCode\nfig, ax = plt.subplots(figsize=(10, 7))\n\ncolors = sns.color_palette('muted', 3)\nfor group, color in zip(['A', 'B', 'C'], colors):\n    subset = df_groups[df_groups['group'] == group]\n    sns.kdeplot(x=subset['x'], y=subset['y'], levels=5,\n                color=color, linewidths=2, label=f'Group {group}', ax=ax)\n\nax.set_xlabel('X Variable')\nax.set_ylabel('Y Variable')\nax.set_title('Density Contours by Group')\nax.legend()\nsns.despine()\n\n\n/var/folders/j7/9dgqq5g53vnbsbmvh2yqtckr0000gr/T/ipykernel_6291/3335885635.py:12: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n  ax.legend()\n\n\n\n\n\nOverlaid density contours reveal different relationship shapes\n\n\n\n\nThis clearly shows that Groups A and B have elongated, correlated distributions. This indicates strong relationships. Group C is more circular, indicating weak correlation.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 2D Data"
    ]
  },
  {
    "objectID": "m02-visualization/2d-data.html#the-bigger-picture",
    "href": "m02-visualization/2d-data.html#the-bigger-picture",
    "title": "2D Data Visualization",
    "section": "The Bigger Picture",
    "text": "The Bigger Picture\nVisualizing bivariate relationships isn‚Äôt just about making pretty pictures. It‚Äôs about seeing patterns that summary statistics conceal.\nWhen you reduce a relationship to a single number (a correlation coefficient, a slope, a p-value), you lose crucial information. Is the relationship linear or curved? Are there outliers driving the result? Are there subgroups with different patterns? Is the relationship consistent across the range of your data?\nAnscombe‚Äôs Quartet taught us this lesson half a century ago, yet papers still report correlations without showing scatter plots. Don‚Äôt make this mistake.\nThe choice of visualization method matters. Use scatter plots for small to moderate datasets where individual points matter. Use hexbin or heatmaps for large datasets where density matters more than individuals. Use 2D KDE and contours for smooth, assumption-light density estimation. Use joint plots for connecting bivariate relationships to univariate distributions.\nBut the most important choice is the simplest. Always plot your data. Let your audience see what you see. Trust them to interpret patterns, not just summary statistics.\nAs statistician John Tukey wrote: ‚ÄúThe greatest value of a picture is when it forces us to notice what we never expected to see.‚Äù",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 2D Data"
    ]
  },
  {
    "objectID": "m01-toolkit/overview.html#the-reproducibility-crisis",
    "href": "m01-toolkit/overview.html#the-reproducibility-crisis",
    "title": "Overview",
    "section": "The Reproducibility Crisis",
    "text": "The Reproducibility Crisis\nImagine spending months on a groundbreaking data analysis, only to find that you can‚Äôt reproduce your own results. Or imagine a colleague asks for your code and data from a project you did last year, and you suddenly realize you can‚Äôt remember where you saved the files or which version produced the final answer. These scenarios are all too common in data science.\nWhat ties all these stories together is the need for something called provenance. This is simply a complete lineage of the data and code from its origin to its final form. It‚Äôs the backbone of good science, allowing others to verify your findings and build upon your work.\nThis module will teach you the tools and principles that make reproducible data science pipelines possible. A little bit of organization upfront can save you hours of pain down the road.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Overview"
    ]
  },
  {
    "objectID": "m01-toolkit/overview.html#version-control",
    "href": "m01-toolkit/overview.html#version-control",
    "title": "Overview",
    "section": "Version Control",
    "text": "Version Control\nLet‚Äôs start with version control. Without proper tracking, accidents happen: you lose days of work to an overwrite, or worse, a security breach because no one knows which version of code is running. Version control transforms chaos into clarity.\nGit and GitHub let you track changes, collaborate without stepping on each other‚Äôs toes, and recover from mistakes. Ready to dive deeper? Learn more about Version Control with Git & GitHub.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Overview"
    ]
  },
  {
    "objectID": "m01-toolkit/overview.html#data-provenance-and-tidy-data",
    "href": "m01-toolkit/overview.html#data-provenance-and-tidy-data",
    "title": "Overview",
    "section": "Data Provenance and Tidy Data",
    "text": "Data Provenance and Tidy Data\nNow shift your attention from the tools to the data itself. You need two things: knowing the history of your data (called data provenance) and structuring it in a way that makes analysis straightforward (called tidy data).\nKnowing where your data came from, how it was collected, and what transformations were applied is critical for trust and reproducibility. When you structure data tidily, analysis becomes faster, clearer, and less error-prone. Ready to explore? Check out Data Provenance and Tidy Data.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Overview"
    ]
  },
  {
    "objectID": "m01-toolkit/overview.html#reproducible-environments",
    "href": "m01-toolkit/overview.html#reproducible-environments",
    "title": "Overview",
    "section": "Reproducible Environments",
    "text": "Reproducible Environments\nNow let‚Äôs think about the environment where your code runs. Your code might work perfectly on your machine today, but will it run on your colleague‚Äôs machine tomorrow? Will it work on your machine six months from now after library updates?\nReproducible environments ensure your work replicates exactly, no matter where or when it runs. This is the final piece of the reproducibility puzzle. Ready to complete the picture? Discover Reproducible Environments & Projects.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Overview"
    ]
  },
  {
    "objectID": "m01-toolkit/overview.html#what-youll-gain",
    "href": "m01-toolkit/overview.html#what-youll-gain",
    "title": "Overview",
    "section": "What You‚Äôll Gain",
    "text": "What You‚Äôll Gain\nBy the end of this module, you‚Äôll have a solid foundation in reproducible data science. You‚Äôll track changes with version control, structure data clearly, and build replicable environments. These practices make you a more effective and trustworthy collaborator.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Overview"
    ]
  },
  {
    "objectID": "m01-toolkit/git-github.html#the-day-the-code-disappeared",
    "href": "m01-toolkit/git-github.html#the-day-the-code-disappeared",
    "title": "Git & GitHub: Your Time Machine for Code",
    "section": "The Day the Code Disappeared",
    "text": "The Day the Code Disappeared\nPicture this: you‚Äôre working late on a project, and you finally fix a major bug. Your fingers are on the keyboard when you suddenly delete a line by accident, then save. You go home. The next morning, you open your laptop and realize all your work from yesterday is gone. The bug fix vanished. You have no idea what you did to fix it, and you have to start from scratch.\nEven big companies get it wrong. In 2017, GitLab, a major code hosting platform, suffered a catastrophic outage. A system administrator accidentally deleted massive amounts of production data. The backups didn‚Äôt work. The company lost six hours of customer data. That‚Äôs a lifetime in the software world.\nThe details are uncomfortable reading. Check out the GitLab post-mortem if you want to see what went wrong.\n\n\nWe accidentally deleted production data and might have to restore from backup. Google Doc with live notes https://t.co/EVRbHzYlk8\n\n‚Äî GitLab.com Status ((gitlabstatus?)) February 1, 2017",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Version Control with Git & GitHub"
    ]
  },
  {
    "objectID": "m01-toolkit/git-github.html#version-control",
    "href": "m01-toolkit/git-github.html#version-control",
    "title": "Git & GitHub: Your Time Machine for Code",
    "section": "Version Control",
    "text": "Version Control\nVersion control is essential for any data-related work. A version control system (VCS) saves ‚Äúsnapshots‚Äù of your files. Think of it as a time machine: you can travel back to any snapshot and see exactly what your code looked like at that moment.\nThe most popular VCS today is Git. When you put Git in the cloud (like on GitHub), something magical happens: you can access your work from anywhere, share it with teammates, and back it up automatically. You‚Äôve gone from a time machine to a collaborative, backed-up time machine.\nHere‚Äôs a quick intro video to get you oriented:",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Version Control with Git & GitHub"
    ]
  },
  {
    "objectID": "m01-toolkit/git-github.html#learning-resources",
    "href": "m01-toolkit/git-github.html#learning-resources",
    "title": "Git & GitHub: Your Time Machine for Code",
    "section": "Learning Resources",
    "text": "Learning Resources\nStart with A Layman‚Äôs Introduction to Git for a readable overview. For hands-on learning, try the Interactive Git Tutorial, which teaches Git visually through interactive exercises.\nIf you want comprehensive documentation, check out the Git Documentation or the Atlassian Git Tutorials for detailed examples.\nFor beginners, I recommend starting with GitHub Desktop instead of the command line. It gives you a graphical way to manage your repositories while you learn Git concepts. See the GitHub Desktop Documentation for more details.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Version Control with Git & GitHub"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html#the-curse-of-dimensionality",
    "href": "m02-visualization/highd-data.html#the-curse-of-dimensionality",
    "title": "High-Dimensional Data Visualization",
    "section": "The Curse of Dimensionality",
    "text": "The Curse of Dimensionality\nBefore we dive into methods, we need to understand what makes high-dimensional data fundamentally different.\nIn high dimensions, everything is far from everything else. This sounds paradoxical, but it‚Äôs mathematically inevitable. As dimensions increase, the volume of space grows exponentially, and data points become increasingly sparse.\nConsider this simple fact: in 1D, if you have 10 points uniformly distributed in [0, 1], the average distance between neighbors is about 0.1. To maintain the same density in 2D, you need 100 points. In 3D, you need 1,000 points. In 10D, you need 10 billion points.\nEven stranger than the sparsity is what happens to distances. In high dimensions, all distances become similar. The nearest and farthest neighbors become roughly equidistant. This makes many of our intuitions about ‚Äúcloseness‚Äù break down.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.metrics.pairwise import euclidean_distances\n\nsns.set_style(\"white\")\nnp.random.seed(42)\n\n# Calculate distance ratio across dimensions\ndimensions = [2, 5, 10, 20, 50, 100, 200]\nn_samples = 100\nratios = []\n\nfor d in dimensions:\n    # Generate random data\n    X = np.random.randn(n_samples, d)\n    # Calculate all pairwise distances\n    distances = euclidean_distances(X)\n    # For each point, find nearest and farthest (excluding self)\n    np.fill_diagonal(distances, np.inf)  # Ignore self-distance\n    nearest = distances.min(axis=1)\n    # For \"farthest,\" ignore inf (self-distance), so set inf entries to -1 and use argmax\n    temp = distances.copy()\n    temp[temp == np.inf] = -1  # Now maximum is truly among finite values\n    farthest = temp.max(axis=1)\n    # Calculate ratio\n    ratio = nearest / farthest\n    ratios.append(ratio)\n\n# Plot\nsns.set(font_scale=2.0)\nsns.set_style(\"white\")\n\nblue, red = sns.color_palette('muted', 2)\n\nfig, ax = plt.subplots(figsize=(10, 5))\npositions = range(len(dimensions))\nbp = ax.boxplot(ratios, positions=positions, widths=0.6, patch_artist=True,\n                boxprops=dict(facecolor=\"#f2f2f2\", alpha=0.7))\nax.set_xticklabels(dimensions)\nax.set_xlabel('Number of Dimensions')\nax.set_ylabel('Nearest Distance / Farthest Distance')\nax.set_title('The Curse of Dimensionality: All Points Become Equidistant')\nax.axhline(y=1.0, color=red, linestyle='--', alpha=0.5, label='Equal distances')\nax.legend(frameon=False)\nsns.despine()\n\n\n\n\n\nAs dimensions increase, the ratio of farthest to nearest distance approaches 1\n\n\n\n\nThe plot shows a striking pattern. As dimensions increase, the ratio of nearest to farthest distance gets closer to 1. At 200 dimensions, nearly every point is equally far from every other point.\nThis curse of dimensionality is useful not just for visualization, but also for analysis. When you want to cluster data points, every point becomes equidistant from every other point, making clustering impossible. By projecting the data into lower dimensions, you can remedy this problem.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html#pairwise-scatter-plots-the-brute-force-approach",
    "href": "m02-visualization/highd-data.html#pairwise-scatter-plots-the-brute-force-approach",
    "title": "High-Dimensional Data Visualization",
    "section": "Pairwise Scatter Plots: The Brute Force Approach",
    "text": "Pairwise Scatter Plots: The Brute Force Approach\nWhen you have a moderate number of dimensions (roughly 3 to 10), you can visualize all pairwise relationships using a scatter plot matrix, also called a pairs plot or SPLOM.\n\n\nCode\n# Load classic iris dataset (4 dimensions)\nfrom sklearn.datasets import load_iris\n\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\n\niris = load_iris()\niris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\niris_df['species'] = iris.target\niris_df['species'] = iris_df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n\n# Create pairplot\ng = sns.pairplot(iris_df, hue='species', diag_kind='kde',\n                 plot_kws={'alpha': 0.6, 's': 50, 'edgecolor': 'white', 'linewidth': 0.5},\n                 diag_kws={'alpha': 0.7, 'linewidth': 2})\ng.fig.suptitle('Iris Dataset: All Pairwise Relationships', y=1.01)\n\n\nText(0.5, 1.01, 'Iris Dataset: All Pairwise Relationships')\nScatter plot matrix showing all pairwise relationships in the Iris dataset\n\n\n\n\n\n\n\n\n\nThe scatter plot matrix shows every possible 2D projection. The diagonal displays the univariate distribution of each feature using KDE, while off-diagonals show bivariate scatter plots. This gives you a complete view of pairwise relationships.\nThe problem is clear: scatter plot matrices don‚Äôt scale. With 10 variables, you have 45 unique pairwise plots, which is manageable but crowded. With 20 variables, you have 190 plots, which becomes overwhelming. And you‚Äôre still only seeing 2D projections, never the full high-dimensional structure.\nThis is where dimensionality reduction becomes essential. Instead of looking at every pairwise combination, we project the data intelligently onto just 2 or 3 dimensions.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html#linear-dimensionality-reduction-pca",
    "href": "m02-visualization/highd-data.html#linear-dimensionality-reduction-pca",
    "title": "High-Dimensional Data Visualization",
    "section": "Linear Dimensionality Reduction: PCA",
    "text": "Linear Dimensionality Reduction: PCA\nPrincipal Component Analysis (PCA) is a linear dimensionality reduction method that finds the directions of maximum variance in your data.\nImagine you have a cloud of points in high-dimensional space. PCA asks a simple question: what direction captures the most variation in the data? This becomes the first principal component (PC1). Then it asks: what direction, perpendicular to the first, captures the most remaining variation? This becomes PC2. And so on.\nMathematically, PCA finds the eigenvectors of the covariance matrix. But conceptually, it‚Äôs rotating your coordinate system to align with the highest variance directions of your data.\n\n\nCode\nfrom sklearn.decomposition import PCA\n\n# Generate correlated 2D data (for visualization)\nnp.random.seed(123)\nmean = [0, 0]\ncov = [[3, 2], [2, 2]]\ndata_2d = np.random.multivariate_normal(mean, cov, 300)\n\n# Fit PCA\npca = PCA(n_components=2)\npca.fit(data_2d)\n\ncolors = [\"#f2f2f2\", sns.color_palette('muted')[0], sns.color_palette('muted')[3]]\n\n# Plot original data with principal components\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(data_2d[:, 0], data_2d[:, 1], alpha=0.9, s=50, color=colors[0], edgecolors='k', linewidth=0.5)\n\n# Draw principal components as arrows\norigin = pca.mean_\nfor i, (component, variance) in enumerate(zip(pca.components_, pca.explained_variance_)):\n    direction = component * np.sqrt(variance) * 3  # Scale for visibility\n    ax.arrow(origin[0], origin[1], direction[0], direction[1],\n             head_width=0.3, head_length=0.3, fc=colors[i+1], ec=colors[i+1], linewidth=3,\n             label=f'PC{i+1} ({variance/pca.explained_variance_.sum()*100:.1f}%)')\n\nax.set_xlabel('Original X')\nax.set_ylabel('Original Y')\nax.set_title('Principal Components: Directions of Maximum Variance')\nax.legend()\nax.axis('equal')\nsns.despine()\n\n\n\n\n\nPCA finds directions of maximum variance. PC1 captures the most variation, PC2 the next most (perpendicular to PC1).\n\n\n\n\nPC1 (orange arrow) points along the direction of greatest spread. PC2 (green arrow) is perpendicular and captures the remaining variation. The percentage in parentheses shows how much variance each component explains. If PC1 explains 90 percent of variance, then projecting onto just PC1 preserves most of your data‚Äôs structure.\n\nApplying PCA to Iris\nLet‚Äôs apply PCA to the 4-dimensional Iris dataset and see how much information we can preserve in just 2 dimensions.\n\n\nCode\n# Prepare data\nX = iris.data\ny = iris.target\n\n# Standardize (important for PCA!)\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Apply PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# Create DataFrame for plotting\npca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\npca_df['species'] = iris.target_names[y]\n\n# Plot\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Left: PCA projection\nfor species, color in zip(['setosa', 'versicolor', 'virginica'], sns.color_palette('muted', 3)):\n    mask = pca_df['species'] == species\n    axes[0].scatter(pca_df.loc[mask, 'PC1'], pca_df.loc[mask, 'PC2'],\n                   label=species, alpha=0.7, s=50, color=color, edgecolors='white', linewidth=0.5)\naxes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)')\naxes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)')\naxes[0].set_title('PCA Projection of Iris Dataset')\naxes[0].legend()\nsns.despine(ax=axes[0])\n\n# Right: Variance explained\nvariances = pca.explained_variance_ratio_\naxes[1].bar([1, 2], variances, color=sns.color_palette('muted', 2), alpha=0.7)\naxes[1].set_xlabel('Principal Component')\naxes[1].set_ylabel('Variance Explained')\naxes[1].set_title('Variance Explained by Each Component')\naxes[1].set_xticks([1, 2])\naxes[1].set_xticklabels(['PC1', 'PC2'])\nfor i, v in enumerate(variances):\n    axes[1].text(i+1, v+0.01, f'{v*100:.1f}%', ha='center', va='bottom', fontsize=11)\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n\n\n\n\n\nPCA projection of Iris dataset to 2D preserves the separation between species\n\n\n\n\nPC1 and PC2 together explain over 95 percent of the variance in the 4D dataset. The 2D projection preserves the main structure beautifully: setosa is well-separated, while versicolor and virginica have some overlap, just as they do in the original high-dimensional space.\nA critical reminder: always standardize before PCA. If features have different units or scales, PCA will be dominated by high-variance features. Standardization (zero mean, unit variance) ensures all features contribute fairly to the analysis.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html#non-linear-dimensionality-reduction-mds",
    "href": "m02-visualization/highd-data.html#non-linear-dimensionality-reduction-mds",
    "title": "High-Dimensional Data Visualization",
    "section": "Non-Linear Dimensionality Reduction: MDS",
    "text": "Non-Linear Dimensionality Reduction: MDS\nMultidimensional Scaling (MDS) takes a different approach than PCA. Instead of finding directions of maximum variance, it tries to preserve distances between points.\nYou give MDS a distance matrix showing the distance between every pair of points in high-dimensional space. MDS then finds a low-dimensional configuration where those distances are preserved as well as possible.\nThink of it like arranging cities on a map. You know the distance between every pair of cities, but not their coordinates. MDS finds positions that preserve those distances. Mathematically, MDS minimizes stress, the difference between high-dimensional distances and low-dimensional distances. Classical MDS has a closed-form solution like PCA, but more flexible variants use iterative optimization.\n\n\nCode\nfrom sklearn.manifold import MDS\n\n# Suppress FutureWarning about n_init in MDS\nimport warnings\nmds = MDS(n_components=2, random_state=42, n_init=1)\nX_mds = mds.fit_transform(X_scaled)\n\n# Create DataFrame\nmds_df = pd.DataFrame(X_mds, columns=['MDS1', 'MDS2'])\nmds_df['species'] = iris.target_names[y]\n\n# Plot both\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# PCA\nfor species, color in zip(['setosa', 'versicolor', 'virginica'], sns.color_palette('muted', 3)):\n    mask = pca_df['species'] == species\n    axes[0].scatter(pca_df.loc[mask, 'PC1'], pca_df.loc[mask, 'PC2'],\n                   label=species, alpha=0.7, s=50, color=color, edgecolors='white', linewidth=0.5)\naxes[0].set_xlabel('PC1')\naxes[0].set_ylabel('PC2')\naxes[0].set_title('PCA: Maximizes Variance')\naxes[0].legend()\nsns.despine(ax=axes[0])\n\n# MDS\nfor species, color in zip(['setosa', 'versicolor', 'virginica'], sns.color_palette('muted', 3)):\n    mask = mds_df['species'] == species\n    axes[1].scatter(mds_df.loc[mask, 'MDS1'], mds_df.loc[mask, 'MDS2'],\n                   label=species, alpha=0.7, s=50, color=color, edgecolors='white', linewidth=0.5)\naxes[1].set_xlabel('MDS1')\naxes[1].set_ylabel('MDS2')\naxes[1].set_title('MDS: Preserves Distances')\naxes[1].legend()\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n\n\n\n\n\nMDS vs PCA on Iris dataset. MDS preserves distances better but looks similar to PCA for this dataset.\n\n\n\n\nFor the Iris dataset, PCA and MDS look very similar. This is because Iris data is fairly linear. The relationships between features don‚Äôt involve complex curves or non-linear structures that would cause MDS to differ significantly from PCA.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html#isomap-preserving-geodesic-distances",
    "href": "m02-visualization/highd-data.html#isomap-preserving-geodesic-distances",
    "title": "High-Dimensional Data Visualization",
    "section": "Isomap: Preserving Geodesic Distances",
    "text": "Isomap: Preserving Geodesic Distances\nMDS preserves Euclidean distances, which are straight-line distances through space. But for curved manifolds, what matters is the geodesic distance: the distance along the surface.\nIsomap (Isometric Mapping) addresses this by approximating geodesic distances using the neighborhood graph. The approach is elegant. First, build a neighborhood graph by connecting each point to its k nearest neighbors. Second, compute shortest paths through this graph. The geodesic distance between points is approximated by the shortest path. Third, apply classical MDS using these geodesic distances instead of Euclidean distances.\nThink of it like this: MDS measures distance ‚Äúas the crow flies,‚Äù while Isomap measures distance ‚Äúas you walk along the surface.‚Äù\n\n\nCode\nfrom sklearn.manifold import Isomap\nfrom sklearn.datasets import make_s_curve\n\n# Generate S-curve data (a 2D manifold embedded in 3D)\nn_samples = 1000\nX_scurve, color = make_s_curve(n_samples, noise=0.1, random_state=42)\n\n# Apply MDS\nmds_scurve = MDS(n_components=2, random_state=42, n_init=1)\nX_scurve_mds = mds_scurve.fit_transform(X_scurve)\n\n# Apply Isomap\nisomap = Isomap(n_components=2, n_neighbors=10)\nX_scurve_isomap = isomap.fit_transform(X_scurve)\n\n# Plot MDS vs Isomap\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# MDS\naxes[0].scatter(X_scurve_mds[:, 0], X_scurve_mds[:, 1], c=color,\n                cmap='viridis', alpha=0.6, s=20)\naxes[0].set_xlabel('MDS1')\naxes[0].set_ylabel('MDS2')\naxes[0].set_title('MDS: Global Euclidean Distances')\nsns.despine(ax=axes[0])\n\n# Isomap\naxes[1].scatter(X_scurve_isomap[:, 0], X_scurve_isomap[:, 1], c=color,\n                cmap='viridis', alpha=0.6, s=20)\naxes[1].set_xlabel('Isomap1')\naxes[1].set_ylabel('Isomap2')\naxes[1].set_title('Isomap: Geodesic Distances')\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n\n\n\n\n\nIsomap uses geodesic distances (along the surface) instead of Euclidean distances (through space), better recovering the S-curve structure\n\n\n\n\nIsomap successfully ‚Äústraightens‚Äù the S-curve because it respects the manifold structure. By computing distances along the neighborhood graph, it avoids the shortcuts across the bend that confused MDS. The key parameter is n_neighbors. Too few neighbors and the graph becomes disconnected with infinite distances. Too many neighbors and you create shortcuts across the manifold, reverting to MDS-like behavior. Getting it just right, typically 5 to 15, captures the local manifold structure perfectly.\nNow we see two extremes emerging. MDS preserves all pairwise distances globally, which works on linear or convex data. Isomap preserves geodesic distances using local neighborhoods, which works on curved manifolds. But what if we only care about local structure? What if global relationships don‚Äôt matter for our purposes?",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html#modern-non-linear-methods-t-sne-and-umap",
    "href": "m02-visualization/highd-data.html#modern-non-linear-methods-t-sne-and-umap",
    "title": "High-Dimensional Data Visualization",
    "section": "Modern Non-Linear Methods: t-SNE and UMAP",
    "text": "Modern Non-Linear Methods: t-SNE and UMAP\nBoth t-SNE (t-Distributed Stochastic Neighbor Embedding) and UMAP (Uniform Manifold Approximation and Projection) take a middle ground between MDS‚Äôs global approach and Isomap‚Äôs geodesic approach. They prioritize local structure while allowing some flexibility in global positioning.\nThe key insight is simple: for visualization, we often care most about which points are neighbors. Whether distant clusters are placed left versus right, or how far apart they are, matters less than preserving the local neighborhood relationships within and between clusters.\n\nHow t-SNE works\nt-SNE converts distances into similarity probabilities and preserves these local relationships. In high dimensions, we define probability p_{ij} that point i picks point j as a neighbor, based on a Gaussian distance. In low dimensions, we define similar probability q_{ij} using a t-distribution with heavy tails. Then we optimize by moving points in 2D to make q_{ij} match p_{ij}, minimizing KL divergence.\nThe t-distribution‚Äôs heavy tails are clever. They let well-separated clusters spread out in 2D without overlapping, while keeping local neighborhoods tight.\n\n\nCode\nfrom sklearn.manifold import TSNE\n\n# Apply t-SNE\ntsne = TSNE(n_components=2, random_state=42, perplexity=30)\nX_scurve_tsne = tsne.fit_transform(X_scurve)\n\n# Plot all three methods\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# MDS - Global Euclidean distances\naxes[0].scatter(X_scurve_mds[:, 0], X_scurve_mds[:, 1], c=color,\n                cmap='viridis', alpha=0.6, s=20)\naxes[0].set_xlabel('MDS1')\naxes[0].set_ylabel('MDS2')\naxes[0].set_title('MDS: Global Distances')\nsns.despine(ax=axes[0])\n\n# Isomap - Geodesic distances\naxes[1].scatter(X_scurve_isomap[:, 0], X_scurve_isomap[:, 1], c=color,\n                cmap='viridis', alpha=0.6, s=20)\naxes[1].set_xlabel('Isomap1')\naxes[1].set_ylabel('Isomap2')\naxes[1].set_title('Isomap: Geodesic Distances')\nsns.despine(ax=axes[1])\n\n# t-SNE - Local neighborhoods\naxes[2].scatter(X_scurve_tsne[:, 0], X_scurve_tsne[:, 1], c=color,\n                cmap='viridis', alpha=0.6, s=20)\naxes[2].set_xlabel('t-SNE1')\naxes[2].set_ylabel('t-SNE2')\naxes[2].set_title('t-SNE: Local Structure')\nsns.despine(ax=axes[2])\n\nplt.tight_layout()\n\n\n\n\n\nComparing global, geodesic, and local approaches on the S-curve\n\n\n\n\nAll three methods successfully straighten the S-curve, but through different philosophies. MDS compromises between all distances. Isomap follows the manifold globally. t-SNE focuses on preserving neighborhoods. Each makes different trade-offs between local and global structure.\nThe key parameter in t-SNE is perplexity, which typically ranges from 30 to 50. Perplexity controls the effective neighborhood size. Too low perplexity fragments clusters. Too high perplexity loses local detail. Finding the right balance is important.\n\n\nWhat t-SNE preserves (and what it doesn‚Äôt)\nt-SNE is powerful but has important limitations. It preserves local structure, keeping points that are neighbors in high dimensions as neighbors in 2D. It preserves clusters, keeping well-separated groups separated. It preserves relative relationships within neighborhoods, so if A is closer to B than to C locally, this is preserved.\nWhat t-SNE does NOT preserve: the actual distance between points is not meaningful. The relative position of distant clusters is arbitrary. Large clusters may appear smaller, and vice versa. Tight clusters may be spread out, and sparse regions may appear dense.\nYou cannot conclude from a t-SNE plot that ‚Äúcluster A is twice as far from B as from C.‚Äù Distances are not preserved. You cannot conclude that ‚Äúcluster A is twice the size of B.‚Äù Sizes are not preserved. You cannot conclude that ‚Äúthe data has exactly 5 clusters.‚Äù Apparent clusters may be visualization artifacts.\nYou can conclude that ‚Äúthese points form a distinct group separate from others.‚Äù You can conclude that ‚Äúthese points are more similar to each other than to distant points.‚Äù You can conclude that ‚Äúthe data has local structure and is not uniformly random.‚Äù\n\n\nApplying t-SNE to real data\nLet‚Äôs apply t-SNE to a more realistic high-dimensional dataset: the digits dataset, which has 64 dimensions (8 by 8 pixel images).\n\n\nCode\nfrom sklearn.datasets import load_digits\n\n# Load digits dataset (8x8 images, 64 dimensions)\ndigits = load_digits()\nX_digits = digits.data\ny_digits = digits.target\n\n# Take a subset for speed (t-SNE is slow on large datasets)\nnp.random.seed(42)\nindices = np.random.choice(len(X_digits), size=1000, replace=False)\nX_subset = X_digits[indices]\ny_subset = y_digits[indices]\n\n# Apply t-SNE\ntsne_digits = TSNE(n_components=2, random_state=42, perplexity=40)\nX_digits_tsne = tsne_digits.fit_transform(X_subset)\n\n# Plot\nfig, ax = plt.subplots(figsize=(12, 10))\nscatter = ax.scatter(X_digits_tsne[:, 0], X_digits_tsne[:, 1],\n                     c=y_subset, cmap='tab10', alpha=0.7, s=30)\nax.set_xlabel('t-SNE1')\nax.set_ylabel('t-SNE2')\nax.set_title('t-SNE Visualization of Handwritten Digits (64D to 2D)')\ncbar = plt.colorbar(scatter, ax=ax, ticks=range(10))\ncbar.set_label('Digit Class')\nsns.despine()\n\n\n\n\n\nt-SNE visualization of handwritten digits (64 dimensions to 2D). Each color represents a digit class.\n\n\n\n\nThe t-SNE projection beautifully separates most digit classes. Digits that look similar, like 3, 5, and 8, cluster near each other. Visually distinct digits, like 0 and 1, are well separated.\nThis demonstrates t-SNE‚Äôs power. From 64 dimensions with no explicit information about what makes digits similar, t-SNE discovers the perceptual structure of handwritten digits. It‚Äôs a remarkable achievement in unsupervised learning.\nAn important note: t-SNE is stochastic. Different runs produce different layouts, though cluster structure remains consistent. Always check multiple runs with different random seeds, especially for important scientific conclusions.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html#umap-a-faster-alternative",
    "href": "m02-visualization/highd-data.html#umap-a-faster-alternative",
    "title": "High-Dimensional Data Visualization",
    "section": "UMAP: A Faster Alternative",
    "text": "UMAP: A Faster Alternative\nUniform Manifold Approximation and Projection (UMAP) is a newer method from 2018 that has become popular as an alternative to t-SNE. Like t-SNE, UMAP preserves local structure, but it‚Äôs based on different mathematical foundations in manifold learning and topological data analysis.\nUMAP has several advantages over t-SNE. It‚Äôs faster, often 10 to 100 times faster than t-SNE on large datasets. It scales better, working well on datasets with millions of points. It preserves more global structure than t-SNE. It‚Äôs also theoretically grounded in Riemannian geometry and fuzzy topology.\nThe trade-offs are worth noting. UMAP is less battle-tested than t-SNE since it‚Äôs newer. It has more hyperparameters to tune, though defaults work well. It often produces similar-looking results to t-SNE, so the choice often comes down to speed.\n\n\nCode\nimport umap\n\n# Apply UMAP\numap_model = umap.UMAP(n_components=2, random_state=42, n_neighbors=30)\nX_digits_umap = umap_model.fit_transform(X_subset)\n\n# Plot comparison\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# t-SNE\nscatter = axes[0].scatter(X_digits_tsne[:, 0], X_digits_tsne[:, 1],\n                          c=y_subset, cmap='tab10', alpha=0.7, s=30)\naxes[0].set_xlabel('t-SNE1')\naxes[0].set_ylabel('t-SNE2')\naxes[0].set_title('t-SNE')\nsns.despine(ax=axes[0])\n\n# UMAP\nscatter = axes[1].scatter(X_digits_umap[:, 0], X_digits_umap[:, 1],\n                          c=y_subset, cmap='tab10', alpha=0.7, s=30)\naxes[1].set_xlabel('UMAP1')\naxes[1].set_ylabel('UMAP2')\naxes[1].set_title('UMAP')\ncbar = plt.colorbar(scatter, ax=axes[1], ticks=range(10))\ncbar.set_label('Digit Class')\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n\n\n/Users/skojaku-admin/Documents/projects/applied-soft-comp/.venv/lib/python3.11/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n  warn(\n\n\n\n\n\nUMAP vs t-SNE on digits dataset. UMAP often preserves more global structure while being much faster.\n\n\n\n\nBoth methods reveal similar cluster structure, but UMAP tends to space clusters more evenly and preserve more of the global topology. Notice how UMAP places similar digits (3, 5, 8) in a connected region, suggesting they share underlying structure.\nUse UMAP when you have very large datasets (over 10,000 points) where t-SNE becomes slow. Use it when you want to preserve more global structure. Use it when you‚Äôre doing exploratory analysis and want fast iteration. UMAP also supports projecting new data onto an existing embedding, which t-SNE doesn‚Äôt easily support.\nStick with t-SNE when you need the most established method with extensive literature. Use t-SNE when you‚Äôre working with moderate-sized datasets where speed isn‚Äôt critical. Use t-SNE when you‚Äôre replicating published work that used t-SNE.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html#the-bigger-picture-choosing-the-right-method",
    "href": "m02-visualization/highd-data.html#the-bigger-picture-choosing-the-right-method",
    "title": "High-Dimensional Data Visualization",
    "section": "The Bigger Picture: Choosing the Right Method",
    "text": "The Bigger Picture: Choosing the Right Method\nDimensionality reduction is not a one-size-fits-all solution. Different methods make different trade-offs:\n\n\n\n\n\n\n\n\n\n\nMethod\nPreserves\nSpeed\nScalability\nWhen to use\n\n\n\n\nScatter plot matrix\nEverything (2D projections)\nFast\n3-10 dimensions\nExploring moderate-dimensional data\n\n\nPCA\nGlobal variance\nVery fast\nExcellent (1000s of dims)\nLinear structure, interpretability needed\n\n\nMDS\nAll distances\nSlow\nPoor (100s of points)\nDistance preservation critical\n\n\nt-SNE\nLocal structure\nSlow\nModerate (10,000s of points)\nRevealing clusters, local relationships\n\n\nUMAP\nLocal plus some global\nFast\nExcellent (millions of points)\nLarge datasets, faster alternative to t-SNE\n\n\n\nA practical workflow begins with PCA. Always run PCA first. It‚Äôs fast, interpretable, and if it works well, you‚Äôre done. Check how much variance the first 2 or 3 components explain.\nNext, check pairwise plots if feasible. If you have fewer than 10 dimensions, look at scatter plot matrices to understand pairwise relationships.\nTry t-SNE or UMAP if PCA doesn‚Äôt reveal clear structure. Run them if the first 2 PCs explain less than 50 percent variance. Try them if you suspect non-linear relationships. Try them if you want to find clusters.\nValidate your findings with multiple approaches. Don‚Äôt trust a single visualization. Try different random seeds for t-SNE and UMAP. Try different hyperparameters like perplexity and number of neighbors. Try different methods and see if t-SNE and PCA agree. Run statistical tests on apparent clusters.\n\n\n\n\n\n\nTry it yourself\n\n\n\nTake a dataset you‚Äôre familiar with and apply all four methods: PCA, MDS, t-SNE, and UMAP. Compare the results. What structure does each method reveal? What structure does each method hide? Which visualization best matches your intuition about the data?\n\n\nDimensionality reduction can create apparent patterns that don‚Äôt exist in the original data. Spurious clusters appear when t-SNE splits continuous data into false groups. Missing relationships occur when two clusters might be connected in high dimensions but appear separated in 2D. Misleading distances happen when distance and size in t-SNE and UMAP are not meaningful.\nAlways validate important findings with statistical tests or domain knowledge. A beautiful t-SNE plot is a starting point for investigation, not a final conclusion.\nVisualizing high-dimensional data is as much art as science. The goal is not to find ‚Äúthe true projection.‚Äù There is no single true way to flatten high-dimensional space onto a page. The goal is to reveal structure that helps you understand your data and ask better questions.\nAs data scientist Jake VanderPlas wrote: ‚ÄúDimensionality reduction is a form of lossy compression. The question is not whether you lose information‚Äîyou always do‚Äîbut whether you lose the information you care about.‚Äù",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m01-toolkit/tidy-data.html#the-80-problem",
    "href": "m01-toolkit/tidy-data.html#the-80-problem",
    "title": "The Tidy Data Philosophy",
    "section": "The 80% Problem",
    "text": "The 80% Problem\nIt is often said that 80% of data analysis is spent cleaning and preparing data. This isn‚Äôt an exaggeration. Getting your data into the right shape makes everything else easier. The good news is that once you understand the tidy data philosophy, you can apply it consistently across projects.\nIf you want to dive deeper, read Tidy Data by Hadley Wickham.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "The Tidy Data Philosophy"
    ]
  },
  {
    "objectID": "m01-toolkit/reproduceability.html#bringing-it-together",
    "href": "m01-toolkit/reproduceability.html#bringing-it-together",
    "title": "Reproducible Environments & Projects",
    "section": "Bringing It Together",
    "text": "Bringing It Together\nBy combining virtual environments, comprehensive documentation, workflow management, and thoughtful project organization, you‚Äôve now built the foundation for reproducible computational projects. These practices protect your work, enable collaboration, and most importantly, help you trust your own results.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Reproducibility"
    ]
  },
  {
    "objectID": "m03-agentic-coding/overview.html#from-copilot-to-agent",
    "href": "m03-agentic-coding/overview.html#from-copilot-to-agent",
    "title": "Overview",
    "section": "From Copilot to Agent",
    "text": "From Copilot to Agent\nThe shift from copilot to agent is not just an upgrade in model size. It is a fundamental change in how you interact with AI.\nA copilot like GitHub Copilot operates on next-token prediction. It looks at your cursor position and uses the probability distribution P(x_{t+1} | x_{0:t}) to guess the next few characters. It is a smart typewriter. Fast, helpful, but ultimately passive. You write, and it completes. It requires your constant attention and cannot act independently.\nAn agent like Claude Code, Google Antigravity, or Cursor operates on task completion. You give it a high-level goal like ‚ÄúRefactor this module,‚Äù and it engages in a loop of reasoning, action, and observation until the task is done. It reads files, runs terminal commands, calls external APIs, and fixes its own errors. The intelligence doesn‚Äôt come from a larger model. It comes from the feedback loop that allows the agent to observe the consequences of its actions and adjust based on what it learns.\nThis shifts your role fundamentally. You are no longer the writer of syntax. You become the manager-architect. Your job is no longer to know the exact syntax of a matplotlib plot, but to know what plot you need, how to clearly specify that requirement, and how to verify that the agent built it correctly. You move from implementation to orchestration.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Overview"
    ]
  },
  {
    "objectID": "m03-agentic-coding/overview.html#what-youll-build",
    "href": "m03-agentic-coding/overview.html#what-youll-build",
    "title": "Overview",
    "section": "What You‚Äôll Build",
    "text": "What You‚Äôll Build\nThis module introduces three operational layers that power agentic AI.\nIn the hands-on session, you‚Äôll use Google Antigravity to build a functional game and refactor a codebase entirely through natural language instructions. This experience grounds the theory that follows.\nPrompt tuning teaches you how to communicate effectively with LLMs by understanding them as stateless pattern matchers sampling from probability distributions. You‚Äôll structure prompts using instruction, data, format, persona, and context to reliably activate desired patterns. This is the interface layer.\nAgentic AI explains the ReAct loop (Reason + Act) that transforms a passive language model into an autonomous agent. You‚Äôll build a working agent using LangGraph that can query and analyze datasets without human intervention. This is the engine layer.\nContext engineering solves the context window problem. LLMs are brilliant but bounded. They have limited working memory that degrades as it fills. You‚Äôll learn to manage context across its lifecycle: write through scratchpads and memories, select using MCP and just-in-time retrieval, compress through summarization, and isolate using multi-agent architectures. This is the operating system layer.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Overview"
    ]
  },
  {
    "objectID": "m03-agentic-coding/agentic-ai.html#the-loop-where-intelligence-emerges",
    "href": "m03-agentic-coding/agentic-ai.html#the-loop-where-intelligence-emerges",
    "title": "From ChatBot to Agentic AI",
    "section": "The Loop: Where Intelligence Emerges",
    "text": "The Loop: Where Intelligence Emerges\n\n\n\nReAct Loop\n\n\nLet‚Äôs shift your attention from what agents are to how they actually work. An agent is not a smarter chatbot. A chatbot generates text and stops. An agent generates text, parses it for actionable commands, executes those commands in the real world, observes the results, and feeds those results back into the next prompt. The intelligence does not come from the model. It comes from the feedback loop.\nThis is the ReAct Pattern, short for Reason + Act. A chatbot is a pure function: \\text{Output} = \\text{Model}(\\text{Input}). An agent is a state machine:\nwhile not task_complete:\n    observation = get_environment_state()\n    thought = model(observation)\n    action = parse_action(thought)\n    result = execute(action)\n    observation = result  # Feedback loop\nThe critical insight is the feedback loop. If the agent tries to import a missing library and receives a ModuleNotFoundError, the next iteration‚Äôs thought will be ‚ÄúI need to install this library.‚Äù It corrects itself not through introspection, but through collision with reality.\nThe ReAct framework interleaves reasoning and action in three steps. First, the model reasons about the current state (Thought). Second, it outputs a specific command to interact with the environment (Action). Third, the environment executes that command and returns the result (Observation). This cycle repeats until the task is solved.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Agentic AI"
    ]
  },
  {
    "objectID": "m03-agentic-coding/context-engineering.html#the-problem-context-decay",
    "href": "m03-agentic-coding/context-engineering.html#the-problem-context-decay",
    "title": "Context Engineering",
    "section": "The Problem: Context Decay",
    "text": "The Problem: Context Decay\nLLMs are brilliant but bounded. Every model has a context window, the set of tokens available during inference. Think of it as working memory. An LLM has an attention budget that degrades as context grows. As token count increases, the model‚Äôs ability to accurately use that context degrades. This phenomenon is called context rot.\n\n\n\n\n\nThe root cause is architectural. LLMs use the transformer architecture where every token attends to every other token. For n tokens, this creates n^2 pairwise relationships. As context length increases, attention gets stretched thin across these relationships. Models are trained predominantly on shorter sequences, so they have less specialized capacity for long-range dependencies. Position encoding tricks help, but performance still decays.\nThis decay manifests in four failure modes. Context poisoning occurs when a hallucination or error enters the context and influences future outputs. Context distraction happens when the volume of context overwhelms the model‚Äôs training distribution, causing it to lose focus. Context confusion arises when superfluous information nudges the model toward irrelevant responses. Context clash occurs when parts of the context contradict each other, forcing the model to arbitrate between conflicting signals.\nThe naive view treats context engineering as simply writing a better prompt. The reality is much broader. Context engineering is the discipline of managing the entire context lifecycle: what tokens go in, what stays, what gets compressed, and what gets isolated elsewhere. The LLM is like a CPU. The context window is like RAM. Context engineering is the operating system that curates what fits.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Context Engineering"
    ]
  },
  {
    "objectID": "m03-agentic-coding/context-engineering.html#the-four-strategies",
    "href": "m03-agentic-coding/context-engineering.html#the-four-strategies",
    "title": "Context Engineering",
    "section": "The Four Strategies",
    "text": "The Four Strategies\nContext engineering breaks into four strategies: write, select, compress, and isolate. Each addresses a different phase of the context lifecycle. Let‚Äôs walk through them in modern agentic AI tools.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Context Engineering"
    ]
  },
  {
    "objectID": "m03-agentic-coding/context-engineering.html#writing-context",
    "href": "m03-agentic-coding/context-engineering.html#writing-context",
    "title": "Context Engineering",
    "section": "Writing Context",
    "text": "Writing Context\nAgents use scratchpads to offload working memory. Instead of keeping every intermediate step in the context window, an agent writes state like a todo list or summary to an external file or variable. Tools like Claude Code, Gemini CLI, Antigravity, and Cursor use this pattern to track multi-step tasks across context resets, maintaining coherence without bloating the prompt.\nFor long-term memory, agents rely on persistent rules files like AGENTS.md, CLAUDE.md, or .cursorrules. These act as procedural memory, storing project-specific instructions that are injected into the context at the start of a session or retrieved when relevant. Here is a sample AGENTS.md file:\n## Project: `AgenticFlow` - Context Engineering Demo\n\n###  Project Goal\nDemonstrate advanced context engineering for LLM agents: writing, selecting, compressing, isolating context to optimize performance and ensure robust multi-step workflows.\n\n### Agent Persona & Principles\n-   **Role**: Senior AI Engineer/Architect.\n-   **Objective**: Develop efficient, reliable, maintainable agentic workflows.\n-   **Style**: Clear, concise, technical, solution-oriented. Justify decisions.\n-   **Principles**: Context Efficiency, Modularity, Transparency, Robustness, Iteration.\n\n### Technical Guidelines\n-   **Language**: Python 3.9+.\n-   **Libraries**: `pydantic`, `pytest`, `black`. `langchain`/`llamaindex` for orchestration (use sparingly).\n-   **Dev Practices**: Git (conventional commits), Markdown/docstring documentation, comprehensive error handling, explicit tool use.\n\n...(continue)\n\nThe AGENTS.md file is an industry standard for agentic AI workflows. It serves as the agent‚Äôs procedural memory, storing project-specific instructions, guidelines, and foundational context. This content is automatically injected into the context window at the start of a session or retrieved on demand. The result is consistent behavior, reduced redundant prompting, and coherence across complex, multi-step workflows. Create an AGENTS.md when you start any new project.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Context Engineering"
    ]
  },
  {
    "objectID": "m03-agentic-coding/context-engineering.html#installing-context7-in-google-antigravity",
    "href": "m03-agentic-coding/context-engineering.html#installing-context7-in-google-antigravity",
    "title": "Context Engineering",
    "section": "Installing Context7 in Google Antigravity",
    "text": "Installing Context7 in Google Antigravity\nGoogle Antigravity connects to external MCP servers through a configuration file. The server exposes tools that the agent calls on demand. Let‚Äôs wire Context7 into Antigravity so the agent can fetch up-to-date library docs without polluting the context window.\nFirst, open Google Antigravity and navigate to the MCP Store. Click Manage MCP Servers at the top, then click View raw config in the main tab. This opens mcp_config.json, which controls all external tools available to your agent.\n\n\n\n\n\n\n\n\n\n\nAdd this block to your mcp_config.json:\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\nThe API key is optional but recommended for higher rate limits. Get one at context7.com/dashboard. Without it, you have access to the free tier.\nSave the config and refresh the MCP servers panel in Antigravity. Two new tools should appear: resolve-library-id to map a library name to its identifier, and get-library-docs to fetch documentation for a resolved library ID.\nNow prompt the agent with:\nUse context7 to get the latest documentation for pandas 2.0 DataFrame.plot() method.\nBehind the scenes, the agent calls resolve-library-id with ‚Äúpandas‚Äù to get the library ID. Then it calls get-library-docs with that ID and your query to retrieve current API docs. Finally, it uses those docs to generate accurate code. The documentation never enters your prompt. The agent retrieves it on demand, uses it, and discards it. Your context window remains clean.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Context Engineering"
    ]
  },
  {
    "objectID": "m03-agentic-coding/prompt-tuning.html#why-phrasing-matters",
    "href": "m03-agentic-coding/prompt-tuning.html#why-phrasing-matters",
    "title": "Prompt Tuning",
    "section": "Why Phrasing Matters",
    "text": "Why Phrasing Matters\nIf a machine can answer questions, it should respond consistently regardless of phrasing. You are asking for the same information, so the answer should not change. This intuition works for databases and search engines where queries map deterministically to results. We expect robustness to variation.\nLLMs shatter this expectation. Ask ‚ÄúSummarize this abstract‚Äù and get a concise two-sentence summary. Ask ‚ÄúWhat is this abstract about?‚Äù and get three rambling paragraphs. Same content, different phrasing, completely different outputs. This is not a bug. It is fundamental to how LLMs work. They do not retrieve information. They sample from probability distributions conditioned on your exact phrasing. Every word in your prompt shifts the distribution.\nLLMs are simultaneously powerful and brittle. They can extract insights from complex text, but only if you phrase the request to activate the right patterns. Prompt engineering is the discipline of designing inputs that reliably activate desired patterns across varied tasks.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Prompt Tuning"
    ]
  },
  {
    "objectID": "m03-agentic-coding/prompt-tuning.html#how-llms-actually-work",
    "href": "m03-agentic-coding/prompt-tuning.html#how-llms-actually-work",
    "title": "Prompt Tuning",
    "section": "How LLMs Actually Work",
    "text": "How LLMs Actually Work\nImagine a word association game. Someone says ‚Äúcapital‚Äù and you must say the next word. If the previous sentence was ‚ÄúThe capital of France is,‚Äù you say ‚ÄúParis.‚Äù If it was ‚ÄúWe need more capital to,‚Äù you say ‚Äúfund‚Äù or ‚Äúinvest.‚Äù The word ‚Äúcapital‚Äù does not have one fixed meaning. It activates different patterns depending on context. LLMs work identically, but at massive scale.\nWhen you submit a prompt, the model converts it into tokens and embeds those tokens in high-dimensional space. Each token‚Äôs position depends on surrounding tokens. Context shapes meaning. The model then samples the next token from a probability distribution over its vocabulary, conditioned on all previous tokens. It repeats this process until it generates a complete response.\nYour exact phrasing determines which region of probability space the model occupies when it begins sampling. Slightly different prompts place the model in different regions where different tokens have high probability. Adding ‚ÄúThink step by step‚Äù shifts the distribution toward reasoning patterns because training data contains many examples where that phrase preceded structured reasoning. Adding ‚ÄúYou are an expert researcher‚Äù shifts toward formal, technical language. Specifying ‚ÄúOutput format: Domain: ‚Ä¶, Methods: ‚Ä¶‚Äù shifts toward structured extraction patterns. The model has no internal representation of what you ‚Äúreally want.‚Äù It only knows which tokens tend to follow which other tokens in which contexts. Prompt engineering exploits this by deliberately activating patterns that produce desired outputs.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Prompt Tuning"
    ]
  },
  {
    "objectID": "m03-agentic-coding/prompt-tuning.html#building-effective-prompts",
    "href": "m03-agentic-coding/prompt-tuning.html#building-effective-prompts",
    "title": "Prompt Tuning",
    "section": "Building Effective Prompts",
    "text": "Building Effective Prompts\n\n\n\n\n\nEffective prompts activate desired patterns by combining structural components that mirror patterns in training data. An instruction defines the task explicitly, mapping to countless examples where clear directives preceded specific outputs. Data provides the input to process. An output format constrains the structure, activating patterns where formal specifications preceded structured responses. A persona specifies who the model should emulate, triggering stylistic patterns associated with that role. Context provides background information about why the task matters, who the response serves, and relevant constraints. This helps the model select appropriate patterns from ambiguous alternatives.\nNot every component is necessary. Simple extraction tasks need only instruction, data, and format. Style-sensitive tasks benefit from persona. Complex scenarios with ambiguity require context to disambiguate. The strategy is to provide exactly enough structure to activate the desired pattern without overloading the prompt with irrelevant information.\nLet‚Äôs build a prompt progressively, adding components one at a time to observe how each shifts the output distribution.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Prompt Tuning"
    ]
  },
  {
    "objectID": "m03-agentic-coding/prompt-tuning.html#building-from-instruction-and-data",
    "href": "m03-agentic-coding/prompt-tuning.html#building-from-instruction-and-data",
    "title": "Prompt Tuning",
    "section": "Building from Instruction and Data",
    "text": "Building from Instruction and Data\nThe most basic prompt consists of an instruction that defines the task and data that provides the input:\n\ninstruction = \"Summarize this abstract\"\ndata = \"\"\"\nWe develop a graph neural network for predicting protein-protein interactions\nfrom sequence data. Our model uses attention mechanisms to identify functionally\nimportant amino acid subsequences. We achieve 89% accuracy on benchmark datasets,\noutperforming previous methods by 7%. The model also provides interpretable\nattention weights showing which protein regions drive predictions.\n\"\"\"\n\nprompt = f\"{instruction}. {data}\"\n\n\n\nCode\nimport ollama\n\nparams_llm = {\"model\": \"gemma3:270m\", \"options\": {\"temperature\": 0.3}}\n\nresponse = ollama.generate(prompt=prompt, **params_llm)\nprint(response.response)\n\n\nThis abstract describes a graph neural network (GNN) for predicting protein-protein interactions. The model utilizes attention mechanisms to identify functionally important amino acid subsequences. The network achieves 89% accuracy on benchmark datasets and provides interpretable attention weights, indicating its effectiveness.\n\n\n\nThis basic prompt works, but output varies. The model might produce a long summary, a short one, or change format across runs. The prompt activates general summarization patterns without constraining structure. Watch what happens when we add an output format specification to narrow the distribution:\n\noutput_format = \"\"\"Provide the summary in exactly 2 sentences:\n- First sentence: What problem and method\n- Second sentence: Key result with numbers\"\"\"\n\nprompt_with_format = f\"\"\"{instruction}. {data}. {output_format}\"\"\"\n\nThe output format constraint produces structured, consistent output by activating patterns where format specifications preceded conforming responses. This becomes critical when processing hundreds of papers. You need programmatically parseable structure, not freeform text.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Prompt Tuning"
    ]
  },
  {
    "objectID": "m03-agentic-coding/prompt-tuning.html#adding-persona-to-control-style",
    "href": "m03-agentic-coding/prompt-tuning.html#adding-persona-to-control-style",
    "title": "Prompt Tuning",
    "section": "Adding Persona to Control Style",
    "text": "Adding Persona to Control Style\nA persona tells the LLM who it should emulate, activating stylistic patterns associated with that role in training data. Imagine a customer support scenario where tone matters:\n\n# New example for persona demonstration\ninstruction = \"Help the customer reconnect to the service by providing troubleshooting instructions.\"\ndata = \"Customer: I cannot see any webpage. Need help ASAP!\"\noutput_format = \"Keep the response concise and polite. Provide a clear resolution in 2-3 sentences.\"\n\nformal_persona = \"You are a professional customer support agent who responds formally and ensures clarity and professionalism.\"\n\nprompt_with_persona = f\"\"\"{formal_persona}. {instruction}. {data}. {output_format}\"\"\"\n\n\n\nCode\nprint(\"BASE (no persona):\")\nprint(ollama.generate(prompt=instruction + \". \" + data + \". \" + output_format, **params_llm).response)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nprint(\"WITH PERSONA:\")\nprint(ollama.generate(prompt=prompt_with_persona, **params_llm).response)\n\n\nBASE (no persona):\nOkay, I understand. Let's try to figure out what's happening. Please try to re-enter the webpage. If that doesn't work, let me know what steps you've already taken to troubleshoot the issue.\n\n\n============================================================\n\nWITH PERSONA:\nHello, I understand you cannot see any webpage. Could you please try re-typing the URL? I'm here to assist you with any troubleshooting steps you need.\n\n\n\nThe persona shifts tone and style. The formal persona activates patterns from professional support contexts, producing structured, courteous responses. Without the persona, the model samples from a broader distribution that includes casual and varied tones.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Prompt Tuning"
    ]
  },
  {
    "objectID": "m03-agentic-coding/prompt-tuning.html#adding-context-to-disambiguate",
    "href": "m03-agentic-coding/prompt-tuning.html#adding-context-to-disambiguate",
    "title": "Prompt Tuning",
    "section": "Adding Context to Disambiguate",
    "text": "Adding Context to Disambiguate\nContext provides additional information that helps the model select appropriate patterns when multiple valid interpretations exist. Context includes background information explaining why the task matters, audience information specifying who the response serves, and constraints defining special circumstances. Let‚Äôs add background urgency:\n\ncontext_background = \"\"\"The customer is extremely frustrated because their internet has been down for three days, and they need it for an important online job interview. They emphasize that 'This is a life-or-death situation for my career!'\"\"\"\n\nprompt_with_context = f\"\"\"{formal_persona}. {instruction}. {data}. {output_format}. Context: {context_background}\"\"\"\n\n\n\nCode\nprint(\"WITH PERSONA:\")\nprint(ollama.generate(prompt=prompt_with_persona, **params_llm).response)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nprint(\"WITH PERSONA + CONTEXT (background):\")\nprint(ollama.generate(prompt=prompt_with_context, **params_llm).response)\n\n\nWITH PERSONA:\nOkay, I understand. I'm here to assist you with your webpage issue. Please try to re-enter the webpage. I'll do my best to find a solution for you within the next 2-3 minutes.\n\n\n============================================================\n\nWITH PERSONA + CONTEXT (background):\nDear [Customer Name],\n\nI understand your frustration with your internet connection and the need for this important job interview. I'm here to assist you in troubleshooting the issue. Please try the following steps:\n1. Check your internet connection.\n2. Try restarting your modem and router.\n3. If the problem persists, please contact our technical support team.\n\nWe'll get this resolved for you as quickly as possible.\n\n\n\nBackground context adds urgency and emotional weight, activating patterns where high-stakes situations preceded empathetic, prioritized responses. The model does not understand emotion, but it has seen urgency markers correlate with specific response patterns.\nAudience information creates even more dramatic shifts. Watch what happens when we tailor the response for different technical levels:\n\n# Context with audience information for non-technical user\ncontext_with_audience_nontech = f\"\"\"{context_background} The customer does not know any technical terms like modem, router, networks, etc.\"\"\"\n\ncontext_with_audience_tech = f\"\"\"{context_background} The customer is Head of IT Infrastructure of our company.\"\"\"\n\nprompt_with_context_nontech = f\"\"\"{formal_persona}. {instruction}. {data}. {output_format}. Context: {context_with_audience_nontech}\"\"\"\nprompt_with_context_tech = f\"\"\"{formal_persona}. {instruction}. {data}. {output_format}. Context: {context_with_audience_tech}\"\"\"\n\n\n\nCode\nprint(\"WITH PERSONA + CONTEXT (background only):\")\nprint(ollama.generate(prompt=prompt_with_context, **params_llm).response)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nprint(\"WITH PERSONA + CONTEXT (background + non-tech audience):\")\nprint(ollama.generate(prompt=prompt_with_context_nontech, **params_llm).response)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nprint(\"WITH PERSONA + CONTEXT (background + tech audience):\")\nprint(ollama.generate(prompt=prompt_with_context_tech, **params_llm).response)\n\n\nWITH PERSONA + CONTEXT (background only):\nThank you for contacting us. I understand your frustration with the internet outage and the need for this vital job interview. I'm here to assist you in troubleshooting the issue and providing you with the necessary information. Please let me know if you have any further questions.\n\n\n============================================================\n\nWITH PERSONA + CONTEXT (background + non-tech audience):\n\"I understand your frustration, and I apologize for the inconvenience this is causing. To help me troubleshoot this, could you please tell me what specific webpage you're having trouble with? Knowing the exact URL will help me pinpoint the issue.\"\n\n\n============================================================\n\nWITH PERSONA + CONTEXT (background + tech audience):\n\"I understand your frustration, and I apologize for the inconvenience this is causing. To help me assist you, could you please provide me with the exact URL of the webpage you're having trouble seeing? I'll do my best to troubleshoot this for you.\"\n\n\nAudience information dramatically shifts technical level and terminology. For non-technical users, the response avoids jargon because training data contains many examples where ‚Äúdoes not know technical terms‚Äù preceded simplified explanations. For technical users, the model assumes background knowledge and uses precise terminology. The mechanism is identical. The patterns activated are different.\nThe complete template combines all components:\n\nprompt_template = \"\"\"\n{persona}\n\n{instruction}\n\n{data}\n\nContext: {context}\n\n{output_format}\n\"\"\"\n\nNot every prompt needs every component. Simple extraction tasks need only instruction, data, and output format. Style-sensitive tasks benefit from persona. Complex scenarios with ambiguity require context.\nResearch shows that personas can improve tone and style but do not necessarily improve performance on factual tasks. In some cases, personas may degrade performance or introduce biases. Use personas when you need specific tone, style tailored to an audience, or a particular perspective. Avoid personas when you need maximum factual accuracy, the task is purely extraction or classification, or you are concerned about bias introduction. When prompted to adopt specific socio-demographic personas, LLMs may produce responses that reflect societal stereotypes. Be careful when designing persona prompts to avoid reinforcing harmful biases.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Prompt Tuning"
    ]
  },
  {
    "objectID": "m03-agentic-coding/prompt-tuning.html#sampling-multiple-times-for-consistency",
    "href": "m03-agentic-coding/prompt-tuning.html#sampling-multiple-times-for-consistency",
    "title": "Prompt Tuning",
    "section": "Sampling Multiple Times for Consistency",
    "text": "Sampling Multiple Times for Consistency\nFor tasks requiring reasoning, generating multiple responses and selecting the most common answer often improves accuracy. The technique is called self-consistency. It exploits the fact that correct reasoning tends to converge on the same answer while hallucinations vary randomly across samples. Let‚Äôs define the prompt:\n\nfrom collections import Counter\n\nprompt_consistency = \"\"\"Three papers study network robustness:\n- Paper A: Targeted attacks are most damaging\n- Paper B: Random failures rarely cause collapse\n- Paper C: Hub nodes are critical for robustness\n\nWhat is the research consensus on network robustness? Give a one-sentence answer.\n\"\"\"\n\nGenerate multiple responses with higher temperature to increase diversity, then identify the most common answer:\n\n\nCode\n# Use higher temperature for diversity\nparams_creative = {\"model\": \"gemma3n:latest\", \"options\": {\"temperature\": 0.7}}\n\n# Generate 5 responses\nresponses = []\nfor i in range(5):\n    response = ollama.generate(prompt=prompt_consistency, **params_creative)\n    responses.append(response.response.strip())\n    print(f\"Response {i+1}: {responses[-1]}\\n\")\n\n# In practice, you'd programmatically identify the most common theme\nprint(\"The most consistent theme across responses would be selected.\")\n\n\nResponse 1: The research consensus on network robustness is that it's a complex issue influenced by both targeted attacks and random failures, with the vulnerability of critical hub nodes playing a significant role in overall network resilience.\n\nResponse 2: The research consensus on network robustness is that it's a complex issue influenced by both targeted attacks and random failures, with the vulnerability of critical hub nodes playing a significant role in overall network resilience.\n\nResponse 3: The research consensus on network robustness is that it's a multifaceted issue, with targeted attacks posing a significant threat, random failures are generally less catastrophic, and the presence of critical hub nodes heavily influences overall network resilience.\n\nResponse 4: The research consensus on network robustness is that it's a complex issue influenced by both targeted attacks and random failures, with the vulnerability of critical hub nodes playing a significant role in overall network resilience.\n\nResponse 5: The research consensus on network robustness is that it's a multifaceted issue, with targeted attacks posing a significant threat, random failures are generally less impactful, and the presence of critical hub nodes heavily influences overall network resilience.\n\nThe most consistent theme across responses would be selected.\n\n\nSelf-consistency works because correct reasoning patterns converge toward the same conclusion when sampled multiple times while fabricated details vary randomly. The tradeoff is significant. Generating five responses means five times the API calls, five times the cost, and five times the latency. Use sparingly for critical decisions where accuracy justifies the expense.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Prompt Tuning"
    ]
  },
  {
    "objectID": "m04-text/overview.html#large-language-models-in-practice",
    "href": "m04-text/overview.html#large-language-models-in-practice",
    "title": "Overview",
    "section": "Large Language Models in Practice",
    "text": "Large Language Models in Practice\nWe start by interacting with the giants. You‚Äôll explore what LLMs are, how they work at a high level, and how to control them effectively. See Large Language Models in Practice.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Overview"
    ]
  },
  {
    "objectID": "m04-text/overview.html#the-mechanics-of-meaning",
    "href": "m04-text/overview.html#the-mechanics-of-meaning",
    "title": "Overview",
    "section": "The Mechanics of Meaning",
    "text": "The Mechanics of Meaning\nHow do computers read? We‚Äôll dive into the tokenization process and the architecture that makes it all possible: the Transformer. The key insight is that meaning emerges through context, not from isolated words. Explore Tokenization: Unboxing How LLMs Read Text, Transformers, and BERT, GPT, and Sentence Transformers.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Overview"
    ]
  },
  {
    "objectID": "m04-text/overview.html#vector-space-models",
    "href": "m04-text/overview.html#vector-space-models",
    "title": "Overview",
    "section": "Vector Space Models",
    "text": "Vector Space Models\nWe‚Äôll uncover the mathematical foundation of modern NLP by representing words as vectors in high-dimensional space where meaning is geometric. Discover how Word Embeddings capture semantic relationships, learn Meaning as Direction with SemAxis, and examine Word Bias in learned representations.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Overview"
    ]
  },
  {
    "objectID": "m04-text/tokenization.html#why-not-just-words",
    "href": "m04-text/tokenization.html#why-not-just-words",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "Why Not Just Words?",
    "text": "Why Not Just Words?\nYou might assume an LLM reads text the way you do, word by word, treating each word as an atomic unit. This assumption is wrong. The model operates on tokens, which are subword chunks. These could be full words like ‚Äúthe‚Äù, word parts like ‚Äúingham‚Äù, or single characters like ‚ÄúB‚Äù. This choice isn‚Äôt arbitrary. It‚Äôs a geometric compression strategy.\nIf we used whole words, the vocabulary would balloon to millions of entries. Each entry requires a row in the embedding matrix, so memory scales linearly with vocabulary size. A 1-million-word vocabulary with 2048-dimensional embeddings would require over 8GB just for the lookup table. Subword tokenization collapses this problem by focusing on frequently occurring fragments. With roughly 50,000 subwords, the model reconstructs both common words (stored as single tokens) and rare words (assembled from parts). The system trades a small increase in sequence length for massive reductions in memory and computational overhead.\nThis compression also explains a quirk: why LLMs sometimes fail at seemingly trivial tasks like counting letters. The word ‚Äústrawberry‚Äù might tokenize as [‚Äústraw‚Äù, ‚Äúberry‚Äù], meaning the model never sees individual ‚Äúr‚Äù characters as separate units. It‚Äôs not stupidity, it‚Äôs compression artifacts.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m04-text/tokenization.html#how-tokenization-works-in-practice",
    "href": "m04-text/tokenization.html#how-tokenization-works-in-practice",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "How Tokenization Works in Practice",
    "text": "How Tokenization Works in Practice\nLet‚Äôs unbox an actual tokenizer from Hugging Face and trace the pipeline from raw text to embeddings. We‚Äôll use Phi-1.5, a compact model from Microsoft. For tokenization experiments, we only need the tokenizer itself, not the full multi-gigabyte model.\n\nfrom transformers import AutoTokenizer\nimport os\n\nmodel_name = \"microsoft/phi-1.5\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nLet‚Äôs inspect the tokenizer‚Äôs constraints.\n\n\nCode\nprint(f\"Vocabulary size: {tokenizer.vocab_size:,} tokens\")\nprint(f\"Max sequence length: {tokenizer.model_max_length} tokens\")\n\n\nVocabulary size: 50,257 tokens\nMax sequence length: 2048 tokens\n\n\nThis tokenizer knows 50,257 unique tokens and enforces a maximum sequence length of 2048 tokens. If your input exceeds this limit, the model will truncate or reject it. This is a hard boundary imposed by the positional encoding system, not a soft guideline.\n\nFrom Text to Tokens\nTokenization splits text into the subword fragments the model actually processes. Watch what happens when we tokenize a university name.\n\ntext = \"Binghamton University.\"\n\ntokens = tokenizer.tokenize(text)\n\n\n\nCode\nprint(f\"Tokens: {tokens}\")\n\n\nTokens: ['B', 'ingham', 'ton', 'ƒ†University', '.']\n\n\nThe rare word ‚ÄúBinghamton‚Äù fractures into [‚ÄòB‚Äô, ‚Äòingham‚Äô, ‚Äòton‚Äô]. The common word ‚ÄúUniversity‚Äù survives intact (with a leading space marker). The tokenizer learned these splits from frequency statistics during training. High-frequency words get dedicated tokens; rare words get decomposed into reusable parts.\nThe ƒ† character (U+0120) is a GPT-style tokenizer convention for encoding spaces. When you see ƒ†University, it means ‚ÄúUniversity‚Äù preceded by a space. This preserves word boundaries while allowing subword splits.\nLet‚Äôs test a few more examples to see the pattern.\n\n\nCode\ntexts = [\n    \"Bearcats\",\n    \"New York\",\n]\n\nprint(\"Word tokenization examples:\\n\")\nfor text in texts:\n    tokens = tokenizer.tokenize(text)\n    print(f\"{text:10s} ‚Üí {tokens}\")\n\n\nWord tokenization examples:\n\nBearcats   ‚Üí ['Bear', 'cats']\nNew York   ‚Üí ['New', 'ƒ†York']\n\n\n‚ÄúBearcats‚Äù splits because it‚Äôs domain-specific jargon. ‚ÄúNew York‚Äù remains whole because it‚Äôs common. The tokenizer‚Äôs behavior directly reflects its training corpus.\nCheck out OpenAI‚Äôs tokenizer to see how different models slice the same text differently.\n\n\nFrom Tokens to Token IDs\nTokens are still strings. The model needs integers. Each token maps to a unique ID in the vocabulary dictionary.\n\n\nCode\ntext = \"Binghamton University\"\n\n# Get token IDs\ntoken_ids = tokenizer.encode(text, add_special_tokens=False)\ntokens = tokenizer.tokenize(text)\n\nprint(\"Token ‚Üí Token ID mapping:\\n\")\nfor token, token_id in zip(tokens, token_ids):\n    print(f\"{token:10s} ‚Üí {token_id:6d}\")\n\n\nToken ‚Üí Token ID mapping:\n\nB          ‚Üí     33\ningham     ‚Üí  25875\nton        ‚Üí   1122\nƒ†University ‚Üí   2059\n\n\nEach token receives a unique integer ID. The vocabulary is a dictionary mapping token strings to integer IDs. Let‚Äôs peek inside.\n\n# Get the full vocabulary\nvocab = tokenizer.get_vocab()\n\n# Sample some tokens\nsample_tokens = list(vocab.items())[:5]\nfor token, id in sample_tokens:\n    print(f\"  {id:6d}: '{token}'\")\n\n   43503: 'ƒ†Lime'\n   29516: 'VO'\n   41002: 'ƒ†UTF'\n   41733: 'Ku'\n   33793: 'ƒ†indent'\n\n\nMost LLMs reserve special tokens for sequence boundaries or control signals. Phi-1.5 uses &lt;|endoftext|&gt; as a separator during training. Let‚Äôs verify.\n\ntoken_id = [50256]\ntoken = tokenizer.convert_ids_to_tokens(token_id)[0]\nprint(f\"Token ID: {token_id} ‚Üí Token: {token}\")\n\nToken ID: [50256] ‚Üí Token: &lt;|endoftext|&gt;\n\n\nToken ID 50256 is Phi-specific. Other models use different conventions (BERT uses [SEP] and [CLS]). Always check your tokenizer‚Äôs special tokens before preprocessing data.\n\n\nFrom Token IDs to Embeddings\n\nNow we need the full model to access the embedding layer, the matrix that converts token IDs into dense vectors.\n\nfrom transformers import AutoModelForCausalLM\nimport torch\n\n# Load the model\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Retrieve the embedding layer\nembedding_layer = model.model.embed_tokens\n\nThe embedding layer is a simple lookup table: a 51,200 √ó 2,048 matrix where each row is the embedding for a token in the vocabulary. Let‚Äôs examine the first few entries.\n\n\nCode\nprint(embedding_layer.weight[:5, :10])\n\n\ntensor([[ 0.0097, -0.0155,  0.0603,  0.0326, -0.0108, -0.0271, -0.0273,  0.0178,\n         -0.0242,  0.0100],\n        [ 0.0243,  0.0543,  0.0178, -0.0679, -0.0130,  0.0265, -0.0423, -0.0287,\n         -0.0051, -0.0179],\n        [-0.0416,  0.0370, -0.0160, -0.0254, -0.0371, -0.0208, -0.0023,  0.0647,\n          0.0468,  0.0013],\n        [-0.0051, -0.0044,  0.0248, -0.0489,  0.0399,  0.0005, -0.0070,  0.0148,\n          0.0030,  0.0070],\n        [ 0.0289,  0.0362, -0.0027, -0.0775, -0.0136, -0.0203, -0.0566, -0.0558,\n          0.0269, -0.0067]], grad_fn=&lt;SliceBackward0&gt;)\n\n\nThese numbers are learned parameters, optimized during training to capture semantic relationships. Token IDs are discrete symbols; embeddings are continuous coordinates in a 2048-dimensional space. This is what the transformer layers operate on.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m04-text/tokenization.html#the-full-pipeline",
    "href": "m04-text/tokenization.html#the-full-pipeline",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "The Full Pipeline",
    "text": "The Full Pipeline\nYou‚Äôve now traced the complete pipeline: raw text fractures into subword tokens, tokens map to integer IDs, and IDs retrieve vector embeddings from a learned matrix. This tokenization step is foundational. Without it, the model cannot begin processing language. The transformer layers come next, using attention mechanisms to extract patterns from these embeddings.\nRemember three key constraints. First, LLMs operate on subwords, not words, because vocabulary size is a memory bottleneck. Second, tokenization is learned from data, not hand-designed, so different models split text differently. Third, compression has side effects. Tasks like character counting fail because the model never sees individual characters as atomic units.\nWith this machinery exposed, we‚Äôre ready to examine the transformer itself. It‚Äôs the architecture that processes these embeddings and enables LLMs to predict the next token.\n\nNext: Transformers: The Architecture Behind the Magic",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m04-text/word-embeddings.html#words-as-relationships-not-containers",
    "href": "m04-text/word-embeddings.html#words-as-relationships-not-containers",
    "title": "Word Embeddings",
    "section": "Words as Relationships, Not Containers",
    "text": "Words as Relationships, Not Containers\nWe intuitively assume words are containers for meaning. ‚ÄúDog‚Äù holds the concept of a canine. This is incorrect. Structural linguistics reveals that a sign is defined solely by its relationships. ‚ÄúDog‚Äù means ‚Äúdog‚Äù only because it is not ‚Äúcat‚Äù, ‚Äúwolf‚Äù, or ‚Äúlog‚Äù. Meaning is differential, not intrinsic.\n\n\n\n\n\n\nFigure¬†1: Green is the color that is not non-green (not red, not blue, not yellow, etc.).\n\n\n\nWord2Vec, the foundational model grounding modern NLP, learns to map the statistical topology of language. Think of it like mapping a city based purely on traffic data. You don‚Äôt know what a ‚Äúschool‚Äù is, but you see that ‚Äúbuses‚Äù and ‚Äúchildren‚Äù congregate there at 8 AM. By placing these entities close together on a map, you reconstruct the city‚Äôs functional structure. Word2Vec does this for language, turning semantic proximity into geometric distance.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Embeddings"
    ]
  },
  {
    "objectID": "m04-text/word-embeddings.html#exploring-word2vec",
    "href": "m04-text/word-embeddings.html#exploring-word2vec",
    "title": "Word Embeddings",
    "section": "Exploring Word2Vec",
    "text": "Exploring Word2Vec\nLet‚Äôs first experience the power of Word2Vec, then understand how it works. We‚Äôll use a pre-trained model trained on 100 billion words of Google News. We aren‚Äôt teaching it anything; we‚Äôre simply inspecting the map it created.\n\nimport gensim.downloader as api\nimport numpy as np\n\n# Load pre-trained Word2vec embeddings\nprint(\"Loading Word2vec model...\")\nmodel = api.load(\"word2vec-google-news-300\")\nprint(f\"Loaded embeddings for {len(model):,} words.\")\n\nLoading Word2vec model...\nLoaded embeddings for 3,000,000 words.\n\n\nIf the map is accurate, ‚Äúdog‚Äù should be surrounded by its semantic kin. We query the nearest neighbors in the vector space.\n\nsimilar_to_dog = model.most_similar(\"dog\", topn=10)\n\nprint(\"Words most similar to 'dog':\")\nfor word, similarity in similar_to_dog:\n    print(f\"  {word:20s} {similarity:.3f}\")\n\nWords most similar to 'dog':\n  dogs                 0.868\n  puppy                0.811\n  pit_bull             0.780\n  pooch                0.763\n  cat                  0.761\n  golden_retriever     0.750\n  German_shepherd      0.747\n  Rottweiler           0.744\n  beagle               0.742\n  pup                  0.741\n\n\nThe model groups ‚Äúdog‚Äù with ‚Äúdogs‚Äù, ‚Äúpuppy‚Äù, and ‚Äúpooch‚Äù not because it knows biology, but because they are statistically interchangeable in sentences. Since words are vectors, we can perform arithmetic on meaning. The relationship between ‚ÄúKing‚Äù and ‚ÄúMan‚Äù is a vector. If we add that vector to ‚ÄúWoman‚Äù, we should arrive at ‚ÄúQueen‚Äù.\n \\vec{\\text{King}} - \\vec{\\text{Man}} + \\vec{\\text{Woman}} \\approx \\vec{\\text{Queen}} \n\nresult = model.most_similar(\n  positive=['king', 'woman'],\n   negative=['man'], topn=5\n)\n\nprint(\"king - man + woman =\")\nfor word, similarity in result:\n    print(f\"  {word:15s} {similarity:.3f}\")\n\nking - man + woman =\n  queen           0.712\n  monarch         0.619\n  princess        0.590\n  crown_prince    0.550\n  prince          0.538\n\n\nWe cannot see in 300 dimensions, but we can project the space down to 2D using PCA. This reveals consistent structures like the ‚Äúcapital city‚Äù relationship that the model has learned.\n\n\nCode\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ncountries = [\"Germany\", \"France\", \"Italy\", \"Spain\", \"Portugal\", \"Greece\"]\ncapitals = [\"Berlin\", \"Paris\", \"Rome\", \"Madrid\", \"Lisbon\", \"Athens\"]\n\n# Get embeddings\ncountry_embeddings = np.array([model[country] for country in countries])\ncapital_embeddings = np.array([model[capital] for capital in capitals])\n\n# PCA to 2D\npca = PCA(n_components=2)\nembeddings = np.vstack([country_embeddings, capital_embeddings])\nembeddings_pca = pca.fit_transform(embeddings)\n\n# Create DataFrame\ndf = pd.DataFrame(embeddings_pca, columns=[\"PC1\", \"PC2\"])\ndf[\"Label\"] = countries + capitals\ndf[\"Type\"] = [\"Country\"] * len(countries) + [\"Capital\"] * len(capitals)\n\n# Plot\nfig, ax = plt.subplots(figsize=(12, 10))\n\nfor idx, row in df.iterrows():\n    color = \"#e74c3c\" if row[\"Type\"] == \"Country\" else \"#3498db\"\n    marker = \"o\" if row[\"Type\"] == \"Country\" else \"s\"\n    ax.scatter(\n        row[\"PC1\"],\n        row[\"PC2\"],\n        c=color,\n        marker=marker,\n        s=200,\n        edgecolors=\"black\",\n        linewidth=1.5,\n        alpha=0.7,\n        zorder=3,\n    )\n    ax.text(\n        row[\"PC1\"],\n        row[\"PC2\"] + 0.15,\n        row[\"Label\"],\n        fontsize=12,\n        ha=\"center\",\n        va=\"bottom\",\n        fontweight=\"bold\",\n        bbox=dict(facecolor=\"white\", edgecolor=\"none\", alpha=0.8),\n    )\n\n# Draw arrows\nfor i in range(len(countries)):\n    country_pos = df.iloc[i][[\"PC1\", \"PC2\"]].values\n    capital_pos = df.iloc[i + len(countries)][[\"PC1\", \"PC2\"]].values\n    ax.arrow(\n        country_pos[0],\n        country_pos[1],\n        capital_pos[0] - country_pos[0],\n        capital_pos[1] - country_pos[1],\n        color=\"gray\",\n        alpha=0.6,\n        linewidth=2,\n        head_width=0.15,\n        head_length=0.1,\n        zorder=2,\n    )\n\nax.set_title(\n    'The \"Capital Of\" Relationship as Parallel Transport',\n    fontsize=16,\n    fontweight=\"bold\",\n    pad=20,\n)\nax.grid(alpha=0.3, linestyle=\"--\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nThe ‚ÄòCapital Of‚Äô relationship appears as a consistent direction in vector space.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Embeddings"
    ]
  },
  {
    "objectID": "m04-text/word-embeddings.html#how-word2vec-learns-meaning",
    "href": "m04-text/word-embeddings.html#how-word2vec-learns-meaning",
    "title": "Word Embeddings",
    "section": "How Word2Vec Learns Meaning",
    "text": "How Word2Vec Learns Meaning\nWe intuitively treat words as containers that hold meaning. ‚ÄúGreen‚Äù contains the visual concept of a specific color. This is incorrect. Nature presents us with a messy, continuous spectrum without hard borders. Language is simply the set of arbitrary cuts we make in that continuum to create order.\nWord2Vec operationalizes this by treating meaning as a game of contrast. It functions as a pair of linguistic scissors. It does not learn what a word is by looking up a definition. It learns what a word is like by pulling it close to neighbors, and more importantly, it learns what a word is not by pushing it away from random noise. The meaning of ‚ÄúGreen‚Äù is simply the geometric region that remains after we have pushed away ‚ÄúRed‚Äù, ‚ÄúPurple‚Äù, and ‚ÄúBanana‚Äù.\n\n\n\n\n\n\nFigure¬†2: Starting from initially random vectors, word2vec learns iteratively to push away the words that are not related and pull words that are related. The resulting vector space is a map of the relationships between words.\n\n\n\nThis process relies on a technique called contrastive learning. We cannot teach the model the exact meaning of each word, but we can let it learn the relationship between words through a binary classification problem: are these two words neighbors, or are they strangers?\nThe training loop provides a positive pair from the text, instructing the model to maximize the similarity between their vectors. Simultaneously, it grabs random negative samples (imposters from the vocabulary) and demands the model minimize their similarity. This push-and-pull mechanic creates the vector space. The ‚ÄúGreen‚Äù cluster forms not because the model understands color, but because those words are statistically interchangeable when opposed to ‚ÄúRed‚Äù.\nTo generate these pairs without human labeling, we employ a sliding window technique. This moves over the raw text corpus, converting a sequence of words into a system of geometric queries.\n\n\n\n\n\n\nFigure¬†3: Without human labeling, word2vec assumes that words in the same context are related. Context is defined as the words within a window of predefined size. For example, in ‚ÄúThe quick brown fox jumps over the lazy dog‚Äù, the context of ‚Äúfox‚Äù includes ‚Äúbrown‚Äù, ‚Äújumps‚Äù, ‚Äúover‚Äù, and ‚Äúlazy‚Äù.\n\n\n\nWord2Vec is a simple neural network with one hidden layer. The input is a one-hot encoded vector of a word, which triggers neurons in the hidden layer to fire. The neural connection strength from the neuron representing the word to the neurons in the hidden layer (marked by red arrows) represents the query vector, u. The hidden layer neurons then trigger the firing of output layer neurons, which represents the probability of word w appearing in the context of word w_i. The connection strength from an output word neuron to the hidden layer neurons represents the key vector, v.\n\nThe word in the center of the window acts as the Query vector (u), broadcasting its position to the surrounding Context words, which act as Keys (v). The neural network adjusts its weights to maximize the dot product u \\cdot v for these specific context pairs while suppressing the dot product for the negative samples. The probability of a word appearing in context is thus a function of their vector alignment.\n\nP(j \\vert i) = \\frac{P(j) \\exp(u_i \\cdot v_j)}{\\sum_{k=1}^{V} P(k) \\exp(u_i \\cdot v_k)}\n\nwhere P(j) is the probability of word j appearing in the vocabulary.\nThe original Word2Vec paper uses a different formulation that omits P(j). This original formulation is correct conceptually but not practically. In practice, word2vec is trained with an efficient but biased training algorithm (negative sampling). The term P(j) enters the P(j \\vert i) when we account for bias, which is why we include it here.\nThis closes the loop between high-level linguistic philosophy and low-level matrix operations. The machine proves the structuralist hypothesis: that meaning is relational. By mechanically slicing the continuum of language and applying the pressure of negative sampling, the model reconstructs a functional map of human concepts. We have successfully turned a philosophy of meaning into a runnable algorithm.\n\n\n\n\n\n\nFigure¬†4",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Embeddings"
    ]
  },
  {
    "objectID": "m04-text/word-embeddings.html#key-takeaway",
    "href": "m04-text/word-embeddings.html#key-takeaway",
    "title": "Word Embeddings",
    "section": "Key Takeaway",
    "text": "Key Takeaway\nYou don‚Äôt need to know what a thing is to understand it. You only need to know where it stands relative to everything it isn‚Äôt. There‚Äôs a nice blog post by Chris McCormick that walks through the inner workings of Word2Vec. See here.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Embeddings"
    ]
  },
  {
    "objectID": "m04-text/word-bias.html#understanding-bias-in-word-embeddings",
    "href": "m04-text/word-bias.html#understanding-bias-in-word-embeddings",
    "title": "Word Bias",
    "section": "Understanding Bias in Word Embeddings",
    "text": "Understanding Bias in Word Embeddings\nWord embeddings can capture and reinforce societal biases from their training data through geometric relationships between word vectors. These relationships often reflect stereotypes about gender, race, age, and other social factors. By using semantic axes, we can analyze gender bias in job-related terms, showing both the benefits and risks of word embeddings capturing these real-world associations.\nSemAxis is a powerful tool to analyze gender bias in word embeddings by measuring word alignments along semantic axes. Using antonym pairs like ‚Äúshe-he‚Äù as poles, it quantifies gender associations in words on a scale from -1 to 1, where positive values indicate feminine associations and negative values indicate masculine associations.\nLet‚Äôs start with a simple example of analyzing gender bias in occupations.\n\nimport gensim.downloader as api\nimport numpy as np\n\n# Load pre-trained Word2vec embeddings\nmodel = api.load(\"word2vec-google-news-300\")\n\n\ndef compute_bias(word, microframe, model):\n    word_vector = model[word]\n    numerator = np.dot(word_vector, microframe)\n    denominator = np.linalg.norm(word_vector) * np.linalg.norm(microframe)\n    return numerator / denominator\n\n\ndef analyze(word, pos_word, neg_word, model):\n    if word not in model:\n        return 0.0\n    microframe = model[pos_word] - model[neg_word]\n    bias = compute_bias(word, microframe, model)\n    return bias\n\nThe compute_bias function calculates the cosine similarity between a word vector and a semantic axis (microframe). The numerator computes the dot product, which projects the word onto the axis. The denominator normalizes by vector lengths to get a score between -1 and 1.\nWe will use the following occupations:\n\n# Occupations from the paper\nshe_occupations = [\n    \"homemaker\",\n    \"nurse\",\n    \"receptionist\",\n    \"librarian\",\n    \"socialite\",\n    \"hairdresser\",\n    \"nanny\",\n    \"bookkeeper\",\n    \"stylist\",\n    \"housekeeper\",\n]\n\nhe_occupations = [\n    \"maestro\",\n    \"skipper\",\n    \"protege\",\n    \"philosopher\",\n    \"captain\",\n    \"architect\",\n    \"financier\",\n    \"warrior\",\n    \"broadcaster\",\n    \"magician\",\n    \"boss\",\n]\n\nWe measure the gender bias in these occupations by measuring how they align with the ‚Äúshe-he‚Äù axis.\n\n\nCode\nprint(\"Gender Bias in Occupations (she-he axis):\")\nprint(\"\\nShe-associated occupations:\")\nfor occupation in she_occupations:\n    bias = analyze(occupation, \"she\", \"he\", model)\n    print(f\"{occupation}: {bias:.3f}\")\n\nprint(\"\\nHe-associated occupations:\")\nfor occupation in he_occupations:\n    bias = analyze(occupation, \"she\", \"he\", model)\n    print(f\"{occupation}: {bias:.3f}\")\n\n\nGender Bias in Occupations (she-he axis):\n\nShe-associated occupations:\nhomemaker: 0.360\nnurse: 0.333\nreceptionist: 0.329\nlibrarian: 0.300\nsocialite: 0.310\nhairdresser: 0.307\nnanny: 0.287\nbookkeeper: 0.264\nstylist: 0.252\nhousekeeper: 0.260\n\nHe-associated occupations:\nmaestro: -0.203\nskipper: -0.177\nprotege: -0.148\nphilosopher: -0.155\ncaptain: -0.130\narchitect: -0.151\nfinancier: -0.145\nwarrior: -0.120\nbroadcaster: -0.124\nmagician: -0.110\nboss: -0.090\n\n\nInterpreting the scores: Positive scores (greater than 0) indicate closer association to ‚Äúshe‚Äù (e.g., nurse, librarian). Negative scores (less than 0) indicate closer association to ‚Äúhe‚Äù (e.g., architect, captain). Magnitude indicates the strength of the gender association. A larger absolute value represents a stronger gender association.\nNotice how occupations historically associated with women (like nurse and librarian) have strong positive scores, while those associated with men (like captain and architect) have negative scores. This confirms that the model has learned these gender stereotypes from the text data.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Bias"
    ]
  },
  {
    "objectID": "m04-text/word-bias.html#stereotype-analogies",
    "href": "m04-text/word-bias.html#stereotype-analogies",
    "title": "Word Bias",
    "section": "Stereotype Analogies",
    "text": "Stereotype Analogies\nSince word embeddings capture semantic relationships learned from large text corpora, they inevitably encode societal biases and stereotypes present in that training data. We can leverage this property to identify pairs of words that exhibit stereotypical gender associations. By measuring how different words align with the gender axis (she-he), we find pairs where one word shows strong feminine bias while its counterpart shows masculine bias, revealing ingrained stereotypes in language use.\n\n# Stereotype analogies from the paper\nstereotype_pairs = [\n    (\"sewing\", \"carpentry\"),\n    (\"nurse\", \"surgeon\"),\n    (\"softball\", \"baseball\"),\n    (\"cosmetics\", \"pharmaceuticals\"),\n    (\"vocalist\", \"guitarist\"),\n]\n\n\n\nCode\nprint(\"\\nAnalyzing Gender Stereotype Pairs:\")\nfor word1, word2 in stereotype_pairs:\n    bias1 = analyze(word1, \"she\", \"he\", model)\n    bias2 = analyze(word2, \"she\", \"he\", model)\n    print(f\"\\n{word1} vs {word2}\")\n    print(f\"{word1}: {bias1:.3f}\")\n    print(f\"{word2}: {bias2:.3f}\")\n\n\n\nAnalyzing Gender Stereotype Pairs:\n\nsewing vs carpentry\nsewing: 0.302\ncarpentry: -0.028\n\nnurse vs surgeon\nnurse: 0.333\nsurgeon: -0.048\n\nsoftball vs baseball\nsoftball: 0.260\nbaseball: -0.066\n\ncosmetics vs pharmaceuticals\ncosmetics: 0.331\npharmaceuticals: -0.011\n\nvocalist vs guitarist\nvocalist: 0.140\nguitarist: -0.041\n\n\nThe results show clear stereotypical alignments. Sewing and nurse align with ‚Äúshe‚Äù, while carpentry and surgeon align with ‚Äúhe‚Äù. This mirrors the ‚Äúman is to computer programmer as woman is to homemaker‚Äù analogy found in early word embedding research.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Bias"
    ]
  },
  {
    "objectID": "m04-text/word-bias.html#indirect-bias-when-neutral-words-become-gendered",
    "href": "m04-text/word-bias.html#indirect-bias-when-neutral-words-become-gendered",
    "title": "Word Bias",
    "section": "Indirect Bias: When Neutral Words Become Gendered",
    "text": "Indirect Bias: When Neutral Words Become Gendered\nIndirect bias occurs when seemingly neutral words or concepts become associated with gender through their relationships with other words. For example, while ‚Äúsoftball‚Äù and ‚Äúfootball‚Äù are not inherently gendered terms, they may show gender associations in word embeddings due to how they‚Äôre used in language and society.\nWe can detect indirect bias by identifying word pairs that form a semantic axis (like softball-football), measuring how other words align with this axis, and examining if alignment correlates with gender bias. This reveals how gender stereotypes can be encoded indirectly through word associations, even when the words themselves don‚Äôt explicitly reference gender.\nLet‚Äôs see how this works in practice. We first measure the gender bias of the following words:\n\n# Words associated with softball-football axis\nsoftball_associations = [\n    \"pitcher\",\n    \"bookkeeper\",\n    \"receptionist\",\n    \"nurse\",\n    \"waitress\"\n]\n\nfootball_associations = [\n    \"footballer\",\n    \"businessman\",\n    \"pundit\",\n    \"maestro\",\n    \"cleric\"\n]\n\n# Calculate biases for all words\ngender_biases = []\nsports_biases = []\nwords = softball_associations + football_associations\n\nfor word in words:\n    gender_bias = analyze(word, \"she\", \"he\", model)\n    sports_bias = analyze(word, \"softball\", \"football\", model)\n    gender_biases.append(gender_bias)\n    sports_biases.append(sports_bias)\n\nLet‚Äôs plot the results:\n\n\nCode\n# Analyze bias along both gender and sports axes\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom adjustText import adjust_text\n\n# Create scatter plot\nfig, ax = plt.subplots(figsize=(6, 6))\nsns.scatterplot(x=gender_biases, y=sports_biases, ax=ax)\nax.set_xlabel(\"Gender Bias (she-he)\")\nax.set_ylabel(\"Sports Bias (softball-football)\")\nax.set_title(\"Indirect Bias Analysis: Gender vs Sports\")\n\n# Add labels for each point\ntexts = []\nfor i, word in enumerate(words):\n    texts.append(ax.text(gender_biases[i], sports_biases[i], word, fontsize=12))\n\nadjust_text(texts, arrowprops=dict(arrowstyle='-', color='gray', lw=0.5))\n\nax.grid(True, alpha=0.3)\nplt.show()\nsns.despine()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nThe plot reveals a correlation: words associated with ‚Äúsoftball‚Äù (y-axis greater than 0) also tend to be associated with ‚Äúshe‚Äù (x-axis greater than 0). Conversely, ‚Äúfootball‚Äù terms align with ‚Äúhe‚Äù. This suggests that even if we remove explicit gender words, the structure of the space still encodes gender through these proxy dimensions.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Bias"
    ]
  },
  {
    "objectID": "m04-text/word-bias.html#the-impact-and-path-forward",
    "href": "m04-text/word-bias.html#the-impact-and-path-forward",
    "title": "Word Bias",
    "section": "The Impact and Path Forward",
    "text": "The Impact and Path Forward\nWord embeddings, while powerful, inevitably capture and reflect societal biases present in the large text corpora they are trained on. We observed both direct bias, where occupations or attributes align strongly with specific gender pronouns, and indirect bias, where seemingly neutral concepts become gendered through their associations with other words. This analysis highlights the importance of understanding and mitigating these biases to prevent the perpetuation of stereotypes in AI systems and ensure fairness in applications like search, recommendation, and hiring.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Bias"
    ]
  },
  {
    "objectID": "m04-text/transformers.html#the-problem-with-one-vector-per-word",
    "href": "m04-text/transformers.html#the-problem-with-one-vector-per-word",
    "title": "Transformers",
    "section": "The Problem with One Vector Per Word",
    "text": "The Problem with One Vector Per Word\n\n\n\n\n\nFor many years, natural language processing treated words as having fixed meanings. Each word (like ‚Äúbank‚Äù) received a single vector of numbers, called a static embedding. But there‚Äôs a hidden catch in this ‚Äúone meaning per word‚Äù mindset. With just a single fixed entry in the dictionary, ‚Äúbank‚Äù means exactly the same thing in ‚ÄúI deposited money at the bank‚Äù as in ‚ÄúWe had a picnic by the bank‚Äù. Every possible meaning gets mashed into a one-size-fits-all average, like describing the population by its average height and pretending nobody‚Äôs shorter or taller. The interesting details, the outliers, the context clues, all vanish in the mix.\nWhat if we simply mixed the target word with its neighbors? For ‚ÄúI deposited money at the bank,‚Äù we could compute a contextualized representation as:\n\n\\vec{v}_{\\text{bank (new)}} = w_1 \\cdot \\vec{v}_{\\text{bank}} + w_2 \\cdot \\vec{v}_{\\text{deposited}} + w_3 \\cdot \\vec{v}_{\\text{money}} + \\cdots\n\nwhere w_i are weights and \\vec{v}_i are word embeddings. The key question: how do we determine these weights? Consider that ‚Äúbank‚Äù sits neutrally between financial terms (money) and geographical terms (river). Try manually adjusting the weights to contextualize ‚Äúbank‚Äù:\n\nd3 = require(\"d3@7\", \"d3-simple-slider@1\")\n\n\n\n\n\n\n\nfunction sliderWithLabel(min, max, step, width, defaultValue, label) {\n  const slider = d3.sliderBottom()\n    .min(min).max(max).step(step).width(width).default(defaultValue);\n  const svg = d3.create(\"svg\").attr(\"width\", width + 50).attr(\"height\", 60);\n  svg.append(\"g\").attr(\"transform\", \"translate(25,20)\").call(slider);\n  svg.append(\"text\").attr(\"x\", (width + 50) / 2).attr(\"y\", 10).attr(\"text-anchor\", \"middle\").style(\"font-size\", \"5px\").text(label);\n  return svg.node();\n}\n\n\n\n\n\n\n\n{\n  function createWeightSlider(min, max, step, width, defaultValue, label) {\n    const slider = d3.sliderBottom()\n      .min(min).max(max).step(step).width(width).default(defaultValue);\n    const svg = d3.create(\"svg\").attr(\"width\", width + 50).attr(\"height\", 60);\n    const g = svg.append(\"g\").attr(\"transform\", \"translate(25,20)\");\n    g.call(slider);\n    svg.append(\"text\").attr(\"x\", (width + 50) / 2).attr(\"y\", 10)\n       .attr(\"text-anchor\", \"middle\").style(\"font-size\", \"12px\").text(label);\n    return { node: svg.node(), slider: slider };\n  }\n\n  const bankSliderObj = createWeightSlider(0, 1, 0.01, 120, 1.0, \"Bank weight\");\n  const moneySliderObj = createWeightSlider(0, 1, 0.01, 120, 0.0, \"Money weight\");\n  const riverSliderObj = createWeightSlider(0, 1, 0.01, 120, 0.0, \"River weight\");\n\n  const contextWords = [\"bank\", \"money\", \"river\"];\n  const contextEmbeddings = [\n    [0.0, 0.0],\n    [-1.6, -0.6],\n    [1.4, -1.0]\n  ];\n\n  const plotContainer = document.createElement(\"div\");\n\n  function update() {\n    const bankWeight = bankSliderObj.slider.value();\n    const moneyWeight = moneySliderObj.slider.value();\n    const riverWeight = riverSliderObj.slider.value();\n\n    const weights = [bankWeight, moneyWeight, riverWeight];\n    const total = weights.reduce((a, b) =&gt; a + b, 0);\n    const normalizedWeights = total &gt; 0 ? weights.map(w =&gt; w / total) : [0, 0, 0];\n\n    const newVec = [\n      normalizedWeights[0] * contextEmbeddings[0][0] +\n      normalizedWeights[1] * contextEmbeddings[1][0] +\n      normalizedWeights[2] * contextEmbeddings[2][0],\n      normalizedWeights[0] * contextEmbeddings[0][1] +\n      normalizedWeights[1] * contextEmbeddings[1][1] +\n      normalizedWeights[2] * contextEmbeddings[2][1]\n    ];\n\n    const originalData = contextWords.map((word, i) =&gt; ({\n      word: word,\n      x: contextEmbeddings[i][0],\n      y: contextEmbeddings[i][1],\n      type: \"Original\"\n    }));\n\n    const contextualizedData = [{\n      word: \"bank (new)\",\n      x: newVec[0],\n      y: newVec[1],\n      type: \"Contextualized\"\n    }];\n\n    const data = [...originalData, ...contextualizedData];\n\n    d3.select(plotContainer).selectAll(\"*\").remove();\n\n    const plot = Plot.plot({\n      width: 300,\n      height: 300,\n      marginTop: 60,\n      marginRight: 20,\n      marginBottom: 50,\n      marginLeft: 60,\n      style: {\n        background: \"white\",\n        color: \"black\"\n      },\n      x: {\n        domain: [-2, 2],\n        label: \"Dimension 1\",\n        grid: true,\n        ticks: 10\n      },\n      y: {\n        domain: [-2, 2],\n        label: \"Dimension 2\",\n        grid: true,\n        ticks: 10\n      },\n      color: {\n        domain: [\"Original\", \"Contextualized\"],\n        range: [\"#dadada\", \"#ff7f0e\"]\n      },\n      marks: [\n        Plot.dot(data, {\n          x: \"x\",\n          y: \"y\",\n          fill: \"type\",\n          r: 8,\n          tip: true\n        }),\n        Plot.text(data, {\n          x: \"x\",\n          y: \"y\",\n          text: \"word\",\n          dy: -15,\n          fontSize: 8,\n          fontWeight: \"bold\",\n          fill: \"black\"\n        }),\n        Plot.text([{x: 0, y: 2.3}], {\n          x: \"x\",\n          y: \"y\",\n          text: () =&gt; `Weights: Bank=${normalizedWeights[0].toFixed(2)}, Money=${normalizedWeights[1].toFixed(2)}, River=${normalizedWeights[2].toFixed(2)}`,\n          fontSize: 11,\n          fill: \"black\"\n        }),\n        Plot.dot([{x: -0.8, y: 2.7, color: \"#dadada\"}, {x: 0.8, y: 2.7, color: \"#ff7f0e\"}], {\n          x: \"x\",\n          y: \"y\",\n          fill: \"color\",\n          r: 6\n        }),\n        Plot.text([{x: -0.5, y: 2.7, label: \"Original\"}, {x: 1.1, y: 2.7, label: \"Contextualized\"}], {\n          x: \"x\",\n          y: \"y\",\n          text: \"label\",\n          fontSize: 10,\n          fill: \"black\",\n          textAnchor: \"start\"\n        })\n      ]\n    });\n\n    d3.select(plotContainer).node().appendChild(plot);\n  }\n\n  bankSliderObj.slider.on(\"onchange\", update);\n  moneySliderObj.slider.on(\"onchange\", update);\n  riverSliderObj.slider.on(\"onchange\", update);\n\n  update();\n\n  return html`&lt;div style=\"display: flex; align-items: center; gap: 40px; justify-content: center;\"&gt;\n    &lt;div style=\"display: flex; flex-direction: column; gap: 10px;\"&gt;\n      ${bankSliderObj.node}\n      ${moneySliderObj.node}\n      ${riverSliderObj.node}\n    &lt;/div&gt;\n    &lt;div&gt;\n      ${plotContainer}\n    &lt;/div&gt;\n  &lt;/div&gt;`;\n}\n\n\n\n\n\n\nBy changing the weights, we see that the vector for ‚Äúbank‚Äù can lean more towards financial terms or geographical terms. So how do we determine the weights? The simplest idea gives each word equal weight: w_i = 1/N. This creates a basic bag-of-words average. But sentences aren‚Äôt this fair. Some words are much more important than others. In ‚ÄúI deposited money at the bank,‚Äù the words ‚Äúdeposited‚Äù and ‚Äúmoney‚Äù are key, while ‚ÄúI‚Äù, ‚Äúat‚Äù, and ‚Äúthe‚Äù add little meaning. If we treat all words the same, we lose the details that matter. We need a way to highlight important words and downplay the rest.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Transformers"
    ]
  },
  {
    "objectID": "m04-text/transformers.html#the-key-insight",
    "href": "m04-text/transformers.html#the-key-insight",
    "title": "Transformers",
    "section": "The Key Insight",
    "text": "The Key Insight\nEvery time you use GPT (ChatGPT, Claude, Gemini, etc.), you‚Äôre seeing transformers in action. Transformers don‚Äôt ‚Äúthink‚Äù. They perform statistical pattern matching at scale.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Transformers"
    ]
  },
  {
    "objectID": "m04-text/llm-intro.html#do-llms-understand-language",
    "href": "m04-text/llm-intro.html#do-llms-understand-language",
    "title": "Large Language Models in Practice",
    "section": "Do LLMs Understand Language?",
    "text": "Do LLMs Understand Language?\nWhat do you think about this question: Can LLMs understand the world and reason about it?\nOne might argue that fluency demonstrates understanding. This is the intuition behind Turing‚Äôs 1950 test: if you can‚Äôt tell it‚Äôs a machine, treat it as intelligent. Fluency implies comprehension. But let‚Äôs examine counter-arguments starting with ELIZA.\nELIZA, developed by Joseph Weizenbaum in the mid-1960s, is widely considered one of the first chatbots. It simulated a Rogerian psychotherapist by using simple pattern matching and keyword substitution to generate responses. Despite its lack of true understanding, ELIZA famously convinced many users that they were conversing with an intelligent entity, highlighting the human tendency to anthropomorphize technology and the limitations of the Turing Test.\n\n\nAnother argument against fluency is the Chinese Room argument, proposed by philosopher John Searle. Imagine a person in a room who receives Chinese characters and, using an English rulebook, manipulates these symbols to produce new Chinese characters. To an outside observer, it appears the room understands Chinese. However, the person inside merely follows instructions to manipulate symbols without understanding their meaning. Searle argues that this is analogous to how computers, including LLMs, operate: they process symbols based on rules without genuine comprehension, raising questions about whether they can truly ‚Äúunderstand‚Äù language or the world.\n\n\nSo do LLMs understand the world? Probably not in the same way we do. LLMs are lossy compression algorithms, compressing data into their parameters to generate fluent outputs. To predict ‚ÄúThe capital of France is __,‚Äù the model must compress not just the fact (Paris) but the statistical regularities governing how facts appear in text. It learns that capitals follow ‚ÄúThe capital of,‚Äù that France is a country, that countries have capitals. This compression is probabilistic, not factual. The model stores P(word{n+1} | word_1, ‚Ä¶, word_n), which words tend to follow which other words in which contexts. Just as a lottery memorizer stores patterns of number sequences, the LLM stores patterns of word sequences.\n\n\n\n\n\nTraining feeds the model billions of sentences. For each sentence, the model predicts the next word, compares its prediction to the actual next word, and adjusts its parameters to increase the probability of the correct word. Repeat trillions of times. The result: a compressed representation of how language behaves statistically. The model doesn‚Äôt learn ‚ÄúParis is the capital of France‚Äù as a fact; it learns that in contexts matching the pattern [The capital of France is], the token ‚ÄúParis‚Äù appears with high probability. The lottery memorizer doesn‚Äôt understand what draws mean; it just knows what patterns appear most often. This is why LLMs create hallucinations‚Äîfluent but false outputs. Truth and fluency correlate in the training data, so the model is mostly truthful. But in the tails‚Äîobscure topics, recent events, precise recall‚Äîfluency diverges from truth, and the model follows fluency.\nKeep this limitation in mind and use LLMs as a tool to scale pattern recognition, not judgment. Let‚Äôs learn how to utilize them.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Large Language Models"
    ]
  },
  {
    "objectID": "m04-text/llm-intro.html#what-comes-next",
    "href": "m04-text/llm-intro.html#what-comes-next",
    "title": "Large Language Models in Practice",
    "section": "What Comes Next",
    "text": "What Comes Next\nYou‚Äôve seen LLMs in practice: setup, summarization, extraction, limitations. But how do they actually work? What happens inside when you send a prompt? The rest of this module unboxes the technology: prompt engineering (communicating with LLMs), embeddings (representing meaning as numbers), transformers (the architecture enabling modern NLP), fundamentals (from word counts to neural representations). First, let‚Äôs master talking to machines.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Large Language Models"
    ]
  },
  {
    "objectID": "m04-text/gpt-inference.html#how-gpt-generates-text",
    "href": "m04-text/gpt-inference.html#how-gpt-generates-text",
    "title": "GPT Inference: Sampling Strategies",
    "section": "How GPT Generates Text",
    "text": "How GPT Generates Text\n\nWhat do you think happens when GPT generates the next word in a sentence? Does it pick the ‚Äúbest‚Äù word, or something else?\nHere‚Äôs the reality: GPT doesn‚Äôt output a single word. It outputs a probability distribution over its entire vocabulary (millions of possible tokens, each with a likelihood). The naive approach is to always pick the highest probability token (greedy sampling), but this creates a deterministic trap. The model falls into repetitive loops because it always makes the same choice. The distribution is high-dimensional, making sampling computationally expensive, but also rich with alternative paths.\nThe solution is controlled randomness. By sampling from the distribution rather than deterministically selecting the peak, we introduce diversity. But blind random sampling produces incoherent text. The challenge is finding the middle ground: sample broadly enough to avoid repetition, but narrowly enough to maintain coherence.\nThink of it like improvisational jazz. A musician playing the same note repeatedly (greedy sampling) is boring. Playing random notes (uniform sampling) is noise. The art is in sampling from the most promising notes while occasionally taking creative risks. This jazz analogy‚Äîthe balance between predictability and surprise‚Äîexplains every sampling strategy we‚Äôll explore.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "GPT Inference: Sampling Strategies"
    ]
  },
  {
    "objectID": "m04-text/gpt-inference.html#sampling-strategies-in-practice",
    "href": "m04-text/gpt-inference.html#sampling-strategies-in-practice",
    "title": "GPT Inference: Sampling Strategies",
    "section": "Sampling Strategies in Practice",
    "text": "Sampling Strategies in Practice\nHere is an interactive demo of GPT inference available online at https://static.marimo.app/static/gpt-ar61. You can try different sampling strategies and see the results. GPT generates text one token at a time, repeatedly sampling from the probability distribution. Let‚Äôs examine the strategies for sampling that balance quality and diversity.\nFirst, let‚Äôs set up our connection to Ollama:\n\nimport ollama\n\n# Make sure you have Ollama running and a model pulled\n# Run: ollama pull mapler/gpt2\nMODEL = \"mapler/gpt2\"\nPROMPT = \"Hi there! \"\n\n\nGreedy Sampling\nGreedy sampling always picks the highest probability token, which is deterministic but can lead to repetitive or trapped text. For example, if the model predicts ‚Äúthe‚Äù with high probability, it will always predict ‚Äúthe‚Äù again. This is the jazz musician stuck on a single note.\n\n\n\n\n\nGPT greedy search.\nLet‚Äôs see greedy sampling in action. We set temperature to 0 to make the sampling deterministic (always picking the highest probability token):\n\ngreedy_response = ollama.generate(\n    model=MODEL,\n    prompt=PROMPT,\n    options={\n        \"temperature\": 0,  # greedy sampling\n        \"num_predict\": 20,  # max tokens to generate\n    }\n)\nprint(greedy_response['response'])\n\n¬†I'm so glad you're here.\nThe first thing I did was to make a list\n\n\nThe output is often repetitive because greedy sampling always selects the most probable token at each step, leading to predictable and repetitive patterns. Try running it multiple times. You‚Äôll get the exact same output each time.\n\n\nBeam Search\nBeam search alleviates the repetition problem by taking into account the high-order dependencies between tokens. For example, in generating ‚ÄúThe cat ran across the ___‚Äú, beam search might preserve a path containing‚Äùmat‚Äù even if ‚Äúfloor‚Äù or ‚Äúroom‚Äù have higher individual probabilities at that position. This is because the complete sequence like ‚Äúmat quickly‚Äù could be more probable when considering the token next after ‚Äúmat‚Äù. ‚ÄúThe cat ran across the mat quickly‚Äù is a more natural phrase than ‚ÄúThe cat ran across the floor quickly‚Äù when considering the full flow and common linguistic patterns.\n\n\n\n\n\nGPT beam search.\nBeam search maintains multiple possible sequences (beams) in parallel, exploring different paths simultaneously. At each step, it expands all current beams, scores the resulting sequences, and keeps only the top-k highest scoring ones. For instance, with a beam width of 3, first beams might be [‚ÄúThe cat ran‚Äù, ‚ÄúThe cat walked‚Äù, ‚ÄúThe cat jumped‚Äù]. Next step: [‚ÄúThe cat ran across‚Äù, ‚ÄúThe cat ran through‚Äù, ‚ÄúThe cat walked across‚Äù]. And so on, keeping the 3 most promising complete sequences at each step.\nThis process continues until reaching the end, finally selecting the sequence with highest overall probability. Beam search can be combined with top-k sampling or nucleus sampling. For example, one can sample a token based on top-k sampling or nucleus sampling to form the next beam.\nWhile beam search often produces high-quality outputs since it considers longer-term coherence, it can still suffer from the problem of repetitive or trapped text. It‚Äôs the jazz ensemble playing in perfect harmony, technically excellent but predictable.\nNote: Ollama doesn‚Äôt natively support beam search through its API. Beam search requires access to the model‚Äôs internal scoring mechanism, which is typically implemented at a lower level (using HuggingFace Transformers or direct PyTorch/TensorFlow implementations). For production beam search, you would use libraries like transformers or vLLM.\n\n\nFrom Deterministic to Stochastic Sampling\nBoth greedy and beam search are deterministic. They pick the most likely token at each step. However, this creates a loop where the model always predicts the same tokens repeatedly. A simple way to alleviate this problem is to sample a token from the distribution.\nTop-k sampling relaxes the deterministic nature of greedy sampling by selecting randomly from the k most likely next tokens at each generation step. While this introduces some diversity compared to greedy sampling, choosing a fixed k can be problematic. Value of k might be too large for some distribution tails (including many poor options) or too small for others (excluding reasonable options). In our jazz analogy, this is like saying ‚Äúyou can only improvise using these five specific notes‚Äù. Sometimes that‚Äôs perfect, sometimes it‚Äôs too limiting.\n\ntop_k_response = ollama.generate(\n    model=MODEL,\n    prompt=PROMPT,\n    options={\n        \"temperature\": 1.0,  # enable stochastic sampling\n        \"top_k\": 10,  # restrict to top 10 tokens\n        \"num_predict\": 20,\n    }\n)\nprint(top_k_response['response'])\n\n~~~\nI was really excited to see that. The girl with a big red face and the\n\n\nTry running this multiple times. You‚Äôll get different outputs each time because the model samples randomly from the top-k tokens.\nNucleus sampling addresses this limitation by dynamically selecting tokens based on cumulative probability. It samples from the smallest set of tokens whose cumulative probability exceeds a threshold p (for example, 0.9). This adapts naturally to different probability distributions, selecting few tokens when the distribution is concentrated and more when it‚Äôs spread out. This approach often provides a good balance between quality and diversity. The jazz musician now has flexibility. When the melody is clear, stick to a few notes; when it‚Äôs time to explore, draw from a wider palette.\n\n\n\n\n\nNucleus sampling. The image is taken from this blog.\n\ntop_p_response = ollama.generate(\n    model=MODEL,\n    prompt=PROMPT,\n    options={\n        \"temperature\": 1.0,\n        \"top_p\": 0.95,  # sample from tokens with cumulative probability &gt;= 0.95\n        \"num_predict\": 20,\n    }\n)\nprint(top_p_response['response'])\n\n¬†I'd like to congratulate @TerraceParks_ for the first time. A lot\n\n\nNucleus sampling dynamically adjusts the number of candidate tokens based on the probability distribution, making it more adaptive than fixed top-k.\n\n\nTemperature Control\nTemperature (\\tau) modifies how ‚Äúconcentrated‚Äù the probability distribution is for sampling by scaling the logits before applying softmax:\n\np_i = \\frac{\\exp(z_i/\\tau)}{\\sum_j \\exp(z_j/\\tau)}\n\nwhere z_i are the logits and \\tau is the temperature parameter. Lower temperatures (\\tau &lt; 1.0) make the distribution more peaked, making high probability tokens even more likely to be chosen, leading to more focused and conservative outputs. Higher temperatures (\\tau &gt; 1.0) flatten the distribution by making the logits more similar, increasing the chances of selecting lower probability tokens and producing more diverse but potentially less coherent text. As \\tau \\to 0, the distribution approaches a one-hot vector (equivalent to greedy search), while as \\tau \\to \\infty, it approaches a uniform distribution.\nIn jazz terms, temperature controls the musician‚Äôs mood. Low temperature is playing it safe (sticking to the melody). High temperature is experimental improvisation (sometimes brilliant, sometimes cacophonous).\n\n\n\n\n\nTemperature controls the concentration of the probability distribution. Lower temperature makes the distribution more peaked, while higher temperature makes the distribution more flat.\nLet‚Äôs see how temperature affects generation:\n\nfor tau in [0.1, 0.5, 1.0, 2.0, 5.0]:\n    response = ollama.generate(\n        model=MODEL,\n        prompt=PROMPT,\n        options={\n            \"temperature\": tau,\n            \"num_predict\": 20,\n        }\n    )\n    print(f\"œÑ = {tau}: {response['response']}\")\n\nœÑ = 0.1: ¬†I'm so glad you're here.\nThe first thing I did was to get a picture\nœÑ = 0.5: ¬†I've been working on this project for almost two years now. I'm so excited to finally\nœÑ = 1.0: ¬†Just so I know, when you've been waiting for the post on my blog to be posted\nœÑ = 2.0: ____________________________________________ Let's see what you've found so far with this tutorial and tips on how to\nœÑ = 5.0: ____ |_/_ ___/ _ \\ .\\:(.* / )?| \\\\ || !\n\n\nNotice how: Low temperature (œÑ = 0.1) produces conservative, focused output. Medium temperature (œÑ = 1.0) provides balanced diversity. High temperature (œÑ = 5.0) produces creative but potentially incoherent output.\n\n\nCombining All Strategies\nYou can combine top-k, top-p, and temperature for fine-grained control:\n\ncombined_response = ollama.generate(\n    model=MODEL,\n    prompt=PROMPT,\n    options={\n        \"temperature\": 0.7,  # moderate randomness\n        \"top_k\": 10,         # restrict to top 10 tokens\n        \"top_p\": 0.95,       # within top-k, use nucleus sampling\n        \"num_predict\": 20,\n    }\n)\nprint(combined_response['response'])\n\n¬†I'm glad that you guys are all enjoying the new release of this book. It's a\n\n\nThis combination restricts candidates to top-k tokens, then applies nucleus sampling, and finally uses temperature to control randomness, giving you maximum control over the generation process. The jazz musician now has a framework: work within these chords (top-k), adapt to the moment (nucleus), and choose your creative intensity (temperature).\n\n\nPractical Recommendations\nFor most applications, use nucleus sampling with p = 0.9 and temperature œÑ = 0.7. This combination provides a good balance between coherence and creativity. For tasks requiring high factual accuracy (e.g., technical documentation), lower the temperature to œÑ = 0.3 to make the model more conservative. For creative writing, increase the temperature to œÑ = 1.0 or higher to encourage exploration.\nBeam search is useful when you need the single most probable sequence (e.g., machine translation), but it sacrifices diversity. Use it when correctness matters more than variety.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "GPT Inference: Sampling Strategies"
    ]
  },
  {
    "objectID": "m04-text/gpt-inference.html#the-key-insight",
    "href": "m04-text/gpt-inference.html#the-key-insight",
    "title": "GPT Inference: Sampling Strategies",
    "section": "The Key Insight",
    "text": "The Key Insight\nGeneration is sampling. Greedy picks the peak, beam search explores multiple peaks, and stochastic sampling adds controlled randomness. Temperature flattens or sharpens the distribution; nucleus sampling adapts to its shape. The right strategy depends on whether you‚Äôre optimizing for accuracy or creativity.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "GPT Inference: Sampling Strategies"
    ]
  },
  {
    "objectID": "m04-text/sentence-transformers.html#the-challenge-from-matrices-to-coordinates",
    "href": "m04-text/sentence-transformers.html#the-challenge-from-matrices-to-coordinates",
    "title": "Sentence Transformers",
    "section": "The Challenge: From Matrices to Coordinates",
    "text": "The Challenge: From Matrices to Coordinates\nBERT gives you a vector for every token in a sentence. If you want to compare two sentences, you‚Äôre stuck comparing two messy matrices of varying sizes. The naive approach (averaging all token vectors) throws away positional information and treats every word equally, which is wrong. The word ‚Äúnot‚Äù in ‚Äúnot good‚Äù should drastically change the sentence embedding, but simple averaging dilutes its impact.\nSentence-BERT (SBERT) solves this by training a Siamese Network. The same BERT model processes two sentences independently, producing their respective token matrices. We then apply pooling (mean, max, or CLS-token extraction) to collapse each matrix into a single vector. The training objective is contrastive: if the sentences are semantically similar (paraphrases), their vectors should be close in Euclidean or cosine space. If they‚Äôre unrelated, their vectors should be distant.\nThink of it like creating a library catalog. Instead of storing every word on every page, you compress each book into a single Dewey Decimal number. Books on similar topics get similar numbers, enabling efficient retrieval. The compression loses fine-grained detail, but gains search speed.\nThe mathematical trick is the Siamese architecture. Weight sharing ensures both sentences are embedded into the same vector space using identical transformations. This makes the distance between vectors meaningful: similar sentences cluster together, dissimilar ones push apart.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Sentence Transformers"
    ]
  },
  {
    "objectID": "m04-text/sentence-transformers.html#how-to-use-sentence-transformers",
    "href": "m04-text/sentence-transformers.html#how-to-use-sentence-transformers",
    "title": "Sentence Transformers",
    "section": "How to Use Sentence Transformers",
    "text": "How to Use Sentence Transformers\nSentence Transformers enable semantic search, clustering, and similarity comparisons. Let‚Äôs see how to use them in practice.\n\nBasic Semantic Search\nHere‚Äôs how to encode sentences and find the most similar matches:\n\nfrom sentence_transformers import SentenceTransformer, util\n\n# Load a pre-trained model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\ncorpus = [\n    \"A man is eating food.\",\n    \"A man is eating a piece of bread.\",\n    \"The girl is carrying a baby.\",\n    \"A man is riding a horse.\",\n    \"A woman is playing violin.\",\n    \"Two men pushed carts through the woods.\",\n    \"A man is riding a white horse on an enclosed ground.\",\n    \"A monkey is playing drums.\",\n    \"Someone in a gorilla costume is playing a set of drums.\"\n]\n\n# Encode all sentences into 384-dimensional vectors\ncorpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n\nquery = \"A man is eating pasta.\"\nquery_embedding = model.encode(query, convert_to_tensor=True)\n\n# Compute cosine similarities\nhits = util.semantic_search(query_embedding, corpus_embeddings, top_k=3)\n\nprint(f\"Query: {query}\")\nprint(\"\\nTop 3 most similar sentences:\")\nfor hit in hits[0]:\n    print(f\"{corpus[hit['corpus_id']]} (Score: {hit['score']:.4f})\")\n\nExpected output shows that the model correctly identifies ‚Äúeating pasta‚Äù is semantically closest to ‚Äúeating food‚Äù and ‚Äúeating bread‚Äù, even though the exact words don‚Äôt match. This is semantic search: matching by meaning, not keywords.\n\n\nClustering Documents\nYou can also cluster documents by their semantic content:\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\nsentences = [\n    \"Python is a programming language\",\n    \"Java is used for software development\",\n    \"The cat sat on the mat\",\n    \"Dogs are loyal animals\",\n    \"Machine learning is a subset of AI\",\n    \"Neural networks mimic the brain\",\n]\n\nembeddings = model.encode(sentences)\n\n# Cluster into 2 groups\nnum_clusters = 2\nclustering_model = KMeans(n_clusters=num_clusters, random_state=42)\nclustering_model.fit(embeddings)\ncluster_assignment = clustering_model.labels_\n\nclustered_sentences = {}\nfor sentence_id, cluster_id in enumerate(cluster_assignment):\n    if cluster_id not in clustered_sentences:\n        clustered_sentences[cluster_id] = []\n    clustered_sentences[cluster_id].append(sentences[sentence_id])\n\nfor cluster_id, cluster_sentences in clustered_sentences.items():\n    print(f\"\\nCluster {cluster_id + 1}:\")\n    for sentence in cluster_sentences:\n        print(f\"  - {sentence}\")\n\nExpected clustering shows the model separates technical/programming sentences from animal-related sentences without any labeled data.\n\n\nChoosing the Right Model\nDifferent Sentence Transformer models optimize for different trade-offs. The all-MiniLM-L6-v2 model is fast and lightweight (384 dimensions), good for most applications. The all-mpnet-base-v2 model offers higher quality (768 dimensions), slower but more accurate. The multi-qa-mpnet-base-dot-v1 model is optimized for question-answering and retrieval tasks. The paraphrase-multilingual-mpnet-base-v2 model supports 50+ languages.\nChoose based on your constraints: speed vs.¬†accuracy, monolingual vs.¬†multilingual, general-purpose vs.¬†domain-specific.\n\n\nArchitecture: The Siamese Network\nThe key innovation is the Siamese Network architecture:\n\n\n\nSiamese Network\n\n\nBoth sentences pass through the same BERT model (shared weights). This ensures they‚Äôre embedded into a common vector space. The pooling layer then collapses each token matrix into a single vector. During training, the loss function pushes similar sentence pairs together and dissimilar pairs apart.\nCommon pooling strategies include mean pooling (average all token vectors, most common), max pooling (take element-wise maximum across tokens), and CLS-token (use the [CLS] token‚Äôs final hidden state, BERT‚Äôs built-in sentence representation).\nMean pooling generally works best because it captures information from all tokens while being robust to varying sentence lengths.\n\n\nWhere This Breaks\nStatic compression is a limitation: a sentence gets exactly one vector, regardless of context. ‚ÄúThe bank‚Äù in ‚Äúthe river bank‚Äù and ‚Äúthe financial bank‚Äù might get similar embeddings if they share enough surrounding words. The model compresses meaning into a fixed point, losing nuance.\nWord order sensitivity is another concern: ‚ÄúThe dog bit the man‚Äù and ‚ÄúThe man bit the dog‚Äù share the same words. If the model relies too heavily on lexical overlap (bag-of-words similarity), they‚Äôll end up dangerously close in vector space. Good models learn syntax, but they‚Äôre not perfect.\nComputational cost matters too. Although retrieval is fast (dot products), encoding large corpora is expensive. Encoding 1 million sentences with a large model can take hours. Pre-compute and cache embeddings whenever possible.\nDomain shift is a practical issue: models trained on general text (Wikipedia, news) may perform poorly on specialized domains (medical, legal). Fine-tuning on domain-specific data helps, but requires labeled sentence pairs.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Sentence Transformers"
    ]
  },
  {
    "objectID": "m04-text/sentence-transformers.html#the-key-takeaway",
    "href": "m04-text/sentence-transformers.html#the-key-takeaway",
    "title": "Sentence Transformers",
    "section": "The Key Takeaway",
    "text": "The Key Takeaway\nSentence Transformers collapse BERT‚Äôs token matrix into a single vector using Siamese Networks and contrastive learning. The result is fast semantic search: encode once, compare with dot products. Choose your pooling strategy and model size based on speed-accuracy trade-offs, and remember that compression always loses information.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Sentence Transformers"
    ]
  }
]
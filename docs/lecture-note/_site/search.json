[
  {
    "objectID": "toc.html",
    "href": "toc.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Home\nWelcome\nAbout\nWhy Applied Soft Computing?\nDiscord\nSetup\nMinidora Usage\nSprint Projects\nDeliverables\n\n\n\n\n\nOverview\nVersion Control with Git & GitHub\nThe Tidy Data Philosophy\nData Provenance\nReproducibility\n\n\n\n\n\nOverview\nPrinciples of Effective Visualization\nVisualizing 1D Data\nVisualizing 2D Data\nVisualizing High-Dimensional Data\nVisualizing Networks\nVisualizing Time-Series\n\n\n\n\n\nOverview\nHands-on\nPrompt Tuning\nAgentic AI\nContext Engineering\n\n\n\n\n\nOverview\nLarge Language Models\nGPT Inference: Sampling Strategies\nTokenization: Unboxing How LLMs Read Text\nTransformers\nBERT & GPT\nSentence Transformers\nWord Embeddings\nSemaxis\nWord Bias\n\n\n\n\n\nOverview\nPart 1: What is an Image?\nPart 2: The Deep Learning Revolution\nPart 3: Using CNN Models\nPart 4: CNN Innovations\n\n\n\n\n\nOverview\nPart 1: From Images to Graphs\nPart 2: The Spectral Perspective\nPart 3: Spatial Graph Networks\nPart 4: Graph Embeddings\n\n\n\n\n\nOverview\nPart 1: When Boundaries Blur\nPart 2: Meaning as Difference\nPart 3: From Categories to Coordinates\nPart 4: Universal Representations"
  },
  {
    "objectID": "toc.html#course-information",
    "href": "toc.html#course-information",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Home\nWelcome\nAbout\nWhy Applied Soft Computing?\nDiscord\nSetup\nMinidora Usage\nSprint Projects\nDeliverables"
  },
  {
    "objectID": "toc.html#module-1-the-data-scientists-toolkit",
    "href": "toc.html#module-1-the-data-scientists-toolkit",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Overview\nVersion Control with Git & GitHub\nThe Tidy Data Philosophy\nData Provenance\nReproducibility"
  },
  {
    "objectID": "toc.html#module-2-visualizing-complexity",
    "href": "toc.html#module-2-visualizing-complexity",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Overview\nPrinciples of Effective Visualization\nVisualizing 1D Data\nVisualizing 2D Data\nVisualizing High-Dimensional Data\nVisualizing Networks\nVisualizing Time-Series"
  },
  {
    "objectID": "toc.html#module-3-agentic-coding",
    "href": "toc.html#module-3-agentic-coding",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Overview\nHands-on\nPrompt Tuning\nAgentic AI\nContext Engineering"
  },
  {
    "objectID": "toc.html#module-4-deep-learning-for-text",
    "href": "toc.html#module-4-deep-learning-for-text",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Overview\nLarge Language Models\nGPT Inference: Sampling Strategies\nTokenization: Unboxing How LLMs Read Text\nTransformers\nBERT & GPT\nSentence Transformers\nWord Embeddings\nSemaxis\nWord Bias"
  },
  {
    "objectID": "toc.html#module-5-deep-learning-for-images",
    "href": "toc.html#module-5-deep-learning-for-images",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Overview\nPart 1: What is an Image?\nPart 2: The Deep Learning Revolution\nPart 3: Using CNN Models\nPart 4: CNN Innovations"
  },
  {
    "objectID": "toc.html#module-6-deep-learning-for-graphs",
    "href": "toc.html#module-6-deep-learning-for-graphs",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Overview\nPart 1: From Images to Graphs\nPart 2: The Spectral Perspective\nPart 3: Spatial Graph Networks\nPart 4: Graph Embeddings"
  },
  {
    "objectID": "toc.html#module-7-representations-structuralism",
    "href": "toc.html#module-7-representations-structuralism",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Overview\nPart 1: When Boundaries Blur\nPart 2: Meaning as Difference\nPart 3: From Categories to Coordinates\nPart 4: Universal Representations"
  },
  {
    "objectID": "m05-images/what-to-learn.html",
    "href": "m05-images/what-to-learn.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "In this module, we will learn how to use neural networks to learn representations of images. We will learn: - Fourier transform on images - Image processing using Fourier transform - Convolutional neural networks - LeNet - AlexNet - VGG - Inception - ResNet - VisTransformer (if time permits)"
  },
  {
    "objectID": "m05-images/what-to-learn.html#what-to-learn-in-this-module",
    "href": "m05-images/what-to-learn.html#what-to-learn-in-this-module",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "In this module, we will learn how to use neural networks to learn representations of images. We will learn: - Fourier transform on images - Image processing using Fourier transform - Convolutional neural networks - LeNet - AlexNet - VGG - Inception - ResNet - VisTransformer (if time permits)"
  },
  {
    "objectID": "m05-images/batch-normalization.html",
    "href": "m05-images/batch-normalization.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Batch Normalization (BN) is a technique used in deep neural networks to stabilize and accelerate training by normalizing the inputs to layers within the network.\n\n\nNormalize the activations coming out of a layer for each feature (channel) independently within the current mini-batch so they have zero mean and unit variance.\n\n\n\nFor a mini-batch B = \\{x_1, ..., x_m\\} and considering a single activation feature:\n\nCalculate Mini-Batch Mean: \\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i\nCalculate Mini-Batch Variance: \\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2\nNormalize: Using the mini-batch mean and variance (adding a small \\epsilon for numerical stability): \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\nScale and Shift: Introduce two learnable parameters, \\gamma (gamma) for scaling and \\beta (beta) for shifting: y_i = \\gamma \\hat{x}_i + \\beta\n\n\nThe parameters \\gamma and \\beta are learned during backpropagation alongside the network’s weights.\nThis process is applied independently to each feature/channel dimension.\n\n\n\n\nIf we just normalized to zero mean and unit variance, the network’s ability to represent information might be limited. Forcing activations into this specific distribution might not always be optimal for the subsequent layers or activation functions.\n\n\\gamma and \\beta give the network flexibility.\nThey allow the network to learn the optimal scale and shift for the normalized activations.\nIf needed, the network can even learn parameters (\\gamma = \\sqrt{\\sigma_B^2 + \\epsilon}, \\beta = \\mu_B) that effectively undo the normalization, restoring the original activation distribution if that proves beneficial.\n\n\n\n\nDuring inference (when making predictions), we often process samples one by one, so calculating mini-batch statistics isn’t feasible or representative.\n\nInstead, BN layers maintain running averages of the mean (\\mu_{pop}) and variance (\\sigma^2_{pop}) encountered across all mini-batches during training.\n\nThese are typically updated using a momentum term:\n\n\\mu_{pop} = \\alpha \\times \\mu_{pop} + (1 - \\alpha) \\times \\mu_B\n\\sigma_{pop}^2 = \\alpha \\times \\sigma_{pop}^2 + (1 - \\alpha) \\times \\sigma_B^2\nwhere \\alpha is a momentum parameter.\n\n\nAt inference time, these fixed population statistics are used for normalization: $ = $\nThe learned \\gamma and \\beta parameters from training are still applied:  y = \\gamma \\hat{x} + \\beta \n\n\n\n\n\nIt’s common practice to place the Batch Norm layer after the Convolutional or Fully Connected layer and before the non-linear activation function (like ReLU).\n\nConv / FC -&gt; Batch Norm -&gt; Activation (ReLU)\n\nHowever, variations in placement exist in different architectures.\n\n(Note: While originally thought to primarily combat “internal covariate shift”, recent research suggests BN’s effectiveness might be more related to smoothing the optimization landscape.)"
  },
  {
    "objectID": "m05-images/batch-normalization.html#the-core-idea",
    "href": "m05-images/batch-normalization.html#the-core-idea",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Normalize the activations coming out of a layer for each feature (channel) independently within the current mini-batch so they have zero mean and unit variance."
  },
  {
    "objectID": "m05-images/batch-normalization.html#how-it-works-during-training",
    "href": "m05-images/batch-normalization.html#how-it-works-during-training",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "For a mini-batch B = \\{x_1, ..., x_m\\} and considering a single activation feature:\n\nCalculate Mini-Batch Mean: \\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i\nCalculate Mini-Batch Variance: \\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2\nNormalize: Using the mini-batch mean and variance (adding a small \\epsilon for numerical stability): \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\nScale and Shift: Introduce two learnable parameters, \\gamma (gamma) for scaling and \\beta (beta) for shifting: y_i = \\gamma \\hat{x}_i + \\beta\n\n\nThe parameters \\gamma and \\beta are learned during backpropagation alongside the network’s weights.\nThis process is applied independently to each feature/channel dimension."
  },
  {
    "objectID": "m05-images/batch-normalization.html#why-scale-and-shift-gamma-and-beta",
    "href": "m05-images/batch-normalization.html#why-scale-and-shift-gamma-and-beta",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "If we just normalized to zero mean and unit variance, the network’s ability to represent information might be limited. Forcing activations into this specific distribution might not always be optimal for the subsequent layers or activation functions.\n\n\\gamma and \\beta give the network flexibility.\nThey allow the network to learn the optimal scale and shift for the normalized activations.\nIf needed, the network can even learn parameters (\\gamma = \\sqrt{\\sigma_B^2 + \\epsilon}, \\beta = \\mu_B) that effectively undo the normalization, restoring the original activation distribution if that proves beneficial."
  },
  {
    "objectID": "m05-images/batch-normalization.html#batch-normalization-during-inference",
    "href": "m05-images/batch-normalization.html#batch-normalization-during-inference",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "During inference (when making predictions), we often process samples one by one, so calculating mini-batch statistics isn’t feasible or representative.\n\nInstead, BN layers maintain running averages of the mean (\\mu_{pop}) and variance (\\sigma^2_{pop}) encountered across all mini-batches during training.\n\nThese are typically updated using a momentum term:\n\n\\mu_{pop} = \\alpha \\times \\mu_{pop} + (1 - \\alpha) \\times \\mu_B\n\\sigma_{pop}^2 = \\alpha \\times \\sigma_{pop}^2 + (1 - \\alpha) \\times \\sigma_B^2\nwhere \\alpha is a momentum parameter.\n\n\nAt inference time, these fixed population statistics are used for normalization: $ = $\nThe learned \\gamma and \\beta parameters from training are still applied:  y = \\gamma \\hat{x} + \\beta"
  },
  {
    "objectID": "m05-images/batch-normalization.html#placement",
    "href": "m05-images/batch-normalization.html#placement",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "It’s common practice to place the Batch Norm layer after the Convolutional or Fully Connected layer and before the non-linear activation function (like ReLU).\n\nConv / FC -&gt; Batch Norm -&gt; Activation (ReLU)\n\nHowever, variations in placement exist in different architectures.\n\n(Note: While originally thought to primarily combat “internal covariate shift”, recent research suggests BN’s effectiveness might be more related to smoothing the optimization landscape.)"
  },
  {
    "objectID": "course/why-applied-soft-computing.html",
    "href": "course/why-applied-soft-computing.html",
    "title": "Why applied soft computing?",
    "section": "",
    "text": "Imagine trying to explain to someone how you recognize your friend’s face. Sure, you do it instantly - but try writing down the exact rules! Should you measure the eye spacing? Check nose shape? It’s nearly impossible to write rigid rules for something our brains do effortlessly.\nThis is where neural networks come in - they learn and adapt like our brains, without needing exact rules.",
    "crumbs": [
      "Home",
      "Course Information",
      "Why applied soft computing?"
    ]
  },
  {
    "objectID": "course/why-applied-soft-computing.html#mind-blowing-neural-network-achievements",
    "href": "course/why-applied-soft-computing.html#mind-blowing-neural-network-achievements",
    "title": "Why applied soft computing?",
    "section": "Mind-Blowing Neural Network Achievements",
    "text": "Mind-Blowing Neural Network Achievements\n\nThe AI Artist Who Beat Human Artists\n\n\n\nAI-generated artwork that won first place in a digital art competition\n\n\nIn 2022, something unexpected happened in the art world: an AI-generated artwork won first place in a digital art competition, beating out human artists! The creator, Jason Allen, used Midjourney AI to generate the winning piece after 80 hours of careful prompting. While critics claimed it was “just pressing buttons,” the win sparked a huge debate about the future of art.\n\n\nFaces That Don’t Exist\n\n\n\nA human face generated by StyleGAN\n\n\nVisit ThisPersonDoesNotExist.com and you’ll see something uncanny - incredibly realistic human faces that never existed! Each refresh shows a new face created by StyleGAN, complete with unique features, expressions, and even tiny details like skin pores. The wild part? Even experts sometimes can’t tell these AI-generated faces from real photos!\n\n\nChatGPT: The AI That Talks Like Us\n\n\n\nChatGPT interface\n\n\nWhen ChatGPT appeared, it shocked everyone with its human-like conversations. It doesn’t just answer questions - it writes poetry, explains complex topics, helps with coding, and even gets jokes! While earlier chatbots were obviously robotic, ChatGPT’s natural responses often make people wonder if they’re really chatting with an AI.\n\n\nThe 50-Year Puzzle Solver\n\n\n\nAlphaFold logo and interface\n\n\nScientists struggled for 50 years to predict how proteins fold - a crucial problem in biology. Then came AlphaFold, which not only solved the problem but did it with near-perfect accuracy! This task was thought to be so complex that it would take decades more to solve. Instead, AlphaFold did in days what used to take months in laboratories.\n\n\nThe Go Master’s Impossible Move\n\n\n\nAlphaGo versus Lee Sedol\n\n\nThe ancient game of Go was considered too complex for AI - until AlphaGo shocked the world by defeating champion Lee Sedol. But the real surprise came in Game 2, with “Move 37” - a play so creative and unexpected that Go experts initially thought it was a mistake! This move, later described as “神の一手” (the divine move), showed that AI could think in ways humans never imagined.\n\n\nSora: Making Movies from Words\n\n\n\nSora video generation example\n\n\nJust when we thought AI couldn’t get more impressive, OpenAI’s Sora arrived in 2024, turning text into realistic 60-second videos. The shocking part? These aren’t just simple animations - they’re physics-accurate scenes with multiple moving elements that look like they were filmed in the real world. It’s like having a movie studio in your pocket!\n\n\nThe Doctor That Sees More\n\n\n\nMedical imaging AI\n\n\nAI systems can now spot cancer in medical scans better than human doctors. Google Health’s system proved more accurate than radiologists at detecting breast cancer, reducing both missed cases and false alarms. It’s not replacing doctors, but it’s giving them a super-powered second opinion.\n\n\nCars That Drive Better Than Us\n\n\n\nTesla self-driving car\n\n\nSelf-driving cars were once science fiction. Now, neural networks help them process information from multiple sensors faster than any human could, making split-second decisions to avoid accidents. In many conditions, they’re already safer drivers than humans, reacting faster and staying alert 100% of the time.",
    "crumbs": [
      "Home",
      "Course Information",
      "Why applied soft computing?"
    ]
  },
  {
    "objectID": "course/why-applied-soft-computing.html#why-this-matters",
    "href": "course/why-applied-soft-computing.html#why-this-matters",
    "title": "Why applied soft computing?",
    "section": "Why This Matters",
    "text": "Why This Matters\nWhat’s truly remarkable is that most of these breakthroughs happened in just the last few years - within our lifetime! Tasks that experts thought would take decades to solve are being conquered by neural networks at an incredible pace. They’re not just matching human abilities - they’re surpassing them in ways that surprise even the leading researchers. ?",
    "crumbs": [
      "Home",
      "Course Information",
      "Why applied soft computing?"
    ]
  },
  {
    "objectID": "course/how-to-submit-assignment.html",
    "href": "course/how-to-submit-assignment.html",
    "title": "How to submit coding assignment",
    "section": "",
    "text": "In this course, we will use GitHub Classroom to submit & grade assignments. Please follow the instructions below to submit your assignment.",
    "crumbs": [
      "Home",
      "Course Information",
      "How to submit coding assignment"
    ]
  },
  {
    "objectID": "course/how-to-submit-assignment.html#option-1-a-simple-workflow-full-local",
    "href": "course/how-to-submit-assignment.html#option-1-a-simple-workflow-full-local",
    "title": "How to submit coding assignment",
    "section": "Option 1: A simple workflow (Full local)",
    "text": "Option 1: A simple workflow (Full local)\nSee the slides for the detailed instructions.\n\nClone the repository from GitHub.\nEdit the assignment.py with marimo editor. Type marimo edit assignment/assignment.py\nSubmit the assignment.py via git. (You can use GitHub Desktop, or command line)\nCheck the grading on the GitHub Classroom.",
    "crumbs": [
      "Home",
      "Course Information",
      "How to submit coding assignment"
    ]
  },
  {
    "objectID": "course/how-to-submit-assignment.html#option-2-github-codespaces-full-cloud",
    "href": "course/how-to-submit-assignment.html#option-2-github-codespaces-full-cloud",
    "title": "How to submit coding assignment",
    "section": "Option 2: Github Codespaces (Full cloud)",
    "text": "Option 2: Github Codespaces (Full cloud)\nSee the slides for the detailed instructions.\n\nGo to your assignment repository on GitHub\nClick the green “Code” button\nClick the “Open with Codespaces” button\nWait for the Codespaces to be ready.\nType ‘marimo edit assignment/assignment.py’. If you cannot find marimo, type “uv run marimo edit assignment/assignment.py” which should work.\nYou will be redirected to a webpage and prompted to enter the access token. The access token can be found on the terminal window in the Codespaces.\nTake the access token in the url “the alphabets after”?access_token=” and enter the token in the webpage.",
    "crumbs": [
      "Home",
      "Course Information",
      "How to submit coding assignment"
    ]
  },
  {
    "objectID": "course/how-to-submit-assignment.html#option-3-local-but-with-docker-machine",
    "href": "course/how-to-submit-assignment.html#option-3-local-but-with-docker-machine",
    "title": "How to submit coding assignment",
    "section": "Option 3: Local but with Docker Machine",
    "text": "Option 3: Local but with Docker Machine\nSee the slides for the detailed instructions.\n\nPreparations\n\nInstall Docker Desktop\nInstall GitHub Desktop\nInstall VS Code\n\n\n\nSteps\n\nClone the repository from GitHub.\nOpen with the VS Code, and click “Reopen in Container”\nOpen the assignment.py with marimo editor.\nSubmit the assignment.py to the repository.",
    "crumbs": [
      "Home",
      "Course Information",
      "How to submit coding assignment"
    ]
  },
  {
    "objectID": "course/deliverables.html",
    "href": "course/deliverables.html",
    "title": "Deliverables",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis guide outlines the expectations for your course project. We will define the philosophy behind a successful report, walk through the essential components of your proposal, and detail the requirements for your final video and code submission.",
    "crumbs": [
      "Home",
      "Course Information",
      "Deliverables"
    ]
  },
  {
    "objectID": "course/deliverables.html#general-remarks-on-the-project-reports",
    "href": "course/deliverables.html#general-remarks-on-the-project-reports",
    "title": "Deliverables",
    "section": "",
    "text": "Project reports are not solely focused on the final results, but also on the process and decisions made along the way. We expect to hear the reasons for your final decisions, for instance the reason why you choose X, over alternative options like Y.\n\nClarify the objectives and goal of your project. What do you want to do it, and why are your questions important to us?\nProvide a detailed description about the data you will use. Where the data are collected from, how they are compiled and preprocessed for your analysis. What are the data type of your focal features, and what features do you think are relevant for your analysis?\nDetermine the appropriate methods. Additionally, consider discussing the methods used in previous studies. Considering the data types and the information you aim to present, what methods could potentially be suitable? It would also be beneficial to explore what approaches others have taken when working with similar datasets.\nClarify the limitation and advantage of your approach. The limitation and advantage stems from data and methodologies, and must be discussed in light of existing works. For instance, you want to develop a link prediction algorithm for a social network based on the common neighbor approach. What are the fundamental assumption underlying the link prediction algorithms? When does the algorithm fail? Can you think of the advantage of your algorithm over other alternatives such as graph neural networks?\nEmbrace failures. As Thomas Edison famously said, “I have not failed. I’ve just found 10,000 ways that won’t work.” In many cases, works and analyses may appear to follow a single pathway, but it is important to recognize that this is just one of many paths that people have taken, many of which have turned out to be unsuccessful. It is crucial to try out multiple candidates, and more importantly, to document your failures and understand why they did not work. Consider using fake data, small subsets, mock-ups, and sketches. These methods can help you iterate and refine your approach, ultimately leading to more successful outcomes.",
    "crumbs": [
      "Home",
      "Course Information",
      "Deliverables"
    ]
  },
  {
    "objectID": "course/deliverables.html#proposal",
    "href": "course/deliverables.html#proposal",
    "title": "Deliverables",
    "section": "Proposal",
    "text": "Proposal\nA document should include the following sections:\n\nProject Title\nTeam Members (1-4 people; keep in mind that a larger team is expected to accomplish more than a smaller one)\nAbstract: A concise summary of your project.\nIntroduction: Provide motivation, background, and objectives for your project. Explain why it is important or interesting and why others should care. Review and discuss relevant existing works, particularly those that have inspired your project. Critique these works substantively. Remember, there is always a wealth of relevant work available.\nQuestions or Objectives: Specify the methods you plan to create and what you hope to discover from the data.\nDatasets and Methods: Identify the dataset you will be using. If you haven’t done so already, I strongly encourage you to reconsider your project. Obtaining and cleaning datasets can be time-consuming. Describe the dataset, including its structure and data types if it is tabular. Explain the methods you plan to apply and why you have chosen them. Finally, provide detailed information about the dataset to convincingly argue that it is suitable for your project and proposed methods.\n\n\nReferences",
    "crumbs": [
      "Home",
      "Course Information",
      "Deliverables"
    ]
  },
  {
    "objectID": "course/deliverables.html#final-presentation",
    "href": "course/deliverables.html#final-presentation",
    "title": "Deliverables",
    "section": "Final Presentation",
    "text": "Final Presentation\nWe ask you to create a 10-minute video and upload it to YouTube. You have the option to publish it or keep it unlisted. You can choose any format you prefer. The goal is to include a thorough analysis while making the content interesting and enjoyable.\nWe evaluate the video on three criteria. First we look at the strength of the case you present. Second we assess the quality of your analysis. Third we judge the production and delivery of your presentation. Once you complete your video, we encourage you to share it on Slack to receive feedback from fellow students and instructors. Seeing what others have accomplished is a great way to learn.",
    "crumbs": [
      "Home",
      "Course Information",
      "Deliverables"
    ]
  },
  {
    "objectID": "course/deliverables.html#final-report",
    "href": "course/deliverables.html#final-report",
    "title": "Deliverables",
    "section": "Final Report",
    "text": "Final Report\nYou will submit your code along with a report on your work. Ideally, you will organize your code in well-documented Jupyter notebooks. You can look at Peter Norvig’s notebooks or high-quality Kaggle kernels for inspiration on structure and clarity.\nThe report has no minimum or maximum length. Your responsibility is to ensure all topics are thoroughly addressed in clear writing. The specific format and ingredients will depend on the type of project you undertake. If you are creating a software package or a website, your report may focus more heavily on the technical aspects of implementation.",
    "crumbs": [
      "Home",
      "Course Information",
      "Deliverables"
    ]
  },
  {
    "objectID": "course/deliverables.html#idea-sketch-template",
    "href": "course/deliverables.html#idea-sketch-template",
    "title": "Deliverables",
    "section": "Idea Sketch Template",
    "text": "Idea Sketch Template\nStarting a project is often the hardest part because ideas feel nebulous in the mind. Writing serves as a scaffolding to think through a research project. We personally use a specific list of questions to materialize ideas. You can use these questions as a living document that specific updates as your project progresses. We suggest answering each question in two to three sentences and setting a timer for 15 minutes per question. If a question takes longer, it often highlights a weakness in the current form of the idea.\nYou start with the Project Overview to define the core focus. You ask if you are developing something new or testing existing ideas. Next is Project Value. You need to articulate what makes this work meaningful. Then you identify Research Gaps by asking what key questions remain unsolved in the area.\nThe next step is to define your Novel Approach. You must explain what makes your solution unique compared to existing methods. This leads to Necessity. You ask why a new solution is needed if others exist and what specific advantages your approach offers.\nYou then define Success Metrics to determine how you will measure success. You also need a Validation Strategy to establish criteria or tests that demonstrate your solution works. You should consider Broader Impact to understand how the work benefits fields beyond your immediate area. Finally you create an Implementation Plan by breaking down each goal into concrete and actionable tasks.",
    "crumbs": [
      "Home",
      "Course Information",
      "Deliverables"
    ]
  },
  {
    "objectID": "m07-representation/overview.html",
    "href": "m07-representation/overview.html",
    "title": "Module 7: Representations & Structuralism",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module explores how machine learning sees the world through a structuralist lens.\nYou’ll learn:\n\nWhy we draw boundaries where none exist in continuous reality and how observation shapes categories.\nHow meaning emerges from relationships rather than fixed definitions in Saussurean linguistics and structural anthropology.\nWhat vector representations reveal about the continuous nature of concepts and why embeddings restore lost gradation.\nHow embeddings dissolve artificial divisions while preserving relational structure across text, images, networks, and time-series.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Overview"
    ]
  },
  {
    "objectID": "m07-representation/overview.html#the-journey",
    "href": "m07-representation/overview.html#the-journey",
    "title": "Module 7: Representations & Structuralism",
    "section": "The Journey",
    "text": "The Journey\nHave you ever wondered where one category ends and another begins? The world is continuous, but we carve it into categories. This module traces a philosophical thread from linguistic structuralism to modern representation learning.\nPart 1: When Boundaries Blur\nWe start by questioning the categories we take for granted. Regional dialects, generational labels, and even physical objects have fuzzy boundaries that we artificially sharpen. Is a 17-year-old fundamentally different from an 18-year-old, and where exactly does “blue” end and “green” begin? These divisions feel natural because language demands them, but they’re artifacts of how we choose to see.\nPart 2: Meaning as Difference\nHere’s a radical idea: language doesn’t label pre-existing categories but creates them through contrast. A word means what it means because of what it is not. We explore how Saussure, Buddhist logic, and structural anthropology all converge on this insight that meaning is relational, not absolute.\nPart 3: From Categories to Coordinates\nWhat happens when we take structuralism seriously? Word2Vec and modern embedding techniques operationalize this philosophical insight by mapping concepts into continuous vector space, restoring the gradation that categorical thinking destroys. “King” - “man” + “woman” ≈ “queen” isn’t magic but geometry encoding relational meaning.\nPart 4: Universal Representations\nThe representational insight extends far beyond language to networks (embedded in continuous space), images (mapped to semantic coordinates), and time-series (trajectories through latent dimensions). You’ll see how this structuralist perspective unifies disparate areas of machine learning.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Overview"
    ]
  },
  {
    "objectID": "m07-representation/overview.html#why-this-matters",
    "href": "m07-representation/overview.html#why-this-matters",
    "title": "Module 7: Representations & Structuralism",
    "section": "Why This Matters",
    "text": "Why This Matters\nTraditional machine learning forces continuous phenomena into discrete boxes through classification boundaries and sharp node edges, but a new generation of techniques embraces continuity. Understanding representation learning changes how you think about data by shifting from “What category does this belong to?” to “What relationships does this participate in?”\nThis perspective matters for fairness. When we recognize that categories are observer-dependent rather than natural, we become more careful about how we define them, and when we see bias in embeddings, we understand it’s reflecting the relational structure of our training data, not objective truth. These ideas connect to broader themes in science and philosophy, as machine learning provides computational answers to questions philosophers have asked for centuries about dividing continuous reality into discrete concepts.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Overview"
    ]
  },
  {
    "objectID": "m07-representation/overview.html#prerequisites",
    "href": "m07-representation/overview.html#prerequisites",
    "title": "Module 7: Representations & Structuralism",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou should have completed Module 4 on text and word embeddings, understanding how word2vec works and what embeddings represent as we build on those concrete techniques to explore their philosophical implications. Basic familiarity with dimensionality reduction (PCA, t-SNE, UMAP) from Module 2 helps, as we’ll discuss how these techniques reveal structure by projecting high-dimensional representations into interpretable spaces. No formal philosophy background is required since we introduce structuralist concepts as needed, but curiosity about the conceptual foundations of machine learning is essential.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Overview"
    ]
  },
  {
    "objectID": "m07-representation/overview.html#what-youll-build",
    "href": "m07-representation/overview.html#what-youll-build",
    "title": "Module 7: Representations & Structuralism",
    "section": "What You’ll Build",
    "text": "What You’ll Build\nBy the end of this module, you’ll develop a conceptual framework for understanding representation learning, seeing connections between linguistic structuralism and modern embeddings while understanding why continuous representations often outperform discrete categories. You’ll analyze how different embedding techniques preserve different aspects of relational structure, recognize when discrete boundaries help and when they hurt, and think critically about how categorization shapes both human cognition and machine learning systems. Most importantly, you’ll gain philosophical depth that complements your technical skills, transforming you from someone who uses tools to someone who understands their foundations.\nLet’s begin by examining the boundaries we take for granted.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Overview"
    ]
  },
  {
    "objectID": "m07-representation/03-embeddings.html",
    "href": "m07-representation/03-embeddings.html",
    "title": "Part 3: From Categories to Coordinates",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module introduces vector embeddings as the computational realization of structuralism.\nYou’ll learn:\n\nWhat vector embeddings are and how they dissolve artificial category boundaries.\nHow word2vec learns meaning from context through the distributional hypothesis.\nThe role of contrastive learning and negative sampling in carving semantic space.\nWhy vector arithmetic captures relationships as geometric directions.\nHow hyperbolic embeddings use geometry to capture hierarchy (metaphor) vs. association (metonymy).\nThe counterintuitive insight that meaning emerges from exclusion (Apoha theory) and the notion of continuous representation.\nPractical consequences for understanding political ideology, semantic similarity, and beyond.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Part 3: From Categories to Coordinates"
    ]
  },
  {
    "objectID": "m07-representation/03-embeddings.html#the-limits-of-traditional-classification",
    "href": "m07-representation/03-embeddings.html#the-limits-of-traditional-classification",
    "title": "Part 3: From Categories to Coordinates",
    "section": "The Limits of Traditional Classification",
    "text": "The Limits of Traditional Classification\nLet’s talk about how traditional machine learning builds decision boundaries. You collect labeled data, extract features, and train a classifier to draw a line (or hyperplane) separating Class A from Class B. The output is binary: spam or not-spam, fraud or legitimate, cat or dog.\nThis approach works when categories are genuinely discrete. But what about political ideology? You might lean liberal on economic issues but conservative on others. Your position isn’t “left or right” but a point in a multidimensional space, and forcing it into a binary choice destroys information.\nWhat about word meanings? Is “dog” more similar to “wolf” or “cat”? The answer is “it depends” on whether you care about biology (dogs and wolves share a genus) or domestication (dogs and cats are both pets). A classification system would force you to choose one dimension, while a continuous representation can capture both simultaneously.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Part 3: From Categories to Coordinates"
    ]
  },
  {
    "objectID": "m07-representation/03-embeddings.html#enter-vector-embeddings",
    "href": "m07-representation/03-embeddings.html#enter-vector-embeddings",
    "title": "Part 3: From Categories to Coordinates",
    "section": "Enter Vector Embeddings",
    "text": "Enter Vector Embeddings\nVector embeddings solve this by mapping each concept to coordinates in a high-dimensional continuous space. Instead of “this word belongs to category X”, you get “this word lives at position [0.23, -0.15, 0.87, ...] in 300-dimensional space”. Similarity becomes geometric distance, and relationships become directional vectors.\n\nThis is the mathematical realization of structuralism. Each word’s meaning is determined by its position relative to all other words. There are no hard boundaries, only neighborhoods of varying density.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Part 3: From Categories to Coordinates"
    ]
  },
  {
    "objectID": "m07-representation/03-embeddings.html#how-word2vec-works",
    "href": "m07-representation/03-embeddings.html#how-word2vec-works",
    "title": "Part 3: From Categories to Coordinates",
    "section": "How Word2Vec Works",
    "text": "How Word2Vec Works\nWord2vec learns through a prediction task. Given a center word, predict the context words around it.\nSpecifically, the model of word2vec is given by\n\nP(j \\vert i) = \\frac{\\exp(u_i^\\top v_j)}{\\sum_{k \\in \\mathcal{V}} \\exp(u_i^\\top v_k)},\n\nwhere u_i is the “in-vector” of center word i and v_j is the “out-vector” of context word j.\nBut here’s the problem. The denominator involves a sum over all words in the dictionary, which is not computable even for a moderate vocabulary size since we need to compute the probability of all words many times during training.\nWhat’s the solution? Negative Sampling offers an elegant answer.\nInstead of calculating probabilities over the entire dictionary, we play a simpler game. We give the model a pair of words and ask: “Is this a real context pair, or did I just randomly grab a word from the dictionary?”\nThe model tries to output 1 for real pairs and 0 for random (negative) pairs. Mathematically, we want to maximize this objective:\n J(\\theta) = \\log \\sigma(\\mathbf{u}_w \\cdot \\mathbf{v}_c) + \\sum_{i=1}^k \\mathbb{E}_{v_n \\sim P_n} [\\log \\sigma(-\\mathbf{u}_w \\cdot \\mathbf{v}_{n})] \nHere, \\sigma is the sigmoid function that squashes values between 0 and 1. The first term encourages the model to put (v_c) close to (v_w). The second term encourages it to push (v_c) away from (v_n).\n\nWhy Does Negative Sampling Work?\nWe can prove that this seemingly ad-hoc contrastive training is actually doing the right thing. Let us write explicitly the sigmoid function:\n\np(y=1 \\vert x) = \\sigma(x) = \\frac{1}{1 + \\exp(-x)}\n\nThe probability that two words x=(i,j) are positive (i.e., y=1) relative to the probability that they are not (i.e., y=0) is given by:\n\n\\log \\frac{p(y=1 \\vert x)}{p(y=0 \\vert x)} = x\n\nThus, the argument of the sigmoid function (i.e., x) represents the log-odds of the positive example, also called logits.\nNow, let us make it clear that the negative samples are sampled from a probability distribution q. When observing a word pair x=(i,j), there are two possible events:\n\ny=1: The pair (i,j) is a real context pair, with probability p(i,j)\ny=0: The pair (i,j) is sampled by random chance, with probability q(i,j)\n\nThe probability that x comes from the positive class is given by:\n\np(y=1 \\vert x) = \\frac{p(i,j)}{p(i,j) + q(i,j)}\n\nThis is a sigmoid function, i.e.,\n\np(y=1 \\vert x) = \\frac{1}{1 + \\exp\\left(-\\ln p(i,j) + \\ln q(i,j)\\right)}\n\nwith the logits given by\n\n\\ln p(i,j) - \\ln q(i,j)\n\nwhich word2vec expresses as the dot similarity of two vectors, i.e.,\n\nu_i^\\top v_j\n\nPutting together we have\n\nu_i^\\top v_j = \\ln p(i,j) - \\ln q(j) \\implies p(i,j) = \\frac{q(i,j)\\exp\\left(u_i^\\top v_j\\right)}{\\sum_{k \\in \\mathcal{V}} q(i,k)\\exp\\left(u_i^\\top v_k\\right)}\n\nTherefore, negative sampling learns the word2vec model when sampling the negatives uniformly at random q(j) = \\frac{1}{|\\mathcal{V}|}.\nIn practice, the noise distribution q is not uniform but is (roughly) proportional to the frequency of the word j. Consequently, the learned model is not precisely the word2vec model but a variant with an inference bias. Yet, this bias is surprisingly useful in downstream tasks. See (Kojaku et al. 2021) and (Kojaku et al. 2023) for more details.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Part 3: From Categories to Coordinates"
    ]
  },
  {
    "objectID": "m07-representation/03-embeddings.html#word2vec-implicitly-factorizes-a-co-occurrence-matrix",
    "href": "m07-representation/03-embeddings.html#word2vec-implicitly-factorizes-a-co-occurrence-matrix",
    "title": "Part 3: From Categories to Coordinates",
    "section": "Word2Vec Implicitly Factorizes a Co-Occurrence Matrix",
    "text": "Word2Vec Implicitly Factorizes a Co-Occurrence Matrix\n\nWe have taken for granted that word2vec learns useful embeddings of words. But why are learned embeddings so useful despite being learned heuristically?\nThe previous section allows us to approach this theoretical question. It provides the analytical form of the similarity as:\n\nu_i^\\top v_j = \\ln p(i,j) - \\ln q(j).\n\nNotice that u_i, v_i have dimension d much smaller than the vocabulary size |\\mathcal{V}|. On the other hand, the right hand side can have distinct values for each pair (i,j), forming a matrix with |\\mathcal{V}|^2 entries.\nFrom this perspective, word2vec is approximating this matrix by a product of two submatrices:\n\n\\mathbf{U} \\cdot \\mathbf{V}^\\top = \\mathbf{R},\n\nwhere\n\n\\begin{align}\n\\mathbf{R} &= \\log p(i,j) - \\log q(j) \\\\\n\\mathbf{U} &= \\begin{bmatrix}\nu_1 \\\\ u_2 \\\\ \\vdots \\\\ u_{|\\mathcal{V}|}\n\\end{bmatrix},\n\\mathbf{V} &= \\begin{bmatrix}\nv_1 \\\\ v_2 \\\\ \\vdots \\\\ v_{|\\mathcal{V}|}\n\\end{bmatrix}\n\\end{align}\n\nWhat does R_{ij} mean? The matrix entry R_{ij} has an information-theoretic meaning.\nIt is called the pointwise mutual information (PMI) between words i and j. In other words, the dot product of the vectors represents this information-theoretic quantity.\nThis is an interesting insight as it provides a clear interpretation of the learned embeddings. Furthermore, it also provides a clear analytical threshold as to when word2vec can capture groups and miss them. See (Kojaku et al. 2023) for more details.\n\nA Universe of Contrast\nThis principle of pulling positive examples together while pushing negative ones apart grew into one of the most successful ideas in modern AI: Contrastive Learning.\nDifferent tasks require different mathematical formulations of this “push-pull” dynamic.\n\nRanking & Triplet Loss\nUsed historically in systems like FaceNet for face recognition. The goal here is relative order.\nWe don’t care about the absolute coordinates, only that your face is closer to your own photos than to anyone else’s.\nWe define an anchor (a), a positive example (p), and a negative example (n). We want the distance d(a, p) to be smaller than d(a, n) by at least a margin m:\n L = \\max(0, d(a, p) - d(a, n) + m) \nThis forces a structured geometry where: d(a, p) + m &lt; d(a, n). The model learns to “rank” the correct match higher than the incorrect one.\n\n\nInfoNCE (SimCLR, CLIP)\nThis is the standard loss for modern self-supervised learning (like SimCLR or CLIP). Instead of comparing one positive against one negative, we play a game of “pick the correct partner from the lineup.”\nGiven a batch of N examples, the model must identify the single correct positive pair (i, j) against all other N-1 “negative” samples in the batch.\n L_{i,j} = -\\log \\frac{\\exp(\\text{sim}(\\mathbf{z}_i, \\mathbf{z}_j) / \\tau)}{\\sum_{k=1}^{2N} \\mathbb{1}_{[k \\neq i]} \\exp(\\text{sim}(\\mathbf{z}_i, \\mathbf{z}_k) / \\tau)} \nIt looks like a Softmax (specifically, categorical cross-entropy), but calculated only over the current batch rather than the full vocabulary. It forces the embedding to filter out specifically hard negatives from the current batch.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Part 3: From Categories to Coordinates"
    ]
  },
  {
    "objectID": "m07-representation/03-embeddings.html#the-magic-of-vector-arithmetic",
    "href": "m07-representation/03-embeddings.html#the-magic-of-vector-arithmetic",
    "title": "Part 3: From Categories to Coordinates",
    "section": "The Magic of Vector Arithmetic",
    "text": "The Magic of Vector Arithmetic\nOnce you have vectors, you can do arithmetic on meaning. The famous example:\n \\vec{\\text{King}} - \\vec{\\text{Man}} + \\vec{\\text{Woman}} \\approx \\vec{\\text{Queen}} \nWhy does this work? Relationships are directions in space.\nThe vector from “Man” to “King” represents the relationship “royal-version-of”. When you apply that same direction to “Woman”, you arrive near “Queen”.\nThe arrows from countries to capitals are parallel. They point in roughly the same direction because they represent the same relationship.\nThis structure wasn’t programmed in. It emerged from observing how words co-occur in text.\nThe model discovered that “France” and “Paris” appear in similar contexts to how “Germany” and “Berlin” appear, and encoded this as geometric parallelism.\nThere are many such examples.\n\nTshitoyan, V., Dagdelen, J., Weston, L., Dunn, A., Rong, Z., Kononova, O., … & Jain, A. (2019). Unsupervised word embeddings capture latent knowledge from materials science literature. Nature, 571(7763), 95-98.\nPeng, H., Ke, Q., Budak, C., Romero, D. M., & Ahn, Y. Y. (2021). Neural embeddings of scholarly periodicals reveal complex disciplinary organizations. Science Advances, 7(17), eabb9004.\nKim, S., Ahn, Y. Y., & Park, J. (2024, May). Labor space: A unifying representation of the labor market via large language models. In Proceedings of the ACM Web Conference 2024 (pp. 2441-2451).\nKe, Q. (2019). Identifying translational science through embeddings of controlled vocabularies. Journal of the American Medical Informatics Association, 26(6), 516-523.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Part 3: From Categories to Coordinates"
    ]
  },
  {
    "objectID": "m07-representation/03-embeddings.html#image-embeddings-also-exhibit-semantic-structure",
    "href": "m07-representation/03-embeddings.html#image-embeddings-also-exhibit-semantic-structure",
    "title": "Part 3: From Categories to Coordinates",
    "section": "Image Embeddings Also Exhibit Semantic Structure",
    "text": "Image Embeddings Also Exhibit Semantic Structure\n\nThis phenomenon isn’t limited to text. Generative models for images, such as VAEs (Variational Autoencoders) and GANs (Generative Adversarial Networks), learn a high-dimensional latent space where every point represents a possible image.\nJust as with word embeddings, this space is semantically structured. Moving in a specific direction corresponds to a meaningful change in the image.\nFor example, we can calculate an attribute vector for “smiling” by taking the average position of all smiling faces and subtracting the average position of non-smiling faces:\n \\vec{v}_{\\text{smile}} = \\text{mean}(\\text{smiling faces}) - \\text{mean}(\\text{neutral faces}) \nAdding this vector to the latent code of a new, neutral face will continuously transform it into a smiling one. This works for glasses, age, gender, and many other attributes.\nThe model doesn’t just memorize pixels. It learns the underlying factors of variation in the visual world. See White, T. (2016). Sampling generative networks. arXiv preprint arXiv:1609.04468..",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Part 3: From Categories to Coordinates"
    ]
  },
  {
    "objectID": "m07-representation/03-embeddings.html#hyperbolic-embeddings",
    "href": "m07-representation/03-embeddings.html#hyperbolic-embeddings",
    "title": "Part 3: From Categories to Coordinates",
    "section": "Hyperbolic Embeddings",
    "text": "Hyperbolic Embeddings\n\n\n\n\n\nShift your attention to a fundamental limitation in everything we’ve seen so far. Models like Word2Vec and GloVe are built on Metonymy (from the Greek meta “change” + onyma “name”), learning from the syntagmatic axis, the “next-to” relationship. “Crown” is embedded near “King” because they appear together in text context. They capture association.\nBut what about Metaphor? Metaphor relies on the paradigmatic axis, the “is-a” or “substitution” relationship. A “Lion” is implicitly like a “Tiger” not because they hang out together, but because they occupy the same structural slot in the hierarchy of life (they are both big cats).\nTo capture this structural similarity, researchers turned to WordNet, a massive database explicitly constructed to map these “is-a” (hypernym) relationships. It is a map of taxonomies, effectively a giant tree of concepts.\nWhen they tried to train standard Euclidean embeddings on WordNet, they hit a wall. Even with hundreds of dimensions, the models struggled to represent the deep, branching hierarchy without distorting the relationships. The problem wasn’t the data but the geometry itself.\nEuclidean space is flat, but in a hierarchy (like a biological taxonomy or a corporate org chart), the number of nodes expands exponentially as you go deeper (Root \\to Children \\to Grandchildren). In flat space, the available volume only grows polynomially, so there simply isn’t enough “room” at the boundaries to fit this exponential growth. You end up crushing the leaf nodes together, losing the distinct structure.\nTo solve this, we need to exit flat space and enter Hyperbolic Space, often visualized as a Poincaré ball. In this non-Euclidean geometry, space itself expands exponentially as you move away from the center. Imagine a disk where, using a specific distance metric (Poincaré distance), the path to the edge is effectively infinite. Because the “room” available grows exactly as fast as the tree branches, you can fit an infinite tree structure within this finite disk by placing the root at the center, with the leaves fitting naturally near the boundary without crushing into each other.\nThis alignment of geometry and data is powerful. While Euclidean models might require 300 dimensions to separate concepts in WordNet, Poincaré embeddings can capture the same complex structure with high precision in as few as 5 dimensions.\nWhen the geometry fits the data, when we model metaphor with a space designed for hierarchies, the representation becomes incredibly efficient.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Part 3: From Categories to Coordinates"
    ]
  },
  {
    "objectID": "m07-representation/01-boundaries.html",
    "href": "m07-representation/01-boundaries.html",
    "title": "Part 1: When Boundaries Blur",
    "section": "",
    "text": "What you’ll learn in this section\n\n\n\nThis section explores the illusion of sharp boundaries in the world around us.\nYou’ll learn:\n\nHow regional dialects blend gradually rather than dividing cleanly at borders.\nWhy generational labels impose arbitrary lines on continuous age cohorts.\nHow color perception varies across languages, revealing subjective categorization.\nThe Ship of Theseus paradox and what it reveals about object identity.\nWhy representation learning offers a way forward by working with continuous variation directly.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Part 1: When Boundaries Blur"
    ]
  },
  {
    "objectID": "m07-representation/01-boundaries.html#the-soft-drink-divide-that-isnt",
    "href": "m07-representation/01-boundaries.html#the-soft-drink-divide-that-isnt",
    "title": "Part 1: When Boundaries Blur",
    "section": "The Soft Drink Divide That Isn’t",
    "text": "The Soft Drink Divide That Isn’t\nHave you ever seen a map dividing America into soda, pop, and Coke regions? These maps circulate widely, showing bold lines that separate linguistic territories. In the Northeast and West Coast, carbonated beverages are “soda”. In the Midwest, they’re “pop”. In the South, everything is “Coke”, even if you’re drinking Sprite.\n\n\n\n\n\n\nSoft drink terminology by region\n\n\n\n\nFigure 1\n\n\n\nHere’s the problem: this boundary doesn’t exist. Drive from Chicago to Atlanta and you won’t cross a line where everyone suddenly switches from “pop” to “Coke”. Instead, you’ll encounter a gradual transition zone spanning hundreds of miles where St. Louis residents might use both terms while Nashville speakers favor “Coke” but still use “soda”.\nThe change is gradational, not binary. We draw boundaries because our analytical apparatus demands nodes, and nodes require boundaries. If everything gradually blends into everything else, we cannot construct the graph that lets us study the system. The boundary is not a feature of reality but a feature of our tools.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Part 1: When Boundaries Blur"
    ]
  },
  {
    "objectID": "m07-representation/01-boundaries.html#the-generation-game",
    "href": "m07-representation/01-boundaries.html#the-generation-game",
    "title": "Part 1: When Boundaries Blur",
    "section": "The Generation Game",
    "text": "The Generation Game\nThis pattern repeats everywhere we look. Consider generational labels like Baby Boomers, Gen X, Millennials, Gen Z. These categories structure entire marketing strategies, political analyses, and cultural narratives.\nThe Pew Research Center officially defines Millennials as those born between 1981 and 1996. This means someone born on December 31, 1996 is a Millennial, but someone born the next day is Gen Z.\n\n\n\n\n\n\nGenerational boundaries impose discrete categories on continuous age distributions\n\n\n\n\nFigure 2\n\n\n\nDoes this make sense? Is there a meaningful discontinuity between these two people that doesn’t exist between someone born in December 1996 and someone born in January 1996? Of course not. The boundary is arbitrary, chosen for analytical convenience, yet once established, it shapes how we think. We ask “what do Millennials believe?” as if the category reflects a natural kind rather than a statistical convenience. ## Colors\n\n\n\n\n\n\nFigure 3\n\n\n\nShift your attention to color. We perceive the continuous variation of light as discrete categories (red, green, blue), but this categorization is not universal. Different languages carve up the color spectrum differently: a traffic light’s middle color is “green” in English but “blue” in Japanese, while English speakers see six colors in a rainbow compared to five in German and seven in Japanese.\n\n\n\n\n\nAll languages describe the same color space but categorize it differently. The boundaries we draw between colors are a choice, not a discovery. The same applies to generational differences: while cultural attitudes, technology exposure, and economic conditions do vary by birth cohort, this variation is continuous. The discrete labels we overlay are a choice about how to slice the continuum, not a discovery of where nature has already sliced it.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Part 1: When Boundaries Blur"
    ]
  },
  {
    "objectID": "m07-representation/01-boundaries.html#the-ship-of-theseus-in-your-body",
    "href": "m07-representation/01-boundaries.html#the-ship-of-theseus-in-your-body",
    "title": "Part 1: When Boundaries Blur",
    "section": "The Ship of Theseus in Your Body",
    "text": "The Ship of Theseus in Your Body\nLet’s push deeper. Perhaps generational boundaries are arbitrary, but surely physical objects have clear boundaries. A ship is a ship. Your body is your body. Right?\nThe Ship of Theseus paradox asks what happens when every plank of a wooden ship gets replaced over time. After thirty years at sea, not a single original board remains. Is it still the same ship?\nIf you gathered all the discarded planks and reassembled them, which ship is the real Theseus?\n\nYour body presents the same puzzle. Nearly every atom in your body gets replaced on a rolling basis. Your skin cells regenerate every few weeks. Your red blood cells last about four months. Even your skeleton completely rebuilds itself every decade.\nThe “you” that exists today shares almost no physical matter with the “you” of ten years ago. Yet you maintain continuity of identity.\nWhere is the boundary of “you”? It’s not spatial, because your atoms disperse and are replaced. It’s not temporal, because you persist despite complete material turnover.\nThe boundary is conceptual. It’s imposed by the observer who needs a stable node labeled “person” to construct social networks, legal systems, and narratives of selfhood.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Part 1: When Boundaries Blur"
    ]
  },
  {
    "objectID": "m07-representation/01-boundaries.html#why-boundaries-matter",
    "href": "m07-representation/01-boundaries.html#why-boundaries-matter",
    "title": "Part 1: When Boundaries Blur",
    "section": "Why Boundaries Matter",
    "text": "Why Boundaries Matter\nThis philosophical excursion has direct implications for how we model systems. Systems consist of parts, and parts require boundaries. But if the phenomena we’re studying are continuous, where do we draw those boundaries?\nTraditional machine learning inherits this problem. Classification algorithms learn decision boundaries that divide feature space into regions, with each region getting a sharp label: you’re either Class A or Class B, with no middle ground. This works when categories are truly discrete (an email is spam or not spam), but it fails when the underlying phenomenon is continuous. Political ideology, disease severity, aesthetic preference all resist binary classification.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Part 1: When Boundaries Blur"
    ]
  },
  {
    "objectID": "m07-representation/01-boundaries.html#the-way-forward",
    "href": "m07-representation/01-boundaries.html#the-way-forward",
    "title": "Part 1: When Boundaries Blur",
    "section": "The Way Forward",
    "text": "The Way Forward\nWhat if we could work with the gradation directly instead of forcing it into boxes? What if meaning didn’t require boundaries? This is the insight that representation learning provides: by mapping concepts into continuous vector spaces, we preserve the relational structure that gives things meaning without imposing artificial divisions.\nThe next section explores where this insight comes from, tracing it back to structural linguistics and the radical claim that meaning emerges from difference rather than essence. Then we’ll see how modern machine learning operationalizes this philosophical stance, turning it from theory into working technology.\n\n\n\n\n\n\nTry it yourself\n\n\n\nThink of three categories that feel natural to you (political party, music genre, personality type). For each one, try to identify specific cases that fall on the boundary. Why is it hard to categorize them? What does this reveal about the nature of the category itself?",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Part 1: When Boundaries Blur"
    ]
  },
  {
    "objectID": "m06-graph/graph-convolutional-network.html",
    "href": "m06-graph/graph-convolutional-network.html",
    "title": "Spatial Graph Convolutional Networks",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module introduces spatial graph convolutional networks and their evolution.\nYou’ll learn:\n\nWhat ChebNet is and how it bridges spectral and spatial domains using Chebyshev polynomials.\nHow Kipf-Welling GCN simplifies convolution with first-order approximation and renormalization tricks.\nThe GraphSAGE approach to sampling and aggregating features for inductive learning on dynamic graphs.\nHow Graph Attention Networks (GAT) learn which neighbors matter most through attention mechanisms.\nThe theoretical foundation of Graph Isomorphism Networks (GIN) and their connection to the Weisfeiler-Lehman test."
  },
  {
    "objectID": "m06-graph/graph-convolutional-network.html#chebnet",
    "href": "m06-graph/graph-convolutional-network.html#chebnet",
    "title": "Spatial Graph Convolutional Networks",
    "section": "ChebNet",
    "text": "ChebNet\nLet’s talk about one of the earliest spatial GCNs that bridges the gap between spectral and spatial domains. ChebNet (Defferrard, Bresson, and Vandergheynst 2016) leverages Chebyshev polynomials to approximate the learned filter {\\bf L}_{\\text{learn}}.\nThe key idea is to express the filter as a polynomial:\n\n{\\bf L}_{\\text{learn}} \\approx \\sum_{k=0}^{K-1} \\theta_k T_k(\\tilde{{\\bf L}}), \\quad \\text{where} \\quad \\tilde{{\\bf L}} = \\frac{2}{\\lambda_{\\text{max}}}{\\bf L} - {\\bf I},\n\nHere \\tilde{{\\bf L}} is the scaled and normalized Laplacian matrix with eigenvalues in the range [-1,1]. The Chebyshev polynomials T_k(\\tilde{{\\bf L}}) transform the eigenvalues recursively:\n\n\\begin{aligned}\nT_0(\\tilde{{\\bf L}}) &= {\\bf I} \\\\\nT_1(\\tilde{{\\bf L}}) &= \\tilde{{\\bf L}} \\\\\nT_k(\\tilde{{\\bf L}}) &= 2\\tilde{{\\bf L}} T_{k-1}(\\tilde{{\\bf L}}) - T_{k-2}(\\tilde{{\\bf L}})\n\\end{aligned}\n\nWe then replace {\\bf L}_{\\text{learn}} in the original spectral GCN with the Chebyshev polynomial approximation:\n\n{\\bf x}^{(\\ell+1)} = h\\left( \\sum_{k=0}^{K-1} \\theta_k T_k(\\tilde{{\\bf L}}){\\bf x}^{(\\ell)}\\right),\n\nWhat does this give us? Each term involves:\n\nT_k(\\tilde{{\\bf L}}) applies the k-th Chebyshev polynomial to the scaled Laplacian matrix\n\\theta_k are the learnable parameters\nK is the order of the polynomial (typically small, e.g., K=3)"
  },
  {
    "objectID": "m06-graph/graph-convolutional-network.html#graph-convolutional-networks-by-kipf-and-welling",
    "href": "m06-graph/graph-convolutional-network.html#graph-convolutional-networks-by-kipf-and-welling",
    "title": "Spatial Graph Convolutional Networks",
    "section": "Graph Convolutional Networks by Kipf and Welling",
    "text": "Graph Convolutional Networks by Kipf and Welling\nWhile ChebNet offers a principled way to approximate spectral convolutions, Kipf and Welling (2017) (Kipf and Welling 2017) proposed an even simpler and highly effective variant called Graph Convolutional Networks (GCN).\n\nFirst-order Approximation\nWhat’s the key departure? GCN uses the first-order approximation of the Chebyshev polynomials.\n\ng_{\\theta'} * x \\approx \\theta'_0x + \\theta'_1(L - I_N)x = \\theta'_0x - \\theta'_1D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}x\n\nThis is a crude approximation but leads to a much simpler form. It leaves only two learnable parameters instead of K parameters in the original ChebNet.\nThe authors further simplify by using the same \\theta for both remaining parameters (i.e., \\theta_0 = \\theta and \\theta_1 = -\\theta). The result is this convolutional filter:\n\ng_{\\theta} * x \\approx \\theta(I_N + D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}})x\n\nWhile this is a very simple filter, one can stack multiple layers of convolutions to perform high-order graph convolutions.\n\n\nStabilizing Deep GCNs\nGCN models can be deep. When they are too deep, they start suffering from an ill-posed problem called gradient vanishing/exploding, where the gradients of the loss function become too small or too large to update the model parameters.\nTo facilitate the training of deep GCNs, the authors introduce a simple trick called renormalization. The idea is to add self-connections to the graph:\n\n\\tilde{A} = A + I_N, \\quad \\text{and} \\quad \\tilde{D}_{ii} = \\sum_j \\tilde{A}_{ij}\n\nWe then use \\tilde{A} and \\tilde{D} to form the convolutional filter.\nAltogether, this leads to the following layer-wise propagation rule:\nX^{(\\ell+1)} = \\sigma(\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}X^{(\\ell)}W^{(\\ell)})\nwhere:\n\nX^{(\\ell)} is the matrix of node features at layer \\ell\nW^{(\\ell)} is the layer’s trainable weight matrix\n\\sigma is a nonlinear activation function (e.g., ReLU)\n\nWhy does this simplification work so well? These simplifications offer several advantages:\n\nEfficiency: Linear complexity in number of edges\nLocalization: Each layer only aggregates information from immediate neighbors\nDepth: Fewer parameters allow building deeper models\nPerformance: Despite (or perhaps due to) its simplicity, it often outperforms more complex models\n\n\n\n\n\n\n\nTry it yourself\n\n\n\nLet’s implement a simple GCN model for node classification. Coding Exercise"
  },
  {
    "objectID": "m06-graph/graph-convolutional-network.html#popular-graph-neural-networks",
    "href": "m06-graph/graph-convolutional-network.html#popular-graph-neural-networks",
    "title": "Spatial Graph Convolutional Networks",
    "section": "Popular Graph Neural Networks",
    "text": "Popular Graph Neural Networks\nShift your attention now to three popular GNN architectures that extend the basic GCN framework. We will introduce GraphSAGE, Graph Attention Networks (GAT), and Graph Isomorphism Network (GIN)."
  },
  {
    "objectID": "m06-graph/graph-convolutional-network.html#graphsage-sample-and-aggregate",
    "href": "m06-graph/graph-convolutional-network.html#graphsage-sample-and-aggregate",
    "title": "Spatial Graph Convolutional Networks",
    "section": "GraphSAGE: Sample and Aggregate",
    "text": "GraphSAGE: Sample and Aggregate\nGraphSAGE (Hamilton, Ying, and Leskovec 2017) introduced a different GCN that can be generalized to unseen nodes (they called it “inductive”). While previous approaches like ChebNet and GCN operate on the entire graph, GraphSAGE proposes an inductive framework that generates embeddings by sampling and aggregating features from a node’s neighborhood.\n\n\n\n\n\n\nFigure 1: GraphSAGE architecture showing neighborhood sampling and aggregation.\n\n\n\n\nNeighborhood Sampling\nThe key idea is neighborhood sampling. Instead of using all neighbors, GraphSAGE samples a fixed-size set of neighbors for each node. This controls memory complexity, a key limitation of the previous GNNs.\nAnother key advantage of neighborhood sampling is that it enables GraphSAGE to handle dynamic, growing networks. Consider a citation network where new papers (nodes) are continuously added. Traditional GCNs would need to recompute filters for the entire network with each new addition. In contrast, GraphSAGE can immediately generate embeddings for new nodes by simply sampling their neighbors, without any retraining or recomputation.\n\n\nAggregation\nHow does GraphSAGE combine information? GraphSAGE makes a distinction between self-information and neighborhood information. While previous GNNs treat them equally and aggregate them, GraphSAGE treats them differently.\nSpecifically, GraphSAGE introduces an additional step. It concatenates the self-information and the neighborhood information as the input of the convolution:\n\nZ_v = \\text{CONCAT}(X_v, X_{\\mathcal{N}(v)})\n\nwhere X_v is the feature of the node itself and X_{\\mathcal{N}(v)} is the aggregation of the features of its neighbors. GraphSAGE introduces different ways to aggregate information from neighbors:\nX_{\\mathcal{N}(v)} = \\text{AGGREGATE}_k(\\{X_u, \\forall u \\in \\mathcal{N}(v)\\})\nCommon aggregation functions include:\n\nMean aggregator: \\text{AGGREGATE} = \\text{mean}(\\{h_u, \\forall u \\in \\mathcal{N}(v)\\})\nMax-pooling: \\text{AGGREGATE} = \\max(\\{\\sigma(W_{\\text{pool}}h_u + b), \\forall u \\in \\mathcal{N}(v)\\})\nLSTM aggregator: Apply LSTM to randomly permuted neighbors\n\nThe concatenated feature Z_v is normalized by the L2 norm:\n\n\\hat{Z}_v = \\frac{Z_v}{\\|Z_v\\|_2}\n\nand then fed into the convolution:\n\nX_v^k = \\sigma(W^k \\hat{Z}_v + b^k)"
  },
  {
    "objectID": "m06-graph/graph-convolutional-network.html#graph-attention-networks-gat-differentiate-individual-neighbors",
    "href": "m06-graph/graph-convolutional-network.html#graph-attention-networks-gat-differentiate-individual-neighbors",
    "title": "Spatial Graph Convolutional Networks",
    "section": "Graph Attention Networks (GAT): Differentiate Individual Neighbors",
    "text": "Graph Attention Networks (GAT): Differentiate Individual Neighbors\nA key innovation of GraphSAGE is to treat the self and neighborhood information differently. But should all neighbors be treated equally? Graph Attention Networks (GAT) address this by letting the model learn which neighbors to pay attention to.\n\nAttention Mechanism\n\n\n\n\n\n\nFigure 2: Graph Attention Network architecture showing attention mechanism between nodes.\n\n\n\nThe core idea is beautifully simple. Instead of using fixed weights like GCN, let’s learn attention weights \\alpha_{ij} that determine how much node i should attend to node j. These weights are computed dynamically based on node features:\n\n\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}(i)} \\exp(e_{ik})}\n\nwhere e_{ij} represents the importance of the edge between node i and node j. Variable e_{ij} is a learnable parameter and can be negative. The exponential function transforms it to a non-negative value, with the normalization term \\sum_{k \\in \\mathcal{N}(i)} \\exp(e_{ik}) ensuring the weights sum to 1.\nHow do we compute e_{ij}? One simple choice is to use a neural network with a shared weight matrix W and a LeakyReLU activation function. Specifically:\nLet’s focus on computing e_{ij} for node i and its neighbor j. We use a shared weight matrix W to transform the features of node i and j:\n\n\\mathbf{\\tilde h}_i  = \\mathbf{h}_i, \\quad \\mathbf{\\tilde h}_j  = W\\mathbf{h}_j\n\nWe concatenate the transformed features and apply a LeakyReLU activation function:\n\ne_{ij} = \\text{LeakyReLU}(\\mathbf{a}^T[\\mathbf{\\tilde h}_i, \\mathbf{\\tilde h}_j])\n\nwhere \\mathbf{a} is a trainable parameter vector that sums the two transformed features.\nOnce we have these attention weights, the node update is straightforward. It’s just a weighted sum of neighbor features:\n\\mathbf{h}'_i = \\sigma\\left(\\sum_{j \\in \\mathcal{N}(i) \\cup \\{i\\}} \\alpha_{ij}{\\bf W}_{\\text{feature}}\\mathbf{h}_j\\right)\nwhere {\\bf W}_{\\text{feature}} is a trainable weight matrix. To stabilize training, GAT uses multiple attention heads and concatenates their outputs:\n\\mathbf{h}'_i = \\parallel_{k=1}^K \\sigma\\left(\\sum_{j \\in \\mathcal{N}(i) \\cup \\{i\\}} \\alpha_{ij}^k{\\bf W}^k_{\\text{feature}}\\mathbf{h}_j\\right)"
  },
  {
    "objectID": "m06-graph/graph-convolutional-network.html#graph-isomorphism-network-gin-differentiate-the-aggregation",
    "href": "m06-graph/graph-convolutional-network.html#graph-isomorphism-network-gin-differentiate-the-aggregation",
    "title": "Spatial Graph Convolutional Networks",
    "section": "Graph Isomorphism Network (GIN): Differentiate the Aggregation",
    "text": "Graph Isomorphism Network (GIN): Differentiate the Aggregation\nGraph Isomorphism Networks (GIN) was born out of a question: what is the maximum discriminative power achievable by Graph Neural Networks? The answer lies in its theoretical connection to the Weisfeiler-Lehman (WL) test, a powerful algorithm for graph isomorphism testing.\n\nWeisfeiler-Lehman Test\nAre two graphs structurally identical? Graph isomorphism testing determines if two graphs are structurally identical, with applications in graph classification, clustering, and other tasks.\n\n\n\n\n\n\nFigure 3: Example of graph isomorphism.\n\n\n\nWhile the general problem has no known polynomial-time solution, the WL test is an efficient heuristic that works well in practice. The WL test iteratively refines node labels by hashing the multiset of neighboring labels.\n\n\n\n\n\n\nFigure 4: Weisfeiler-Lehman test iteratively refining node labels.\n\n\n\nHow does the process work? The WL test works as follows:\nFirst, assign all nodes the same initial label. For each node, collect the labels of all its neighbors and aggregate them into a hash (e.g., new label). For example, the top node gets {0} from its neighbors, resulting in a collection {0,0}. A new label is created via a hash function h that maps {0, {0, 0}} to a new label 1.\nRepeat the process for a fixed number of iterations or until convergence.\nAfter these iterations:\n\nNodes with the same label are structurally identical, meaning that they are indistinguishable unless we label them differently.\nTwo graphs are structurally identical if and only if they have the same node labels after the WL test.\n\nThe WL test is a heuristic and can fail on some graphs. For example, it cannot distinguish regular graphs with the same number of nodes and edges. The WL test above is called the 1-WL test. There are higher-order WL tests that can distinguish more graphs, which are the basis of advanced GNNs. Check out this note for more details.\n\n\nGIN\nGIN (Xu et al. 2019) is a GNN that is based on the WL test. The key idea is to focus on the parallel between the WL test and the GNN update rule.\nWhat’s the crucial difference? In the WL test, we iteratively collect the labels of neighbors and aggregate them through a hash function. In the GraphSAGE and GAT, the labels are the nodes’ features, and the aggregation is some arithmetic operations such as mean or max.\nThe key difference is that the hash function in the WL test always distinguishes different sets of neighbors’ labels, while the aggregation in GraphSAGE and GAT does not always do so. For example, if all nodes have the same feature (e.g., all 1), the aggregation by the mean or max will result in the same value for all nodes. The hash function in the WL test can still distinguish different sets of neighbors’ labels by the count of each label.\nHow does GIN solve this? The resulting convolution update rule is:\n\nh_v^{(k+1)} = \\text{MLP}^{(k)}\\left((1 + \\epsilon^{(k)}) \\cdot h_v^{(k)} + \\sum_{u \\in \\mathcal{N}(v)} h_u^{(k)}\\right)\n\nwhere \\text{MLP}^{(k)} is a multi-layer perceptron (MLP) with k layers, and \\epsilon^{(k)} is a fixed or trainable parameter."
  },
  {
    "objectID": "m06-graph/04-graph-embeddings.html",
    "href": "m06-graph/04-graph-embeddings.html",
    "title": "Graph Embeddings",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module introduces graph embeddings that transform networks into continuous vector spaces.\nYou’ll learn:\n\nWhat network embedding means and how to compress graphs into low-dimensional representations.\nHow to use spectral methods (adjacency, modularity, Laplacian) for finding optimal embeddings through eigendecomposition.\nThe power of neural embeddings with word2vec, DeepWalk, and node2vec for learning from random walks.\nPractical techniques for clustering, visualization, and downstream machine learning tasks on graph data.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 4: Graph Embeddings"
    ]
  },
  {
    "objectID": "m06-graph/04-graph-embeddings.html#what-is-network-embedding",
    "href": "m06-graph/04-graph-embeddings.html#what-is-network-embedding",
    "title": "Graph Embeddings",
    "section": "What is Network Embedding?",
    "text": "What is Network Embedding?\n\n\n\n\n\n\nFigure 1: This figure is taken from DeepWalk: Online Learning of Social Representations.\n\n\n\nNetworks are high-dimensional discrete structures that resist traditional machine learning algorithms designed for continuous data. Network embedding transforms graphs into low-dimensional continuous spaces where each node becomes a point in \\mathbb{R}^d (typically d \\ll N) while preserving important structural properties.\nThe goal is simple but powerful. Map nodes to vectors such that similar nodes have similar embeddings.\nWhat does “similar” mean? It can mean many things: connected by edges, sharing neighbors, playing similar structural roles, or belonging to the same community.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 4: Graph Embeddings"
    ]
  },
  {
    "objectID": "m06-graph/04-graph-embeddings.html#spectral-embeddings",
    "href": "m06-graph/04-graph-embeddings.html#spectral-embeddings",
    "title": "Graph Embeddings",
    "section": "Spectral Embeddings",
    "text": "Spectral Embeddings\nLet’s talk about spectral methods. They use eigendecomposition to find low-dimensional representations. We explore three approaches: adjacency-based, modularity-based, and Laplacian-based.\n\nCompressing Networks\nSuppose we have an adjacency matrix {\\bf A} of size N \\times N. We want to compress it into a matrix {\\bf U} of size N \\times d where d \\ll N.\nGood embeddings should reconstruct the original network well:\n\n\\min_{{\\bf U}} J({\\bf U}), \\quad J({\\bf U}) = \\| {\\bf A} - {\\bf U}{\\bf U}^\\top \\|_F^2\n\nwhere \\|\\cdot\\|_F is the Frobenius norm (sum of squared elements). The outer product {\\bf U}{\\bf U}^\\top reconstructs the adjacency matrix from embeddings. Minimizing the difference between {\\bf A} and this reconstruction yields the best low-dimensional representation.\n\n\nSpectral Decomposition Solution\nHow do we solve this optimization problem? Consider the eigendecomposition of {\\bf A}:\n\n{\\bf A} = \\sum_{i=1}^N \\lambda_i {\\bf u}_i {\\bf u}_i^\\top\n\nEach term \\lambda_i {\\bf u}_i {\\bf u}_i^\\top is a rank-one matrix capturing part of the network structure. Eigenvalues \\lambda_i indicate importance.\n\nTo compress the network, select the d eigenvectors with largest eigenvalues and form embedding matrix {\\bf U} = [{\\bf u}_1, {\\bf u}_2, \\ldots, {\\bf u}_d]. This is provably optimal for minimizing reconstruction error.\n\n\nModularity Embedding\nWhat if we want to emphasize community structure? Instead of the adjacency matrix, we can embed the modularity matrix:\n\nQ_{ij} = \\frac{1}{2m}A_{ij} - \\frac{k_i k_j}{4m^2}\n\nwhere k_i is node degree and m is the number of edges. The modularity matrix emphasizes connections beyond what random chance predicts.\nIts eigenvectors naturally separate communities. A simple community detection algorithm: group nodes by the sign of the second eigenvector (Newman 2006).\n\n\nLaplacian Eigenmap\nLaplacian Eigenmap (Belkin and Niyogi 2003) positions connected nodes close together. The optimization problem is:\n\n\\min_{{\\bf U}} J_{LE}({\\bf U}), \\quad J_{LE}({\\bf U}) = \\frac{1}{2}\\sum_{i,j} A_{ij} \\| {\\bf u}_i - {\\bf u}_j \\|^2\n\nThrough algebraic manipulation, this becomes:\n\nJ_{LE}({\\bf U}) = \\text{Tr}({\\bf U}^\\top {\\bf L} {\\bf U})\n\nwhere {\\bf L} = {\\bf D} - {\\bf A} is the graph Laplacian and \\text{Tr} denotes the trace. The derivation proceeds as follows:\n\n\\begin{aligned}\nJ_{LE} &= \\frac{1}{2}\\sum_{i,j} A_{ij} (\\| {\\bf u}_i \\|^2 - 2 {\\bf u}_i^\\top {\\bf u}_j + \\| {\\bf u}_j \\|^2) \\\\\n&= \\sum_i k_i \\| {\\bf u}_i \\|^2 - \\sum_{i,j} A_{ij} {\\bf u}_i^\\top {\\bf u}_j \\\\\n&= \\text{Tr}({\\bf U}^\\top {\\bf D} {\\bf U}) - \\text{Tr}({\\bf U}^\\top {\\bf A} {\\bf U}) \\\\\n&= \\text{Tr}({\\bf U}^\\top {\\bf L} {\\bf U})\n\\end{aligned}\n\nThe solution is the d eigenvectors corresponding to the smallest eigenvalues of {\\bf L} (excluding the trivial zero eigenvalue). The smallest eigenvalue is always zero with eigenvector of all ones. In practice, compute d+1 smallest eigenvectors and discard the first.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 4: Graph Embeddings"
    ]
  },
  {
    "objectID": "m06-graph/04-graph-embeddings.html#neural-embeddings-with-word2vec",
    "href": "m06-graph/04-graph-embeddings.html#neural-embeddings-with-word2vec",
    "title": "Graph Embeddings",
    "section": "Neural Embeddings with word2vec",
    "text": "Neural Embeddings with word2vec\nSpectral methods are elegant but computationally expensive for large graphs. What’s the alternative?\nNeural methods learn embeddings using neural networks trained on data. Before applying these to graphs, we introduce word2vec, which forms the foundation for many graph embedding techniques.\n\nHow word2vec Works\nword2vec (Mikolov et al. 2013) learns word meanings from context, following the principle: “You shall know a word by the company it keeps” (church1988word?). This phrase comes from Aesop’s fable The Ass and his Purchaser. A man brings an ass to his farm on trial. The ass immediately joins the laziest, greediest ass in the herd. The man returns it, knowing its character by the company it chose.\nThe core idea is simple. Given a target word, predict surrounding context words within a fixed window. For example, in “The quick brown fox jumps over a lazy dog,” the context of fox (window size 2) includes quick, brown, jumps, over.\nWords appearing in similar contexts get similar embeddings. Both fox and dog appear with “quick,” “brown,” and action verbs, so they have similar embeddings. But student appears with “studies” and “library,” giving it a distant embedding.\n\n\n\n\n\nThe network architecture has three layers. The input layer uses one-hot encoding of the target word. The hidden layer holds the learned word embedding (low-dimensional). The output layer produces a probability distribution over context words using softmax.\nThe hidden layer activations become dense, low-dimensional vectors capturing semantic relationships. For a visual walkthrough, see The Illustrated Word2vec by Jay Alammar.\n\n\nEfficient Training\nWhy don’t we use this architecture directly? Computing the full softmax over vocabulary is expensive:\n\nP(w_c | w_t) = \\frac{\\exp({\\bf v}_{w_c} \\cdot {\\bf v}_{w_t})}{\\sum_{w \\in V} \\exp({\\bf v}_w \\cdot {\\bf v}_{w_t})}\n\nThe denominator sums over all words (100,000+ terms), making training slow.\nTwo solutions exist:\nHierarchical Softmax: Organizes vocabulary as a binary tree. Computing probability becomes traversing root-to-leaf paths, reducing complexity from \\mathcal{O}(|V|) to \\mathcal{O}(\\log |V|).\n\n\n\n\n\n\nFigure 2\n\n\n\nNegative Sampling: Instead of normalizing over all words, sample a few “negative” (non-context) words and contrast them with true context words. This approximates the full softmax efficiently with 5-20 negative samples.\n\n\nword2vec’s Power\nWhat makes word2vec special? word2vec embeddings capture semantic relationships through simple linear algebra:\n\n\n\n\n\n\nFigure 3\n\n\n\nFamous examples include analogies: man is to woman as king is to queen. Relationships like countries and capitals form parallel vectors in the embedding space.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 4: Graph Embeddings"
    ]
  },
  {
    "objectID": "m06-graph/04-graph-embeddings.html#graph-embedding-with-word2vec",
    "href": "m06-graph/04-graph-embeddings.html#graph-embedding-with-word2vec",
    "title": "Graph Embeddings",
    "section": "Graph Embedding with word2vec",
    "text": "Graph Embedding with word2vec\nHow can we apply word2vec to graphs? The challenge is that word2vec expects sequences, while graphs are unordered.\nThe solution: random walks transform graphs into sequences of nodes. Treat walks as sentences and nodes as words. Random walks create “sentences” from graphs where each walk is a sequence of nodes, just like a sentence is a sequence of words.\n\nDeepWalk\n\nDeepWalk (Perozzi, Al-Rfou, and Skiena 2014) pioneered applying word2vec to graphs:\n\nSample multiple random walks from the graph\nTreat walks as sentences and feed them to word2vec\n\nDeepWalk uses skip-gram with hierarchical softmax for efficient training. Nodes appearing in similar walk contexts get similar embeddings.\n\n\nnode2vec\nWhat if we want more control over the random walks? node2vec (Grover and Leskovec 2016) extends DeepWalk with biased random walks controlled by parameters p and q:\n\nP(v_{t+1} = x | v_t = v, v_{t-1} = t) \\propto\n\\begin{cases}\n\\frac{1}{p} & \\text{if } d(t,x) = 0 \\text{ (return to previous)} \\\\\n1 & \\text{if } d(t,x) = 1 \\text{ (close neighbor)} \\\\\n\\frac{1}{q} & \\text{if } d(t,x) = 2 \\text{ (explore further)}\n\\end{cases}\n\nwhere d(t,x) is the shortest path from previous node t to candidate x. Low p creates return bias (local revisiting). Low q creates outward bias (exploration, DFS-like). High q creates inward bias (stay local, BFS-like).\n\nThese parameters enable two exploration strategies. BFS-like walks (low q) explore immediate neighborhoods and capture community structure. DFS-like walks (high q) explore deep paths and capture structural roles.\n\nNote that node2vec uses negative sampling instead of hierarchical softmax, affecting embedding characteristics (Kojaku et al. 2021; Dyer 2014).\n\n\nLINE\nLINE (Tang et al. 2015) is equivalent to node2vec with p=1, q=1, and window size 1. It directly optimizes graph structure preservation.\nNeural methods seem less transparent than spectral methods, but recent work establishes equivalences under specific conditions (Qiu et al. 2018; kojaku2024network?). Surprisingly, DeepWalk, node2vec, and LINE are provably optimal for stochastic block models (kojaku2024network?).",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 4: Graph Embeddings"
    ]
  },
  {
    "objectID": "m06-graph/04-graph-embeddings.html#hands-on-implementing-embeddings",
    "href": "m06-graph/04-graph-embeddings.html#hands-on-implementing-embeddings",
    "title": "Graph Embeddings",
    "section": "Hands-On: Implementing Embeddings",
    "text": "Hands-On: Implementing Embeddings\nLet’s implement these methods on the Karate Club network.\n\nData Preparation\n\n# Load the karate club network\ng = igraph.Graph.Famous(\"Zachary\")\nA = g.get_adjacency_sparse()\n\n# Get community labels (Mr. Hi = 0, Officer = 1)\nlabels = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\ng.vs[\"label\"] = labels\n\n# Visualize the network\npalette = sns.color_palette().as_hex()\nigraph.plot(g, vertex_color=[palette[label] for label in labels], bbox=(300, 300))\n\n\n\n\n\n\n\n\n\n\nSpectral Embedding Example\n\n# Compute the spectral decomposition\nA_dense = A.toarray()\neigvals, eigvecs = np.linalg.eig(A_dense)\n\n# Find the top d eigenvectors\nd = 2\nsorted_indices = np.argsort(eigvals)[::-1][:d]\neigvecs_top = eigvecs[:, sorted_indices]\n\n\n\nShow visualization code\n# Plot the results\nfig, ax = plt.subplots(figsize=(7, 5))\nsns.scatterplot(x=eigvecs_top[:, 0], y=eigvecs_top[:, 1], hue=labels, ax=ax)\nax.set_title('Spectral Embedding')\nax.set_xlabel('Eigenvector 1')\nax.set_ylabel('Eigenvector 2')\nplt.show()\n\n\n/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/matplotlib/cbook.py:1709: ComplexWarning: Casting complex values to real discards the imaginary part\n  return math.isfinite(val)\n/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/matplotlib/cbook.py:1709: ComplexWarning: Casting complex values to real discards the imaginary part\n  return math.isfinite(val)\n/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:133: ComplexWarning: Casting complex values to real discards the imaginary part\n  return arr.astype(dtype, copy=True)\n/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:133: ComplexWarning: Casting complex values to real discards the imaginary part\n  return arr.astype(dtype, copy=True)\n\n\n\n\n\n\n\n\n\nThe first eigenvector corresponds to eigencentrality. The second eigenvector captures community structure, clearly separating the two groups.\n\n\nLaplacian Eigenmap Example\n\n# Compute the Laplacian\nD = np.diag(np.sum(A_dense, axis=1))\nL = D - A_dense\n\neigvals_L, eigvecs_L = np.linalg.eig(L)\n\n# Sort and select eigenvalues (exclude first)\nsorted_indices_L = np.argsort(eigvals_L)[1:d+1]\neigvecs_L_top = eigvecs_L[:, sorted_indices_L]\n\n\n\nShow visualization code\n# Plot the results\nfig, ax = plt.subplots(figsize=(7, 5))\nsns.scatterplot(x=eigvecs_L_top[:, 0], y=eigvecs_L_top[:, 1], hue=labels, ax=ax)\nax.set_title('Laplacian Eigenmap')\nax.set_xlabel('Eigenvector 2')\nax.set_ylabel('Eigenvector 3')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nDeepWalk Example\n\n\nShow random walk implementation\ndef random_walk(net, start_node, walk_length):\n    \"\"\"Generate a random walk starting from start_node.\"\"\"\n    walk = [start_node]\n    while len(walk) &lt; walk_length:\n        cur = walk[-1]\n        cur_nbrs = list(net[cur].indices)\n        if len(cur_nbrs) &gt; 0:\n            walk.append(np.random.choice(cur_nbrs))\n        else:\n            break\n    return walk\n\n# Generate random walks\nn_nodes = g.vcount()\nn_walkers_per_node = 10\nwalk_length = 50\n\nwalks = []\nfor i in range(n_nodes):\n    for _ in range(n_walkers_per_node):\n        walks.append(random_walk(A, i, walk_length))\n\nprint(f\"Generated {len(walks)} random walks\")\nprint(f\"Example walk: {walks[0][:10]}...\")\n\n\nGenerated 340 random walks\nExample walk: [0, 5, 10, 4, 6, 0, 13, 3, 12, 3]...\n\n\n\n# Train word2vec on the walks\nmodel = Word2Vec(\n    walks,\n    vector_size=32,\n    window=3,\n    min_count=1,\n    sg=1,  # Skip-gram\n    hs=1,  # Hierarchical softmax\n    workers=1,\n)\n\n# Extract embeddings\nembedding = np.array([model.wv[i] for i in range(n_nodes)])\nprint(f\"Embedding matrix shape: {embedding.shape}\")\n\nEmbedding matrix shape: (34, 32)\n\n\n\n\n/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n  warn(\nOMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n\n\n    \n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\n\n  \n\n\n\n\n\nNodes from the same community cluster together, showing that DeepWalk captures community structure.\n\n\nClustering with K-means\n\n\nShow clustering implementation\ndef find_optimal_clusters(embedding, n_clusters_range=(2, 10)):\n    \"\"\"Find optimal number of clusters using silhouette score.\"\"\"\n    silhouette_scores = []\n    for n_clusters in range(*n_clusters_range):\n        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n        cluster_labels = kmeans.fit_predict(embedding)\n        score = silhouette_score(embedding, cluster_labels)\n        silhouette_scores.append((n_clusters, score))\n        print(f\"k={n_clusters}: silhouette score = {score:.3f}\")\n\n    optimal_k = max(silhouette_scores, key=lambda x: x[1])[0]\n    print(f\"\\nOptimal number of clusters: {optimal_k}\")\n\n    kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n    return kmeans.fit_predict(embedding)\n\n# Find clusters\ncluster_labels = find_optimal_clusters(embedding)\n\n\nk=2: silhouette score = 0.474\nk=3: silhouette score = 0.510\nk=4: silhouette score = 0.499\nk=5: silhouette score = 0.452\nk=6: silhouette score = 0.392\nk=7: silhouette score = 0.289\nk=8: silhouette score = 0.291\nk=9: silhouette score = 0.300\n\nOptimal number of clusters: 3\n\n\n\n\nShow clustering visualization\n# Visualize clustering\ncmap = sns.color_palette().as_hex()\nigraph.plot(\n    g,\n    vertex_color=[cmap[label] for label in cluster_labels],\n    bbox=(500, 500),\n    vertex_size=20\n)\n\n\n\n\n\n\n\n\n\nK-means successfully identifies communities using only learned embeddings, demonstrating that DeepWalk captures meaningful structural properties.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 4: Graph Embeddings"
    ]
  },
  {
    "objectID": "m06-graph/04-graph-embeddings.html#comparing-approaches",
    "href": "m06-graph/04-graph-embeddings.html#comparing-approaches",
    "title": "Graph Embeddings",
    "section": "Comparing Approaches",
    "text": "Comparing Approaches\nWhich method should you use? We have explored multiple embedding methods, each with distinct trade-offs:\n\n\n\n\n\n\n\n\n\nMethod\nApproach\nStrengths\nLimitations\n\n\n\n\nSpectral (Adjacency)\nEigendecomposition\nPrincipled, captures centrality\nExpensive, requires full graph\n\n\nLaplacian Eigenmap\nMinimize edge distances\nPreserves local structure\nSensitive to disconnected components\n\n\nDeepWalk\nRandom walks + word2vec\nScalable, flexible\nRandom initialization sensitive\n\n\nnode2vec\nBiased random walks\nControls exploration\nMore hyperparameters\n\n\n\nSpectral methods offer mathematical guarantees but scale poorly. Neural methods scale to large graphs and generalize to unseen nodes but require careful hyperparameter tuning.\nAre these methods fundamentally different? Recent work shows these methods are more connected than they appear. Under specific conditions, neural embeddings provably approximate spectral embeddings (Qiu et al. 2018; kojaku2024network?). This bridges the gap between elegant theory and practical scalability.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 4: Graph Embeddings"
    ]
  },
  {
    "objectID": "m06-graph/04-graph-embeddings.html#key-takeaways",
    "href": "m06-graph/04-graph-embeddings.html#key-takeaways",
    "title": "Graph Embeddings",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nGraph embeddings transform discrete networks into continuous vector spaces, enabling standard machine learning algorithms. Spectral methods use eigendecomposition to find optimal low-dimensional representations. Neural methods learn embeddings by treating random walks as training data for word2vec.\nThese embeddings power downstream tasks: node classification, link prediction, community detection, and visualization. They bridge the gap between graph theory and deep learning, showing that geometric intuitions about similarity and distance extend naturally to network data.\nWhat have we learned in this module? We journeyed from pixels to nodes, extending convolution beyond regular grids. We explored spectral and spatial perspectives on graph neural networks. We learned how embeddings compress networks into vectors while preserving structure.\nThe principles transcend specific architectures. Locality matters. Parameter sharing generalizes. Hierarchical features extract increasingly abstract patterns. These insights apply wherever relationships exist, from molecules to social networks to knowledge graphs.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 4: Graph Embeddings"
    ]
  },
  {
    "objectID": "m06-graph/02-spectral-perspective.html",
    "href": "m06-graph/02-spectral-perspective.html",
    "title": "The Spectral Perspective",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module introduces the spectral approach to graph convolution.\nYou’ll learn:\n\nHow to define frequency for graph signals using the Laplacian matrix.\nWhy eigenvectors act as basis functions and eigenvalues measure roughness.\nHow to design spectral filters that control which frequencies pass through.\nHow to build learnable spectral graph convolutional networks.\nThe computational limitations that motivate spatial methods.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 2: The Spectral Perspective"
    ]
  },
  {
    "objectID": "m06-graph/02-spectral-perspective.html#finding-frequency-in-graphs",
    "href": "m06-graph/02-spectral-perspective.html#finding-frequency-in-graphs",
    "title": "The Spectral Perspective",
    "section": "Finding Frequency in Graphs",
    "text": "Finding Frequency in Graphs\nImages live in a spatial domain, but the Fourier transform reveals their frequency content. Low frequencies capture smooth gradients and overall structure. High frequencies capture sharp edges and fine details.\nGraphs need their own notion of frequency. Consider a network of nodes where each node has a feature value. What makes a signal “smooth” on a graph?\nIntuitively, smoothness means connected nodes have similar values. Roughness means neighbors differ significantly. Let’s formalize this intuition.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 2: The Spectral Perspective"
    ]
  },
  {
    "objectID": "m06-graph/02-spectral-perspective.html#total-variation-measuring-roughness",
    "href": "m06-graph/02-spectral-perspective.html#total-variation-measuring-roughness",
    "title": "The Spectral Perspective",
    "section": "Total Variation: Measuring Roughness",
    "text": "Total Variation: Measuring Roughness\nConsider a network of N nodes where each node i has a scalar feature x_i \\in \\mathbb{R}. Define the total variation as:\n\nJ = \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N A_{ij}(x_i - x_j)^2\n\nwhere A_{ij} is the adjacency matrix. This quantity sums the squared differences between connected nodes.\nSmall J means smooth variation (low frequency). Large J means rough variation (high frequency).\nWe can rewrite J in matrix form. Through algebraic manipulation, we obtain:\n\nJ = {\\bf x}^\\top {\\bf L} {\\bf x}\n\nwhere {\\bf x} = [x_1, x_2, \\ldots, x_N]^\\top and {\\bf L} is the graph Laplacian:\n\nL_{ij} = \\begin{cases}\nk_i & \\text{if } i = j \\\\\n-1 & \\text{if } i \\text{ and } j \\text{ are connected} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\nwith k_i being the degree of node i.\n\n\n\n\n\n\nDetailed derivation\n\n\n\nStarting from the definition of total variation:\n\n\\begin{aligned}\nJ &= \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N A_{ij}(x_i - x_j)^2 \\\\\n&= \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N A_{ij}(x_i^2 + x_j^2 - 2x_ix_j) \\\\\n&= \\sum_{i=1}^N x_i^2 \\sum_{j=1}^N A_{ij} - \\sum_{i=1}^N\\sum_{j=1}^N A_{ij}x_ix_j \\\\\n&= \\sum_{i=1}^N x_i^2 k_i - \\sum_{i=1}^N\\sum_{j=1}^N A_{ij}x_ix_j \\\\\n&= {\\bf x}^\\top {\\bf D} {\\bf x} - {\\bf x}^\\top {\\bf A} {\\bf x} \\\\\n&= {\\bf x}^\\top ({\\bf D} - {\\bf A}) {\\bf x} \\\\\n&= {\\bf x}^\\top {\\bf L} {\\bf x}\n\\end{aligned}\n\nwhere {\\bf D} is the diagonal degree matrix with D_{ii} = k_i.\n\n\nThe Laplacian {\\bf L} plays the same role for graphs that the derivative operator plays for continuous signals. It measures how much a signal varies across edges.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 2: The Spectral Perspective"
    ]
  },
  {
    "objectID": "m06-graph/02-spectral-perspective.html#eigenvectors-as-basis-functions",
    "href": "m06-graph/02-spectral-perspective.html#eigenvectors-as-basis-functions",
    "title": "The Spectral Perspective",
    "section": "Eigenvectors as Basis Functions",
    "text": "Eigenvectors as Basis Functions\nIn Fourier analysis, we decompose signals into sinusoidal basis functions. For graphs, eigenvectors of the Laplacian serve as basis functions. How does this work?\nConsider the eigendecomposition of {\\bf L}:\n\n{\\bf L} = \\sum_{i=1}^N \\lambda_i {\\bf u}_i {\\bf u}_i^\\top\n\nwhere \\lambda_i are eigenvalues and {\\bf u}_i are eigenvectors. We can decompose the total variation J as:\n\nJ = {\\bf x}^\\top {\\bf L} {\\bf x} = \\sum_{i=1}^N \\lambda_i ({\\bf x}^\\top {\\bf u}_i)^2\n\nEach term ({\\bf x}^\\top {\\bf u}_i)^2 measures how much signal {\\bf x} aligns with eigenvector {\\bf u}_i. The eigenvalue \\lambda_i weights this contribution.\nWhat do eigenvalues tell us? Compute the total variation of eigenvector {\\bf u}_i itself:\n\nJ_i = {\\bf u}_i^\\top {\\bf L} {\\bf u}_i = \\lambda_i\n\nThis reveals the key insight. Eigenvalues measure the total variation of their corresponding eigenvectors. Small eigenvalues correspond to smooth eigenvectors (low frequency). Large eigenvalues correspond to rough eigenvectors (high frequency).\nNow consider any signal {\\bf x}. If {\\bf x} aligns strongly with a low-eigenvalue eigenvector, {\\bf x} is smooth. If {\\bf x} aligns with a high-eigenvalue eigenvector, {\\bf x} is rough.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 2: The Spectral Perspective"
    ]
  },
  {
    "objectID": "m06-graph/02-spectral-perspective.html#spectral-filtering",
    "href": "m06-graph/02-spectral-perspective.html#spectral-filtering",
    "title": "The Spectral Perspective",
    "section": "Spectral Filtering",
    "text": "Spectral Filtering\nEigenvalues act as natural filters. We can design custom filters h(\\lambda_i) to control which frequencies pass through. Let’s look at two fundamental filters.\nLow-pass Filter (smoothing): \nh_{\\text{low}}(\\lambda) = \\frac{1}{1 + \\alpha\\lambda}\n\nThis suppresses high frequencies (large \\lambda) and preserves low frequencies (small \\lambda), resulting in smoother signals.\nHigh-pass Filter (edge detection): \nh_{\\text{high}}(\\lambda) = \\frac{\\alpha\\lambda}{1 + \\alpha\\lambda}\n\nThis suppresses low frequencies and emphasizes high frequencies. The result highlights rapid variations like boundaries between communities.\n\n\n\n\n\n\n\n\n\nThe parameter \\alpha controls the filter’s strength. Larger \\alpha makes the transition between frequencies sharper.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 2: The Spectral Perspective"
    ]
  },
  {
    "objectID": "m06-graph/02-spectral-perspective.html#learnable-spectral-filters",
    "href": "m06-graph/02-spectral-perspective.html#learnable-spectral-filters",
    "title": "The Spectral Perspective",
    "section": "Learnable Spectral Filters",
    "text": "Learnable Spectral Filters\nHand-designed filters work for specific tasks, but what if we want to learn filters from data? This question leads to spectral graph convolutional networks.\nThe simplest learnable filter uses a linear combination of eigenvectors:\n\n{\\bf L}_{\\text{learn}} = \\sum_{k=1}^K \\theta_k {\\bf u}_k {\\bf u}_k^\\top\n\nwhere \\theta_k are learnable parameters and K is the filter rank. Instead of fixed weights \\lambda_k, we learn weights \\theta_k that maximize performance on downstream tasks. This shifts from hand-designed to data-driven filters.\nBuilding on this idea, Bruna et al. (2014) proposed spectral convolutional neural networks by adding nonlinearity:\n\n{\\bf x}^{(\\ell+1)} = h\\left( {\\bf L}_{\\text{learn}} {\\bf x}^{(\\ell)}\\right)\n\nwhere h is an activation function (e.g., ReLU) and {\\bf x}^{(\\ell)} is the feature vector at layer \\ell.\nFor multidimensional features {\\bf X} \\in \\mathbb{R}^{N \\times f_{\\text{in}}}, we extend this to produce outputs {\\bf X}' \\in \\mathbb{R}^{N \\times f_{\\text{out}}}:\n\n{\\bf X}^{(\\ell+1)}_j = h\\left( \\sum_{i=1}^{f_{\\text{in}}} {\\bf L}_{\\text{learn}}^{(i,j)} {\\bf X}^{(\\ell)}_i\\right)\n\nwhere {\\bf L}_{\\text{learn}}^{(i,j)} = \\sum_{k=1}^K \\theta_{k, (i,j)} {\\bf u}_k {\\bf u}_k^\\top is a separate learnable filter for each input-output dimension pair.\nThis architecture stacks multiple layers, each learning which spectral components matter for the task. Deeper layers capture increasingly complex patterns.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 2: The Spectral Perspective"
    ]
  },
  {
    "objectID": "m06-graph/02-spectral-perspective.html#the-computational-challenge",
    "href": "m06-graph/02-spectral-perspective.html#the-computational-challenge",
    "title": "The Spectral Perspective",
    "section": "The Computational Challenge",
    "text": "The Computational Challenge\nSpectral methods face two critical limitations. What are they?\nComputational cost: Computing eigendecomposition requires \\mathcal{O}(N^3) operations, prohibitive for large graphs with millions of nodes.\nLack of spatial locality: Learned filters operate on all eigenvectors, meaning every node can influence every other node. This loses the inductive bias of locality that makes CNNs powerful.\nThese limitations motivated the development of spatial methods. Spatial approaches define convolution directly in the graph domain without requiring eigendecomposition. We explore this alternative in the next part.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 2: The Spectral Perspective"
    ]
  },
  {
    "objectID": "m06-graph/02-spectral-perspective.html#key-takeaways",
    "href": "m06-graph/02-spectral-perspective.html#key-takeaways",
    "title": "The Spectral Perspective",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nThe spectral perspective reveals that graphs have frequency domains. The Laplacian’s eigenvectors serve as basis functions, and eigenvalues indicate how rapidly signals vary.\nSpectral filters control which frequencies pass through. We can learn these filters for specific tasks. This mathematical elegance comes at a cost.\nEigendecomposition is expensive, and spectral filters lack spatial locality. But the insights gained inform spatial methods. In Part 3, we see how spatial methods address these limitations while preserving the power of learned graph convolution.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 2: The Spectral Perspective"
    ]
  },
  {
    "objectID": "m05-images/overview.html",
    "href": "m05-images/overview.html",
    "title": "Module 5: Deep Learning for Images",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module takes you from pixels to state-of-the-art vision models.\nYou’ll learn:\n\nWhat images really are as data structures and why spatial structure matters.\nHow the deep learning revolution shifted computer vision from hand-crafted to learned features.\nPractical skills for using CNNs: building blocks, pre-trained models, and transfer learning.\nThe innovation timeline from VGG to Vision Transformers and the architectural insights that made them possible.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Overview"
    ]
  },
  {
    "objectID": "m05-images/overview.html#the-journey",
    "href": "m05-images/overview.html#the-journey",
    "title": "Module 5: Deep Learning for Images",
    "section": "The Journey",
    "text": "The Journey\nLet’s talk about where this module takes you. We begin with the fundamentals and build up to cutting-edge architectures. Each part answers a crucial question.\nPart 1: Understanding Images\nBefore we can process images with neural networks, we need to understand what images are. How do computers represent visual information, and why does spatial structure matter? We answer these questions by examining images as multidimensional arrays.\nPart 2: The Deep Learning Revolution\nComputer vision didn’t always work this way. Shift your attention to the historical moment when neural networks transformed the field, contrasting the old paradigm of hand-crafted features with learned representations as we follow the path from LeNet to AlexNet’s breakthrough.\nPart 3: Becoming a Practitioner\nNow you’ll learn the skills to actually use these models, covering CNN building blocks like convolution and pooling. You’ll work with pre-trained models, master transfer learning, and gain hands-on implementation experience.\nPart 4: The Innovation Timeline\nThe very best way to understand modern architectures is to see them as solutions to specific problems. Why did networks need to get deeper, and how did researchers overcome training difficulties? We trace the quest for better networks through VGG, Inception, ResNet, and Vision Transformers.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Overview"
    ]
  },
  {
    "objectID": "m05-images/overview.html#why-this-matters",
    "href": "m05-images/overview.html#why-this-matters",
    "title": "Module 5: Deep Learning for Images",
    "section": "Why This Matters",
    "text": "Why This Matters\nHere’s something remarkable: computer vision is no longer about manually designing features, as modern systems learn representations automatically from data. This shift changed everything about how we build vision applications, with what used to require expert knowledge and careful tuning now happening through learning. This module gives you both conceptual understanding and practical skills so you’ll know why architectures evolved the way they did and be able to use state-of-the-art vision models in your own projects.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Overview"
    ]
  },
  {
    "objectID": "m05-images/overview.html#prerequisites",
    "href": "m05-images/overview.html#prerequisites",
    "title": "Module 5: Deep Learning for Images",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou should be comfortable with basic Python programming and NumPy arrays, plus neural network fundamentals like forward propagation, backpropagation, and gradient descent. You’ll also need PyTorch basics like tensors, autograd, and simple model training (review the earlier modules in this course if you need to refresh these topics).",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Overview"
    ]
  },
  {
    "objectID": "m05-images/overview.html#what-youll-build",
    "href": "m05-images/overview.html#what-youll-build",
    "title": "Module 5: Deep Learning for Images",
    "section": "What You’ll Build",
    "text": "What You’ll Build\nBy the end of this module, you will understand how images are represented as tensors, implement classic CNN architectures from scratch, use pre-trained models for transfer learning, and make informed decisions about architecture selection with practical hands-on experience using real vision models.\nLet’s begin by understanding what an image really is.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Overview"
    ]
  },
  {
    "objectID": "m05-images/03-using-cnn-models.html",
    "href": "m05-images/03-using-cnn-models.html",
    "title": "Part 3: Using CNN Models",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis section transforms you into a CNN practitioner.\nYou’ll learn:\n\nWhat convolution operations do and how learnable kernels detect visual patterns.\nThe key properties of translation equivariance and parameter sharing that make CNNs efficient.\nHow receptive fields grow with depth and why this enables hierarchical feature learning.\nThe role of stride, padding, and pooling in controlling dimensions and creating invariance.\nHow to use pre-trained models from torchvision for immediate image classification.\nTwo approaches to transfer learning: feature extraction and fine-tuning for custom tasks.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 3: Using CNN Models"
    ]
  },
  {
    "objectID": "m05-images/03-using-cnn-models.html#understanding-cnn-building-blocks",
    "href": "m05-images/03-using-cnn-models.html#understanding-cnn-building-blocks",
    "title": "Part 3: Using CNN Models",
    "section": "Understanding CNN Building Blocks",
    "text": "Understanding CNN Building Blocks\nAlexNet proved that deep learning works at scale, but how do these networks actually process images? Let’s break down the fundamental operations that make CNNs powerful.\n\nConvolutional Layers: Learnable Pattern Detectors\nAt the heart of CNNs lies a remarkably elegant operation called convolution. Imagine sliding a small window (a kernel or filter) across an image. At each position, we multiply the kernel values by the overlapping image pixels and sum the results to produce a single output value. Repeating this across all positions creates an output feature map.\n\n\n\n\n\n\nConvolution operation. The kernel slides across the input, computing weighted sums at each position to produce a feature map.\n\n\n\n\nFigure 1\n\n\n\nMathematically, for a single-channel input (grayscale image), 2D convolution is:\n\n(I * K)_{i,j} = \\sum_{m=0}^{L-1}\\sum_{n=0}^{L-1} I_{i+m,j+n} \\cdot K_{m,n}\n\nwhere I is the input image, K is the kernel of size L \\times L, and (i,j) specifies the output position.\nWhat makes CNNs powerful is that these kernels are learnable parameters. During training, each kernel evolves to detect specific visual patterns like edges, textures, colors, or more complex features. The network discovers useful features automatically rather than relying on hand-crafted filters. Real-world images have multiple channels (RGB), and convolution extends naturally to 3D inputs using 3D kernels:\n\n(I * K)_{i,j} = \\sum_{c=1}^{C}\\sum_{m=0}^{L-1}\\sum_{n=0}^{L-1} I_{c,i+m,j+n} \\cdot K_{c,m,n}\n\nwhere C is the number of input channels. Each kernel processes all channels simultaneously, combining color information into a single output value.\n\n\n\n\n\n\nMulti-channel convolution. Each kernel processes all input channels, producing one output feature map.\n\n\n\n\nFigure 2\n\n\n\n\n\n\n\n\n\nInteractive visualizations\n\n\n\nExplore CNN operations interactively. The CNN Explainer shows how convolution, activation, and pooling work step-by-step. The Interactive Node-Link Visualization lets you see activations flow through a trained network.\n\n\n\n\nTranslation Equivariance: A Key Property\nA crucial feature of convolutional layers is translation equivariance: if you shift the input, the output shifts by the same amount. When detecting a vertical edge, if the edge moves one pixel to the right in the input, the detected feature also moves one pixel to the right in the output. The detection operation cares only about relative patterns, not absolute position.\n\n\n\n\n\n\nTranslation equivariance. The same kernel detects the same feature regardless of position in the input.\n\n\n\n\nFigure 3\n\n\n\nThis property allows CNNs to recognize objects anywhere in an image. A cat detector learned on centered cats will also detect cats in image corners, and the network doesn’t need to learn separate detectors for every possible position.\n\n\nParameter Sharing: Efficient Learning\nUnlike fully connected networks where each weight is used once, convolutional layers reuse kernel weights across all spatial positions. A 3×3 kernel applied to a 224×224 RGB image uses just 27 parameters (3×3×3), not the millions required by a fully connected layer. This weight-sharing dramatically reduces parameter count while preserving spatial relationships, a key reason CNNs can process high-resolution images efficiently.\n\n\nReceptive Field: Seeing More with Depth\nThe receptive field is the region of input pixels that influence each output pixel. In the first convolutional layer, a 3×3 kernel has a receptive field of 3×3 pixels. As we stack layers, the receptive field grows. Consider two 3×3 convolutional layers: each output pixel in the second layer depends on a 3×3 region in the first layer’s output, but each of those positions depends on a 3×3 region in the input, so the second layer’s receptive field is 5×5 in the original input.\n\n\n\n\n\n\nReceptive field grows with network depth. Deeper layers see increasingly large regions of the input image.\n\n\n\n\nFigure 4\n\n\n\nWhy does this matter? This hierarchical structure allows CNNs to detect increasingly complex, abstract features: early layers detect edges and simple patterns, middle layers combine these into textures and parts, and deep layers recognize complete objects and scenes.\n\n\nStride and Padding: Controlling Dimensions\nStride determines how many pixels we skip when sliding the kernel. With stride 1, we move one pixel at a time creating dense feature maps, while stride 2 skips every other position, effectively downsampling the output. For a 1D example with input [a,b,c,d,e,f] and kernel [1,2]:\nStride 1: \n[1a + 2b, 1b + 2c, 1c + 2d, 1d + 2e, 1e + 2f]\n\nStride 2: \n[1a + 2b, 1c + 2d, 1e + 2f]\n\nLarger strides reduce computational cost and increase the receptive field but might miss fine details.\n\n\n\n\n\n\nStride controls how far the kernel moves at each step. Stride 2 produces half the spatial dimensions of stride 1.\n\n\n\n\nFigure 5\n\n\n\nPadding addresses information loss at borders. Without padding (called “valid” padding), the output shrinks after each convolution because the kernel can’t fully overlap with border pixels. Zero padding adds a border of zeros around the input, allowing the kernel to process edge pixels and control output dimensions.\n\n\n\n\n\n\nZero padding extends the input with zeros, preserving spatial dimensions and processing border pixels.\n\n\n\n\nFigure 6\n\n\n\nFor a square input of size W with kernel size K, stride S, and padding P, the output dimension is:\n\nO = \\left\\lfloor\\frac{W - K + 2P}{S}\\right\\rfloor + 1\n\nExample: 224×224 input, 3×3 kernel, stride 2, padding 1:\n\nO = \\left\\lfloor\\frac{224 - 3 + 2(1)}{2}\\right\\rfloor + 1 = 112\n\nThe interplay between stride and padding lets network designers control spatial dimensions and computational efficiency. Try the Convolution Visualizer to experiment with different stride and padding settings interactively.\n\n\nPooling Layers: Downsampling with Invariance\nPooling layers downsample feature maps, reducing spatial dimensions while preserving important information. Max pooling selects the maximum value in each local window:\n\nP_{i,j} = \\max_{m,n} F_{si+m,sj+n}\n\nwhere F is the feature map, s is the stride (typically equal to the window size), and (m,n) range over the pooling window.\nAverage pooling computes the mean instead:\n\nP_{i,j} = \\frac{1}{w^2}\\sum_{m=0}^{w-1}\\sum_{n=0}^{w-1} F_{si+m,sj+n}\n\nMax pooling creates local translation invariance: if an edge moves slightly within a pooling window, the maximum value remains unchanged, helping the network focus on whether a feature is present rather than its exact position. Pooling also reduces computational cost by decreasing spatial dimensions, and a common pattern is to double the number of channels while halving spatial dimensions, maintaining roughly constant computational load across layers. Some recent architectures replace pooling with strided convolutions, arguing that learnable downsampling might be more effective (Springenberg et al. 2015), though the choice involves trade-offs between parameter efficiency and flexibility.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 3: Using CNN Models"
    ]
  },
  {
    "objectID": "m05-images/03-using-cnn-models.html#using-pre-trained-models",
    "href": "m05-images/03-using-cnn-models.html#using-pre-trained-models",
    "title": "Part 3: Using CNN Models",
    "section": "Using Pre-Trained Models",
    "text": "Using Pre-Trained Models\nNow that we understand CNN building blocks, let’s use them in practice. Training a CNN from scratch on ImageNet requires weeks of GPU time, but we can leverage pre-trained models trained by research labs with vast computational resources.\n\nLoading Models from torchvision\nPyTorch’s torchvision.models provides pre-trained implementations of major architectures:\n\nimport torch\nimport torchvision.models as models\nfrom torchvision import transforms\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\n# Load a pre-trained ResNet-50 model\nresnet50 = models.resnet50(weights='IMAGENET1K_V1')\nresnet50.eval()  # Set to evaluation mode\n\nprint(f\"Model type: {type(resnet50)}\")\nprint(f\"Number of parameters: {sum(p.numel() for p in resnet50.parameters()):,}\")\n\nModel type: &lt;class 'torchvision.models.resnet.ResNet'&gt;\nNumber of parameters: 25,557,032\n\n\nThe model is trained on ImageNet with 1000 classes, and we can use it to classify images directly.\n\n\nImage Classification Example\nTo use a pre-trained model, we must preprocess images the same way they were during training. ImageNet models expect images resized to 224×224 (or 299×299 for some models) with pixel values normalized to mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225].\n\n# Define the preprocessing transform pipeline\npreprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    ),\n])\n\n\n\nShow image display code\n# Display the original image\nplt.figure(figsize=(6, 6))\nplt.imshow(img)\nplt.title(\"Input Image (from CIFAR-10)\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n# Apply preprocessing and prepare for model input\ninput_tensor = preprocess(img)\ninput_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n\nprint(f\"Input shape: {input_batch.shape}\")  # [1, 3, 224, 224]\n\nInput shape: torch.Size([1, 3, 224, 224])\n\n\nNow classify the image:\n\n# Perform inference\nwith torch.no_grad():\n    output = resnet50(input_batch)\n\n# Output is logits for 1000 classes\nprint(f\"Output shape: {output.shape}\")  # [1, 1000]\n\n# Convert to probabilities and get top 5 predictions\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\n\n# Display predictions\nprint(\"\\nTop 5 predictions:\")\nfor i in range(5):\n    print(f\"{i+1}. {labels[top5_catid[i]]}: {top5_prob[i].item()*100:.2f}%\")\n\nOutput shape: torch.Size([1, 1000])\n\nTop 5 predictions:\n1. macaque: 25.89%\n2. frilled-necked lizard: 24.47%\n3. consomme: 12.40%\n4. patas monkey: 12.35%\n5. hot pot: 11.11%\n\n\nThe model correctly identifies the object with high confidence, demonstrating the power of pre-trained networks that have learned rich visual representations from ImageNet’s diverse images.\n\n\nWhen to Use Which Architecture\nDifferent architectures offer trade-offs between accuracy, speed, and memory. ResNet-50 provides excellent general-purpose performance with good accuracy and reasonable speed. EfficientNet is optimized for mobile and edge devices with the best accuracy-per-parameter ratio. VGG-16 has a simple, easy-to-understand architecture but is larger and slower than modern alternatives. MobileNet is designed for fast mobile inference at the cost of some accuracy. Vision Transformer (ViT) achieves state-of-the-art accuracy on large datasets but requires more data and compute. For most applications, start with ResNet-50 and optimize for speed or accuracy later if needed.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 3: Using CNN Models"
    ]
  },
  {
    "objectID": "m05-images/03-using-cnn-models.html#transfer-learning-adapting-pre-trained-models",
    "href": "m05-images/03-using-cnn-models.html#transfer-learning-adapting-pre-trained-models",
    "title": "Part 3: Using CNN Models",
    "section": "Transfer Learning: Adapting Pre-Trained Models",
    "text": "Transfer Learning: Adapting Pre-Trained Models\nPre-trained models learn general visual features from ImageNet’s 1000 categories, but what if you want to classify different objects? Transfer learning adapts these models to new tasks.\n\nWhy Transfer Learning Works\nImageNet-trained models learn a hierarchy of features: early layers detect edges, colors, and simple textures that are universal across tasks; middle layers detect patterns, parts, and compositions that are somewhat task-specific; late layers detect complete objects specific to ImageNet categories. The early and middle layers learn representations useful for many vision tasks, so we can reuse these features and only retrain the final layers for our specific problem.\n\n\nTwo Approaches: Feature Extraction vs. Fine-Tuning\nFeature Extraction freezes all convolutional layers and trains only a new classifier head, running fast and working well with small datasets. Fine-Tuning initializes with pre-trained weights then trains the entire network (or parts of it) on your data, achieving better accuracy but requiring more data and computation.\n\n\nExample: Fine-Tuning for Custom Classification\nLet’s adapt ResNet-50 to classify 10 animal species:\n\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Load pre-trained ResNet-50\nmodel = models.resnet50(weights='IMAGENET1K_V1')\n\n# Replace the final fully connected layer\n# Original: 2048 -&gt; 1000 (ImageNet classes)\n# New: 2048 -&gt; 10 (our custom classes)\nnum_classes = 10\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nprint(f\"Modified final layer: {model.fc}\")\n\nModified final layer: Linear(in_features=2048, out_features=10, bias=True)\n\n\nFor feature extraction, freeze early layers:\n\n# Freeze all layers except the final classifier\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Unfreeze the final layer\nfor param in model.fc.parameters():\n    param.requires_grad = True\n\n# Count trainable parameters\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(f\"Trainable parameters: {trainable_params:,} / {total_params:,}\")\n\nTrainable parameters: 20,490 / 23,528,522\n\n\nOnly 20,490 parameters (the final layer) are trainable, making training fast and preventing overfitting on small datasets.\n\n\nTraining Loop\n\n\nShow training loop implementation\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n\n# Training loop (pseudo-code, requires actual data)\ndef train_epoch(model, dataloader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for inputs, labels in dataloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    epoch_loss = running_loss / len(dataloader)\n    epoch_acc = correct / total\n    return epoch_loss, epoch_acc\n\n# For fine-tuning instead of feature extraction:\n# 1. Unfreeze all or some layers\n# 2. Use a smaller learning rate (e.g., 1e-4 or 1e-5)\n# 3. Train for more epochs\n\n\n\n\nData Augmentation: Essential for Small Datasets\nWhen training on limited data, augmentation is crucial: transform each image differently each epoch to artificially expand the training set.\n\n\nShow training augmentation pipeline\n# Training transforms with aggressive augmentation\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(224),      # Random crop and resize\n    transforms.RandomHorizontalFlip(),       # Flip with 50% probability\n    transforms.ColorJitter(                  # Random brightness, contrast\n        brightness=0.2,\n        contrast=0.2,\n        saturation=0.2\n    ),\n    transforms.RandomRotation(15),           # Rotate up to 15 degrees\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    ),\n])\n\n\n\n\nShow validation transform pipeline\n# Validation transforms (deterministic, no randomness)\nval_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    ),\n])\n\n\nNote that validation uses deterministic transforms (no randomness) for reproducible evaluation.\n\n\nBest Practices for Transfer Learning\nStart with feature extraction by training only the final layer first, which is fast and often achieves good results. If accuracy is insufficient, try fine-tuning by unfreezing earlier layers and training with a small learning rate (10× smaller than initial training). Use learning rate schedules to reduce the learning rate when validation loss plateaus. Monitor for overfitting using validation data, and apply more augmentation or stronger regularization (dropout, weight decay) if needed. Always match preprocessing to the pre-training dataset, using ImageNet statistics for most models.\n\n\n\n\n\n\nTry it yourself\n\n\n\nPractice transfer learning on your own image dataset. Collect 100-500 images per class (even phone camera photos work), then split into train/val/test sets (70/15/15). Start with ResNet-50 feature extraction and train for 10-20 epochs before evaluating on the test set. You’ll likely achieve 80-90%+ accuracy with just a few hundred images per class, demonstrating the power of pre-trained features.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 3: Using CNN Models"
    ]
  },
  {
    "objectID": "m05-images/03-using-cnn-models.html#visualizing-what-networks-learn",
    "href": "m05-images/03-using-cnn-models.html#visualizing-what-networks-learn",
    "title": "Part 3: Using CNN Models",
    "section": "Visualizing What Networks Learn",
    "text": "Visualizing What Networks Learn\nWhat features do trained networks actually detect? Let’s peek inside a trained network to see:\n\n\nShow activation visualization code\n# Extract intermediate feature maps\ndef get_activation(name, activations):\n    def hook(model, input, output):\n        activations[name] = output.detach()\n    return hook\n\n# Register hooks to capture activations\nactivations = {}\nmodel.layer1[0].conv1.register_forward_hook(get_activation('layer1', activations))\nmodel.layer2[0].conv1.register_forward_hook(get_activation('layer2', activations))\nmodel.layer3[0].conv1.register_forward_hook(get_activation('layer3', activations))\n\n# Run inference\nwith torch.no_grad():\n    _ = model(input_batch)\n\n# Visualize first layer activations\nlayer1_act = activations['layer1'][0]  # [C, H, W]\n\nfig, axes = plt.subplots(4, 8, figsize=(16, 8))\nfor i, ax in enumerate(axes.flat):\n    if i &lt; layer1_act.shape[0]:\n        ax.imshow(layer1_act[i].cpu(), cmap='viridis')\n    ax.axis('off')\nplt.suptitle(\"Layer 1 Feature Maps\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nEarly layers show edge detection and simple patterns, while deeper layers show increasingly abstract features that are harder to interpret but encode high-level semantic information.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 3: Using CNN Models"
    ]
  },
  {
    "objectID": "m05-images/03-using-cnn-models.html#summary",
    "href": "m05-images/03-using-cnn-models.html#summary",
    "title": "Part 3: Using CNN Models",
    "section": "Summary",
    "text": "Summary\nWe explored the building blocks that make CNNs powerful: convolution operations with learnable kernels, translation equivariance for position-invariant recognition, parameter sharing for efficiency, growing receptive fields through depth, stride and padding for dimension control, and pooling for downsampling with invariance. We learned to use pre-trained models from torchvision and adapt them to custom tasks through transfer learning, either by feature extraction (training only the classifier) or fine-tuning (training the entire network with small learning rates). Data augmentation artificially expands small datasets to prevent overfitting. These practical skills let you load state-of-the-art models, adapt them to your data, and achieve strong results with limited computational resources.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 3: Using CNN Models"
    ]
  },
  {
    "objectID": "m05-images/01-what-is-an-image.html",
    "href": "m05-images/01-what-is-an-image.html",
    "title": "Part 1: What is an Image?",
    "section": "",
    "text": "What you’ll learn in this section\n\n\n\nThis section introduces images as structured data.\nYou’ll learn:\n\nWhat pixels are and how computers represent visual information as grids of numbers.\nHow color channels combine to create RGB images.\nWhy spatial structure matters fundamentally for machine perception.\nHow to convert between NumPy arrays and PyTorch tensors for deep learning.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 1: What is an Image?"
    ]
  },
  {
    "objectID": "m05-images/01-what-is-an-image.html#a-photograph-is-just-numbers",
    "href": "m05-images/01-what-is-an-image.html#a-photograph-is-just-numbers",
    "title": "Part 1: What is an Image?",
    "section": "A Photograph is Just Numbers",
    "text": "A Photograph is Just Numbers\nLet’s talk about what an image really is from a computer’s perspective. When you look at a photograph, you see faces, objects, and scenes, but to a machine, that same photograph is simply a grid of numbers where each value represents the brightness or color at a specific location. How can numbers capture the richness of visual information? The answer lies in spatial structure. Unlike a spreadsheet where row order doesn’t matter, the arrangement of numbers in an image is everything, with neighboring pixels relating to each other to form edges, textures, and patterns.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 1: What is an Image?"
    ]
  },
  {
    "objectID": "m05-images/01-what-is-an-image.html#grayscale-images-the-simplest-case",
    "href": "m05-images/01-what-is-an-image.html#grayscale-images-the-simplest-case",
    "title": "Part 1: What is an Image?",
    "section": "Grayscale Images: The Simplest Case",
    "text": "Grayscale Images: The Simplest Case\nThe very first step in understanding images is to examine the grayscale case, where an image contains only brightness information with no color. We can think of it as a 2D matrix where each entry is a pixel intensity value. Consider a tiny 6×6 grayscale image:\n\nX = \\begin{bmatrix}\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10\n\\end{bmatrix}\n\nHere, the value 10 represents dark pixels, while 80 represents bright pixels. The third column forms a bright vertical line. This simple example shows how spatial patterns emerge from the arrangement of numbers.\n\n\n\n\n\n\nA grayscale image represented as a matrix of pixel intensity values. Each number encodes brightness at that location.\n\n\n\n\nFigure 1",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 1: What is an Image?"
    ]
  },
  {
    "objectID": "m05-images/01-what-is-an-image.html#loading-and-inspecting-real-images",
    "href": "m05-images/01-what-is-an-image.html#loading-and-inspecting-real-images",
    "title": "Part 1: What is an Image?",
    "section": "Loading and Inspecting Real Images",
    "text": "Loading and Inspecting Real Images\nLet’s make this concrete by loading an actual image and examining its structure. We’ll use Python with standard libraries to see what an image really looks like under the hood.\n\n\nShow image display code\n# Display the image\nplt.figure(figsize=(8, 6))\nplt.imshow(img_array)\nplt.title(\"Original Image\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n# Examine the image properties\nprint(f\"Image shape: {img_array.shape}\")\nprint(f\"Data type: {img_array.dtype}\")\nprint(f\"Value range: [{img_array.min()}, {img_array.max()}]\")\n\nImage shape: (300, 600, 3)\nData type: uint8\nValue range: [0, 255]\n\n\nWhat does the shape tell us? The output (height, width, 3) reveals three dimensions: the first two specify spatial location, while the third holds three color channels. Let’s zoom into a small patch to see the actual numbers:\n\n# Extract a tiny 5x5 patch from the center\ncenter_y, center_x = img_array.shape[0] // 2, img_array.shape[1] // 2\npatch = img_array[center_y:center_y+5, center_x:center_x+5, 0]  # Red channel only\n\nprint(\"A 5x5 patch of pixel values (Red channel):\")\nprint(patch)\n\nA 5x5 patch of pixel values (Red channel):\n[[ 45  49  40  93 141]\n [ 55  52  55 113 102]\n [ 35  46  63 121 104]\n [136  52  48  84 124]\n [225  90  41  73 136]]\n\n\nThese are the actual numbers the computer sees. Each value between 0 and 255 represents brightness in the red channel for that pixel location.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 1: What is an Image?"
    ]
  },
  {
    "objectID": "m05-images/01-what-is-an-image.html#color-images-three-stacked-layers",
    "href": "m05-images/01-what-is-an-image.html#color-images-three-stacked-layers",
    "title": "Part 1: What is an Image?",
    "section": "Color Images: Three Stacked Layers",
    "text": "Color Images: Three Stacked Layers\nColor images extend the grayscale concept by using three separate matrices, one for each color channel: Red, Green, and Blue (RGB). Think of these as three grayscale images stacked on top of each other. When you combine the values from all three channels at a given location, you get the color for that pixel: [255, 0, 0] is pure red, [0, 255, 0] is pure green, and [255, 255, 255] is white. How do the channels look separately? Let’s visualize them:\n\n\nShow channel visualization code\nfig, axes = plt.subplots(1, 4, figsize=(16, 4))\n\n# Original image\naxes[0].imshow(img_array)\naxes[0].set_title(\"Original Image\")\naxes[0].axis(\"off\")\n\n# Individual channels\nchannel_names = ['Red', 'Green', 'Blue']\ncolors = ['Reds', 'Greens', 'Blues']\n\nfor i, (name, cmap) in enumerate(zip(channel_names, colors)):\n    axes[i+1].imshow(img_array[:, :, i], cmap=cmap)\n    axes[i+1].set_title(f\"{name} Channel\")\n    axes[i+1].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNotice how each channel emphasizes different aspects of the scene. The red channel might be bright where red objects appear, while the blue channel highlights sky and water.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 1: What is an Image?"
    ]
  },
  {
    "objectID": "m05-images/01-what-is-an-image.html#why-spatial-structure-matters",
    "href": "m05-images/01-what-is-an-image.html#why-spatial-structure-matters",
    "title": "Part 1: What is an Image?",
    "section": "Why Spatial Structure Matters",
    "text": "Why Spatial Structure Matters\nShift your attention from individual pixel values to relationships between pixels. This is what makes images fundamentally different from tabular data: in a spreadsheet, you can shuffle the rows without losing information, but in an image, shuffling pixels destroys everything because the spatial arrangement is the information itself. An edge appears when neighboring pixels have very different values, a texture emerges from repeating patterns across nearby locations, and an object is a coherent region of similar pixels. Why does this matter for machine learning? Fully connected networks treat every input independently, ignoring spatial relationships, while convolutional networks (which we’ll explore soon) are designed specifically to exploit this structure.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 1: What is an Image?"
    ]
  },
  {
    "objectID": "m05-images/01-what-is-an-image.html#converting-between-representations",
    "href": "m05-images/01-what-is-an-image.html#converting-between-representations",
    "title": "Part 1: What is an Image?",
    "section": "Converting Between Representations",
    "text": "Converting Between Representations\nLet’s practice manipulating image representations to build intuition:\n\n# Convert to grayscale by averaging channels\ngrayscale = np.mean(img_array, axis=2).astype(np.uint8)\n\nprint(f\"RGB shape: {img_array.shape}\")\nprint(f\"Grayscale shape: {grayscale.shape}\")\n\nRGB shape: (300, 600, 3)\nGrayscale shape: (300, 600)\n\n\n\n\nShow RGB vs grayscale comparison\n# Create a side-by-side comparison\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\naxes[0].imshow(img_array)\naxes[0].set_title(\"RGB Image (3 channels)\")\naxes[0].axis(\"off\")\n\naxes[1].imshow(grayscale, cmap='gray')\naxes[1].set_title(\"Grayscale Image (1 channel)\")\naxes[1].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe grayscale version loses color information but preserves spatial structure. For many computer vision tasks, this simplified representation is sufficient.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 1: What is an Image?"
    ]
  },
  {
    "objectID": "m05-images/01-what-is-an-image.html#image-dimensions-in-deep-learning",
    "href": "m05-images/01-what-is-an-image.html#image-dimensions-in-deep-learning",
    "title": "Part 1: What is an Image?",
    "section": "Image Dimensions in Deep Learning",
    "text": "Image Dimensions in Deep Learning\nWhen we feed images into neural networks, we need to be precise about dimensions. Different frameworks use different conventions. PyTorch expects images in (batch_size, channels, height, width) format, often abbreviated as NCHW:\n\nN: Batch size (number of images processed together)\nC: Channels (3 for RGB, 1 for grayscale)\nH: Height in pixels\nW: Width in pixels\n\nLet’s convert our image to PyTorch format:\n\nimport torch\n\n# Original numpy array is (H, W, C)\nprint(f\"NumPy format (H, W, C): {img_array.shape}\")\n\n# Convert to PyTorch format (C, H, W) for a single image\nimg_tensor = torch.from_numpy(img_array).permute(2, 0, 1).float() / 255.0\nprint(f\"PyTorch format (C, H, W): {img_tensor.shape}\")\n\n# Add batch dimension to get (N, C, H, W)\nimg_batch = img_tensor.unsqueeze(0)\nprint(f\"Batch format (N, C, H, W): {img_batch.shape}\")\n\nNumPy format (H, W, C): (300, 600, 3)\nPyTorch format (C, H, W): torch.Size([3, 300, 600])\nBatch format (N, C, H, W): torch.Size([1, 3, 300, 600])\n\n\nWe also normalized pixel values from [0, 255] to [0, 1] by dividing by 255. Why do this? This normalization helps neural networks train more stably.\n\n\n\n\n\n\nTry it yourself\n\n\n\nLoad your own image and explore its properties. Extract and visualize a small patch of pixels as numbers to see the underlying data structure. Modify some pixel values directly and observe how the image changes. Swap the red and blue channels to see the dramatic color shift this creates. Convert between NumPy and PyTorch formats to build fluency with both representations.\nUnderstanding these representations at a hands-on level will make everything that follows more intuitive.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 1: What is an Image?"
    ]
  },
  {
    "objectID": "m05-images/01-what-is-an-image.html#from-pixels-to-patterns",
    "href": "m05-images/01-what-is-an-image.html#from-pixels-to-patterns",
    "title": "Part 1: What is an Image?",
    "section": "From Pixels to Patterns",
    "text": "From Pixels to Patterns\nNow that we understand what images are as data structures, the next question becomes: how do we detect patterns in them? Human vision effortlessly recognizes edges, textures, and objects, but what computational operations allow machines to do the same? This question leads us to convolution, feature extraction, and ultimately to the deep learning revolution. In the next section, we’ll explore how computer vision evolved from hand-crafted feature detectors to learned representations that can match or exceed human performance. The key insight to carry forward is this: images are spatial data where relationships between neighboring pixels encode visual information.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 1: What is an Image?"
    ]
  },
  {
    "objectID": "m05-images/01-what-is-an-image.html#summary",
    "href": "m05-images/01-what-is-an-image.html#summary",
    "title": "Part 1: What is an Image?",
    "section": "Summary",
    "text": "Summary\nWe explored images as structured numerical data: a grayscale image is a 2D matrix of brightness values, while a color image adds two more matrices for the other color channels. Spatial relationships between pixels encode edges, textures, and objects, and unlike tabular data, the arrangement of values matters fundamentally. We saw how to load, inspect, and manipulate images in Python, converting between NumPy arrays and PyTorch tensors. This hands-on understanding prepares us to work with the deep learning models that process these image representations. Images are not just collections of numbers but spatially organized data where local patterns combine into global structure.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 1: What is an Image?"
    ]
  },
  {
    "objectID": "m04-text/word-bias.html",
    "href": "m04-text/word-bias.html",
    "title": "Word Bias",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module explores how word embeddings capture and reinforce societal biases.\nYou’ll learn:\n\nHow semantic axes reveal gender bias in occupations and concepts through geometric relationships.\nHow to measure bias using cosine similarity and the SemAxis framework.\nThe difference between direct bias (explicit gender associations) and indirect bias (gender encoded through proxy dimensions).\nWhy understanding these biases matters for building fair AI systems.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Bias"
    ]
  },
  {
    "objectID": "m04-text/word-bias.html#understanding-bias-in-word-embeddings",
    "href": "m04-text/word-bias.html#understanding-bias-in-word-embeddings",
    "title": "Word Bias",
    "section": "Understanding Bias in Word Embeddings",
    "text": "Understanding Bias in Word Embeddings\nHave you ever wondered what biases might be hiding in word embeddings? Word embeddings capture and reinforce societal biases from their training data through geometric relationships between word vectors, often reflecting stereotypes about gender, race, age, and other social factors.\nSemAxis is a powerful tool to analyze gender bias in word embeddings by measuring word alignments along semantic axes. Using antonym pairs like “she-he” as poles, it quantifies gender associations in words on a scale from -1 to 1, where positive values indicate feminine associations while negative values indicate masculine associations. Let’s start with a simple example of analyzing gender bias in occupations.\n\nimport gensim.downloader as api\nimport numpy as np\n\n# Load pre-trained Word2vec embeddings\nmodel = api.load(\"word2vec-google-news-300\")\n\n\ndef compute_bias(word, microframe, model):\n    word_vector = model[word]\n    numerator = np.dot(word_vector, microframe)\n    denominator = np.linalg.norm(word_vector) * np.linalg.norm(microframe)\n    return numerator / denominator\n\n\ndef analyze(word, pos_word, neg_word, model):\n    if word not in model:\n        return 0.0\n    microframe = model[pos_word] - model[neg_word]\n    bias = compute_bias(word, microframe, model)\n    return bias\n\nWhat’s happening in compute_bias? The function calculates the cosine similarity between a word vector and a semantic axis (microframe), where the numerator computes the dot product (projecting the word onto the axis) and the denominator normalizes by vector lengths to get a score between -1 and 1. Let’s use these occupations:\n\n# Occupations from the paper\nshe_occupations = [\n    \"homemaker\",\n    \"nurse\",\n    \"receptionist\",\n    \"librarian\",\n    \"socialite\",\n    \"hairdresser\",\n    \"nanny\",\n    \"bookkeeper\",\n    \"stylist\",\n    \"housekeeper\",\n]\n\nhe_occupations = [\n    \"maestro\",\n    \"skipper\",\n    \"protege\",\n    \"philosopher\",\n    \"captain\",\n    \"architect\",\n    \"financier\",\n    \"warrior\",\n    \"broadcaster\",\n    \"magician\",\n    \"boss\",\n]\n\nWe measure the gender bias in these occupations by measuring how they align with the “she-he” axis.\n\n\nCode\nprint(\"Gender Bias in Occupations (she-he axis):\")\nprint(\"\\nShe-associated occupations:\")\nfor occupation in she_occupations:\n    bias = analyze(occupation, \"she\", \"he\", model)\n    print(f\"{occupation}: {bias:.3f}\")\n\nprint(\"\\nHe-associated occupations:\")\nfor occupation in he_occupations:\n    bias = analyze(occupation, \"she\", \"he\", model)\n    print(f\"{occupation}: {bias:.3f}\")\n\n\nGender Bias in Occupations (she-he axis):\n\nShe-associated occupations:\nhomemaker: 0.360\nnurse: 0.333\nreceptionist: 0.329\nlibrarian: 0.300\nsocialite: 0.310\nhairdresser: 0.307\nnanny: 0.287\nbookkeeper: 0.264\nstylist: 0.252\nhousekeeper: 0.260\n\nHe-associated occupations:\nmaestro: -0.203\nskipper: -0.177\nprotege: -0.148\nphilosopher: -0.155\ncaptain: -0.130\narchitect: -0.151\nfinancier: -0.145\nwarrior: -0.120\nbroadcaster: -0.124\nmagician: -0.110\nboss: -0.090\n\n\nHow do we interpret these scores? Positive scores indicate closer association to “she” (e.g., nurse, librarian), while negative scores indicate closer association to “he” (e.g., architect, captain), with magnitude showing the strength of association. Notice how occupations historically associated with women have strong positive scores while those associated with men have negative scores, confirming that the model has learned these gender stereotypes from the text data.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Bias"
    ]
  },
  {
    "objectID": "m04-text/word-bias.html#stereotype-analogies",
    "href": "m04-text/word-bias.html#stereotype-analogies",
    "title": "Word Bias",
    "section": "Stereotype Analogies",
    "text": "Stereotype Analogies\nWhat happens when we look at word pairs? Since word embeddings capture semantic relationships from large text corpora, they inevitably encode societal biases and stereotypes. Let’s measure how different words align with the gender axis (she-he) to find pairs where one word shows strong feminine bias while its counterpart shows masculine bias, revealing ingrained stereotypes in language use.\n\n# Stereotype analogies from the paper\nstereotype_pairs = [\n    (\"sewing\", \"carpentry\"),\n    (\"nurse\", \"surgeon\"),\n    (\"softball\", \"baseball\"),\n    (\"cosmetics\", \"pharmaceuticals\"),\n    (\"vocalist\", \"guitarist\"),\n]\n\n\n\nCode\nprint(\"\\nAnalyzing Gender Stereotype Pairs:\")\nfor word1, word2 in stereotype_pairs:\n    bias1 = analyze(word1, \"she\", \"he\", model)\n    bias2 = analyze(word2, \"she\", \"he\", model)\n    print(f\"\\n{word1} vs {word2}\")\n    print(f\"{word1}: {bias1:.3f}\")\n    print(f\"{word2}: {bias2:.3f}\")\n\n\n\nAnalyzing Gender Stereotype Pairs:\n\nsewing vs carpentry\nsewing: 0.302\ncarpentry: -0.028\n\nnurse vs surgeon\nnurse: 0.333\nsurgeon: -0.048\n\nsoftball vs baseball\nsoftball: 0.260\nbaseball: -0.066\n\ncosmetics vs pharmaceuticals\ncosmetics: 0.331\npharmaceuticals: -0.011\n\nvocalist vs guitarist\nvocalist: 0.140\nguitarist: -0.041\n\n\nThe results show clear stereotypical alignments where sewing and nurse align with “she” while carpentry and surgeon align with “he”, mirroring the “man is to computer programmer as woman is to homemaker” analogy found in early word embedding research.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Bias"
    ]
  },
  {
    "objectID": "m04-text/word-bias.html#indirect-bias-when-neutral-words-become-gendered",
    "href": "m04-text/word-bias.html#indirect-bias-when-neutral-words-become-gendered",
    "title": "Word Bias",
    "section": "Indirect Bias: When Neutral Words Become Gendered",
    "text": "Indirect Bias: When Neutral Words Become Gendered\nWhat about words that don’t explicitly reference gender? Indirect bias occurs when seemingly neutral words become associated with gender through their relationships with other words. For example, while “softball” and “football” are not inherently gendered terms, they may show gender associations due to how they’re used in language and society.\nWe can detect indirect bias by identifying word pairs that form a semantic axis (like softball-football), measuring how other words align with this axis, and examining if alignment correlates with gender bias. Let’s see how this works by measuring the gender bias of these words:\n\n# Words associated with softball-football axis\nsoftball_associations = [\n    \"pitcher\",\n    \"bookkeeper\",\n    \"receptionist\",\n    \"nurse\",\n    \"waitress\"\n]\n\nfootball_associations = [\n    \"footballer\",\n    \"businessman\",\n    \"pundit\",\n    \"maestro\",\n    \"cleric\"\n]\n\n# Calculate biases for all words\ngender_biases = []\nsports_biases = []\nwords = softball_associations + football_associations\n\nfor word in words:\n    gender_bias = analyze(word, \"she\", \"he\", model)\n    sports_bias = analyze(word, \"softball\", \"football\", model)\n    gender_biases.append(gender_bias)\n    sports_biases.append(sports_bias)\n\nLet’s plot the results:\n\n\nCode\n# Analyze bias along both gender and sports axes\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom adjustText import adjust_text\n\n# Create scatter plot\nfig, ax = plt.subplots(figsize=(6, 6))\nsns.scatterplot(x=gender_biases, y=sports_biases, ax=ax)\nax.set_xlabel(\"Gender Bias (she-he)\")\nax.set_ylabel(\"Sports Bias (softball-football)\")\nax.set_title(\"Indirect Bias Analysis: Gender vs Sports\")\n\n# Add labels for each point\ntexts = []\nfor i, word in enumerate(words):\n    texts.append(ax.text(gender_biases[i], sports_biases[i], word, fontsize=12))\n\nadjust_text(texts, arrowprops=dict(arrowstyle='-', color='gray', lw=0.5))\n\nax.grid(True, alpha=0.3)\nplt.show()\nsns.despine()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nThe plot reveals a correlation where words associated with “softball” (y-axis greater than 0) also tend to be associated with “she” (x-axis greater than 0), while “football” terms align with “he”. This suggests that even if we remove explicit gender words, the structure of the space still encodes gender through these proxy dimensions.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Bias"
    ]
  },
  {
    "objectID": "m04-text/word-bias.html#the-impact-and-path-forward",
    "href": "m04-text/word-bias.html#the-impact-and-path-forward",
    "title": "Word Bias",
    "section": "The Impact and Path Forward",
    "text": "The Impact and Path Forward\nWord embeddings, while powerful, inevitably capture and reflect societal biases present in the large text corpora they are trained on. We observed both direct bias (where occupations or attributes align strongly with specific gender pronouns) and indirect bias (where seemingly neutral concepts become gendered through their associations with other words). This analysis highlights the importance of understanding and mitigating these biases to prevent the perpetuation of stereotypes in AI systems and ensure fairness in applications like search, recommendation, and hiring.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Bias"
    ]
  },
  {
    "objectID": "m04-text/tokenization.html",
    "href": "m04-text/tokenization.html",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module introduces tokenization, the subword compression strategy that powers modern language models.\nYou’ll learn:\n\nWhat tokens are and why LLMs use subwords instead of whole words.\nHow subword tokenization reduces memory costs while enabling rare word handling.\nThe complete pipeline from raw text to token IDs to embeddings.\nWhy compression artifacts cause strange LLM behaviors like failing at character counting.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m04-text/tokenization.html#why-not-just-words",
    "href": "m04-text/tokenization.html#why-not-just-words",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "Why Not Just Words?",
    "text": "Why Not Just Words?\nHave you ever wondered how an LLM reads text? You might assume it reads word by word, treating each word as an atomic unit. This assumption is wrong.\nThe model operates on tokens, which are subword chunks. These could be full words like “the”, word parts like “ingham”, or single characters like “B”. This choice isn’t arbitrary. It’s a geometric compression strategy.\nWhy compress? If we used whole words, the vocabulary would balloon to millions of entries. Each entry requires a row in the embedding matrix, so memory scales linearly with vocabulary size. A 1-million-word vocabulary with 2048-dimensional embeddings would require over 8GB just for the lookup table.\nSubword tokenization collapses this problem by focusing on frequently occurring fragments. With roughly 50,000 subwords, the model reconstructs both common words (stored as single tokens) and rare words (assembled from parts). The system trades a small increase in sequence length for massive reductions in memory and computational overhead.\nThis compression explains a strange quirk. Why do LLMs sometimes fail at seemingly trivial tasks like counting letters? The word “strawberry” might tokenize as [“straw”, “berry”], meaning the model never sees individual “r” characters as separate units. It’s not stupidity. It’s compression artifacts.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m04-text/tokenization.html#how-tokenization-works-in-practice",
    "href": "m04-text/tokenization.html#how-tokenization-works-in-practice",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "How Tokenization Works in Practice",
    "text": "How Tokenization Works in Practice\nLet’s unbox an actual tokenizer from Hugging Face and trace the pipeline from raw text to embeddings. We’ll use Phi-1.5, a compact model from Microsoft.\nFor tokenization experiments, we only need the tokenizer itself, not the full multi-gigabyte model.\n\nfrom transformers import AutoTokenizer\nimport os\n\nmodel_name = \"microsoft/phi-1.5\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nLet’s inspect the tokenizer’s constraints.\n\n\nCode\nprint(f\"Vocabulary size: {tokenizer.vocab_size:,} tokens\")\nprint(f\"Max sequence length: {tokenizer.model_max_length} tokens\")\n\n\nVocabulary size: 50,257 tokens\nMax sequence length: 2048 tokens\n\n\nThis tokenizer knows 50,257 unique tokens and enforces a maximum sequence length of 2048 tokens. If your input exceeds this limit, the model will truncate or reject it. This is a hard boundary imposed by the positional encoding system, not a soft guideline.\n\nFrom Text to Tokens\nTokenization splits text into the subword fragments the model actually processes. Watch what happens when we tokenize a university name.\n\ntext = \"Binghamton University.\"\n\ntokens = tokenizer.tokenize(text)\n\n\n\nCode\nprint(f\"Tokens: {tokens}\")\n\n\nTokens: ['B', 'ingham', 'ton', 'ĠUniversity', '.']\n\n\nThe rare word “Binghamton” fractures into [‘B’, ‘ingham’, ‘ton’]. The common word “University” survives intact (with a leading space marker). The tokenizer learned these splits from frequency statistics during training. High-frequency words get dedicated tokens. Rare words get decomposed into reusable parts.\nWhat’s that Ġ character? The Ġ character (U+0120) is a GPT-style tokenizer convention for encoding spaces. When you see ĠUniversity, it means “University” preceded by a space. This preserves word boundaries while allowing subword splits.\nLet’s test a few more examples to see the pattern.\n\n\nCode\ntexts = [\n    \"Bearcats\",\n    \"New York\",\n]\n\nprint(\"Word tokenization examples:\\n\")\nfor text in texts:\n    tokens = tokenizer.tokenize(text)\n    print(f\"{text:10s} → {tokens}\")\n\n\nWord tokenization examples:\n\nBearcats   → ['Bear', 'cats']\nNew York   → ['New', 'ĠYork']\n\n\n“Bearcats” splits because it’s domain-specific jargon. “New York” remains whole because it’s common. The tokenizer’s behavior directly reflects its training corpus.\nCheck out OpenAI’s tokenizer to see how different models slice the same text differently.\n\n\nFrom Tokens to Token IDs\nTokens are still strings. The model needs integers. Each token maps to a unique ID in the vocabulary dictionary.\n\n\nCode\ntext = \"Binghamton University\"\n\n# Get token IDs\ntoken_ids = tokenizer.encode(text, add_special_tokens=False)\ntokens = tokenizer.tokenize(text)\n\nprint(\"Token → Token ID mapping:\\n\")\nfor token, token_id in zip(tokens, token_ids):\n    print(f\"{token:10s} → {token_id:6d}\")\n\n\nToken → Token ID mapping:\n\nB          →     33\ningham     →  25875\nton        →   1122\nĠUniversity →   2059\n\n\nEach token receives a unique integer ID. The vocabulary is a dictionary mapping token strings to integer IDs. Let’s peek inside.\n\n# Get the full vocabulary\nvocab = tokenizer.get_vocab()\n\n# Sample some tokens\nsample_tokens = list(vocab.items())[:5]\nfor token, id in sample_tokens:\n    print(f\"  {id:6d}: '{token}'\")\n\n   31818: 'Ġinconvenience'\n   39472: 'Ġunknow'\n   31083: 'Ġconcluding'\n   35540: 'Ġ;)'\n    5506: 'ĠAnn'\n\n\nMost LLMs reserve special tokens for sequence boundaries or control signals. Phi-1.5 uses &lt;|endoftext|&gt; as a separator during training. Let’s verify.\n\ntoken_id = [50256]\ntoken = tokenizer.convert_ids_to_tokens(token_id)[0]\nprint(f\"Token ID: {token_id} → Token: {token}\")\n\nToken ID: [50256] → Token: &lt;|endoftext|&gt;\n\n\nToken ID 50256 is Phi-specific. Other models use different conventions (BERT uses [SEP] and [CLS]). Always check your tokenizer’s special tokens before preprocessing data.\n\n\nFrom Token IDs to Embeddings\n\nNow we need the full model to access the embedding layer, the matrix that converts token IDs into dense vectors.\n\nfrom transformers import AutoModelForCausalLM\nimport torch\n\n# Load the model\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Retrieve the embedding layer\nembedding_layer = model.model.embed_tokens\n\nThe embedding layer is a simple lookup table. It’s a 51,200 × 2,048 matrix where each row is the embedding for a token in the vocabulary. Let’s examine the first few entries.\n\n\nCode\nprint(embedding_layer.weight[:5, :10])\n\n\ntensor([[ 0.0097, -0.0155,  0.0603,  0.0326, -0.0108, -0.0271, -0.0273,  0.0178,\n         -0.0242,  0.0100],\n        [ 0.0243,  0.0543,  0.0178, -0.0679, -0.0130,  0.0265, -0.0423, -0.0287,\n         -0.0051, -0.0179],\n        [-0.0416,  0.0370, -0.0160, -0.0254, -0.0371, -0.0208, -0.0023,  0.0647,\n          0.0468,  0.0013],\n        [-0.0051, -0.0044,  0.0248, -0.0489,  0.0399,  0.0005, -0.0070,  0.0148,\n          0.0030,  0.0070],\n        [ 0.0289,  0.0362, -0.0027, -0.0775, -0.0136, -0.0203, -0.0566, -0.0558,\n          0.0269, -0.0067]], grad_fn=&lt;SliceBackward0&gt;)\n\n\nThese numbers are learned parameters, optimized during training to capture semantic relationships. Token IDs are discrete symbols. Embeddings are continuous coordinates in a 2048-dimensional space. This is what the transformer layers operate on.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m04-text/tokenization.html#the-full-pipeline",
    "href": "m04-text/tokenization.html#the-full-pipeline",
    "title": "Tokenization: Unboxing How LLMs Read Text",
    "section": "The Full Pipeline",
    "text": "The Full Pipeline\nYou’ve now traced the complete pipeline. Raw text fractures into subword tokens, tokens map to integer IDs, and IDs retrieve vector embeddings from a learned matrix. This tokenization step is foundational. Without it, the model cannot begin processing language.\nWhat comes next? The transformer layers come next, using attention mechanisms to extract patterns from these embeddings.\nRemember three key constraints. First, LLMs operate on subwords, not words, because vocabulary size is a memory bottleneck. Second, tokenization is learned from data, not hand-designed, so different models split text differently. Third, compression has side effects. Tasks like character counting fail because the model never sees individual characters as atomic units.\nWith this machinery exposed, we’re ready to examine the transformer itself. It’s the architecture that processes these embeddings and enables LLMs to predict the next token.\n\nNext: Transformers: The Architecture Behind the Magic",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Tokenization: Unboxing How LLMs Read Text"
    ]
  },
  {
    "objectID": "m04-text/semaxis.html",
    "href": "m04-text/semaxis.html",
    "title": "SemAxis: Meaning as Direction",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module introduces SemAxis, a framework for measuring semantic meaning through contrast.\nYou’ll learn:\n\nHow meaning emerges from contrast rather than inherent properties.\nHow to construct semantic axes by subtracting antonym vectors.\nHow to project words onto axes to measure alignment with concepts like sentiment or intensity.\nHow to build robust axes using centroids of synonym clusters.\nHow to visualize semantic relationships in 2D space by crossing two axes.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Semaxis"
    ]
  },
  {
    "objectID": "m04-text/semaxis.html#embedding-space-as-contrast",
    "href": "m04-text/semaxis.html#embedding-space-as-contrast",
    "title": "SemAxis: Meaning as Direction",
    "section": "Embedding Space as Contrast",
    "text": "Embedding Space as Contrast\nHave you ever wondered how to extract specific meanings from word embeddings? We intuitively treat word embeddings as static maps where “king” is simply near “queen”, assuming the meaning is inherent to the coordinate itself, much like a city has a fixed latitude and longitude. This is a convenient fiction. In embedding space, meaning emerges entirely from contrast.\nSemAxis defines a semantic axis by subtracting one vector from its opposite (e.g., v_{good} - v_{bad}), isolating a single semantic dimension that ignores all other information.\nGiven two pole words w_+ and w_-, the axis is defined as:\n\nv_{\\text{axis}} = \\frac{v_{w_+} - v_{w_-}}{||v_{w_+} - v_{w_-}\\||_2}\n\nThe denominator is the L_2 norm of the difference vector, ensuring v_{\\text{axis}} is a unit vector. Using this “ruler”, we project words onto the axis with cosine similarity between v_{w} and v_{\\text{axis}}:\n\n\\text{Position of w on axis } v_{\\text{axis}} = \\cos(v_{\\text{axis}},v_{w})\n\nLet’s build a “Sentiment Compass” to measure the emotional charge of words that aren’t explicitly emotional. First, we load the standard GloVe embeddings.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gensim.downloader as api\n\n# Download and load pre-trained GloVe embeddings\nmodel = api.load(\"glove-wiki-gigaword-100\")",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Semaxis"
    ]
  },
  {
    "objectID": "m04-text/semaxis.html#defining-the-axis",
    "href": "m04-text/semaxis.html#defining-the-axis",
    "title": "SemAxis: Meaning as Direction",
    "section": "Defining the Axis",
    "text": "Defining the Axis\nWe define the axis not as a point, but as the difference vector between two poles. This vector points from “bad” to “good”.\n\ndef create_axis(pos_word, neg_word, model):\n    return model[pos_word] - model[neg_word]\n\n\n# The \"Sentiment\" Axis\nsentiment_axis = create_axis(\"good\", \"bad\", model)",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Semaxis"
    ]
  },
  {
    "objectID": "m04-text/semaxis.html#measuring-alignment",
    "href": "m04-text/semaxis.html#measuring-alignment",
    "title": "SemAxis: Meaning as Direction",
    "section": "Measuring Alignment",
    "text": "Measuring Alignment\nHow do we see where a word falls on this axis? We project it using the normalized dot product. If the vector points in the same direction, the score is positive; if it points away, it is negative.\n\ndef get_score(word, axis, model):\n    v_word = model[word]\n    # Cosine similarity is just a normalized dot product\n    return np.dot(v_word, axis) / (np.linalg.norm(v_word) * np.linalg.norm(axis))\n\n\nwords = [\"excellent\", \"terrible\", \"mediocre\", \"stone\", \"flower\"]\nfor w in words:\n    print(f\"{w}: {get_score(w, sentiment_axis, model):.3f}\")\n\nexcellent: 0.523\nterrible: -0.208\nmediocre: -0.001\nstone: 0.181\nflower: 0.204",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Semaxis"
    ]
  },
  {
    "objectID": "m04-text/semaxis.html#robustness-via-centroids",
    "href": "m04-text/semaxis.html#robustness-via-centroids",
    "title": "SemAxis: Meaning as Direction",
    "section": "Robustness via Centroids",
    "text": "Robustness via Centroids\nSingle words are noisy since “bad” might carry connotations of “naughty” or “poor quality”. The solution is to use the centroid of a cluster of synonyms, averaging out the noise to leave only the pure semantic signal.\n\ndef create_robust_axis(pos_word, neg_word, model, k=5):\n    # Get k nearest neighbors for both poles\n    pos_group = [pos_word]\n    pos_words = model.most_similar(pos_word, topn=k)\n    for word, _ in pos_words:\n        pos_group.append(word)\n\n    neg_group = [neg_word]\n    neg_words = model.most_similar(neg_word, topn=k)\n    for word, _ in neg_words:\n        neg_group.append(word)\n\n    # Average them to find the centroid\n    pos_vec = np.mean([model[w] for w in pos_group], axis=0)\n    neg_vec = np.mean([model[w] for w in neg_group], axis=0)\n\n    return pos_vec - neg_vec\n\n\nrobust_axis = create_robust_axis(\"good\", \"bad\", model)",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Semaxis"
    ]
  },
  {
    "objectID": "m04-text/semaxis.html#the-2d-semantic-space",
    "href": "m04-text/semaxis.html#the-2d-semantic-space",
    "title": "SemAxis: Meaning as Direction",
    "section": "The 2D Semantic Space",
    "text": "The 2D Semantic Space\nWhen does the real power emerge? When we cross two axes, plotting words against “Sentiment” and “Intensity” (Strong vs. Weak) to reveal relationships that a single list hides.\n\n\nCode\ndef plot_2d(words, axis_x, axis_y, model):\n    x_scores = [get_score(w, axis_x, model) for w in words]\n    y_scores = [get_score(w, axis_y, model) for w in words]\n\n    plt.figure(figsize=(5, 5))\n    plt.scatter(x_scores, y_scores)\n\n    for i, w in enumerate(words):\n        plt.annotate(\n            w,\n            (x_scores[i], y_scores[i]),\n            xytext=(5, 5),\n            textcoords=\"offset points\",\n            fontsize=16,\n        )\n\n    plt.axhline(0, color=\"k\", alpha=0.3)\n    plt.axvline(0, color=\"k\", alpha=0.3)\n    plt.xlabel(\"Sentiment (Bad -&gt; Good)\")\n    plt.ylabel(\"Intensity (Weak -&gt; Strong)\")\n    plt.show()\n\n\nintensity_axis = create_axis(\"strong\", \"weak\", model)\ntest_words = [\n    \"excellent\",\n    \"terrible\",\n    \"mediocre\",\n    \"mild\",\n    \"extreme\",\n    \"murder\",\n    \"charity\",\n]\n\nplot_2d(test_words, sentiment_axis, intensity_axis, model)\n\n\n\n\n\n2D Semantic Space",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Semaxis"
    ]
  },
  {
    "objectID": "m04-text/semaxis.html#the-key-insight",
    "href": "m04-text/semaxis.html#the-key-insight",
    "title": "SemAxis: Meaning as Direction",
    "section": "The Key Insight",
    "text": "The Key Insight\nTo define a concept, you must first define its opposite. Meaning isn’t stored in the word itself but lives in the contrast space, where the relationship between poles defines an axis. SemAxis operationalizes this principle, isolating the dimension that matters by defining opposition.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Semaxis"
    ]
  },
  {
    "objectID": "m04-text/llm-intro.html",
    "href": "m04-text/llm-intro.html",
    "title": "Large Language Models in Practice",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module introduces large language models as practical research tools.\nYou’ll learn:\n\nWhat LLMs really are and why they don’t understand language the way we do.\nHow to set up and use Ollama for local model inference with Python.\nPractical workflows for summarization, extraction, and hypothesis generation in research.\nThe failure modes and boundaries of LLMs, and when to trust (or verify) their outputs.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Large Language Models"
    ]
  },
  {
    "objectID": "m04-text/llm-intro.html#do-llms-understand-language",
    "href": "m04-text/llm-intro.html#do-llms-understand-language",
    "title": "Large Language Models in Practice",
    "section": "Do LLMs Understand Language?",
    "text": "Do LLMs Understand Language?\nLet’s talk about the most fundamental question: Can LLMs understand the world and reason about it?\nOne might argue that fluency demonstrates understanding. This is the intuition behind Turing’s 1950 test: if you can’t tell it’s a machine, treat it as intelligent. But let’s examine counter-arguments starting with ELIZA, developed by Joseph Weizenbaum in the mid-1960s as one of the first chatbots. It simulated a Rogerian psychotherapist using simple pattern matching and keyword substitution. Despite its lack of true understanding, ELIZA famously convinced many users that they were conversing with an intelligent entity, highlighting the human tendency to anthropomorphize technology and the limitations of the Turing Test.\n\n\nAnother argument against fluency is the Chinese Room argument, proposed by philosopher John Searle. Imagine a person in a room who receives Chinese characters and, using an English rulebook, manipulates these symbols to produce new Chinese characters. To an outside observer, it appears the room understands Chinese, yet the person inside merely follows instructions to manipulate symbols without understanding their meaning. Searle argues that this is analogous to how computers, including LLMs, operate: they process symbols based on rules without genuine comprehension.\n\n\nSo do LLMs understand the world? Probably not in the same way we do. LLMs are lossy compression algorithms, compressing data into their parameters to generate fluent outputs. To predict “The capital of France is ___,” the model must compress not just the fact (Paris) but the statistical regularities governing how facts appear in text: that capitals follow “The capital of,” that France is a country, that countries have capitals. The model stores P(\\text{word}_{n+1} \\mid \\text{word}_1, \\ldots, \\text{word}_n), which words tend to follow which other words in which contexts, just as a lottery memorizer stores patterns of number sequences.\n\n\n\n\n\nTraining feeds the model billions of sentences. For each sentence, the model predicts the next word, compares its prediction to the actual next word, and adjusts its parameters to increase the probability of the correct word. Repeat trillions of times. The result: a compressed representation of how language behaves statistically. The model doesn’t learn “Paris is the capital of France” as a fact but rather that in contexts matching the pattern [The capital of France is], the token “Paris” appears with high probability.\nThe lottery memorizer doesn’t understand what draws mean but just knows what patterns appear most often. This is why LLMs create hallucinations: fluent but false outputs. Truth and fluency correlate in the training data, so the model is mostly truthful, but in the tails (obscure topics, recent events, precise recall), fluency diverges from truth, and the model follows fluency.\nKeep this limitation in mind and use LLMs as a tool to scale pattern recognition, not judgment. Let’s learn how to utilize them.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Large Language Models"
    ]
  },
  {
    "objectID": "m04-text/llm-intro.html#setting-up-ollama",
    "href": "m04-text/llm-intro.html#setting-up-ollama",
    "title": "Large Language Models in Practice",
    "section": "Setting Up Ollama",
    "text": "Setting Up Ollama\nFor this course, we use Ollama, a tool for running LLMs locally, with Gemma 3N, a 4-billion parameter open-source model. It’s free, private, and capable enough for research tasks.\nVisit ollama.ai, download the installer, and verify installation.\nollama --version\nollama pull gemma3n:latest\nollama run gemma3n:latest \"What is a complex system?\"\nIf you receive a coherent response, install the Python client and send your first prompt.\npip install ollama\n\nimport ollama\n\nparams_llm = {\"model\": \"gemma3n:latest\", \"options\": {\"temperature\": 0.3}}\n\nresponse = ollama.generate(\n    prompt=\"Explain emergence in two sentences.\",\n    **params_llm\n)\n\nprint(response.response)\n\nEmergence is when complex patterns and behaviors arise from simple interactions between individual components in a system. These emergent properties are not predictable from the properties of the individual parts alone, representing a novel level of organization. \n\n\n\nRun this code twice and you’ll get different outputs. Why? Because LLMs sample from probability distributions. The temperature parameter controls this randomness: lower values (0.1) make outputs more deterministic, higher values (1.0) increase diversity. You’re controlling how far into the tail of the probability distribution the model samples. Low temperature means the model picks the most likely next word, while high temperature ventures into less probable territory, sometimes producing creativity, sometimes nonsense.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Large Language Models"
    ]
  },
  {
    "objectID": "m04-text/llm-intro.html#research-applications",
    "href": "m04-text/llm-intro.html#research-applications",
    "title": "Large Language Models in Practice",
    "section": "Research Applications",
    "text": "Research Applications\nThe strategy is simple: use LLMs for tasks where speed trumps precision, then verify the outputs that matter. Three workflows demonstrate this pattern.\n\nAbstract Summarization\nYou collected 50 papers on network science. Which deserve detailed reading?\nYou don’t have time to read all 50 abstracts carefully. An LLM scans them in seconds.\n\nabstract = \"\"\"\nCommunity detection in networks is a fundamental problem in complex systems.\nWhile many algorithms exist, most assume static networks. We propose a dynamic\ncommunity detection algorithm that tracks evolving communities over time using\na temporal smoothness constraint. We evaluate our method on synthetic and real\ntemporal networks, showing it outperforms static methods applied to temporal\nsnapshots. Our approach reveals how communities merge, split, and persist in\nsocial networks, biological systems, and transportation networks.\n\"\"\"\n\nprompt = f\"Summarize this abstract in one sentence:\\n\\n{abstract}\"\nresponse = ollama.generate(prompt=prompt, **params_llm)\nprint(response.response)\n\nThis paper introduces a novel dynamic community detection algorithm that effectively tracks evolving communities in networks over time, outperforming static methods and revealing community dynamics in various real-world systems.\n\n\n\n\n\n\nThe model captures the pattern: propose method, evaluate, outperform baselines. It doesn’t understand the paper but has seen enough academic abstracts to recognize the structure. For multiple abstracts, loop through them.\n\nfor i, abstract in enumerate([\"Abstract 1...\", \"Abstract 2...\"], 1):\n    response = ollama.generate(prompt=f\"Summarize:\\n\\n{abstract}\", **params_llm)\n    print(f\"{i}. {response.response}\")\n\n1. Please provide me with \"Abstract 1\"! I need the text of the abstract to be able to summarize it for you. \n\nJust paste the abstract here, and I'll give you a concise summary. 😊 \n\nI'm ready when you are!\n2. Please provide me with the content of \"Abstract 2\"! I need the text of the abstract to be able to summarize it for you. \n\nJust paste the abstract here, and I'll give you a concise summary. 😊 \n\n\n\n\nLocal models are slow (2–5 seconds per abstract). For thousands of papers, switch to cloud APIs. But the workflow scales: delegate skimming to the model, retain judgment for yourself.\nI ran this on 200 abstracts about power-law distributions. Gemma flagged the 15 that used preferential attachment models. Saved me 4 hours. I still read all 15 myself.\n\n\nStructured Extraction\nTurn unstructured text into structured data automatically.\n\nabstract = \"\"\"\nWe analyze scientific collaboration networks using 5 million papers from\n2000-2020. Using graph neural networks and community detection, we identify\ndisciplinary boundaries and interdisciplinary bridges. Interdisciplinarity\nincreased 25%, with physics and CS showing strongest cross-connections.\n\"\"\"\n\nprompt = f\"\"\"Extract: Domain, Methods, Key Finding\\n\\n{abstract}\\n\\nFormat:\\nDomain:...\\nMethods:...\\nKey Finding:...\"\"\"\nresponse = ollama.generate(prompt=prompt, **params_llm)\nprint(response.response)\n\nHere's the extraction in the requested format:\n\nDomain: Scientific Collaboration Networks\nMethods: Graph Neural Networks, Community Detection, Analysis of 5 million papers (2000-2020)\nKey Finding: Interdisciplinarity increased by 25% between 2000-2020, with the strongest cross-connections observed between Physics and Computer Science.\n\n\n\n\n\n\nScale this to hundreds of papers for meta-analysis, but always verify. LLMs misinterpret obscure terminology and fabricate plausible-sounding technical details when uncertain, pattern-matching against academic writing they’ve seen rather than reasoning about your domain.\n\n\nHypothesis Generation\nLLMs pattern-match against research questions they’ve encountered in training data.\n\ncontext = \"\"\"I study concept spread in citation networks. Highly cited papers\ncombine existing concepts novelty. What should I study next?\"\"\"\n\nprompt = f\"\"\"Suggest three follow-up research questions:\\n\\n{context}\"\"\"\nresponse = ollama.generate(prompt=prompt, **params_llm)\nprint(response.response)\n\nOkay, here are three follow-up research questions, building on your work on concept spread in citation networks, focusing on highly cited papers and the interplay of existing concepts and novelty.  I've tried to offer a mix of methodological and theoretical directions:\n\n**1.  How does the *type* of novelty (e.g., incremental, radical, convergent) in highly cited papers influence the rate and direction of concept spread?**\n\n*   **Rationale:** You've identified that highly cited papers combine existing concepts with novelty.  However, the *nature* of that novelty likely matters.  Is it a small tweak to an existing idea (incremental), a completely new paradigm (radical), or a synthesis of multiple existing ideas (convergent)?  Different types of novelty might spread differently through the citation network.\n*   **Methodology:**  This could involve:\n    *   **Concept Extraction & Categorization:**  Develop a method (potentially using NLP techniques like topic modeling or knowledge graph extraction) to identify and categorize the types of novelty present in highly cited papers.\n    *   **Network Analysis:**  Analyze the citation network to see if papers with different types of novelty have different citation patterns (e.g., different citation paths, different communities of citing papers).\n    *   **Temporal Analysis:** Track the spread of concepts over time, looking for differences in the spread dynamics based on the type of novelty.\n*   **Potential Insights:**  This could reveal whether incremental novelty spreads quickly within a well-established field, while radical novelty requires more time and a different set of initial citations to gain traction.\n\n**2.  To what extent does the *citation context* (i.e., how a highly cited paper is cited) mediate the relationship between novelty and concept spread?**\n\n*   **Rationale:**  It's not just *that* a paper is highly cited, but *how* it's cited that matters.  Is it cited as a foundational work, a contrasting viewpoint, a building block for further research, or something else? The citation context could significantly influence how the novelty is perceived and incorporated by subsequent researchers.\n*   **Methodology:**\n    *   **Citation Context Analysis:**  Develop a method to classify the citation context of highly cited papers (e.g., using NLP to analyze the surrounding text in citations).\n    *   **Network Analysis:**  Analyze the citation network to see if the citation context of a paper is correlated with the subsequent spread of concepts.\n    *   **Sentiment Analysis:**  Use sentiment analysis on the citation text to gauge the attitude towards the novelty being presented.\n*   **Potential Insights:**  This could reveal whether a paper's novelty is more likely to spread if it's cited as a key foundational work, or if it's more likely to be incorporated if it's cited as a contrasting viewpoint that sparks debate.\n\n**3.  Can we identify \"concept amplifiers\" – papers that, due to their specific combination of existing concepts and novelty, act as particularly effective catalysts for concept spread?**\n\n*   **Rationale:**  Not all highly cited papers are created equal in terms of their ability to spread concepts. Some papers might be inherently more influential due to their specific combination of existing knowledge and new ideas.\n*   **Methodology:**\n    *   **Feature Engineering:**  Develop a set of features that capture the combination of existing concepts and novelty in a paper (e.g., the number of distinct concepts introduced, the degree of overlap with existing concepts, the \"surprise\" or unexpectedness of the novelty).\n    *   **Machine Learning:**  Use machine learning techniques (e.g., regression, classification) to identify papers that are strong predictors of concept spread, based on their feature values.\n    *   **Network Analysis:**  Analyze the citation network to see if papers identified as \"concept amplifiers\" have distinct network properties (e.g., high betweenness centrality, strong connections to diverse communities).\n*   **Potential Insights:**  This could lead to a better understanding of the factors that contribute to the influence of scientific papers and potentially inform strategies for promoting impactful research.\n\n\n\nThese questions are designed to be relatively focused and address different aspects of your initial research.  They also offer opportunities to combine quantitative network analysis with qualitative analysis of the content and context of citations.  I hope this helps! Let me know if you'd like me to elaborate on any of these or suggest alternative directions.\n\n\n\n\n\n\nTreat the model as a thought partner, not an oracle. It helps structure thinking but doesn’t possess domain expertise, reflecting patterns in how research questions are framed rather than deep knowledge of your field.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Large Language Models"
    ]
  },
  {
    "objectID": "m04-text/llm-intro.html#failure-modes-and-boundaries",
    "href": "m04-text/llm-intro.html#failure-modes-and-boundaries",
    "title": "Large Language Models in Practice",
    "section": "Failure Modes and Boundaries",
    "text": "Failure Modes and Boundaries\nThe failure modes follow directly from the mechanism. LLMs fabricate plausibly because they optimize for fluency, not truth. Ask about a non-existent “Smith et al. quantum paper” and receive fluent academic prose describing results that never happened. Always verify citations: the model has seen thousands of papers cited in the format “Smith et al. (2023) demonstrated that…” and generates outputs matching that pattern even when the citation is fictional.\nContext limits are architectural. Models see only 2,000–8,000 tokens at once. Paste 100 abstracts and early ones are mathematically evicted from working memory, gone. Knowledge cutoffs are temporal: Gemma 3N’s training ended early 2024, so asking about recent events yields outdated information or plausible fabrications constructed from pre-cutoff patterns.\nReasoning is absent. LLMs pattern-match, they don’t reason. Ask “How many r’s in ‘Strawberry’?” and the model might answer correctly via pattern matching against similar questions in training data, not by counting letters. Sometimes right, often wrong, with no internal representation of what counting means. These aren’t bugs to be fixed but intrinsic to the architecture.\nUse LLMs to accelerate work, not replace judgment. They excel at summarizing text, extracting structure, reformulating concepts, brainstorming, generating synthetic examples, and translation. They fail at literature reviews without verification, factual claims without sources, statistical analysis, and ethical decisions.\nHarvest the center of the distribution where fluency and truth correlate. Defend against the tails where they diverge.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Large Language Models"
    ]
  },
  {
    "objectID": "m04-text/llm-intro.html#what-comes-next",
    "href": "m04-text/llm-intro.html#what-comes-next",
    "title": "Large Language Models in Practice",
    "section": "What Comes Next",
    "text": "What Comes Next\nYou’ve seen LLMs in practice: setup, summarization, extraction, limitations. But how do they actually work?\nWhat happens inside when you send a prompt? The rest of this module unboxes the technology: prompt engineering (communicating with LLMs), embeddings (representing meaning as numbers), transformers (the architecture enabling modern NLP), fundamentals (from word counts to neural representations).\nFirst, let’s master talking to machines.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Large Language Models"
    ]
  },
  {
    "objectID": "m04-text/bert-gpt.html",
    "href": "m04-text/bert-gpt.html",
    "title": "BERT, GPT, and Sentence Transformers",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nBERT and GPT are not variants of one architecture. They represent fundamentally different information flows.\nYou’ll learn:\n\nWhat makes BERT and GPT fundamentally different architectures, and why bidirectional vs. causal attention determines what each model can learn.\nHow BERT’s encoder stack sees everything at once for deep understanding, while GPT’s decoder stack processes text sequentially for generation.\nThe training objectives that shape these models: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) for BERT, versus Causal Language Modeling (CLM) for GPT.\nHow to visualize attention patterns to understand what BERT learns about word relationships.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "BERT & GPT"
    ]
  },
  {
    "objectID": "m04-text/bert-gpt.html#two-siblings-bert-and-gpt",
    "href": "m04-text/bert-gpt.html#two-siblings-bert-and-gpt",
    "title": "BERT, GPT, and Sentence Transformers",
    "section": "Two Siblings, BERT and GPT",
    "text": "Two Siblings, BERT and GPT\n\nWe instinctively think of “Transformers” as a single unified model, but this is wrong. The original Transformer paper proposed an Encoder-Decoder architecture, a two-part machine. Modern models split this architecture in half, creating two distinct lineages with fundamentally different information flows.\nBERT (Bidirectional Encoder Representations from Transformers) uses the encoder stack and sees everything at once, like reading a completed sentence. GPT (Generative Pre-trained Transformer) uses the decoder stack and processes text causally, like improvising a story where you can only react to what’s already been said. This architectural choice determines what the model can learn and what tasks it excels at.\nThink of it like two different reading strategies: BERT is the student who reads the entire paragraph, then goes back to understand each word in context, while GPT is the actor performing a cold read, processing each line sequentially without peeking ahead at the script. The first strategy gives deeper understanding, the second gives the ability to continue the story.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "BERT & GPT"
    ]
  },
  {
    "objectID": "m04-text/bert-gpt.html#architecture",
    "href": "m04-text/bert-gpt.html#architecture",
    "title": "BERT, GPT, and Sentence Transformers",
    "section": "Architecture",
    "text": "Architecture\n\nWhat’s the most important difference? The attention mechanism. BERT uses bidirectional attention, meaning every token at position t can attend to every other token. This allows BERT to understand the context of a word by looking at all the words in the sentence, capturing full context.\nGPT uses masked (or causal) attention, meaning a token at position t can only attend to previous tokens. This masking imposes a causal constraint, making it ideal for language generation where the model must predict future tokens based only on past context. Although less globally context-aware than BERT, this causal processing allows GPT to generate remarkably fluent and coherent text sequentially.\n\nMore on BERT\nLet’s talk about BERT’s special tokens. The [CLS] token represents the start of the sentence, [SEP] marks sentence boundaries, [MASK] represents masked words, and [UNK] represents unknown words. For example, the sentence “The cat sat on the mat. It then went to sleep.” becomes “[CLS] The cat sat on the mat [SEP] It then went to sleep [SEP]”.\nWhy does the [CLS] token matter? BERT learns to encode a summary of the input into the [CLS] token, making it particularly useful when we want the embedding of the whole input text rather than token-level embeddings.\nBERT uses position and segment embeddings to provide contextual information. Unlike the sinusoidal position embedding used in the original transformer paper, BERT uses learnable position embeddings. Segment embeddings distinguish sentences in the input: tokens in the first sentence receive segment embedding 0, tokens in the second receive segment embedding 1. Both types of embeddings are learned during pre-training.\nSeveral BERT variants have been developed. RoBERTa (Robustly Optimized BERT Approach) improved upon BERT by removing Next Sentence Prediction, using dynamic masking, training with larger batches and datasets, leading to significant performance improvements. DistilBERT focused on efficiency through knowledge distillation, achieving 95% of BERT’s performance while being 40% smaller and 60% faster. ALBERT introduced parameter reduction techniques using factorized embedding parameterization and cross-layer parameter sharing while replacing NSP with Sentence Order Prediction.\nDomain-specific BERT models have been trained on specialized corpora for specific fields (BioBERT for biomedical text, SciBERT for scientific papers, FinBERT for financial documents), demonstrating superior performance in their respective domains. Multilingual BERT (mBERT) was trained on Wikipedia data from 104 languages and shows remarkable zero-shot cross-lingual transfer abilities despite lacking explicit cross-lingual objectives, making it valuable for low-resource languages.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "BERT & GPT"
    ]
  },
  {
    "objectID": "m04-text/bert-gpt.html#training",
    "href": "m04-text/bert-gpt.html#training",
    "title": "BERT, GPT, and Sentence Transformers",
    "section": "Training",
    "text": "Training\n\n\n\n\n\n\nBERT\nHow do you train models with such different architectures? BERT, with its encoder-only design, is trained using two primary unsupervised tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP).\n\n\n\n\n\nIn MLM, a percentage of input tokens are randomly masked, and the model predicts the original tokens based on full bidirectional context. For example, given “The quick brown fox jumps over the lazy dog,” BERT might see “The quick brown [MASK] jumps over the lazy dog” and predict “fox.”\n\nNSP involves presenting the model with two sentences and asking it to predict whether the second sentence logically follows the first. Given “The cat sat on the mat.” followed by “It was a sunny day.”, BERT would predict ‘IsNextSentence = No’, but for “It was purring softly.”, BERT would predict ‘IsNextSentence = Yes’. These tasks enable BERT to learn deep contextual representations for understanding text.\n\n\nRecipe for MLM\nTo generate training data for MLM, BERT randomly masks 15% of the tokens in each sequence. The masking strategy uses three approaches: 80% of the time, replace the word with [MASK] (“the cat sat on the mat” becomes “the cat [MASK] on the mat”), 10% of the time, replace with a random word (“the cat dog on the mat”), and 10% of the time, keep unchanged (“the cat sat on the mat”).\nWhy this strange mix? The model must predict the original token for all selected positions regardless of how they were modified, preventing the model from simply learning to detect replaced tokens. During training, the model processes the modified sequence through its transformer layers and predicts the original token at each masked position using contextual representations. This approach, while counterintuitive, has proven effective and become essential to BERT’s pre-training.\n\n\nRecipe for NSP\nNext Sentence Prediction (NSP) trains BERT to understand relationships between sentences by predicting whether two sentences naturally follow each other. During training, half of the pairs are consecutive sentences from documents (labeled IsNext), the other half are random pairs (labeled NotNext).\nThe input format uses special tokens: [CLS] at the start, the first sentence, [SEP], the second sentence, and a final [SEP]. For instance:\n\n\\text{``[CLS] }\\underbrace{\\text{I went to the store}}_{\\text{Sentence 1}}\\text{ [SEP] }\\underbrace{\\text{They were out of milk}}_{\\text{Sentence 2}}\\text{ [SEP]''}.\n\nBERT uses the final hidden state of the [CLS] token to classify whether the sentences are consecutive, helping the model develop broader understanding of language context and relationships.\n\n\nGPT\nWhat about GPT? GPT uses its decoder-only architecture for Causal Language Modeling (CLM), where the model learns to predict the next token given all previous tokens. More formally, given a sequence (x_1, x_2, ..., x_n), the model maximizes:\n\nP(x_1, ..., x_n) = \\prod_{i=1}^n P(x_i|x_1, ..., x_{i-1})\n\nFor example, given “The cat sat on”, the model calculates probability distributions over its vocabulary: “mat” might have high probability while “laptop” has lower probability. This autoregressive nature means GPT processes text left to right, learning to generate coherent and grammatically correct continuations that directly align with its strength in text generation.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "BERT & GPT"
    ]
  },
  {
    "objectID": "m04-text/bert-gpt.html#visualizing-attention-what-is-bert-looking-at",
    "href": "m04-text/bert-gpt.html#visualizing-attention-what-is-bert-looking-at",
    "title": "BERT, GPT, and Sentence Transformers",
    "section": "Visualizing Attention: What Is BERT Looking At?",
    "text": "Visualizing Attention: What Is BERT Looking At?\nBERT produces attention weights, a matrix showing which tokens influence each other. We can extract these weights and visualize them to understand how the model disambiguates meaning.\n\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load a small BERT model\nmodel_name = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name, output_attentions=True)\n\ntext = \"The bank of the river.\"\ninputs = tokenizer(text, return_tensors=\"pt\")\noutputs = model(**inputs)\n\n# Get attention from the last layer\n# Shape: (batch, heads, seq_len, seq_len)\nattention = outputs.attentions[-1].squeeze(0)\n\n# Average attention across all heads for simplicity\nmean_attention = attention.mean(dim=0).detach().numpy()\ntokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n\n# Plot\nplt.figure(figsize=(8, 6))\nsns.heatmap(mean_attention, xticklabels=tokens, yticklabels=tokens, cmap=\"viridis\")\nplt.title(\"BERT Attention Map (Last Layer)\")\nplt.xlabel(\"Key (Source)\")\nplt.ylabel(\"Query (Target)\")\nplt.show()\n\nIn this heatmap, a bright spot at row “bank” and column “river” reveals that BERT is using “river” to understand “bank”, disambiguating it from a financial institution. This bidirectional flow is why BERT excels at tasks requiring deep contextual understanding like question answering and named entity recognition.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "BERT & GPT"
    ]
  },
  {
    "objectID": "m04-text/bert-gpt.html#the-takeaway",
    "href": "m04-text/bert-gpt.html#the-takeaway",
    "title": "BERT, GPT, and Sentence Transformers",
    "section": "The Takeaway",
    "text": "The Takeaway\nBERT reads to understand. GPT writes to create. Choose the architecture that matches your information flow: bidirectional for deep contextual analysis, causal for sequential generation.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "BERT & GPT"
    ]
  },
  {
    "objectID": "m03-agentic-coding/overview.html",
    "href": "m03-agentic-coding/overview.html",
    "title": "Module 3: Agentic Coding",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module transforms you from a coder writing syntax to a manager orchestrating intelligence.\nYou’ll learn:\n\nThe fundamental shift from copilots (next-token predictors) to agents (autonomous task completers) and why this changes your role from writer to architect.\nHow to communicate effectively with LLMs through structured prompt engineering using instruction, data, format, persona, and context.\nThe ReAct loop (Reason + Act) that transforms a passive language model into an autonomous agent capable of task completion.\nContext engineering techniques to manage the LLM’s working memory across its lifecycle through scratchpads, retrieval, summarization, and multi-agent architectures.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Overview"
    ]
  },
  {
    "objectID": "m03-agentic-coding/overview.html#the-journey",
    "href": "m03-agentic-coding/overview.html#the-journey",
    "title": "Module 3: Agentic Coding",
    "section": "The Journey",
    "text": "The Journey\nLet’s talk about where this module takes you. We start with hands-on experience building with agents, then unpack the three operational layers that power agentic AI: the interface, the engine, and the operating system.\nHands-on: Building with Agents is practice-first. You’ll use Google Antigravity to build a functional game and refactor a codebase entirely through natural language instructions. This experience grounds the theory that follows.\nPrompt Tuning: The Interface Layer teaches you how to communicate effectively with LLMs by understanding them as stateless pattern matchers sampling from probability distributions. You’ll structure prompts using instruction, data, format, persona, and context to reliably activate desired patterns.\nAgentic AI: The Engine Layer explains the ReAct loop (Reason + Act) that transforms a passive language model into an autonomous agent. You’ll build a working agent using LangGraph that can query and analyze datasets without human intervention.\nContext Engineering: The Operating System Layer solves the context window problem. LLMs are brilliant but bounded with limited working memory that degrades as it fills. You’ll learn to manage context across its lifecycle: write through scratchpads and memories, select using MCP and just-in-time retrieval, compress through summarization, and isolate using multi-agent architectures.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Overview"
    ]
  },
  {
    "objectID": "m03-agentic-coding/overview.html#why-this-matters",
    "href": "m03-agentic-coding/overview.html#why-this-matters",
    "title": "Module 3: Agentic Coding",
    "section": "Why This Matters",
    "text": "Why This Matters\nHave you ever wished your coding assistant could actually finish the task instead of just suggesting the next line? The shift from copilot to agent is not just an upgrade in model size, it is a fundamental change in how you interact with AI. This shift changes your role from writer of syntax to manager-architect. Your job is no longer to know the exact syntax of a matplotlib plot. Instead, you know what plot you need, how to clearly specify that requirement, and how to verify that the agent built it correctly.\nThis transformation matters for productivity, but also for how you think about software development. Understanding agentic systems means understanding feedback loops, context management, and task decomposition. These concepts extend far beyond coding assistants to any system that makes decisions autonomously.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Overview"
    ]
  },
  {
    "objectID": "m03-agentic-coding/overview.html#prerequisites",
    "href": "m03-agentic-coding/overview.html#prerequisites",
    "title": "Module 3: Agentic Coding",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou should be comfortable with basic Python programming and familiar with API usage. Prior exposure to language models helps but isn’t required (we’ll introduce LLM concepts as needed). If you completed Module 4 on text and transformers, you’ll have deeper insight into what’s happening under the hood, but this module stands alone.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Overview"
    ]
  },
  {
    "objectID": "m03-agentic-coding/overview.html#what-youll-build",
    "href": "m03-agentic-coding/overview.html#what-youll-build",
    "title": "Module 3: Agentic Coding",
    "section": "What You’ll Build",
    "text": "What You’ll Build\nBy the end of this module, you’ll build a working agent using LangGraph, design effective prompts that reliably produce desired outputs, and implement context management strategies like scratchpads and retrieval systems. You’ll gain practical skills in crafting clear instructions, structuring agent workflows, debugging when agents fail, and managing computational costs. Most importantly, you’ll develop the mindset shift from “How do I code this?” to “How do I specify this clearly enough for an agent to execute?”\nLet’s begin by getting your hands dirty with agentic systems.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Overview"
    ]
  },
  {
    "objectID": "m03-agentic-coding/context-engineering.html",
    "href": "m03-agentic-coding/context-engineering.html",
    "title": "Context Engineering",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module introduces context engineering, the discipline of managing an LLM’s working memory.\nYou’ll learn:\n\nWhat context decay means and how it degrades model performance as token count grows.\nThe four strategies: write (scratchpads, rules files), select (progressive disclosure, MCP), compress (summarization, trimming), and isolate (multi-agent architectures).\nHow to set up Model Context Protocol (MCP) servers like Context7 for on-demand documentation retrieval.\nPractical techniques to prevent context rot and optimize your agent’s attention budget.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Context Engineering"
    ]
  },
  {
    "objectID": "m03-agentic-coding/context-engineering.html#the-problem-context-decay",
    "href": "m03-agentic-coding/context-engineering.html#the-problem-context-decay",
    "title": "Context Engineering",
    "section": "The Problem: Context Decay",
    "text": "The Problem: Context Decay\nLLMs are brilliant but bounded. Every model has a context window, the set of tokens available during inference. Think of it as working memory.\nAn LLM has an attention budget that degrades as context grows. As token count increases, the model’s ability to accurately use that context degrades. This phenomenon is called context rot.\n\n\n\n\n\n\nContext rot degrades attention as token count increases\n\n\n\n\nFigure 1\n\n\n\nWhy does this happen? The root cause is architectural. LLMs use the transformer architecture where every token attends to every other token. For n tokens, this creates n^2 pairwise relationships.\nAs context length increases, attention gets stretched thin across these relationships. Models are trained predominantly on shorter sequences, so they have less specialized capacity for long-range dependencies. Position encoding tricks help, but performance still decays.\nThis decay manifests in four failure modes. Context poisoning occurs when a hallucination or error enters the context and influences future outputs. Context distraction happens when the volume of context overwhelms the model’s training distribution, causing it to lose focus.\nContext confusion arises when superfluous information nudges the model toward irrelevant responses. Context clash occurs when parts of the context contradict each other, forcing the model to arbitrate between conflicting signals.\nWhat’s the solution? The naive view treats context engineering as simply writing a better prompt. The reality is much broader.\nContext engineering is the discipline of managing the entire context lifecycle: what tokens go in, what stays, what gets compressed, and what gets isolated elsewhere. Think of it this way: the LLM is like a CPU, the context window is like RAM, and context engineering is the operating system that curates what fits.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Context Engineering"
    ]
  },
  {
    "objectID": "m03-agentic-coding/context-engineering.html#the-four-strategies",
    "href": "m03-agentic-coding/context-engineering.html#the-four-strategies",
    "title": "Context Engineering",
    "section": "The Four Strategies",
    "text": "The Four Strategies\nContext engineering breaks into four strategies: write, select, compress, and isolate. Each addresses a different phase of the context lifecycle. Let’s walk through them in modern agentic AI tools.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Context Engineering"
    ]
  },
  {
    "objectID": "m03-agentic-coding/context-engineering.html#writing-context",
    "href": "m03-agentic-coding/context-engineering.html#writing-context",
    "title": "Context Engineering",
    "section": "Writing Context",
    "text": "Writing Context\nLet’s talk about the first strategy: writing. Agents use scratchpads to offload working memory. Instead of keeping every intermediate step in the context window, an agent writes state like a todo list or summary to an external file or variable.\nTools like Claude Code, Gemini CLI, Antigravity, and Cursor use this pattern to track multi-step tasks across context resets. They maintain coherence without bloating the prompt.\nWhat about long-term memory? Agents rely on persistent rules files like AGENTS.md, CLAUDE.md, or .cursorrules. These act as procedural memory, storing project-specific instructions that are injected into the context at the start of a session or retrieved when relevant.\nHere is a sample AGENTS.md file:\n## Project: `AgenticFlow` - Context Engineering Demo\n\n###  Project Goal\nDemonstrate advanced context engineering for LLM agents: writing, selecting, compressing, isolating context to optimize performance and ensure robust multi-step workflows.\n\n### Agent Persona & Principles\n-   **Role**: Senior AI Engineer/Architect.\n-   **Objective**: Develop efficient, reliable, maintainable agentic workflows.\n-   **Style**: Clear, concise, technical, solution-oriented. Justify decisions.\n-   **Principles**: Context Efficiency, Modularity, Transparency, Robustness, Iteration.\n\n### Technical Guidelines\n-   **Language**: Python 3.9+.\n-   **Libraries**: `pydantic`, `pytest`, `black`. `langchain`/`llamaindex` for orchestration (use sparingly).\n-   **Dev Practices**: Git (conventional commits), Markdown/docstring documentation, comprehensive error handling, explicit tool use.\n\n...(continue)\n\nThe AGENTS.md file is an industry standard for agentic AI workflows. It serves as the agent’s procedural memory, storing project-specific instructions, guidelines, and foundational context. This content is automatically injected into the context window at the start of a session or retrieved on demand.\nThe result is consistent behavior, reduced redundant prompting, and coherence across complex, multi-step workflows. Create an AGENTS.md when you start any new project.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Context Engineering"
    ]
  },
  {
    "objectID": "m03-agentic-coding/context-engineering.html#selecting-context",
    "href": "m03-agentic-coding/context-engineering.html#selecting-context",
    "title": "Context Engineering",
    "section": "Selecting Context",
    "text": "Selecting Context\n\n\n\n\n\n\nModel Context Protocol enables on-demand data retrieval\n\n\n\n\nFigure 2\n\n\n\nNow shift your attention to the second strategy: selecting. Selecting context means pulling it into the context window at runtime. The key insight is progressive disclosure: the agent does not need all the data upfront.\nIt explores incrementally, using lightweight identifiers like file paths, URLs, or database queries to fetch data only when needed.\nImagine you want to update a file A.py that depends on B.py. A bad approach loads the entire content of B.py into the context window.\nWhat’s a better approach? Give the agent the file path and let it fetch content on demand.\nThe Model Context Protocol (MCP) is the standard mechanism for this. Instead of copy-pasting data into the prompt, MCP gives the agent tools to pull data on demand.\nContext7 is a good example. It is an MCP server that fetches documentation for a library to ground agents on the latest features. An agent trained in 2024 may not know about 2025 features, but context7 solves this.\nIt offers two tools: resolve-library-id and get-library-docs. These tool names are injected into the context window. When the agent calls them, it retrieves documentation on demand without polluting the context window, allowing the agent to focus on the task.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Context Engineering"
    ]
  },
  {
    "objectID": "m03-agentic-coding/context-engineering.html#installing-context7-in-google-antigravity",
    "href": "m03-agentic-coding/context-engineering.html#installing-context7-in-google-antigravity",
    "title": "Context Engineering",
    "section": "Installing Context7 in Google Antigravity",
    "text": "Installing Context7 in Google Antigravity\nGoogle Antigravity connects to external MCP servers through a configuration file. The server exposes tools that the agent calls on demand. Let’s wire Context7 into Antigravity so the agent can fetch up-to-date library docs without polluting the context window.\nHow do you set this up? First, open Google Antigravity and navigate to the MCP Store. Click Manage MCP Servers at the top, then click View raw config in the main tab. This opens mcp_config.json, which controls all external tools available to your agent.\n\n\n\n\n\n\n\n\n\n\n\n(a) Digital Garden MCP interface\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Managing MCP servers in Antigravity\n\n\n\n\n\n\n\nFigure 3: Setting up Context7 in Google Antigravity\n\n\n\nAdd this block to your mcp_config.json:\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\nThe API key is optional but recommended for higher rate limits. Get one at context7.com/dashboard. Without it, you have access to the free tier.\nSave the config and refresh the MCP servers panel in Antigravity. What should you see? Two new tools should appear: resolve-library-id to map a library name to its identifier, and get-library-docs to fetch documentation for a resolved library ID.\nNow prompt the agent with:\nUse context7 to get the latest documentation for pandas 2.0 DataFrame.plot() method.\nWhat happens behind the scenes? The agent calls resolve-library-id with “pandas” to get the library ID. Then it calls get-library-docs with that ID and your query to retrieve current API docs.\nFinally, it uses those docs to generate accurate code. The documentation never enters your prompt. The agent retrieves it on demand, uses it, and discards it. Your context window remains clean.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Context Engineering"
    ]
  },
  {
    "objectID": "m03-agentic-coding/context-engineering.html#compressing-context",
    "href": "m03-agentic-coding/context-engineering.html#compressing-context",
    "title": "Context Engineering",
    "section": "Compressing Context",
    "text": "Compressing Context\nThe third strategy is compressing. Long-running tasks generate more context than the window can hold. When you approach the limit, you have two choices: summarize or trim.\nCompaction through summarization distills a conversation into its essential elements. Claude Code does this automatically. When you exceed 95% of the context window, it triggers auto-compact: the message history is passed to the model to summarize architectural decisions, unresolved bugs, and implementation details while discarding redundant tool outputs.\nThe agent continues with the compressed context plus the five most recently accessed files.\nWhat’s the art of compaction? Deciding what to keep versus discard. Overly aggressive compaction loses subtle details whose importance only becomes apparent later. Start by maximizing recall, capturing everything relevant, then iterate to improve precision by eliminating fluff.\nA lighter form of compaction is tool result clearing. Once a tool has been called and its result used, the raw output can be removed from the message history. The decision or action taken from that result matters. The ten thousand tokens of JSON it returned does not.\nTrimming is a simpler strategy. It uses heuristics to prune context without LLM involvement. Remove messages older than N turns, or keep only the system prompt and the last K user-agent exchanges.\nThis approach requires no model computation but loses more information than summarization.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Context Engineering"
    ]
  },
  {
    "objectID": "m03-agentic-coding/context-engineering.html#isolating-context",
    "href": "m03-agentic-coding/context-engineering.html#isolating-context",
    "title": "Context Engineering",
    "section": "Isolating Context",
    "text": "Isolating Context\nThe fourth strategy is isolating. Isolation means splitting context across boundaries so the model doesn’t drown in a single monolithic window.\nThe most common pattern is multi-agent architectures. Instead of one agent maintaining state across an entire project, specialized sub-agents handle focused sub-tasks with clean context windows.\nAnthropic’s multi-agent researcher demonstrates this well. A lead agent coordinates with a high-level plan and spawns sub-agents that explore different aspects of a question in parallel, each with its own context window. A sub-agent might use ten thousand or more tokens to explore a research thread, but it returns only a one thousand to two thousand token summary to the lead agent.\nThe detailed search context remains isolated. The lead agent synthesizes the compressed results without ever seeing the full exploration.\nThis approach achieves separation of concerns. Each sub-agent has a narrow scope, reducing context confusion and clash.\nWhat’s the cost? Coordination complexity. Spawning agents, managing handoffs, and aggregating results all add overhead. Anthropic reports that multi-agent systems can use up to fifteen times more tokens than single-agent systems. The performance gain on complex tasks justifies it.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Context Engineering"
    ]
  },
  {
    "objectID": "m03-agentic-coding/context-engineering.html#the-takeaway",
    "href": "m03-agentic-coding/context-engineering.html#the-takeaway",
    "title": "Context Engineering",
    "section": "The Takeaway",
    "text": "The Takeaway\nContext is a finite resource. The bottleneck in agentic systems is rarely the model’s reasoning. It is the poverty or pollution of its inputs.\nContext engineering is the discipline of managing this resource across its lifecycle. Write what you need to remember. Select what you need now. Compress what you need later. Isolate what you don’t need yet.\nThe most powerful agent is not the one with the highest IQ in parameters. It is the one with the most disciplined context management.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Context Engineering"
    ]
  },
  {
    "objectID": "m02-visualization/time-series.html",
    "href": "m02-visualization/time-series.html",
    "title": "Time Series Visualization",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module explores how to visualize time series data effectively, moving beyond simple line charts to reveal true underlying patterns.\nYou’ll learn:\n\nHow line plots create continuity illusions and when to use discrete representations instead.\nWhy small multiples solve the spaghetti problem when comparing multiple time series.\nThe crucial difference between linear and log scales and when each tells the honest story.\nHow moving averages balance noise and trend to reveal underlying patterns.\nTechniques for showing uncertainty through ribbon plots and prediction intervals.\nHow heatmaps and cycle plots expose temporal rhythms like daily patterns and seasonality.\nWhat lag plots reveal about autocorrelation and the memory of time series data.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Time-Series"
    ]
  },
  {
    "objectID": "m02-visualization/time-series.html#the-nature-of-time",
    "href": "m02-visualization/time-series.html#the-nature-of-time",
    "title": "Time Series Visualization",
    "section": "The nature of time",
    "text": "The nature of time\nLet’s talk about time. In March 2020, charts of COVID-19 cases told vastly different stories depending on how they were visualized. Some used linear scales, showing a terrifying vertical wall. Others used log scales, showing a straight line.\nWhy did the same data produce such different narratives? Time series data is special because it implies causality and momentum. Unlike other variables, time flows in one direction. Your choices of scale, aggregation, and geometry determine whether you reveal a genuine pattern or manufacture a misleading narrative.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Time-Series"
    ]
  },
  {
    "objectID": "m02-visualization/time-series.html#line-plots-and-the-continuity-illusion",
    "href": "m02-visualization/time-series.html#line-plots-and-the-continuity-illusion",
    "title": "Time Series Visualization",
    "section": "Line plots and the continuity illusion",
    "text": "Line plots and the continuity illusion\nThe most fundamental choice is whether to connect the dots. A line plot suggests continuity, implying that a value exists at every moment between your measurements. This works for temperature or stock prices, where the variable has momentum.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set style\nsns.set_style(\"white\")\nsns.set(font_scale=1.2)\n\n# Generate synthetic time series with trend and seasonality\nnp.random.seed(42)\nn_points = 365\ndates = pd.date_range('2023-01-01', periods=n_points, freq='D')\ntrend = np.linspace(100, 150, n_points)\nseasonal = 10 * np.sin(2 * np.pi * np.arange(n_points) / 365 * 4)  # Quarterly seasonality\nnoise = np.random.normal(0, 3, n_points)\nvalues = trend + seasonal + noise\n\ndf = pd.DataFrame({'date': dates, 'value': values})\n\n# Create line plot\nfig, ax = plt.subplots(figsize=(12, 5))\nax.plot(df['date'], df['value'], linewidth=1.5, color=sns.color_palette()[0])\nax.set_xlabel('Date')\nax.set_ylabel('Value')\nax.set_title('Daily Time Series: Line Plot Shows Trend and Seasonality')\nax.grid(True, alpha=0.3)\nsns.despine()\n\n\n\n\n\nBasic line plot showing a time series with trend and seasonality\n\n\n\n\nBut what if your data is discrete? If you plot distinct sales events or email arrivals as a line, you create a false narrative of values existing in the gaps. In those cases, let the silence between points speak.\n\n\nCode\n# Generate sparse discrete event data\nnp.random.seed(123)\nevent_dates = pd.to_datetime(['2023-01-15', '2023-03-10', '2023-05-22',\n                               '2023-07-08', '2023-09-30', '2023-11-15'])\nevent_values = np.random.randint(20, 80, len(event_dates))\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Line plot (implies continuity - misleading for discrete events)\naxes[0].plot(event_dates, event_values, marker='o', linewidth=2, markersize=8)\naxes[0].set_xlabel('Date')\naxes[0].set_ylabel('Event Count')\naxes[0].set_title('Line Plot: Implies Values Between Events (Misleading)')\naxes[0].grid(True, alpha=0.3)\n\n# Scatter plot (appropriate for discrete events)\naxes[1].scatter(event_dates, event_values, s=100, alpha=0.7)\naxes[1].set_xlabel('Date')\naxes[1].set_ylabel('Event Count')\naxes[1].set_title('Scatter Plot: Shows Only Observed Events (Honest)')\naxes[1].grid(True, alpha=0.3)\n\nfor ax in axes:\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n\n\n\n\n\nLine plot vs scatter plot: connecting points implies continuity",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Time-Series"
    ]
  },
  {
    "objectID": "m02-visualization/time-series.html#comparing-series-the-spaghetti-problem",
    "href": "m02-visualization/time-series.html#comparing-series-the-spaghetti-problem",
    "title": "Time Series Visualization",
    "section": "Comparing series: The spaghetti problem",
    "text": "Comparing series: The spaghetti problem\nOften you need to compare multiple series. The natural instinct is to overlay them on the same plot. This works for two or three variables.\nBut what happens when the count rises? You fall into the spaghetti trap where individual trends get lost in the tangle.\n\n\nCode\n# Generate three related time series\nnp.random.seed(42)\ndates = pd.date_range('2023-01-01', periods=200, freq='D')\n\nseries_a = 100 + np.linspace(0, 30, 200) + np.random.normal(0, 5, 200)\nseries_b = 95 + np.linspace(0, 20, 200) + np.random.normal(0, 4, 200)\nseries_c = 110 + np.linspace(0, 10, 200) + np.random.normal(0, 6, 200)\n\ndf_multi = pd.DataFrame({\n    'date': dates,\n    'Product A': series_a,\n    'Product B': series_b,\n    'Product C': series_c\n})\n\n# Overlay plot\nfig, ax = plt.subplots(figsize=(12, 6))\nfor column in ['Product A', 'Product B', 'Product C']:\n    ax.plot(df_multi['date'], df_multi[column], linewidth=2, label=column, alpha=0.8)\n\nax.set_xlabel('Date')\nax.set_ylabel('Sales')\nax.set_title('Multiple Time Series: Overlaid Comparison')\nax.legend(loc='upper left')\nax.grid(True, alpha=0.3)\nsns.despine()\n\n\n\n\n\nMultiple time series overlaid with different colors\n\n\n\n\nThe solution is small multiples (or faceting). By giving each series its own stage while locking the axes, you preserve both the individual trends and the ability to compare them.\n\n\nCode\n# Generate multiple time series\nnp.random.seed(42)\nn_series = 6\ndates = pd.date_range('2023-01-01', periods=150, freq='D')\n\ndata_list = []\nfor i in range(n_series):\n    values = 50 + np.random.randn(150).cumsum() + 10 * np.sin(2 * np.pi * np.arange(150) / 30)\n    data_list.append(pd.DataFrame({\n        'date': dates,\n        'value': values,\n        'series': f'Region {i+1}'\n    }))\n\ndf_many = pd.concat(data_list, ignore_index=True)\n\n# Small multiples using seaborn FacetGrid\ng = sns.FacetGrid(df_many, col='series', col_wrap=3, height=3, aspect=1.5, sharey=True)\ng.map_dataframe(sns.lineplot, x='date', y='value', linewidth=2, color=sns.color_palette()[0])\ng.set_axis_labels('Date', 'Value')\ng.set_titles('Region {col_name}')\nfor ax in g.axes.flat:\n    ax.grid(True, alpha=0.3)\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n\n\n\n\n\nSmall multiples avoid spaghetti plots when comparing many time series",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Time-Series"
    ]
  },
  {
    "objectID": "m02-visualization/time-series.html#the-power-of-scale-linear-vs-log",
    "href": "m02-visualization/time-series.html#the-power-of-scale-linear-vs-log",
    "title": "Time Series Visualization",
    "section": "The power of scale: Linear vs Log",
    "text": "The power of scale: Linear vs Log\nPerhaps the most consequential choice in time series visualization is the y-axis scale. Your choice defines the question you are answering.\nA linear scale asks “How much did it increase?” A log scale asks “How fast is it growing?” In the example below, the linear scale suggests an explosive crisis at the end. The log scale reveals that the growth rate has been constant the entire time.\n\n\nCode\n# Generate exponential growth data (e.g., epidemic spread)\nnp.random.seed(42)\ndays = np.arange(0, 100)\ncases = 10 * np.exp(0.05 * days) * (1 + np.random.normal(0, 0.1, len(days)))\n\ndf_exp = pd.DataFrame({'day': days, 'cases': cases})\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Linear scale\naxes[0].plot(df_exp['day'], df_exp['cases'], linewidth=2, color=sns.color_palette()[0])\naxes[0].set_xlabel('Days')\naxes[0].set_ylabel('Cases')\naxes[0].set_title('Linear Scale: Exponential Growth Looks Explosive')\naxes[0].grid(True, alpha=0.3)\n\n# Log scale\naxes[1].plot(df_exp['day'], df_exp['cases'], linewidth=2, color=sns.color_palette()[1])\naxes[1].set_xlabel('Days')\naxes[1].set_ylabel('Cases (log scale)')\naxes[1].set_yscale('log')\naxes[1].set_title('Log Scale: Exponential Growth Appears Linear')\naxes[1].grid(True, alpha=0.3, which='both')\n\nfor ax in axes:\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n\n\n\n\n\nThe same exponential growth looks different on linear vs. log scales\n\n\n\n\n\n\n\n\n\n\nWhen to use log scales\n\n\n\nLog scales are essential for data spanning orders of magnitude or when percentage changes matter more than absolute units. However, they can downplay absolute magnitude.\nA jump from 100 to 1,000 looks the same as 10,000 to 100,000. This equality can be honest or deceptive depending on your question.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Time-Series"
    ]
  },
  {
    "objectID": "m02-visualization/time-series.html#smoothing-and-trends",
    "href": "m02-visualization/time-series.html#smoothing-and-trends",
    "title": "Time Series Visualization",
    "section": "Smoothing and trends",
    "text": "Smoothing and trends\nReal data is messy. Smoothing via moving averages mimics how we squint at a chart to blur out the details and see the trend.\nThe window size controls the trade-off. A small window keeps the texture, showing volatility. A large window reveals the structure, showing the trend.\n\n\nCode\n# Generate noisy time series\nnp.random.seed(42)\ndates = pd.date_range('2023-01-01', periods=200, freq='D')\ntrend = 50 + 0.2 * np.arange(200)\nseasonal = 8 * np.sin(2 * np.pi * np.arange(200) / 30)\nnoise = np.random.normal(0, 5, 200)\nvalues = trend + seasonal + noise\n\ndf_noisy = pd.DataFrame({'date': dates, 'value': values})\n\n# Calculate moving averages\ndf_noisy['MA_7'] = df_noisy['value'].rolling(window=7, center=True).mean()\ndf_noisy['MA_30'] = df_noisy['value'].rolling(window=30, center=True).mean()\n\n# Plot\nfig, ax = plt.subplots(figsize=(12, 6))\nax.plot(df_noisy['date'], df_noisy['value'], linewidth=0.8, alpha=0.3, label='Raw Data', color='gray')\nax.plot(df_noisy['date'], df_noisy['MA_7'], linewidth=2, label='7-Day Moving Average', color=sns.color_palette()[0])\nax.plot(df_noisy['date'], df_noisy['MA_30'], linewidth=2, label='30-Day Moving Average', color=sns.color_palette()[1])\n\nax.set_xlabel('Date')\nax.set_ylabel('Value')\nax.set_title('Moving Averages Reveal Trends by Smoothing Noise')\nax.legend()\nax.grid(True, alpha=0.3)\nsns.despine()\n\n\n\n\n\nMoving averages smooth noise to reveal underlying trends",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Time-Series"
    ]
  },
  {
    "objectID": "m02-visualization/time-series.html#showing-uncertainty",
    "href": "m02-visualization/time-series.html#showing-uncertainty",
    "title": "Time Series Visualization",
    "section": "Showing uncertainty",
    "text": "Showing uncertainty\nPredicting the future is an exercise in humility. A forecast without an error bar is a lie of precision. Use ribbon plots to visualize the widening cone of uncertainty as time moves forward.\n\n\nCode\n# Generate data with trend\nnp.random.seed(42)\nn = 150\nx = np.arange(n)\ntrue_trend = 50 + 0.3 * x\nobserved = true_trend + np.random.normal(0, 5, n)\n\n# Simple linear forecast\nfrom scipy import stats\nslope, intercept, r_value, p_value, std_err = stats.linregress(x[:100], observed[:100])\n\n# Forecast period\nx_future = np.arange(100, 150)\ny_pred = slope * x_future + intercept\n\n# Estimate prediction interval (simplified)\nresiduals = observed[:100] - (slope * x[:100] + intercept)\nstd_residual = np.std(residuals)\nmargin = 1.96 * std_residual  # 95% prediction interval\n\n# Plot\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Historical data\nax.plot(x[:100], observed[:100], linewidth=2, label='Historical Data', color=sns.color_palette()[0])\n\n# Forecast with uncertainty\nax.plot(x_future, y_pred, linewidth=2, label='Forecast', color=sns.color_palette()[1], linestyle='--')\nax.fill_between(x_future, y_pred - margin, y_pred + margin,\n                alpha=0.3, color=sns.color_palette()[1], label='95% Prediction Interval')\n\n# Actual future (for comparison)\nax.plot(x_future, observed[100:], linewidth=1.5, alpha=0.5, label='Actual (for comparison)',\n        color='gray', linestyle=':')\n\nax.axvline(x=100, color='black', linestyle=':', alpha=0.5, label='Forecast Start')\nax.set_xlabel('Time')\nax.set_ylabel('Value')\nax.set_title('Time Series Forecast with Uncertainty Bands')\nax.legend()\nax.grid(True, alpha=0.3)\nsns.despine()\n\n\n\n\n\nRibbon plots show uncertainty bands around predictions",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Time-Series"
    ]
  },
  {
    "objectID": "m02-visualization/time-series.html#the-rhythm-of-time-heatmaps-and-cycles",
    "href": "m02-visualization/time-series.html#the-rhythm-of-time-heatmaps-and-cycles",
    "title": "Time Series Visualization",
    "section": "The rhythm of time: Heatmaps and cycles",
    "text": "The rhythm of time: Heatmaps and cycles\nTime often cycles rather than marches. Does your data have a heartbeat? Heatmaps and cycle plots break the linear narrative to reveal patterns like daily lulls, weekend spikes, or seasonal waves.\n\n\nCode\n# Generate synthetic hourly data with daily and weekly patterns\nnp.random.seed(42)\nhours = pd.date_range('2023-01-01', periods=24*7*4, freq='H')  # 4 weeks\n\n# Patterns: higher activity during business hours and weekdays\nhour_of_day = hours.hour\nday_of_week = hours.dayofweek\n\n# Activity pattern\nbase_activity = 20\nhour_effect = 30 * np.exp(-((hour_of_day - 14)**2) / 20)  # Peak at 2 PM\nweekday_effect = np.where(day_of_week &lt; 5, 20, -10)  # Weekdays higher\nnoise = np.random.normal(0, 5, len(hours))\n\nactivity = base_activity + hour_effect + weekday_effect + noise\n\ndf_hourly = pd.DataFrame({\n    'datetime': hours,\n    'activity': activity,\n    'hour': hour_of_day,\n    'day_name': hours.day_name(),\n    'week': (hours.day // 7) + 1\n})\n\n# Take first week for heatmap\ndf_week = df_hourly[df_hourly['week'] == 1].copy()\n\n# Pivot for heatmap\nheatmap_data = df_week.pivot_table(values='activity',\n                                     index='hour',\n                                     columns='day_name',\n                                     aggfunc='mean')\n\n# Reorder columns to start with Monday\nday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\nheatmap_data = heatmap_data[[day for day in day_order if day in heatmap_data.columns]]\n\n# Plot heatmap\nfig, ax = plt.subplots(figsize=(12, 8))\nsns.heatmap(heatmap_data, cmap='YlOrRd', annot=False, fmt='.0f',\n            cbar_kws={'label': 'Activity Level'}, ax=ax)\nax.set_xlabel('Day of Week')\nax.set_ylabel('Hour of Day')\nax.set_title('Temporal Heatmap: Activity by Hour and Day of Week')\nplt.tight_layout()\n\n\n/var/folders/j7/9dgqq5g53vnbsbmvh2yqtckr0000gr/T/ipykernel_9497/3637488205.py:3: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n  hours = pd.date_range('2023-01-01', periods=24*7*4, freq='H')  # 4 weeks\n\n\n\n\n\nHeat map reveals daily and weekly patterns in temporal data\n\n\n\n\n\n\nCode\n# Generate monthly data with strong annual seasonality\nnp.random.seed(42)\nmonths = pd.date_range('2020-01-01', periods=48, freq='M')\nmonth_num = np.tile(np.arange(1, 13), 4)  # 4 years of monthly data\n\n# Seasonal pattern (higher in summer, lower in winter)\nseasonal_effect = 20 * np.sin(2 * np.pi * (month_num - 3) / 12)\ntrend_effect = 0.5 * np.arange(48)\nnoise = np.random.normal(0, 3, 48)\n\nvalues = 50 + seasonal_effect + trend_effect + noise\n\ndf_seasonal = pd.DataFrame({\n    'date': months,\n    'value': values,\n    'month': month_num,\n    'year': months.year,\n    'month_name': months.month_name()\n})\n\n# Create cycle plot\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Traditional time series\naxes[0].plot(df_seasonal['date'], df_seasonal['value'], marker='o', linewidth=2)\naxes[0].set_xlabel('Date')\naxes[0].set_ylabel('Value')\naxes[0].set_title('Traditional Time Series: Seasonality Repeats')\naxes[0].grid(True, alpha=0.3)\n\n# Cycle plot\nmonth_names_short = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n                     'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\nfor year in df_seasonal['year'].unique():\n    year_data = df_seasonal[df_seasonal['year'] == year]\n    axes[1].plot(year_data['month'], year_data['value'], marker='o',\n                linewidth=2, label=str(year), alpha=0.7)\n\naxes[1].set_xlabel('Month')\naxes[1].set_ylabel('Value')\naxes[1].set_xticks(range(1, 13))\naxes[1].set_xticklabels(month_names_short)\naxes[1].set_title('Cycle Plot: Each Year Overlaid to Show Seasonal Pattern')\naxes[1].legend(title='Year')\naxes[1].grid(True, alpha=0.3)\n\nfor ax in axes:\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n\n\n/var/folders/j7/9dgqq5g53vnbsbmvh2yqtckr0000gr/T/ipykernel_9497/3601020590.py:3: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n  months = pd.date_range('2020-01-01', periods=48, freq='M')\n\n\n\n\n\nCycle plot reveals seasonal patterns by separating each cycle",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Time-Series"
    ]
  },
  {
    "objectID": "m02-visualization/time-series.html#the-memory-of-the-past-autocorrelation",
    "href": "m02-visualization/time-series.html#the-memory-of-the-past-autocorrelation",
    "title": "Time Series Visualization",
    "section": "The memory of the past: Autocorrelation",
    "text": "The memory of the past: Autocorrelation\nDoes the past predict the future? Lag plots visualize the system’s memory by plotting x_t against x_{t-1}. A tight diagonal implies strong memory (autocorrelation) while a scattered cloud implies random noise.\n\n\nCode\n# Generate time series with autocorrelation\nnp.random.seed(42)\nn = 200\n\n# AR(1) process: strong autocorrelation\nar_series = np.zeros(n)\nar_series[0] = np.random.normal(0, 1)\nfor i in range(1, n):\n    ar_series[i] = 0.7 * ar_series[i-1] + np.random.normal(0, 1)\n\n# Random walk: perfect autocorrelation at lag 1\nrandom_walk = np.random.normal(0, 1, n).cumsum()\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Lag-1 plot for AR(1) series\naxes[0].scatter(ar_series[:-1], ar_series[1:], alpha=0.6, s=30)\naxes[0].set_xlabel('Value at time t')\naxes[0].set_ylabel('Value at time t+1')\naxes[0].set_title('Lag-1 Plot: Strong Autocorrelation (AR Process)')\naxes[0].plot([-3, 3], [-3, 3], 'r--', alpha=0.5, linewidth=1)\naxes[0].grid(True, alpha=0.3)\n\n# Lag-1 plot for random walk\naxes[1].scatter(random_walk[:-1], random_walk[1:], alpha=0.6, s=30, color=sns.color_palette()[1])\naxes[1].set_xlabel('Value at time t')\naxes[1].set_ylabel('Value at time t+1')\naxes[1].set_title('Lag-1 Plot: Perfect Autocorrelation (Random Walk)')\naxes[1].plot([random_walk.min(), random_walk.max()],\n            [random_walk.min(), random_walk.max()], 'r--', alpha=0.5, linewidth=1)\naxes[1].grid(True, alpha=0.3)\n\nfor ax in axes:\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n\n\n\n\n\nLag plots reveal autocorrelation structure in time series\n\n\n\n\n\n\n\n\n\n\nSummary\n\n\n\nTime series visualization is about making choices that honestly represent temporal patterns. By following these principled visualization practices, you ensure your temporal data tells its true story, not the story you wish it told.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Time-Series"
    ]
  },
  {
    "objectID": "m02-visualization/overview.html",
    "href": "m02-visualization/overview.html",
    "title": "Module 2: Data Visualization",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module transforms you from someone who makes charts to someone who reveals insight through visual design.\nYou’ll learn:\n\nHow perception psychology shapes what we see in visualizations and why color, Gestalt principles, and preattentive attributes matter.\nThe golden rule: show all the data whenever possible, and why summary statistics alone mislead.\nHow to choose the right visualization for one-dimensional, two-dimensional, and high-dimensional datasets.\nSpecialized techniques for networks and time-series data that reveal structure and patterns.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Overview"
    ]
  },
  {
    "objectID": "m02-visualization/overview.html#the-journey",
    "href": "m02-visualization/overview.html#the-journey",
    "title": "Module 2: Data Visualization",
    "section": "The Journey",
    "text": "The Journey\nLet’s talk about where this module takes you. We begin with the surprising ways your visual system deceives you, then build up practical skills for every major data type.\nEach part answers a crucial question about effective visualization.\nPrinciples of Effective Visualization\nBefore we can make good visualizations, we need to understand how humans see. Your visual perception is contextual, subjective, and driven by unconscious processes.\nYou’ll learn how color fools us, how Gestalt principles organize information, and how preattentive attributes guide attention.\nVisualizing 1D Data\nThe golden rule of visualization: show all the data, whenever possible. Why do dynamite plots (bar charts with error bars) persist despite being misleading?\nYou’ll discover swarm plots, violin plots, and kernel density estimation. The goal is honesty.\nVisualizing 2D Data\nShift your attention to relationships between two variables. Scatter plots are just the beginning.\nYou’ll learn when to use hexbin plots for dense data, how to add marginal distributions, and why correlation doesn’t imply causation.\nVisualizing High-Dimensional Data\nWhat happens when you have dozens or hundreds of variables? Parallel coordinates, radar charts, and dimensionality reduction techniques like PCA, t-SNE, and UMAP reveal structure in high-dimensional spaces.\nYou’ll understand when each method shines and when it misleads.\nVisualizing Networks\nNetworks capture relationships: social connections, citations, biological interactions. Node-link diagrams are intuitive but quickly become hairballs.\nYou’ll explore force-directed layouts, matrix visualizations, and when to use network embeddings instead.\nVisualizing Time-Series\nTime-series data has special structure. Trends, seasonality, and anomalies emerge through clever visual design.\nYou’ll learn about scale choices (linear vs. log), smoothing techniques, and how to handle multiple time-series simultaneously.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Overview"
    ]
  },
  {
    "objectID": "m02-visualization/overview.html#why-this-matters",
    "href": "m02-visualization/overview.html#why-this-matters",
    "title": "Module 2: Data Visualization",
    "section": "Why This Matters",
    "text": "Why This Matters\nHere’s something remarkable. Visualization is not decoration. It’s exploration, communication, and decision-making rolled into one. When you visualize data well, patterns jump out immediately, outliers reveal themselves, and relationships become obvious.\nQuestions you didn’t know to ask suddenly demand answers. Exploratory visualization drives discovery. But visualization also communicates: a well-designed figure conveys your finding instantly, while a poorly designed figure obscures truth (or worse, misleads).\nUnderstanding perception psychology and design principles separates effective communication from noise. These skills matter beyond academia, where data-driven organizations make decisions based on dashboards and reports. The ability to transform raw numbers into clear visual stories is increasingly valuable. Whether you’re analyzing experimental results, monitoring systems, or presenting to stakeholders, visualization is your interface to insight.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Overview"
    ]
  },
  {
    "objectID": "m02-visualization/overview.html#prerequisites",
    "href": "m02-visualization/overview.html#prerequisites",
    "title": "Module 2: Data Visualization",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou should be comfortable with basic Python programming and familiar with NumPy arrays and Pandas DataFrames. Experience with matplotlib or seaborn helps but isn’t required. We’ll teach you the visualization libraries as we go, focusing on principles and design choices rather than syntax. Understanding when to use a particular chart type matters more than memorizing function parameters.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Overview"
    ]
  },
  {
    "objectID": "m02-visualization/overview.html#what-youll-build",
    "href": "m02-visualization/overview.html#what-youll-build",
    "title": "Module 2: Data Visualization",
    "section": "What You’ll Build",
    "text": "What You’ll Build\nBy the end of this module, you’ll have a diverse visualization portfolio: honest one-dimensional visualizations that show all data points, effective scatter plots with marginal distributions, high-dimensional datasets reduced to interpretable 2D projections, networks visualized using multiple complementary approaches, and time-series data handled with appropriate scale and smoothing choices. Most importantly, you’ll develop intuition for chart selection. Given a dataset and a question, you’ll know which visualization reveals the answer.\nLet’s begin by understanding how your visual system really works.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Overview"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html",
    "href": "m02-visualization/highd-data.html",
    "title": "High-Dimensional Data Visualization",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module introduces dimensionality reduction, a fundamental technique for visualizing and understanding high-dimensional data.\nYou’ll learn:\n\nWhat the curse of dimensionality means and why high-dimensional space is counterintuitive.\nHow to use PCA to preserve global variance structure in linear projections.\nThe difference between local methods (t-SNE, UMAP) and global methods (MDS, Isomap).\nHow to choose the right method for your data and avoid misleading visualizations.\nHigh-dimensional space is hostile territory. In our comfortable 3D world, we understand distance and density. But in 50 dimensions, your intuition betrays you: the volume of space explodes, points become lonely wanderers, and “distance” loses its meaning. This is the Curse of Dimensionality.\nYou can’t plot 50 dimensions directly. Dimensionality reduction bridges this gap, projecting high-dimensional data into 2 or 3 seeable dimensions. But this projection always involves a trade-off. Some methods preserve global structure (the big picture), while others preserve local structure (the neighborhood relationships). Understanding this trade-off is the key to avoiding misleading visualizations.\nConsider this geometrical nightmare: In 1D, 10 points cover the unit interval nicely. To maintain that same density in 10 dimensions, you don’t need 100 points—you need 10 billion. Even stranger, as dimensions increase, all points become roughly equidistant from each other. The concept of “nearest neighbor” dissolves.\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.metrics.pairwise import euclidean_distances\n\nsns.set_style(\"white\")\nnp.random.seed(42)\n\n# Calculate distance ratio across dimensions\ndimensions = [2, 5, 10, 20, 50, 100, 200]\nn_samples = 100\nratios = []\n\nfor d in dimensions:\n    # Generate random data\n    X = np.random.randn(n_samples, d)\n    # Calculate all pairwise distances\n    distances = euclidean_distances(X)\n    # For each point, find nearest and farthest (excluding self)\n    np.fill_diagonal(distances, np.inf)  # Ignore self-distance\n    nearest = distances.min(axis=1)\n    # For \"farthest,\" ignore inf (self-distance), so set inf entries to -1 and use argmax\n    temp = distances.copy()\n    temp[temp == np.inf] = -1  # Now maximum is truly among finite values\n    farthest = temp.max(axis=1)\n    # Calculate ratio\n    ratio = nearest / farthest\n    ratios.append(ratio)\n\n# Plot\nsns.set(font_scale=2.0)\nsns.set_style(\"white\")\n\nblue, red = sns.color_palette('muted', 2)\n\nfig, ax = plt.subplots(figsize=(10, 5))\npositions = range(len(dimensions))\nbp = ax.boxplot(ratios, positions=positions, widths=0.6, patch_artist=True,\n                boxprops=dict(facecolor=\"#f2f2f2\", alpha=0.7))\nax.set_xticklabels(dimensions)\nax.set_xlabel('Number of Dimensions')\nax.set_ylabel('Nearest Distance / Farthest Distance')\nax.set_title('The Curse of Dimensionality: All Points Become Equidistant')\nax.axhline(y=1.0, color=red, linestyle='--', alpha=0.5, label='Equal distances')\nax.legend(frameon=False)\nsns.despine()\n\n\n\n\n\nAs dimensions increase, the ratio of farthest to nearest distance approaches 1\nThe plot shows a striking pattern: As dimensions increase, the ratio of nearest to farthest distance gets closer to 1. At 200 dimensions, nearly every point is equally far from every other point. This makes clustering impossible since every point becomes equidistant from every other point. By projecting the data into lower dimensions, you can remedy this problem. This is why dimensionality reduction matters for analysis, not just visualization.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html#the-curse-of-dimensionality",
    "href": "m02-visualization/highd-data.html#the-curse-of-dimensionality",
    "title": "High-Dimensional Data Visualization",
    "section": "The Curse of Dimensionality",
    "text": "The Curse of Dimensionality\nLet’s talk about what makes high-dimensional data fundamentally different. In high dimensions, everything is far from everything else. This sounds paradoxical, but it’s mathematically inevitable. As dimensions increase, the volume of space grows exponentially, and data points become increasingly sparse.\nConsider this simple fact: In 1D, if you have 10 points uniformly distributed in [0, 1], the average distance between neighbors is about 0.1. To maintain the same density in 2D, you need 100 points. In 3D, you need 1,000 points. In 10D, you need 10 billion points. Even stranger is what happens to distances. In high dimensions, all distances become similar, the nearest and farthest neighbors becoming roughly equidistant. This makes many of our intuitions about “closeness” break down.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.metrics.pairwise import euclidean_distances\n\nsns.set_style(\"white\")\nnp.random.seed(42)\n\n# Calculate distance ratio across dimensions\ndimensions = [2, 5, 10, 20, 50, 100, 200]\nn_samples = 100\nratios = []\n\nfor d in dimensions:\n    # Generate random data\n    X = np.random.randn(n_samples, d)\n    # Calculate all pairwise distances\n    distances = euclidean_distances(X)\n    # For each point, find nearest and farthest (excluding self)\n    np.fill_diagonal(distances, np.inf)  # Ignore self-distance\n    nearest = distances.min(axis=1)\n    # For \"farthest,\" ignore inf (self-distance), so set inf entries to -1 and use argmax\n    temp = distances.copy()\n    temp[temp == np.inf] = -1  # Now maximum is truly among finite values\n    farthest = temp.max(axis=1)\n    # Calculate ratio\n    ratio = nearest / farthest\n    ratios.append(ratio)\n\n# Plot\nsns.set(font_scale=2.0)\nsns.set_style(\"white\")\n\nblue, red = sns.color_palette('muted', 2)\n\nfig, ax = plt.subplots(figsize=(10, 5))\npositions = range(len(dimensions))\nbp = ax.boxplot(ratios, positions=positions, widths=0.6, patch_artist=True,\n                boxprops=dict(facecolor=\"#f2f2f2\", alpha=0.7))\nax.set_xticklabels(dimensions)\nax.set_xlabel('Number of Dimensions')\nax.set_ylabel('Nearest Distance / Farthest Distance')\nax.set_title('The Curse of Dimensionality: All Points Become Equidistant')\nax.axhline(y=1.0, color=red, linestyle='--', alpha=0.5, label='Equal distances')\nax.legend(frameon=False)\nsns.despine()\n\n\n\n\n\nAs dimensions increase, the ratio of farthest to nearest distance approaches 1\n\n\n\n\nThe plot shows a striking pattern: As dimensions increase, the ratio of nearest to farthest distance gets closer to 1. At 200 dimensions, nearly every point is equally far from every other point. This makes clustering impossible since every point becomes equidistant from every other point. By projecting the data into lower dimensions, you can remedy this problem. This is why dimensionality reduction matters for analysis, not just visualization.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html#pairwise-scatter-plots-the-brute-force-approach",
    "href": "m02-visualization/highd-data.html#pairwise-scatter-plots-the-brute-force-approach",
    "title": "High-Dimensional Data Visualization",
    "section": "Pairwise Scatter Plots: The Brute Force Approach",
    "text": "Pairwise Scatter Plots: The Brute Force Approach\nWhen you have a moderate number of dimensions (roughly 3 to 10), you can visualize all pairwise relationships using a scatter plot matrix, also called a pairs plot or SPLOM.\n\n\nCode\n# Load classic iris dataset (4 dimensions)\nfrom sklearn.datasets import load_iris\n\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\n\niris = load_iris()\niris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\niris_df['species'] = iris.target\niris_df['species'] = iris_df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n\n# Create pairplot\ng = sns.pairplot(iris_df, hue='species', diag_kind='kde',\n                 plot_kws={'alpha': 0.6, 's': 50, 'edgecolor': 'white', 'linewidth': 0.5},\n                 diag_kws={'alpha': 0.7, 'linewidth': 2})\ng.fig.suptitle('Iris Dataset: All Pairwise Relationships', y=1.01)\n\n\nText(0.5, 1.01, 'Iris Dataset: All Pairwise Relationships')\nScatter plot matrix showing all pairwise relationships in the Iris dataset\n\n\n\n\n\n\n\n\n\nThe scatter plot matrix shows every possible 2D projection: the diagonal displays the univariate distribution of each feature using KDE, while off-diagonals show bivariate scatter plots. The problem is clear: scatter plot matrices don’t scale. With 10 variables, you have 45 unique pairwise plots (manageable but crowded). With 20 variables, you have 190 plots (overwhelming). And you’re still only seeing 2D projections, never the full high-dimensional structure. This is where dimensionality reduction becomes essential, projecting the data intelligently onto just 2 or 3 dimensions.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html#linear-dimensionality-reduction-pca",
    "href": "m02-visualization/highd-data.html#linear-dimensionality-reduction-pca",
    "title": "High-Dimensional Data Visualization",
    "section": "Linear Dimensionality Reduction: PCA",
    "text": "Linear Dimensionality Reduction: PCA\nPrincipal Component Analysis (PCA) is the workhorse of dimensionality reduction. It asks one simple question: “which direction has the most variance?” It rotates your coordinate system so that the first axis (PC1) aligns with the widest spread of data, and the second axis (PC2) aligns with the second widest, perpendicular spread.\nThe Math: If we center our data matrix X (so columns have zero mean), the covariance matrix is\nC = \\frac{1}{n-1} X^T X.\nWe want to find a vector v that maximizes the variance of the projection Xv. This turns out to be an eigenvalue problem:\n C v = \\lambda v \nThe eigenvector v with the largest eigenvalue \\lambda is the first principal component. The eigenvalue \\lambda represents the amount of variance captured by that component.\n\n\nCode\nfrom sklearn.decomposition import PCA\n\n# Generate correlated 2D data (for visualization)\nnp.random.seed(123)\nmean = [0, 0]\ncov = [[3, 2], [2, 2]]\ndata_2d = np.random.multivariate_normal(mean, cov, 300)\n\n# Fit PCA\npca = PCA(n_components=2)\npca.fit(data_2d)\n\ncolors = [\"#f2f2f2\", sns.color_palette('muted')[0], sns.color_palette('muted')[3]]\n\n# Plot original data with principal components\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(data_2d[:, 0], data_2d[:, 1], alpha=0.9, s=50, color=colors[0], edgecolors='k', linewidth=0.5)\n\n# Draw principal components as arrows\norigin = pca.mean_\nfor i, (component, variance) in enumerate(zip(pca.components_, pca.explained_variance_)):\n    direction = component * np.sqrt(variance) * 3  # Scale for visibility\n    ax.arrow(origin[0], origin[1], direction[0], direction[1],\n             head_width=0.3, head_length=0.3, fc=colors[i+1], ec=colors[i+1], linewidth=3,\n             label=f'PC{i+1} ({variance/pca.explained_variance_.sum()*100:.1f}%)')\n\nax.set_xlabel('Original X')\nax.set_ylabel('Original Y')\nax.set_title('Principal Components: Directions of Maximum Variance')\nax.legend()\nax.axis('equal')\nsns.despine()\n\n\n\n\n\nPCA finds directions of maximum variance. PC1 captures the most variation, PC2 the next most (perpendicular to PC1).\n\n\n\n\nPC1 (orange arrow) points along the direction of greatest spread, while PC2 (green arrow) is perpendicular and captures the remaining variation. The percentage shows how much variance each component explains. If PC1 explains 90 percent of variance, projecting onto just PC1 preserves most of your data’s structure.\n\nApplying PCA to Iris\nLet’s apply PCA to the 4-dimensional Iris dataset and see how much information we can preserve in just 2 dimensions.\n\n\nCode\n# Prepare data\nX = iris.data\ny = iris.target\n\n# Standardize (important for PCA!)\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Apply PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# Create DataFrame for plotting\npca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\npca_df['species'] = iris.target_names[y]\n\n# Plot\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Left: PCA projection\nfor species, color in zip(['setosa', 'versicolor', 'virginica'], sns.color_palette('muted', 3)):\n    mask = pca_df['species'] == species\n    axes[0].scatter(pca_df.loc[mask, 'PC1'], pca_df.loc[mask, 'PC2'],\n                   label=species, alpha=0.7, s=50, color=color, edgecolors='white', linewidth=0.5)\naxes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)')\naxes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)')\naxes[0].set_title('PCA Projection of Iris Dataset')\naxes[0].legend()\nsns.despine(ax=axes[0])\n\n# Right: Variance explained\nvariances = pca.explained_variance_ratio_\naxes[1].bar([1, 2], variances, color=sns.color_palette('muted', 2), alpha=0.7)\naxes[1].set_xlabel('Principal Component')\naxes[1].set_ylabel('Variance Explained')\naxes[1].set_title('Variance Explained by Each Component')\naxes[1].set_xticks([1, 2])\naxes[1].set_xticklabels(['PC1', 'PC2'])\nfor i, v in enumerate(variances):\n    axes[1].text(i+1, v+0.01, f'{v*100:.1f}%', ha='center', va='bottom', fontsize=11)\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n\n\n\n\n\nPCA projection of Iris dataset to 2D preserves the separation between species\n\n\n\n\nPC1 and PC2 together explain over 95 percent of the variance in the 4D dataset, with the 2D projection preserving the main structure beautifully (Setosa well-separated, versicolor and virginica showing some overlap just as in the original high-dimensional space). A critical reminder: always standardize before PCA. If features have different units or scales, PCA will be dominated by high-variance features. Standardization (zero mean, unit variance) ensures all features contribute fairly.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html#non-linear-dimensionality-reduction-mds",
    "href": "m02-visualization/highd-data.html#non-linear-dimensionality-reduction-mds",
    "title": "High-Dimensional Data Visualization",
    "section": "Non-Linear Dimensionality Reduction: MDS",
    "text": "Non-Linear Dimensionality Reduction: MDS\nShift your perspective from variance to distances. Multidimensional Scaling (MDS) tries to preserve the pairwise distances between points. Think of it like making a map from a mileage table: you know the distance between every pair of cities in a spreadsheet, but you don’t know where they are on a map. MDS works backward to find coordinates that satisfy those distances.\nThe Math: Given a high-dimensional distance matrix D where d_{ij} is the distance between points i and j, MDS seeks low-dimensional coordinates y_1, \\dots, y_n that minimize the Stress function:  \\text{Stress} = \\sqrt{ \\frac{\\sum_{i&lt;j} (d_{ij} - \\|y_i - y_j\\|)^2}{\\sum_{i&lt;j} d_{ij}^2} }  Minimizing this stress forces the Euclidean distances in the low-dimensional map \\|y_i - y_j\\| to approximate the original distances d_{ij}.\n\n\nCode\nfrom sklearn.manifold import MDS\n\n# Suppress FutureWarning about n_init in MDS\nimport warnings\nmds = MDS(n_components=2, random_state=42, n_init=1)\nX_mds = mds.fit_transform(X_scaled)\n\n# Create DataFrame\nmds_df = pd.DataFrame(X_mds, columns=['MDS1', 'MDS2'])\nmds_df['species'] = iris.target_names[y]\n\n# Plot both\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# PCA\nfor species, color in zip(['setosa', 'versicolor', 'virginica'], sns.color_palette('muted', 3)):\n    mask = pca_df['species'] == species\n    axes[0].scatter(pca_df.loc[mask, 'PC1'], pca_df.loc[mask, 'PC2'],\n                   label=species, alpha=0.7, s=50, color=color, edgecolors='white', linewidth=0.5)\naxes[0].set_xlabel('PC1')\naxes[0].set_ylabel('PC2')\naxes[0].set_title('PCA: Maximizes Variance')\naxes[0].legend()\nsns.despine(ax=axes[0])\n\n# MDS\nfor species, color in zip(['setosa', 'versicolor', 'virginica'], sns.color_palette('muted', 3)):\n    mask = mds_df['species'] == species\n    axes[1].scatter(mds_df.loc[mask, 'MDS1'], mds_df.loc[mask, 'MDS2'],\n                   label=species, alpha=0.7, s=50, color=color, edgecolors='white', linewidth=0.5)\naxes[1].set_xlabel('MDS1')\naxes[1].set_ylabel('MDS2')\naxes[1].set_title('MDS: Preserves Distances')\naxes[1].legend()\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n\n\n\n\n\nMDS vs PCA on Iris dataset. MDS preserves distances better but looks similar to PCA for this dataset.\n\n\n\n\nFor the Iris dataset, PCA and MDS look very similar. This is because Iris data is fairly linear. The relationships between features don’t involve complex curves or non-linear structures that would cause MDS to differ significantly from PCA.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html#isomap-preserving-geodesic-distances",
    "href": "m02-visualization/highd-data.html#isomap-preserving-geodesic-distances",
    "title": "High-Dimensional Data Visualization",
    "section": "Isomap: Preserving Geodesic Distances",
    "text": "Isomap: Preserving Geodesic Distances\nMDS fails on curved surfaces (manifolds) because it respects Euclidean “straight-line” distance. Isomap (Isometric Mapping) fixes this by measuring distance along the surface. It builds a neighborhood graph—connecting only nearby points—and measures distance by hopping along the graph edges. It’s the difference between flying (MDS) and walking (Isomap).\n\n\n\nIsomap\n\n\nThe Math: Isomap transforms the problem into MDS by changing the distance metric to follow the data’s underlying manifold. It begins by constructing a neighborhood graph, where points i and j are connected if d_{ij} &lt; \\epsilon (epsilon-Isomap) or if j is among the k-nearest neighbors (k-Isomap). Next, it computes the geodesic distances d_G(i, j) as the shortest path distance between nodes in the graph using algorithms like Dijkstra’s or Floyd-Warshall. Finally, classical MDS is applied to this matrix of geodesic distances D_G to find the low-dimensional coordinates.\n\n\nCode\nfrom sklearn.manifold import Isomap\nfrom sklearn.datasets import make_s_curve\n\n# Generate S-curve data (a 2D manifold embedded in 3D)\nn_samples = 1000\nX_scurve, color = make_s_curve(n_samples, noise=0.1, random_state=42)\n\n# Apply MDS\nmds_scurve = MDS(n_components=2, random_state=42, n_init=1)\nX_scurve_mds = mds_scurve.fit_transform(X_scurve)\n\n# Apply Isomap\nisomap = Isomap(n_components=2, n_neighbors=10)\nX_scurve_isomap = isomap.fit_transform(X_scurve)\n\n# Plot MDS vs Isomap\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# MDS\naxes[0].scatter(X_scurve_mds[:, 0], X_scurve_mds[:, 1], c=color,\n                cmap='viridis', alpha=0.6, s=20)\naxes[0].set_xlabel('MDS1')\naxes[0].set_ylabel('MDS2')\naxes[0].set_title('MDS: Global Euclidean Distances')\nsns.despine(ax=axes[0])\n\n# Isomap\naxes[1].scatter(X_scurve_isomap[:, 0], X_scurve_isomap[:, 1], c=color,\n                cmap='viridis', alpha=0.6, s=20)\naxes[1].set_xlabel('Isomap1')\naxes[1].set_ylabel('Isomap2')\naxes[1].set_title('Isomap: Geodesic Distances')\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n\n\n\n\n\nIsomap uses geodesic distances (along the surface) instead of Euclidean distances (through space), better recovering the S-curve structure\n\n\n\n\nIsomap successfully “straightens” the S-curve because it respects the manifold structure, computing distances along the neighborhood graph to avoid shortcuts across the bend that confused MDS. The key parameter is n_neighbors. Too few and the graph becomes disconnected with infinite distances. Too many and you create shortcuts across the manifold, reverting to MDS-like behavior. Getting it just right (typically 5 to 15) captures the local manifold structure perfectly.\nNow we see two extremes emerging: MDS preserves all pairwise distances globally (working on linear or convex data), while Isomap preserves geodesic distances using local neighborhoods (working on curved manifolds). But what if we only care about local structure and global relationships don’t matter?",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html#modern-non-linear-methods-t-sne-and-umap",
    "href": "m02-visualization/highd-data.html#modern-non-linear-methods-t-sne-and-umap",
    "title": "High-Dimensional Data Visualization",
    "section": "Modern Non-Linear Methods: t-SNE and UMAP",
    "text": "Modern Non-Linear Methods: t-SNE and UMAP\nGlobal methods like MDS can crush local details. Modern non-linear methods like t-SNE and UMAP take the opposite approach: they prioritize the local neighborhood above all else. They care intensely about keeping friends together, but they care much less about where those groups of friends end up relative to each other. The key insight: For visualization, we almost always care most about neighbors.\n\nHow t-SNE works\nt-SNE defines “similarity” as a probability. In high dimensions, it measures how likely point i would pick point j as its neighbor (based on a Gaussian curve). In low dimensions, it tries to match that probability using a Student’s t-distribution.\nThe Math:\n\nHigh-dimensional Probabilities (P): We define the conditional probability p_{j|i} using a Gaussian kernel centered at x_i, where \\sigma_i is tuned to match the desired “perplexity”:  p_{j|i} = \\frac{\\exp(-\\|x_i - x_j\\|^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-\\|x_i - x_k\\|^2 / 2\\sigma_i^2)}  We symmetrize this to get p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2n}.\nLow-dimensional Probabilities (Q): In the low-dimensional space y, we use a Student t-distribution with 1 degree of freedom (which has heavier tails than a Gaussian):  q_{ij} = \\frac{(1 + \\|y_i - y_j\\|^2)^{-1}}{\\sum_{k \\neq l} (1 + \\|y_k - y_l\\|^2)^{-1}} \nOptimization: We minimize the Kullback-Leibler (KL) divergence between P and Q using gradient descent:  C = KL(P||Q) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}} \n\nThe t-distribution’s heavy tails are clever. They let well-separated clusters spread out in 2D without overlapping, while keeping local neighborhoods tight.\n\n\nCode\nfrom sklearn.manifold import TSNE\n\n# Apply t-SNE\ntsne = TSNE(n_components=2, random_state=42, perplexity=30)\nX_scurve_tsne = tsne.fit_transform(X_scurve)\n\n# Plot all three methods\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# MDS - Global Euclidean distances\naxes[0].scatter(X_scurve_mds[:, 0], X_scurve_mds[:, 1], c=color,\n                cmap='viridis', alpha=0.6, s=20)\naxes[0].set_xlabel('MDS1')\naxes[0].set_ylabel('MDS2')\naxes[0].set_title('MDS: Global Distances')\nsns.despine(ax=axes[0])\n\n# Isomap - Geodesic distances\naxes[1].scatter(X_scurve_isomap[:, 0], X_scurve_isomap[:, 1], c=color,\n                cmap='viridis', alpha=0.6, s=20)\naxes[1].set_xlabel('Isomap1')\naxes[1].set_ylabel('Isomap2')\naxes[1].set_title('Isomap: Geodesic Distances')\nsns.despine(ax=axes[1])\n\n# t-SNE - Local neighborhoods\naxes[2].scatter(X_scurve_tsne[:, 0], X_scurve_tsne[:, 1], c=color,\n                cmap='viridis', alpha=0.6, s=20)\naxes[2].set_xlabel('t-SNE1')\naxes[2].set_ylabel('t-SNE2')\naxes[2].set_title('t-SNE: Local Structure')\nsns.despine(ax=axes[2])\n\nplt.tight_layout()\n\n\n\n\n\nComparing global, geodesic, and local approaches on the S-curve\n\n\n\n\nAll three methods successfully straighten the S-curve through different philosophies: MDS compromises between all distances, Isomap follows the manifold globally, and t-SNE focuses on preserving neighborhoods. The key parameter in t-SNE is perplexity (typically 30 to 50), which controls the effective neighborhood size. Too low perplexity fragments clusters, too high perplexity loses local detail.\n\n\n\n\n\n\nWhen t-SNE works and fails\n\n\n\nt-SNE is useful and widely used for high-dimensional data visualization. Yet, it sometimes falls in short. See this blog for more details, which shows numerious examples of t-SNE visualization that are not what we expect.\n\n\n\n\nWhat t-SNE preserves (and what it doesn’t)\nt-SNE is powerful but has important limitations. It preserves local structure (keeping points that are neighbors in high dimensions as neighbors in 2D), preserves clusters (keeping well-separated groups separated), and preserves relative relationships within neighborhoods (if A is closer to B than to C locally, this is preserved). What t-SNE does NOT preserve: actual distances between points, the relative position of distant clusters (arbitrary), cluster sizes (large clusters may appear smaller and vice versa), or densities (tight clusters may spread out, sparse regions may appear dense).\nYou cannot conclude that “cluster A is twice as far from B as from C” (distances not preserved), “cluster A is twice the size of B” (sizes not preserved), or “the data has exactly 5 clusters” (apparent clusters may be visualization artifacts). You can conclude that “these points form a distinct group separate from others,” “these points are more similar to each other than to distant points,” and “the data has local structure and is not uniformly random.”\n\n\nApplying t-SNE to real data\nLet’s apply t-SNE to a more realistic high-dimensional dataset: the digits dataset, which has 64 dimensions (8 by 8 pixel images).\n\n\nCode\nfrom sklearn.datasets import load_digits\n\n# Load digits dataset (8x8 images, 64 dimensions)\ndigits = load_digits()\nX_digits = digits.data\ny_digits = digits.target\n\n# Take a subset for speed (t-SNE is slow on large datasets)\nnp.random.seed(42)\nindices = np.random.choice(len(X_digits), size=1000, replace=False)\nX_subset = X_digits[indices]\ny_subset = y_digits[indices]\n\n# Apply t-SNE\ntsne_digits = TSNE(n_components=2, random_state=42, perplexity=40)\nX_digits_tsne = tsne_digits.fit_transform(X_subset)\n\n# Plot\nfig, ax = plt.subplots(figsize=(12, 10))\nscatter = ax.scatter(X_digits_tsne[:, 0], X_digits_tsne[:, 1],\n                     c=y_subset, cmap='tab10', alpha=0.7, s=30)\nax.set_xlabel('t-SNE1')\nax.set_ylabel('t-SNE2')\nax.set_title('t-SNE Visualization of Handwritten Digits (64D to 2D)')\ncbar = plt.colorbar(scatter, ax=ax, ticks=range(10))\ncbar.set_label('Digit Class')\nsns.despine()\n\n\n\n\n\nt-SNE visualization of handwritten digits (64 dimensions to 2D). Each color represents a digit class.\n\n\n\n\nThe t-SNE projection beautifully separates most digit classes, with similar digits (3, 5, and 8) clustering near each other while visually distinct digits (0 and 1) are well separated. This demonstrates t-SNE’s power: from 64 dimensions with no explicit information about what makes digits similar, t-SNE discovers the perceptual structure of handwritten digits. An important note: t-SNE is stochastic. Different runs produce different layouts (though cluster structure remains consistent), so always check multiple runs with different random seeds, especially for scientific conclusions.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html#umap-a-faster-alternative",
    "href": "m02-visualization/highd-data.html#umap-a-faster-alternative",
    "title": "High-Dimensional Data Visualization",
    "section": "UMAP: A Faster Alternative",
    "text": "UMAP: A Faster Alternative\nUniform Manifold Approximation and Projection (UMAP) is a newer method from 2018 that has become popular as an alternative to t-SNE. Like t-SNE, UMAP preserves local structure, but it’s based on different mathematical foundations in manifold learning and topological data analysis. UMAP has several advantages over t-SNE: it’s faster (often 10 to 100 times faster on large datasets), scales better (working well on datasets with millions of points), preserves more global structure, and is theoretically grounded in Riemannian geometry and fuzzy topology.\nThe Math: UMAP relies on fuzzy simplicial sets and a different cost function.\n\nLocal Connectivity: It computes high-dimensional probabilities p_{ij} using a specific local connectivity constraint:  p_{j|i} = \\exp\\left(-\\frac{d(x_i, x_j) - \\rho_i}{\\sigma_i}\\right)  where \\rho_i is the distance to the nearest neighbor. This ensures every point is connected to at least one neighbor.\nBinary Cross-Entropy: Instead of KL divergence, UMAP minimizes the fuzzy set cross-entropy:  C = \\sum_{i \\neq j} \\left[ p_{ij} \\log \\left(\\frac{p_{ij}}{q_{ij}}\\right) + (1 - p_{ij}) \\log \\left(\\frac{1 - p_{ij}}{1 - q_{ij}}\\right) \\right]  The second term (1 - p_{ij}) \\log (\\dots) forces points that are far in high dimensions to also be far in low dimensions, helping UMAP preserve more global structure than t-SNE.\n\n\n\nCode\nimport umap\n\n# Apply UMAP\numap_model = umap.UMAP(n_components=2, random_state=42, n_neighbors=30)\nX_digits_umap = umap_model.fit_transform(X_subset)\n\n# Plot comparison\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# t-SNE\nscatter = axes[0].scatter(X_digits_tsne[:, 0], X_digits_tsne[:, 1],\n                          c=y_subset, cmap='tab10', alpha=0.7, s=30)\naxes[0].set_xlabel('t-SNE1')\naxes[0].set_ylabel('t-SNE2')\naxes[0].set_title('t-SNE')\nsns.despine(ax=axes[0])\n\n# UMAP\nscatter = axes[1].scatter(X_digits_umap[:, 0], X_digits_umap[:, 1],\n                          c=y_subset, cmap='tab10', alpha=0.7, s=30)\naxes[1].set_xlabel('UMAP1')\naxes[1].set_ylabel('UMAP2')\naxes[1].set_title('UMAP')\ncbar = plt.colorbar(scatter, ax=axes[1], ticks=range(10))\ncbar.set_label('Digit Class')\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n\n\n\n\n\nUMAP vs t-SNE on digits dataset. UMAP often preserves more global structure while being much faster.\n\n\n\n\nBoth methods reveal similar cluster structure, but UMAP tends to space clusters more evenly and preserve more global topology (notice how UMAP places similar digits like 3, 5, 8 in a connected region, suggesting they share underlying structure).",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/highd-data.html#the-bigger-picture-choosing-the-right-method",
    "href": "m02-visualization/highd-data.html#the-bigger-picture-choosing-the-right-method",
    "title": "High-Dimensional Data Visualization",
    "section": "The Bigger Picture: Choosing the Right Method",
    "text": "The Bigger Picture: Choosing the Right Method\nLet’s step back and see the full landscape. Dimensionality reduction is not a one-size-fits-all solution. Different methods make different trade-offs:\n\n\n\n\n\n\n\n\n\n\nMethod\nPreserves\nSpeed\nScalability\nWhen to use\n\n\n\n\nScatter plot matrix\nEverything (2D projections)\nFast\n3-10 dimensions\nExploring moderate-dimensional data\n\n\nPCA\nGlobal variance\nVery fast\nExcellent (1000s of dims)\nLinear structure, interpretability needed\n\n\nMDS\nAll distances\nSlow\nPoor (100s of points)\nDistance preservation critical\n\n\nt-SNE\nLocal structure\nSlow\nModerate (10,000s of points)\nRevealing clusters, local relationships\n\n\nUMAP\nLocal plus some global\nFast\nExcellent (millions of points)\nLarge datasets, faster alternative to t-SNE\n\n\n\nA practical workflow begins with PCA: always run PCA first. It’s fast, deterministic, and interpretable. If the first few components capture enough variance, you’re done. If the data is nonlinear or you need to separate complex clusters, move to t-SNE or UMAP.\nValidate your findings. Don’t trust a single visualization: try different random seeds, vary the perplexity, and compare methods.\n\n\n\n\n\n\nTry it yourself\n\n\n\nTake a dataset you’re familiar with and apply all four methods: PCA, MDS, t-SNE, and UMAP. Compare the results. What structure does each method reveal? What structure does each method hide? Which visualization best matches your intuition about the data?\n\n\nRemember the golden rule: Dimensionality reduction is a lie. It lies about distance, density, or topology to fit a complex world onto a flat screen. Use these visualizations to generate hypotheses, but never trust them blindly. As Jake VanderPlas wrote, “The question is not whether you lose information (you always do) but whether you lose the information you care about.”",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing High-Dimensional Data"
    ]
  },
  {
    "objectID": "m02-visualization/2d-data.html",
    "href": "m02-visualization/2d-data.html",
    "title": "2D Data Visualization",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module teaches how to visualize relationships between two variables.\nYou’ll learn:\n\nWhat scatter plots reveal about relationships and how to handle overlapping points with transparency and jittering.\nHow binning methods (heatmaps, hexbin plots) make sense of massive datasets where individual points disappear.\nThe power of density estimation techniques like 2D KDE and contour plots for smooth, assumption-light visualization.\nPractical strategies for comparing relationships across groups without misleading your audience.\nYou’ve probably heard that “correlation doesn’t equal causation.” Here’s an even more fundamental problem: a correlation coefficient doesn’t tell you what your data actually looks like.\nLet’s talk about one of the most famous examples in data visualization. In 1973, statistician Francis Anscombe created four datasets that shattered the myth of summary statistics.\nEach dataset has 11 (x, y) pairs. Each has identical means, variances, correlation coefficients (r = 0.816), and linear regression lines. Identical on paper.\nBut when you plot them? Completely different stories.\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Load Anscombe's quartet\nanscombe = sns.load_dataset(\"anscombe\")\n\n# Create the plot\nsns.set_style(\"white\")\ng = sns.FacetGrid(anscombe, col=\"dataset\", col_wrap=2, height=4, aspect=1.2)\ng.map_dataframe(sns.scatterplot, x=\"x\", y=\"y\", s=100)\ng.map_dataframe(sns.regplot, x=\"x\", y=\"y\", scatter=False, color=\"red\")\ng.set_axis_labels(\"X\", \"Y\")\ng.set_titles(\"Dataset {col_name}\")\n\n# Add correlation to each subplot\nfor ax, dataset in zip(g.axes.flat, [\"I\", \"II\", \"III\", \"IV\"]):\n    data_subset = anscombe[anscombe[\"dataset\"] == dataset]\n    r = np.corrcoef(data_subset[\"x\"], data_subset[\"y\"])[0, 1]\n    ax.text(0.05, 0.95, f'r = {r:.3f}', transform=ax.transAxes,\n            verticalalignment='top', fontsize=12, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\nsns.despine()\nplt.tight_layout()\n\n\n\n\n\nAnscombe’s Quartet: Four datasets with identical summary statistics but completely different relationships\nLook at what the plots reveal. Dataset I shows a nice linear relationship. Dataset II? Clearly non-linear. It’s a parabola that a linear model completely misses.\nDataset III has a perfect linear relationship except for one outlier that changes everything. Dataset IV shows no relationship at all. A single influential point creates the illusion of correlation.\nSame statistics. Same regression line. Completely different data.\nThis is why we visualize relationships. Always plot your bivariate data. Summary statistics conceal structure.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 2D Data"
    ]
  },
  {
    "objectID": "m02-visualization/2d-data.html#showing-all-points-scatter-plots",
    "href": "m02-visualization/2d-data.html#showing-all-points-scatter-plots",
    "title": "2D Data Visualization",
    "section": "Showing All Points: Scatter Plots",
    "text": "Showing All Points: Scatter Plots\nLet’s start with the most direct way to show a relationship between two variables. A scatter plot plots every point, so each observation becomes a single dot in 2D space.\n\n\nCode\n# Generate sample data with clear relationship\nnp.random.seed(42)\nn_points = 200\nx = np.random.normal(50, 15, n_points)\ny = 1.5 * x + np.random.normal(0, 10, n_points)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(x, y, alpha=0.6, s=50, edgecolors='white', linewidth=0.5)\nax.set_xlabel('X Variable')\nax.set_ylabel('Y Variable')\nax.set_title('Scatter Plot: Every Point Visible')\nsns.despine()\n\n\n\n\n\nBasic scatter plot showing relationship between two variables\n\n\n\n\nFor small to moderate datasets (up to roughly 1,000 points), scatter plots are perfect. You can see everything at once: the relationship’s strength and direction, the spread around the trend, individual outliers, non-linear patterns, clusters, and subgroups.\nWhat happens when points overlap heavily? Use transparency (alpha).\nTransparency creates natural density shading. Areas with many overlapping points appear darker. Sparse areas appear lighter.\n\n\nCode\n# Generate data with heavy overlap\nnp.random.seed(123)\nn_points = 1000\nx_overlap = np.random.normal(50, 10, n_points)\ny_overlap = 0.8 * x_overlap + np.random.normal(0, 8, n_points)\n\nfig, axes = plt.subplots(1, 3, figsize=(14, 5))\nalphas = [1.0, 0.5, 0.1]\n\nfor ax, alpha in zip(axes, alphas):\n    ax.scatter(x_overlap, y_overlap, alpha=alpha, s=30)\n    ax.set_xlabel('X Variable')\n    ax.set_ylabel('Y Variable')\n    ax.set_title(f'Alpha = {alpha}')\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n\n\n\n\n\nScatter plots with different alpha values showing how transparency reveals density\n\n\n\n\nLook at the difference. With alpha = 1.0 (opaque), the center is a solid blob. You can’t tell if there are 10 points or 100.\nWith alpha = 0.1, the density gradient becomes visible. Dark regions have many points. Light regions have few.\n\n\n\n\n\n\nNote\n\n\n\n\nA figure from Metaanalysis of faculty’s teaching effectiveness showing the relationship between student evaluation of teaching and actual learning. Each bubble represents a course section, with size proportional to the number of students. Notice how transparency reveals the density of observations.\n\n\nWhat if even transparency doesn’t help? For extremely dense data, jittering can separate overlapping points.\nJittering adds small random noise to point positions. This spreads them out while keeping their relative positions meaningful.\n\n\nCode\n# Generate data with discrete values (common in survey data)\nnp.random.seed(456)\nn_points = 500\nx_discrete = np.random.choice([1, 2, 3, 4, 5], n_points)\ny_discrete = x_discrete + np.random.choice([-1, 0, 1], n_points) + np.random.normal(0, 0.3, n_points)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Without jitter\naxes[0].scatter(x_discrete, y_discrete, alpha=0.5, s=50)\naxes[0].set_xlabel('X Variable (Discrete)')\naxes[0].set_ylabel('Y Variable')\naxes[0].set_title('Without Jittering')\nsns.despine(ax=axes[0])\n\n# With jitter\njitter_x = x_discrete + np.random.normal(0, 0.1, n_points)\njitter_y = y_discrete + np.random.normal(0, 0.1, n_points)\naxes[1].scatter(jitter_x, jitter_y, alpha=0.5, s=50)\naxes[1].set_xlabel('X Variable (Discrete)')\naxes[1].set_ylabel('Y Variable')\naxes[1].set_title('With Jittering')\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n\n\n\n\n\nJittering helps separate discrete or overlapping points\n\n\n\n\nSee the problem? Without jittering, many points stack on top of each other. You might think there are only 25 data points (5 times 5) when there are actually 500.\nJittering reveals the true sample size and density at each location.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 2D Data"
    ]
  },
  {
    "objectID": "m02-visualization/2d-data.html#when-points-overlap-binning-methods",
    "href": "m02-visualization/2d-data.html#when-points-overlap-binning-methods",
    "title": "2D Data Visualization",
    "section": "When Points Overlap: Binning Methods",
    "text": "When Points Overlap: Binning Methods\nWhat happens when you have tens of thousands of points? Even transparency and jittering don’t fully reveal the density structure.\nShift your attention from individual points to the overall pattern. This is when we need to bin the data.\nBinning divides the 2D space into regions and counts observations in each region. Think of it as creating a grid over your data and counting how many points fall in each cell.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 2D Data"
    ]
  },
  {
    "objectID": "m02-visualization/2d-data.html#d-histograms-heatmaps",
    "href": "m02-visualization/2d-data.html#d-histograms-heatmaps",
    "title": "2D Data Visualization",
    "section": "2D Histograms (Heatmaps)",
    "text": "2D Histograms (Heatmaps)\nA 2D histogram extends the 1D histogram concept to two dimensions. The plane is divided into rectangular bins, and each bin’s color represents the number of points it contains.\n\n\nCode\n# Generate large dataset\nnp.random.seed(789)\nn_large = 10000\nx_large = np.random.normal(50, 15, n_large)\ny_large = 0.8 * x_large + np.random.normal(0, 12, n_large)\n\nfig, ax = plt.subplots(figsize=(10, 7))\nhb = ax.hexbin(x_large, y_large, gridsize=30, cmap='YlOrRd', mincnt=1)\nax.set_xlabel('X Variable')\nax.set_ylabel('Y Variable')\nax.set_title('2D Histogram: Density Through Binning')\ncb = plt.colorbar(hb, ax=ax)\ncb.set_label('Count')\nsns.despine()\n\n\n\n\n\n2D histogram showing density through rectangular bins\n\n\n\n\nThe key parameter is bin size (or gridsize). Too few bins and you lose detail. Too many bins and the plot becomes noisy.\nLike 1D histograms, this requires experimentation.\n\n\nCode\nfig, axes = plt.subplots(1, 3, figsize=(14, 5))\ngridsizes = [10, 30, 60]\n\nfor ax, gridsize in zip(axes, gridsizes):\n    hb = ax.hexbin(x_large, y_large, gridsize=gridsize, cmap='YlOrRd', mincnt=1)\n    ax.set_xlabel('X Variable')\n    ax.set_ylabel('Y Variable')\n    ax.set_title(f'Gridsize = {gridsize}')\n    plt.colorbar(hb, ax=ax)\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n\n\n\n\n\nEffect of bin size on 2D histograms\n\n\n\n\nLook at the trade-off. With gridsize = 10, we see only coarse structure. With gridsize = 60, the plot is noisy. Some bins have few points just by chance.\nGridsize = 30 provides a good balance between detail and stability.\n\nHexbin Plots\nWhy use rectangles when hexagons are better? Hexagonal binning uses hexagons instead of rectangles.\nHexagons are closer to circles. Every edge is equidistant from the center. This reduces bias in how we perceive density.\n\n\nCode\n# Generate data with interesting structure\nnp.random.seed(101)\nn = 8000\n\n# Create two clusters\ncluster1_x = np.random.normal(30, 8, n // 2)\ncluster1_y = np.random.normal(40, 8, n // 2)\ncluster2_x = np.random.normal(60, 10, n // 2)\ncluster2_y = np.random.normal(70, 10, n // 2)\n\nx_clusters = np.concatenate([cluster1_x, cluster2_x])\ny_clusters = np.concatenate([cluster1_y, cluster2_y])\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Scatter plot (for reference)\naxes[0].scatter(x_clusters, y_clusters, alpha=0.1, s=10)\naxes[0].set_xlabel('X Variable')\naxes[0].set_ylabel('Y Variable')\naxes[0].set_title('Scatter Plot (alpha = 0.1)')\nsns.despine(ax=axes[0])\n\n# Hexbin plot\nhb = axes[1].hexbin(x_clusters, y_clusters, gridsize=25, cmap='viridis', mincnt=1)\naxes[1].set_xlabel('X Variable')\naxes[1].set_ylabel('Y Variable')\naxes[1].set_title('Hexbin Plot')\nplt.colorbar(hb, ax=axes[1], label='Count')\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n\n\n\n\n\nHexbin plot provides more perceptually uniform density representation\n\n\n\n\nThe hexbin plot clearly reveals the two clusters and their relative densities. That’s harder to see in the scatter plot, even with low alpha.\nHexbin plots are particularly powerful for very large datasets (100,000+ points). At that scale, scatter plots become computationally expensive and visually overwhelming.\n\n\n\n\n\n\nChoosing colors for density plots\n\n\n\nWhen showing density or counts, use sequential colormaps that vary in lightness: light equals low density, dark equals high density. Good choices include 'YlOrRd' (yellow-orange-red), 'viridis' (purple-blue-green-yellow, perceptually uniform), and 'Blues' or 'Reds' (single hue).\nAvoid rainbow colormaps like 'jet'. They create artificial boundaries where none exist and are not perceptually uniform.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 2D Data"
    ]
  },
  {
    "objectID": "m02-visualization/2d-data.html#smooth-density-estimation-2d-kde",
    "href": "m02-visualization/2d-data.html#smooth-density-estimation-2d-kde",
    "title": "2D Data Visualization",
    "section": "Smooth Density Estimation: 2D KDE",
    "text": "Smooth Density Estimation: 2D KDE\nJust as 1D kernel density estimation (KDE) provides a smooth alternative to histograms, 2D KDE smooths 2D histograms by placing a kernel at each data point and summing them.\n\n\nCode\n# Use the clustered data\nfig, ax = plt.subplots(figsize=(10, 7))\nsns.kdeplot(x=x_clusters, y=y_clusters, cmap='viridis', fill=True, thresh=0.05, levels=20, ax=ax)\nax.set_xlabel('X Variable')\nax.set_ylabel('Y Variable')\nax.set_title('2D Kernel Density Estimation')\nsns.despine()\n\n\n\n\n\n2D kernel density estimation provides smooth density surface\n\n\n\n\nKDE reveals smooth density gradients without arbitrary binning decisions. The key parameter is bandwidth, which controls how wide each kernel is.\nSmall bandwidth gives high detail but can be noisy. Large bandwidth is smooth but may blur important features.\n\n\nCode\nfig, axes = plt.subplots(1, 3, figsize=(14, 5))\nbandwidths = [0.5, 1.5, 3.0]\n\nfor ax, bw in zip(axes, bandwidths):\n    sns.kdeplot(x=x_clusters, y=y_clusters, cmap='viridis', fill=True,\n                bw_adjust=bw, thresh=0.05, levels=15, ax=ax)\n    ax.set_xlabel('X Variable')\n    ax.set_ylabel('Y Variable')\n    ax.set_title(f'Bandwidth adjustment = {bw}')\n    sns.despine(ax=ax)\n\nplt.tight_layout()\n\n\n\n\n\nEffect of bandwidth on 2D KDE smoothness\n\n\n\n\nSee the bandwidth effect. With bw_adjust=0.5 (narrow bandwidth), we see fine detail but some noise. With bw_adjust=3.0 (wide bandwidth), the plot is very smooth but the two clusters nearly merge.\nThe default bw_adjust=1.0 (or around 1.5 here) balances detail and smoothness.\n\nContour Plots\nA contour plot represents the density surface as lines of equal density. Think of it like a topographic map where each contour line represents an “elevation” of density.\n\n\nCode\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Filled contours\nsns.kdeplot(x=x_clusters, y=y_clusters, cmap='viridis', fill=True,\n            thresh=0.05, levels=10, ax=axes[0])\naxes[0].set_xlabel('X Variable')\naxes[0].set_ylabel('Y Variable')\naxes[0].set_title('Filled Contour Plot')\nsns.despine(ax=axes[0])\n\n# Line contours with scatter\naxes[1].scatter(x_clusters, y_clusters, alpha=0.1, s=5, c='gray')\nsns.kdeplot(x=x_clusters, y=y_clusters, levels=8, color='red', linewidths=2, ax=axes[1])\naxes[1].set_xlabel('X Variable')\naxes[1].set_ylabel('Y Variable')\naxes[1].set_title('Contour Lines Over Scatter Plot')\nsns.despine(ax=axes[1])\n\nplt.tight_layout()\n\n\n\n\n\nContour plot shows density as topographic lines\n\n\n\n\nWhen do contour plots shine? They excel at overlaying density information on scatter plots, comparing multiple groups with different colored contours, and showing the “shape” of relationships clearly.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 2D Data"
    ]
  },
  {
    "objectID": "m02-visualization/2d-data.html#joint-distributions-combining-2d-and-1d",
    "href": "m02-visualization/2d-data.html#joint-distributions-combining-2d-and-1d",
    "title": "2D Data Visualization",
    "section": "Joint Distributions: Combining 2D and 1D",
    "text": "Joint Distributions: Combining 2D and 1D\nHere’s a powerful approach: show both the joint (2D) distribution and the marginal (1D) distributions of each variable. This connects 2D visualization back to the 1D methods we learned earlier.\nWhat’s a joint plot? It combines a central 2D plot with 1D histograms or density plots along the margins.\n\n\nCode\n# Generate data with interesting marginals\nnp.random.seed(202)\nn = 1000\nx_joint = np.concatenate([np.random.normal(30, 10, n//2), np.random.normal(70, 8, n//2)])\ny_joint = np.concatenate([np.random.normal(40, 12, n//2), np.random.normal(60, 10, n//2)])\n\n# Create joint plot\ng = sns.jointplot(x=x_joint, y=y_joint, kind='scatter', alpha=0.5, height=10)\ng.set_axis_labels('X Variable', 'Y Variable')\ng.fig.suptitle('Joint Distribution with Marginal Histograms', y=1.01)\n\n\nText(0.5, 1.01, 'Joint Distribution with Marginal Histograms')\nJoint plot combining 2D scatter with marginal 1D distributions\n\n\n\n\n\n\n\n\n\nLook at what this reveals. The marginal distributions (top and right) show that both X and Y are bimodal. There are two peaks.\nBut the scatter plot reveals that the peaks are correlated. When X is low, Y tends to be low. When X is high, Y tends to be high.\nThis relationship is invisible in the marginals alone.\nJoint plots can use different visualizations in the center:\n\n\nCode\n# Create joint plot with hexbin and KDE\ng = sns.jointplot(x=x_large, y=y_large, kind='hex', height=10,\n                  marginal_kws=dict(bins=30, fill=True))\ng.set_axis_labels('X Variable', 'Y Variable')\ng.fig.suptitle('Joint Plot: Hexbin Center with KDE Margins', y=1.01)\n\n\nText(0.5, 1.01, 'Joint Plot: Hexbin Center with KDE Margins')\nJoint plot with hexbin center and KDE margins\n\n\n\n\n\n\n\n\n\nOr with KDE everywhere:\n\n\nCode\ng = sns.jointplot(x=x_clusters, y=y_clusters, kind='kde', height=10,\n                  fill=True, cmap='viridis', thresh=0.05)\ng.set_axis_labels('X Variable', 'Y Variable')\ng.fig.suptitle('Joint Plot: All KDE', y=1.01)\n\n\nText(0.5, 1.01, 'Joint Plot: All KDE')\nJoint plot with 2D KDE center and 1D KDE margins\n\n\n\n\n\n\n\n\n\nWhy use joint plots? They’re particularly useful for understanding if marginal distributions are misleading about the relationship, seeing if there’s correlation between variables with interesting univariate structure, and presenting a complete picture of a bivariate relationship.\n\n\n\n\n\n\nNote\n\n\n\nPro tip: When presenting data, start with marginal distributions to establish what each variable looks like. Then show the joint distribution to reveal the relationship. This guides your audience from the familiar (1D) to the complex (2D).",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 2D Data"
    ]
  },
  {
    "objectID": "m02-visualization/2d-data.html#visualizing-relationships-across-groups",
    "href": "m02-visualization/2d-data.html#visualizing-relationships-across-groups",
    "title": "2D Data Visualization",
    "section": "Visualizing Relationships Across Groups",
    "text": "Visualizing Relationships Across Groups\nOften we want to compare relationships across multiple groups or categories. There are several effective approaches.\n\nColor coding by group\nThe simplest approach is to use different colors for different groups:\n\n\nCode\n# Generate multi-group data\nnp.random.seed(303)\nn_per_group = 150\n\ngroup_a_x = np.random.normal(40, 12, n_per_group)\ngroup_a_y = 0.7 * group_a_x + np.random.normal(0, 8, n_per_group)\n\ngroup_b_x = np.random.normal(55, 10, n_per_group)\ngroup_b_y = 1.2 * group_b_x + np.random.normal(-20, 10, n_per_group)\n\ngroup_c_x = np.random.normal(60, 15, n_per_group)\ngroup_c_y = 0.3 * group_c_x + np.random.normal(30, 12, n_per_group)\n\ndf_groups = pd.DataFrame({\n    'x': np.concatenate([group_a_x, group_b_x, group_c_x]),\n    'y': np.concatenate([group_a_y, group_b_y, group_c_y]),\n    'group': ['A'] * n_per_group + ['B'] * n_per_group + ['C'] * n_per_group\n})\n\nfig, ax = plt.subplots(figsize=(10, 6))\nfor group, color in zip(['A', 'B', 'C'], sns.color_palette('muted', 3)):\n    subset = df_groups[df_groups['group'] == group]\n    ax.scatter(subset['x'], subset['y'], label=f'Group {group}',\n               alpha=0.6, s=50, color=color, edgecolors='white', linewidth=0.5)\n\nax.set_xlabel('X Variable')\nax.set_ylabel('Y Variable')\nax.set_title('Relationships Across Groups')\nax.legend()\nsns.despine()\n\n\n\n\n\nScatter plot with color-coded groups\n\n\n\n\nSee the differences? The three groups have different relationships. Group A has a positive moderate slope. Group B has a steeper positive relationship.\nGroup C? Almost no relationship at all.\n\n\n\n\n\n\nSimpson’s Paradox\n\n\n\nBe careful! Sometimes the overall trend (pooling all groups) can be opposite to the trend within each group. This is called Simpson’s Paradox. Always visualize groups separately to check if pooling is appropriate.\n\n\n\n\nSmall multiples (faceting)\nWhen groups overlap heavily or there are many groups, small multiples work better than color coding. Small multiples are separate plots for each group.\n\n\nCode\ng = sns.FacetGrid(df_groups, col='group', height=4, aspect=1.3)\ng.map_dataframe(sns.scatterplot, x='x', y='y', alpha=0.6, s=50)\ng.map_dataframe(sns.regplot, x='x', y='y', scatter=False, color='red')\ng.set_axis_labels('X Variable', 'Y Variable')\ng.set_titles('Group {col_name}')\nsns.despine()\nplt.tight_layout()\n\n\n\n\n\nSmall multiples showing relationship for each group separately\n\n\n\n\nSmall multiples make it easy to compare the strength and direction of relationships across groups without visual clutter.\n\n\nContour overlays\nFor large datasets, overlaying density contours for each group can be very effective:\n\n\nCode\nfig, ax = plt.subplots(figsize=(10, 7))\n\ncolors = sns.color_palette('muted', 3)\nfor group, color in zip(['A', 'B', 'C'], colors):\n    subset = df_groups[df_groups['group'] == group]\n    sns.kdeplot(x=subset['x'], y=subset['y'], levels=5,\n                color=color, linewidths=2, label=f'Group {group}', ax=ax)\n\nax.set_xlabel('X Variable')\nax.set_ylabel('Y Variable')\nax.set_title('Density Contours by Group')\nax.legend()\nsns.despine()\n\n\n/var/folders/j7/9dgqq5g53vnbsbmvh2yqtckr0000gr/T/ipykernel_7560/3335885635.py:12: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n  ax.legend()\n\n\n\n\n\nOverlaid density contours reveal different relationship shapes\n\n\n\n\nLook at the shapes. Groups A and B have elongated, correlated distributions. This indicates strong relationships.\nGroup C is more circular, indicating weak correlation.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 2D Data"
    ]
  },
  {
    "objectID": "m02-visualization/2d-data.html#the-bigger-picture",
    "href": "m02-visualization/2d-data.html#the-bigger-picture",
    "title": "2D Data Visualization",
    "section": "The Bigger Picture",
    "text": "The Bigger Picture\nVisualizing bivariate relationships isn’t just about making pretty pictures. It’s about seeing patterns that summary statistics conceal.\nThink about what happens when you reduce a relationship to a single number. A correlation coefficient, a slope, a p-value. You lose crucial information.\nIs the relationship linear or curved? Are there outliers driving the result? Are there subgroups with different patterns? Is the relationship consistent across the range of your data?\nAnscombe’s Quartet taught us this lesson half a century ago. Yet papers still report correlations without showing scatter plots. Don’t make this mistake.\nThe choice of visualization method matters. Use scatter plots for small to moderate datasets where individual points matter. Use hexbin or heatmaps for large datasets where density matters more than individuals.\nUse 2D KDE and contours for smooth, assumption-light density estimation. Use joint plots for connecting bivariate relationships to univariate distributions.\nBut the most important choice is the simplest: always plot your data. Let your audience see what you see. Trust them to interpret patterns, not just summary statistics.\nAs statistician John Tukey wrote: “The greatest value of a picture is when it forces us to notice what we never expected to see.”",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 2D Data"
    ]
  },
  {
    "objectID": "m01-toolkit/tidy-data.html",
    "href": "m01-toolkit/tidy-data.html",
    "title": "The Tidy Data Philosophy",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module introduces the tidy data philosophy. You’ll learn what makes data “tidy” and what pitfalls to avoid, explore practical tools like melt and pivot to reshape your data, and understand why standardizing data structure makes analysis faster and more reliable.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "The Tidy Data Philosophy"
    ]
  },
  {
    "objectID": "m01-toolkit/tidy-data.html#the-80-problem",
    "href": "m01-toolkit/tidy-data.html#the-80-problem",
    "title": "The Tidy Data Philosophy",
    "section": "The 80% Problem",
    "text": "The 80% Problem\nHave you ever spent hours wrestling with a dataset before you could even start analyzing it? It is often said that 80% of data analysis is spent cleaning and preparing data. This isn’t an exaggeration.\nGetting your data into the right shape makes everything else easier. The good news is that once you understand the tidy data philosophy, you can apply it consistently across projects. If you want to dive deeper, read Tidy Data by Hadley Wickham.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "The Tidy Data Philosophy"
    ]
  },
  {
    "objectID": "m01-toolkit/tidy-data.html#what-is-tidy-data",
    "href": "m01-toolkit/tidy-data.html#what-is-tidy-data",
    "title": "The Tidy Data Philosophy",
    "section": "What is Tidy Data?",
    "text": "What is Tidy Data?\nAt its core, tidy data is a standard way of mapping the meaning of a dataset to its structure. Whether your data is messy or tidy depends entirely on how rows, columns, and tables match up with observations, variables, and types.\nLet’s talk about the three core principles. First, each variable forms its own column. A variable measures the same underlying attribute (like height, temperature, or duration) across different units.\nSecond, each observation forms a row. An observation captures all measurements on the same unit (like a person, a day, or a race) across different attributes.\nThird, each type of observational unit gets its own table. In a study of allergy medication, you’d have separate tables for demographic data, daily medical data, and meteorological data, not one giant table mixing everything together.\nWhy does this matter? Tidy datasets are dramatically easier to manipulate, model, and visualize. They make exploration faster and analysis clearer. Most importantly, they standardize data organization, making your code reusable and reliable.\n\nCommon Pitfalls\nNow let’s flip the perspective and look at the most common mistakes. When you first encounter messy data, it usually falls into one of five patterns.\nThe first problem is that column headers often contain values instead of variable names. Imagine a table where months (“Jan”, “Feb”, “Mar”) are the column headers, rather than having a single “Month” column with those values.\nThe second problem is multiple variables stored in one column. You might find a column like “height_weight” containing values like “5.5_130” instead of splitting those into separate “height” and “weight” columns.\nThe third problem is variables scattered across both rows and columns. A piece of information like gender might be encoded in a specific column and also hidden within the values of another column.\nThe fourth problem is mixing different types of observational units in one table. For example, a single table containing both patient demographic information and medical test results mashes two fundamentally different kinds of data together.\nThe fifth and final problem is splitting a single observational unit across multiple tables. Patient information scattered across one table for addresses, another for test results, and another for appointments, with no clean way to link them together, makes every analysis painful.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "The Tidy Data Philosophy"
    ]
  },
  {
    "objectID": "m01-toolkit/tidy-data.html#tidy-tools",
    "href": "m01-toolkit/tidy-data.html#tidy-tools",
    "title": "The Tidy Data Philosophy",
    "section": "Tidy Tools",
    "text": "Tidy Tools\nHow do we fix messy data? Let’s learn practical tools to reshape your messy data into tidy form. The examples here are adapted from Python for Data Science.\n\nMelt: From Wide to Long\nPicture data stored in “wide” format, where different columns represent different variables of the same type. Consider this simple example:\n\n\nCode\nimport pandas as pd\ndf = pd.DataFrame({'first': ['John', 'Mary'],\n                   'last': ['Smith', 'Doe'],\n                   'height': [5.5, 5.0],\n                   'weight': [130, 110]})\ndf\n\n\n\n\n\n\n\n\n\nfirst\nlast\nheight\nweight\n\n\n\n\n0\nJohn\nSmith\n5.5\n130\n\n\n1\nMary\nDoe\n5.0\n110\n\n\n\n\n\n\n\nNotice how “height” and “weight” sit in separate columns. This wide format breaks tidy principles. It makes comparisons awkward when you want to plot or analyze these measurements together.\nThe pandas.DataFrame.melt() method solves this by transforming data from wide to long format. After melting, instead of separate columns for “height” and “weight”, you get one column for the variable type and another for the value.\nLet’s see it in action:\n\nimport pandas as pd\ndf_melted = df.melt(\n    id_vars=['first', 'last'],\n    var_name='quantity',\n    value_name='value'\n)\ndf_melted\n\n\n\n\n\n\n\n\nfirst\nlast\nquantity\nvalue\n\n\n\n\n0\nJohn\nSmith\nheight\n5.5\n\n\n1\nMary\nDoe\nheight\n5.0\n\n\n2\nJohn\nSmith\nweight\n130.0\n\n\n3\nMary\nDoe\nweight\n110.0\n\n\n\n\n\n\n\nNow each row represents a single measurement for an individual. If you want to compare height and weight, they’re in the same column format, making analysis natural.\nThis is the essence of tidy data.\n\n\nPivot: From Long to Wide\nWhat if you need to go the other direction? Sometimes the opposite problem arises. Your data starts in “long” format, with a separate row for each measurement type (like “cases” or “population”) for each country and year.\nThis scatters information about a single observation across multiple rows, making it hard to see all statistics for country A in 2020 at once.\n\n\nCode\nimport numpy as np\n\n# Long format: each row is a different variable for country and year\ndf = pd.DataFrame({\n    'country': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],\n    'year': [2020, 2021, 2020, 2021, 2020, 2021, 2020, 2021],\n    'variable': ['cases', 'cases', 'population', 'population', 'cases', 'cases', 'population', 'population'],\n    'value': [100, 200, 120, 220, 130, 230, 140, 240]\n})\ndf\n\n\n\n\n\n\n\n\n\ncountry\nyear\nvariable\nvalue\n\n\n\n\n0\nA\n2020\ncases\n100\n\n\n1\nA\n2021\ncases\n200\n\n\n2\nA\n2020\npopulation\n120\n\n\n3\nA\n2021\npopulation\n220\n\n\n4\nB\n2020\ncases\n130\n\n\n5\nB\n2021\ncases\n230\n\n\n6\nB\n2020\npopulation\n140\n\n\n7\nB\n2021\npopulation\n240\n\n\n\n\n\n\n\nThe pivot() function reshapes the data so that each observation (a country-year combination) has its measurements spread across columns. This transforms long data back to a wider, more readable format.\nNow each row shows all measurements for a single country and year, making analysis straightforward.\n\n\nStack and Unstack\nWhat about more complex cases? A trickier situation emerges when your data has multi-level column headers, with variables split across two or more header rows. Imagine measurements for different people and types, with test results for multiple groups all shown as columns.\nThis nested structure makes it hard to access and visualize data cleanly.\n\n\nCode\n# Example: multi-level columns for two participants (P1, P2) and two attributes (A, B)\nheader = pd.MultiIndex.from_product([['P1','P2'],['A','B']])\ndf = pd.DataFrame(np.random.rand(4, 4),\n                  columns=header)\ndf\n\n\n\n\n\n\n\n\n\nP1\nP2\n\n\n\nA\nB\nA\nB\n\n\n\n\n0\n0.489253\n0.878847\n0.547561\n0.729908\n\n\n1\n0.773649\n0.175704\n0.996851\n0.252101\n\n\n2\n0.907468\n0.775998\n0.194114\n0.752454\n\n\n3\n0.949542\n0.154380\n0.728268\n0.137646\n\n\n\n\n\n\n\nThe stack() method solves this by converting one level of column headers into a row index, transforming wide data to long. Now each row represents a single measurement, and all values of the same variable sit together in one column.\nThe unstack() method does the reverse, spreading data back out from the index into columns. Let’s see both methods in action:\n\ndf.stack(future_stack=True)\n\n\n\n\n\n\n\n\n\nP1\nP2\n\n\n\n\n0\nA\n0.489253\n0.547561\n\n\nB\n0.878847\n0.729908\n\n\n1\nA\n0.773649\n0.996851\n\n\nB\n0.175704\n0.252101\n\n\n2\nA\n0.907468\n0.194114\n\n\nB\n0.775998\n0.752454\n\n\n3\nA\n0.949542\n0.728268\n\n\nB\n0.154380\n0.137646\n\n\n\n\n\n\n\n\ndf.stack(future_stack=True).unstack()\n\n\n\n\n\n\n\n\nP1\nP2\n\n\n\nA\nB\nA\nB\n\n\n\n\n0\n0.489253\n0.878847\n0.547561\n0.729908\n\n\n1\n0.773649\n0.175704\n0.996851\n0.252101\n\n\n2\n0.907468\n0.775998\n0.194114\n0.752454\n\n\n3\n0.949542\n0.154380\n0.728268\n0.137646",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "The Tidy Data Philosophy"
    ]
  },
  {
    "objectID": "m01-toolkit/overview.html",
    "href": "m01-toolkit/overview.html",
    "title": "Module 1: Data Science Toolkit",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module introduces the tools and principles of reproducible data science.\nYou’ll learn:\n\nHow version control with Git and GitHub tracks changes and enables effective collaboration.\nWhat data provenance is and why knowing your data’s history builds trust and enables reproducibility.\nHow tidy data principles structure datasets to make analysis faster, clearer, and less error-prone.\nHow to build reproducible environments so others can replicate your work exactly, regardless of platform or time.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Overview"
    ]
  },
  {
    "objectID": "m01-toolkit/overview.html#the-journey",
    "href": "m01-toolkit/overview.html#the-journey",
    "title": "Module 1: Data Science Toolkit",
    "section": "The Journey",
    "text": "The Journey\nLet’s talk about where this module takes you. We begin with the reproducibility crisis and build up to a complete toolkit for trustworthy data science. Each part solves a specific problem that haunts researchers and practitioners.\nVersion Control with Git & GitHub\nHave you ever lost days of work to an accidental overwrite? Version control transforms chaos into clarity. You’ll learn how Git tracks every change, enables collaboration without conflicts, and lets you recover from mistakes instantly.\nData Provenance\nShift your attention from tools to the data itself. Where did your data come from? How was it collected? What transformations were applied? Knowing your data’s complete history is the backbone of good science.\nTidy Data\nStructure matters. When you organize data tidily, analysis becomes straightforward. When data is messy, simple tasks become painful. You’ll learn the principles that distinguish clean datasets from nightmares.\nReproducible Environments & Projects\nYour code works perfectly on your machine today. But will it run on your colleague’s machine tomorrow? Will it work six months from now after library updates? Reproducible environments ensure your work replicates exactly, no matter where or when it runs.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Overview"
    ]
  },
  {
    "objectID": "m01-toolkit/overview.html#why-this-matters",
    "href": "m01-toolkit/overview.html#why-this-matters",
    "title": "Module 1: Data Science Toolkit",
    "section": "Why This Matters",
    "text": "Why This Matters\nHere’s something remarkable. The reproducibility crisis isn’t about fraud. It’s about losing track of details. A colleague asks for your code from last year. You search frantically. Which files? Which version? Which environment produced that final figure?\nThese stories are all too common. What ties them together? The need for provenance, a complete lineage of data and code from origin to final form. Provenance lets others verify your findings and build upon your work.\nA little organization upfront saves hours of pain later. These practices make you a more effective and trustworthy collaborator. They transform scattered files into coherent projects. They turn “it works on my machine” into “it works everywhere.”",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Overview"
    ]
  },
  {
    "objectID": "m01-toolkit/overview.html#prerequisites",
    "href": "m01-toolkit/overview.html#prerequisites",
    "title": "Module 1: Data Science Toolkit",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou should be comfortable with basic Python programming. Familiarity with the command line helps but isn’t required. We’ll teach you the essential terminal operations you need.\nNo prior experience with Git, data management, or environment tools is necessary. This module assumes you’re starting from scratch. By the end, you’ll have solid foundations in reproducible practices.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Overview"
    ]
  },
  {
    "objectID": "m01-toolkit/overview.html#what-youll-build",
    "href": "m01-toolkit/overview.html#what-youll-build",
    "title": "Module 1: Data Science Toolkit",
    "section": "What You’ll Build",
    "text": "What You’ll Build\nBy the end of this module, you’ll track changes with Git and collaborate through GitHub. You’ll structure data clearly using tidy principles. You’ll build replicable environments using conda or virtual environments.\nYou’ll gain practical skills: creating repositories, writing clear commit messages, managing branches, documenting data sources, and configuring project dependencies. Most importantly, you’ll develop habits that make reproducibility automatic rather than an afterthought.\nLet’s begin by tackling the reproducibility crisis head-on.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Overview"
    ]
  },
  {
    "objectID": "m01-toolkit/data-provenance.html",
    "href": "m01-toolkit/data-provenance.html",
    "title": "Data Provenance: The Secret Life of Data",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module introduces data provenance, the complete record of where your data came from and what happened to it.\nYou’ll learn:\n\nWhat data provenance means and why it’s critical for reproducible science.\nHow a spreadsheet error by Harvard economists reshaped economic policy worldwide.\nThe three essential tools for tracking data transformations in your projects.\nPractical consequences for trust, error prevention, and scientific integrity.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Data Provenance"
    ]
  },
  {
    "objectID": "m01-toolkit/data-provenance.html#the-mystery-of-the-disappearing-data",
    "href": "m01-toolkit/data-provenance.html#the-mystery-of-the-disappearing-data",
    "title": "Data Provenance: The Secret Life of Data",
    "section": "The Mystery of the Disappearing Data",
    "text": "The Mystery of the Disappearing Data\nHave you ever opened a dataset you worked on a few months ago, only to find that you have no idea where it came from? You can’t remember what the columns mean or what transformations you applied. This experience is surprisingly common in data science.\nThink of data provenance as the complete story of your data. It answers the who, what, when, where, and why of your data’s journey from raw form to its current state. Understanding this story is crucial for doing good science and catching errors before they spread.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Data Provenance"
    ]
  },
  {
    "objectID": "m01-toolkit/data-provenance.html#a-tale-of-two-economists-and-a-spreadsheet-error",
    "href": "m01-toolkit/data-provenance.html#a-tale-of-two-economists-and-a-spreadsheet-error",
    "title": "Data Provenance: The Secret Life of Data",
    "section": "A Tale of Two Economists and a Spreadsheet Error",
    "text": "A Tale of Two Economists and a Spreadsheet Error\n\nPicture this: in 2010, two Harvard economists named Carmen Reinhart and Kenneth Rogoff published a paper called “Growth in a Time of Debt.” Their main argument was simple and powerful: countries with high government debt have lower economic growth. The paper became incredibly influential, shaping policy decisions around the world and justifying austerity measures in multiple countries.\nThen in 2013, a graduate student named Thomas Herndon decided to reproduce their results. He couldn’t. After persistent effort, he obtained their original spreadsheet and discovered a simple but catastrophic error: they had accidentally excluded the first five countries from their analysis.\nWhen Herndon corrected that single mistake, the paper’s main finding vanished. Without a clear, documented record of how data was processed, errors slip through unnoticed and spread far. The consequences can reshape entire economies.\nWant the full story? Read The Reinhart-Rogoff Error.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Data Provenance"
    ]
  },
  {
    "objectID": "m01-toolkit/data-provenance.html#the-data-detectives-toolkit",
    "href": "m01-toolkit/data-provenance.html#the-data-detectives-toolkit",
    "title": "Data Provenance: The Secret Life of Data",
    "section": "The Data Detective’s Toolkit",
    "text": "The Data Detective’s Toolkit\nLet’s talk about how you actually track provenance. There are three essential approaches.\nFirst, keep a lab notebook. This can be physical or digital, but it should record where your data came from, what you did to it, and why you made each decision. A good lab notebook becomes the narrative companion to your code.\nSecond, embrace scripting. When you process data with code (Python, R, or anything else), your scripts become documentation. They show exactly what transformations happened. If you version control those scripts, you have a complete history of changes.\nThird, for complex projects, consider workflow management tools like Snakemake or Nextflow. These tools let you define and track your entire analysis pipeline, automatically recording which data went into which step and what came out.\nWhy do all this? When you embrace these provenance practices, you become a more trustworthy data scientist. You’ll trust your own results because you know exactly how they were created. Others will trust them too.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Data Provenance"
    ]
  },
  {
    "objectID": "m01-toolkit/data-provenance.html#further-reading",
    "href": "m01-toolkit/data-provenance.html#further-reading",
    "title": "Data Provenance: The Secret Life of Data",
    "section": "Further Reading",
    "text": "Further Reading\nReady to go deeper? Check out What is Data Provenance and Why is it Important? and Data Provenance: What It Is, Why It Matters, and How to Implement It for more perspectives.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Data Provenance"
    ]
  },
  {
    "objectID": "course/setup.html",
    "href": "course/setup.html",
    "title": "Setup",
    "section": "",
    "text": "We’ll use Python to work with data throughout this course. Python is an excellent choice for deep learning and complex systems analysis for its rich ecosystem of libraries, readable and intuitive syntax, and well-documented documentation.",
    "crumbs": [
      "Home",
      "Course Information",
      "Setup"
    ]
  },
  {
    "objectID": "course/setup.html#programming-language",
    "href": "course/setup.html#programming-language",
    "title": "Setup",
    "section": "",
    "text": "We’ll use Python to work with data throughout this course. Python is an excellent choice for deep learning and complex systems analysis for its rich ecosystem of libraries, readable and intuitive syntax, and well-documented documentation.",
    "crumbs": [
      "Home",
      "Course Information",
      "Setup"
    ]
  },
  {
    "objectID": "course/setup.html#create-a-virtual-environment-via-uv",
    "href": "course/setup.html#create-a-virtual-environment-via-uv",
    "title": "Setup",
    "section": "Create a Virtual Environment via uv",
    "text": "Create a Virtual Environment via uv\nWe strongly recommend using virtual environments to manage your Python packages. Virtual environments create isolated Python installations for each project, avoiding dependency hell and providing several key benefits:\n\n\n\n\n\n\nNote\n\n\n\nDon’t confuse Python virtual environments with virtual machines (VMs). Python virtual environments are lightweight isolation tools that only separate Python packages and dependencies within the same operating system. Virtual machines, on the other hand, create complete isolated operating systems.\n\n\n\n\n\n\n\n\nFigure 1: Without virtual environments, you risk dependency hell where package conflicts make your projects unusable.\n\n\n\nWe recommend using uv. uv is a fast Python package and project manager. While we won’t be running uv commands directly in this course, you’ll need uv to properly run Marimo notebooks, which provides a much better development experience. See here for installation instructions.\nFollow the following steps to install uv, along with the minimum Python packages required for this course.\n\nInstall uv\nRun the following command to create a new environment with the minimum Python packages required for this course.\n\nuv venv -p 3.11\nuv pip install matplotlib scipy numpy pandas seaborn pytorch torchvision marimo\n\nActivate the environment.\n\nsource .venv/bin/activate",
    "crumbs": [
      "Home",
      "Course Information",
      "Setup"
    ]
  },
  {
    "objectID": "course/setup.html#jupyter-marimo-notebook",
    "href": "course/setup.html#jupyter-marimo-notebook",
    "title": "Setup",
    "section": "Jupyter Marimo Notebook",
    "text": "Jupyter Marimo Notebook\nWe’ll use Marimo (GitHub) notebooks for assignments and interactive exercises throughout the course. Marimo is a reactive Python notebook that automatically updates when you change code, making it perfect for exploring deep learning models and seeing results in real-time.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\nMarimo integrates especially tightly with uv and provides a package sandbox feature that lets you inline dependencies directly in notebook files. This is the easiest way to get started - no prior uv knowledge required.\nCreating a sandboxed notebook:\nuvx run --sandbox my_notebook.py\nThis command installs marimo in a temporary environment, tracks your dependencies and stores them in the notebook file, and automatically downloads any existing dependencies.",
    "crumbs": [
      "Home",
      "Course Information",
      "Setup"
    ]
  },
  {
    "objectID": "course/minidora-usage.html#getting-started-with-minidora",
    "href": "course/minidora-usage.html#getting-started-with-minidora",
    "title": "Using Minidora",
    "section": "Getting Started with Minidora",
    "text": "Getting Started with Minidora\nMinidora is your personal AI tutor available 24/7 through Discord to help you master deep learning and complex systems concepts. She’s designed to provide personalized learning support, answer questions about course materials, and guide you through challenging topics with patience and clarity. To interact with Minidora, simply use Discord slash commands or mention her directly in any channel or thread where she’s present.\nCheck out the instruction here on how to use Minidora: Minidora Usage. Minidora is available on Discord, and you can find the invitation link on the email sent in the first week of the semester. Or you can find the invitation link on the Brightspace.\n\n\n\n\n\n\nNote\n\n\n\nSome students could not find Minidora on Discord. The easiest way to get around this is:\n\nGo to the course discord server\nOpen the “applied-soft-computing” channel.\nClick the Minidora icon and send a direct message\nType “/” and see if the available commands are shown.",
    "crumbs": [
      "Home",
      "Course Information",
      "Using Minidora"
    ]
  },
  {
    "objectID": "course/minidora-usage.html#asking-questions",
    "href": "course/minidora-usage.html#asking-questions",
    "title": "Using Minidora",
    "section": "Asking Questions",
    "text": "Asking Questions\nThe most straightforward way to get help is using the /ask command followed by your question. For example, suppose that you want to ask about a subject (Word embeddings) in module 3.\n\nType /ask then type space.\nType your question (e.g., What are word embeddings and how does Word2Vec work?)\nType space\nYou will be prompted to specify the module id. The id consists of “m”. For example, if it is module 3, you should type m03. Type the module id.\nThen type enter.\n\nMinidora will then read the lecture content and provide an explanation.",
    "crumbs": [
      "Home",
      "Course Information",
      "Using Minidora"
    ]
  },
  {
    "objectID": "course/minidora-usage.html#natural-conversations-and-interactive-learning",
    "href": "course/minidora-usage.html#natural-conversations-and-interactive-learning",
    "title": "Using Minidora",
    "section": "Natural Conversations and Interactive Learning",
    "text": "Natural Conversations and Interactive Learning\nFor a more conversational experience, use the /chat command which allows you to interact with Minidora in a natural, free-flowing manner. You can say things like /chat I'm confused about transformers, can you explain the attention mechanism step by step? or /chat Can you help me debug this Python code for training a CNN? Minidora will engage in back-and-forth dialogue, ask clarifying questions, and adapt her explanations based on your responses.\nNote that /chat does not contextualize the Minidora to the course materials. That means that it does not read the lecture content and interact with the students with its built-in knowledge.",
    "crumbs": [
      "Home",
      "Course Information",
      "Using Minidora"
    ]
  },
  {
    "objectID": "course/minidora-usage.html#quizzes-and-assessment",
    "href": "course/minidora-usage.html#quizzes-and-assessment",
    "title": "Using Minidora",
    "section": "Quizzes and Assessment",
    "text": "Quizzes and Assessment\nTo test your understanding and reinforce learning, Minidora offers intelligent quiz features through the /quiz command. She can generate concept-based questions using /concept-quiz m03 multiple-choice for theoretical understanding, or coding challenges with /code-quiz m03 to practice implementation skills. Minidora tracks your progress and adapts quiz difficulty based on your performance, focusing on areas where you need more practice. You can also request quizzes on specific topics by adding subject keywords, such as /quiz m04 convolutional neural networks.",
    "crumbs": [
      "Home",
      "Course Information",
      "Using Minidora"
    ]
  },
  {
    "objectID": "course/minidora-usage.html#tracking-your-progress",
    "href": "course/minidora-usage.html#tracking-your-progress",
    "title": "Using Minidora",
    "section": "Tracking Your Progress",
    "text": "Tracking Your Progress\nUse the /status command to monitor your learning journey and see detailed insights about your progress. Minidora provides different status views: /status summary gives you a quick overview of questions asked and concepts mastered, while /status concepts shows which topics you’ve learned and what to study next. The /status profile command reveals your personalized learning profile, including your preferred difficulty level, learning style, and areas where you excel or need additional support. This helps Minidora provide increasingly personalized assistance as you continue learning.",
    "crumbs": [
      "Home",
      "Course Information",
      "Using Minidora"
    ]
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Home",
    "section": "Course Overview",
    "text": "Course Overview\nThis course explores how deep learning serves as a powerful tool to operationalize high-dimensional data from social and physical systems. You’ll learn to model complex system dynamics using modern computational intelligence, and in turn, use perspectives from complexity science to understand the emergent behaviors of large-scale models.\nThe course combines:\n\nHands-on coding with real data from text, images, and networks\nTheoretical foundations of deep learning and complex systems\nReproducible data science practices with modern tools\nEthical considerations in AI and computational modeling",
    "crumbs": [
      "Home",
      "Course Information",
      "Home"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Home",
    "section": "Getting Started",
    "text": "Getting Started\n\nRead the Welcome page\nLearn About Us\nJoin our Discord server\nFollow the Setup Guide\nLearn How to Submit Assignments",
    "crumbs": [
      "Home",
      "Course Information",
      "Home"
    ]
  },
  {
    "objectID": "m01-toolkit/git-github.html",
    "href": "m01-toolkit/git-github.html",
    "title": "Git & GitHub: Your Time Machine for Code",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module introduces Git and GitHub, your tools for tracking code changes and collaborating.\nYou’ll learn:\n\nWhy version control matters through real-world disasters.\nHow Git saves snapshots of your work like a time machine.\nHow to use GitHub to backup your code and work with others.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Version Control with Git & GitHub"
    ]
  },
  {
    "objectID": "m01-toolkit/git-github.html#the-day-the-code-disappeared",
    "href": "m01-toolkit/git-github.html#the-day-the-code-disappeared",
    "title": "Git & GitHub: Your Time Machine for Code",
    "section": "The Day the Code Disappeared",
    "text": "The Day the Code Disappeared\nPicture this: you’re working late on a project, and you finally fix a major bug. Your fingers are on the keyboard when you suddenly delete a line by accident, then save. You go home.\nThe next morning, you open your laptop and realize all your work from yesterday is gone. The bug fix vanished. You have no idea what you did to fix it, and you have to start from scratch.\nEven big companies get it wrong. In 2017, GitLab, a major code hosting platform, suffered a catastrophic outage. A system administrator accidentally deleted massive amounts of production data.\nThe backups didn’t work. The company lost six hours of customer data. That’s a lifetime in the software world. The details are uncomfortable reading. Check out the GitLab post-mortem if you want to see what went wrong.\n\n\nWe accidentally deleted production data and might have to restore from backup. Google Doc with live notes https://t.co/EVRbHzYlk8\n\n— GitLab.com Status ((gitlabstatus?)) February 1, 2017",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Version Control with Git & GitHub"
    ]
  },
  {
    "objectID": "m01-toolkit/git-github.html#lets-talk-about-version-control",
    "href": "m01-toolkit/git-github.html#lets-talk-about-version-control",
    "title": "Git & GitHub: Your Time Machine for Code",
    "section": "Let’s Talk About Version Control",
    "text": "Let’s Talk About Version Control\nVersion control is essential for any data-related work. A version control system (VCS) saves “snapshots” of your files. Think of it as a time machine: you can travel back to any snapshot and see exactly what your code looked like at that moment.\nThe most popular VCS today is Git. When you put Git in the cloud (like on GitHub), something magical happens. You can access your work from anywhere, share it with teammates, and back it up automatically.\nWhy does this matter so much? Because code changes constantly. You fix one bug and create another. You experiment with new features that don’t work out. Without version control, you’re always one mistake away from losing everything.\nHere’s a quick intro video to get you oriented:",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Version Control with Git & GitHub"
    ]
  },
  {
    "objectID": "m01-toolkit/git-github.html#where-to-start-learning",
    "href": "m01-toolkit/git-github.html#where-to-start-learning",
    "title": "Git & GitHub: Your Time Machine for Code",
    "section": "Where to Start Learning",
    "text": "Where to Start Learning\nThe best way to learn Git is by doing. Start with A Layman’s Introduction to Git for a readable overview. Then move to the Interactive Git Tutorial, which teaches Git visually through interactive exercises.\nWhat if you want deeper documentation? The Git Documentation provides comprehensive coverage of every feature. The Atlassian Git Tutorials offer detailed examples with clear explanations.\nHere’s a practical tip. If you’re just starting out, use GitHub Desktop instead of the command line. It gives you a graphical interface that makes Git concepts visible.\nYou can see branches, commits, and changes in a way that’s much easier to understand than terminal output. The GitHub Desktop Documentation walks you through the basics.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Version Control with Git & GitHub"
    ]
  },
  {
    "objectID": "m01-toolkit/reproduceability.html",
    "href": "m01-toolkit/reproduceability.html",
    "title": "Reproducible Environments & Projects",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module teaches you how to build truly reproducible projects.\nYou’ll learn:\n\nWhat reproducibility means in computational research and why it matters.\nHow to use virtual environments with tools like uv to isolate project dependencies.\nHow workflow management tools like Snakemake automate complex analysis pipelines.\nThe essential components of comprehensive documentation that make projects understandable.\nBest practices for project organization that prevent errors and enable collaboration.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Reproducibility"
    ]
  },
  {
    "objectID": "m01-toolkit/reproduceability.html#the-importance-of-reproducibility",
    "href": "m01-toolkit/reproduceability.html#the-importance-of-reproducibility",
    "title": "Reproducible Environments & Projects",
    "section": "The Importance of Reproducibility",
    "text": "The Importance of Reproducibility\nHave you ever tried to run someone else’s code, only to watch it fail mysteriously on your machine? Or worse, have you ever returned to your own project after a few months and found it no longer works?\nReproducibility is a cornerstone of good science and computational research. At its core, it means this: an independent researcher should be able to duplicate your results using the same materials and procedures. In data science, your code must produce the exact same outputs every single time, given the same input data, regardless of which machine runs it.\nLet’s clarify a key distinction. Reproducibility asks: can an independent researcher achieve the exact same results using your data and code? This is a computational challenge. Replicability asks: can an independent researcher confirm your scientific conclusions by conducting a brand new, independent study? This is a scientific challenge.\nWe focus here on computational reproducibility. It’s the essential first step. Without it, replication becomes impossible.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Reproducibility"
    ]
  },
  {
    "objectID": "m01-toolkit/reproduceability.html#pillars-of-a-reproducible-project",
    "href": "m01-toolkit/reproduceability.html#pillars-of-a-reproducible-project",
    "title": "Reproducible Environments & Projects",
    "section": "Pillars of a Reproducible Project",
    "text": "Pillars of a Reproducible Project\nLet’s talk about what makes a project truly reproducible. Achieving reproducibility requires a conscious effort and the right set of tools.\nWe can think of a reproducible project as having three main pillars: Virtual Environments, Workflow automation, and Comprehensive documentation.\n\nVirtual Environments\nA virtual environment is an isolated container holding all the packages and dependencies your project needs. Without one, you risk “dependency hell.”\nPicture the problem. You have project A using pandas 1.3 and project B using pandas 2.0. Without virtual environments, your system has only one pandas version installed, and one project breaks. Virtual environments solve this by giving each project its own isolated package space.\nFor this course, we recommend uv. It’s a modern, blazingly fast Python package and environment manager written in Rust. Unlike older tools like pip and venv, uv combines both package installation and environment creation into one tool.\nThis integrated approach and high performance can dramatically speed up your Python workflows. Ready to get started? Check the uv documentation.\n\n\n\n\n\n\nNote\n\n\n\nWhy not conda? Conda is a mature, well-used tool in the Python community. Unlike uv, conda can manage non-Python dependencies like compilers or optimized BLAS libraries. This flexibility is powerful if you need system-level packages.\nHowever, this same flexibility creates complexity. Conda environments can develop intricate dependencies that make them harder to replicate. uv focuses solely on Python packages, which actually simplifies reproducibility. Less flexibility sometimes means better reproducibility.\n\n\n\n\nWorkflow Management\nAs your project grows, the order of steps matters. One script preprocesses data, another trains a model, a third generates figures. Run them manually in the wrong order, or forget which ones depend on which, and errors creep in silently.\n\n\n\n\n\n\nFigure 1: This diagram shows a research project as a workflow. Each box is a script, and connections show data flowing from one script to the next. As projects grow, these dependencies become impossible to track mentally.\n\n\n\nWhat’s the solution? A workflow management tool automates this complexity. You define the steps and their dependencies, and the tool figures out the correct order, runs them all, and even parallelizes where possible.\nSnakemake is excellent for this. It uses a readable, Python-based language where you define rules: here’s how to create output files from input files. Snakemake handles the rest. Want to learn more? Watch this introductory video.\nOne principle matters deeply: make individual scripts atomic. One script preprocesses data. Another trains a model. A third generates figures. When scripts are minimal and focused, they become readable, reusable, and easy to debug.\n\n\nComprehensive Documentation\nCode and data are not self-explanatory. Comprehensive documentation is crucial for anyone reading your project (including your future self) to understand the what, why, and how.\nEvery project should have a README.md file in its root directory. This file explains what the project is about, what the files are, and how to run the analysis.\nA strong README includes a few key sections. Start with a clear project description explaining what it does and its goals. Add a file structure summary so readers can navigate. If you include example datasets, describe the data table structure, including column names and formats.\nProvide installation instructions so people can set up the environment. Explain usage with examples of how to run the main scripts. Finally, add contact and license information so readers know who to ask and under what terms they can use your work.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Reproducibility"
    ]
  },
  {
    "objectID": "m01-toolkit/reproduceability.html#project-organization-best-practices",
    "href": "m01-toolkit/reproduceability.html#project-organization-best-practices",
    "title": "Reproducible Environments & Projects",
    "section": "Project Organization Best Practices",
    "text": "Project Organization Best Practices\nBeyond the core tools, organizing your project thoughtfully is crucial for long-term reproducibility and collaboration.\n\nFile and Directory Structure\nA logical file structure makes it easy for others (and for future you) to find things. Start with descriptive naming. Give files and directories clear, self-explanatory names so you can find them via search.\nInstead of analysis.py, use preprocess_customer_data.py. For files that evolve over time (like datasets or reports), consider adding timestamps. Name a report 2025-10-20_survey_analysis.qmd instead of report.qmd. This lets you instantly identify which version is which.\nWant to dive deeper? Check out How to name files and A Simple File Management System.\n\n\nWriting Clean Code\nHere’s a simple truth: code quality directly impacts reproducibility. If your code is hard to read, it’s hard to verify. It’s hard to reuse. And it’s hard for someone else (or your future self) to understand why you made each choice.\nThe book Clean Code: A Handbook of Agile Software Craftsmanship by Robert C. Martin is an excellent guide to writing code that’s readable, maintainable, and trustworthy.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Reproducibility"
    ]
  },
  {
    "objectID": "m01-toolkit/reproduceability.html#bringing-it-together",
    "href": "m01-toolkit/reproduceability.html#bringing-it-together",
    "title": "Reproducible Environments & Projects",
    "section": "Bringing It Together",
    "text": "Bringing It Together\nBy combining virtual environments, comprehensive documentation, workflow management, and thoughtful project organization, you’ve now built the foundation for reproducible computational projects. These practices protect your work, enable collaboration, and most importantly, help you trust your own results.",
    "crumbs": [
      "Home",
      "Module 1: The Data Scientist's Toolkit",
      "Reproducibility"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html",
    "href": "m02-visualization/1d-data.html",
    "title": "Visualizing One-Dimensional Data: Show Me the Data!",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module teaches you how to visualize one-dimensional data honestly and clearly.\nYou’ll learn:\n\nWhy hiding raw data behind summary statistics is misleading and how to show all the data whenever possible.\nHow to use swarm plots, strip plots, and barcode plots to visualize individual data points effectively.\nThe principles of histograms and kernel density estimation for summarizing distributions.\nHow cumulative distribution functions reveal patterns in heavy-tailed data without arbitrary parameter choices.\nPractical strategies for choosing the right visualization based on your dataset’s size and characteristics.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#the-case-of-the-missing-data-points",
    "href": "m02-visualization/1d-data.html#the-case-of-the-missing-data-points",
    "title": "Visualizing One-Dimensional Data: Show Me the Data!",
    "section": "The Case of the Missing Data Points",
    "text": "The Case of the Missing Data Points\nImagine you’re reading a research paper that claims “Treatment A is significantly better than Treatment B.” The paper shows a bar chart with two bars and error bars. The difference looks impressive. But here’s the question: what does the actual data look like? Are there 5 data points per group? 500? Are they normally distributed, or are there outliers? Are most points clustered together, or spread out?\n\n\n\n\n\nWithout seeing the raw data, you’re flying blind. And unfortunately, many scientific papers and reports make this same mistake: they summarize data without showing it.\nThe golden rule of data visualization: Show all the data, whenever possible.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#why-showing-all-data-matters",
    "href": "m02-visualization/1d-data.html#why-showing-all-data-matters",
    "title": "Visualizing One-Dimensional Data: Show Me the Data!",
    "section": "Why Showing All Data Matters",
    "text": "Why Showing All Data Matters\nIn 2016, a group of researchers analyzed 118 papers in leading neuroscience journals and found something disturbing: when they requested the raw data and re-analyzed it, they found that the bar charts in many papers were misleading. The bar charts suggested clear differences between groups, but the raw data often told a different story. There was substantial overlap between groups, unexpected distributions, or influential outliers.\nThis isn’t about fraud. It’s about the limitations of summary statistics. When you reduce your data to a mean and a standard error, you lose a tremendous amount of information. The data might be bimodal, skewed, or contain outliers. These patterns are invisible in a bar chart, but they’re crucial for understanding what’s really going on.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#dynamite-plots-must-die",
    "href": "m02-visualization/1d-data.html#dynamite-plots-must-die",
    "title": "Visualizing One-Dimensional Data: Show Me the Data!",
    "section": "Dynamite Plots Must Die",
    "text": "Dynamite Plots Must Die\nStatisticians have been campaigning against bar charts with error bars—called “dynamite plots”—for years. Yet a systematic review found that 85.6% of papers in top physiology journals still use them. They appear everywhere: Nature, Science, Cell.\nWhy is this a problem? A dynamite plot shows you exactly four numbers (two means and two standard errors), regardless of sample size. But worse, completely different datasets produce identical bar charts. A dataset with outliers, a uniform distribution, or a bimodal distribution can all generate the same plot.\nWhen Rafael Irizarry showed the actual data behind a blood pressure comparison, the story changed dramatically. The bar chart showed a clear, significant difference. But the raw data revealed an extreme outlier (possibly a data entry error) and substantial overlap between groups. Remove that single outlier, and the result was no longer significant.\nAs Irizarry put it in his open letter to journal editors: dynamite plots conceal the data rather than showing it. The solution? Show the actual data points whenever possible, and use distributions (boxplots, histograms, density plots) when you can’t.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#start-simple-show-every-point",
    "href": "m02-visualization/1d-data.html#start-simple-show-every-point",
    "title": "Visualizing One-Dimensional Data: Show Me the Data!",
    "section": "Start Simple: Show Every Point",
    "text": "Start Simple: Show Every Point\n\nSwarm Plots (Beeswarm Plots)\nThe most straightforward approach is to plot every single data point. A swarm plot (also called a beeswarm plot) does exactly this: it displays each observation as a point, with points arranged to avoid overlap.\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate sample data\nnp.random.seed(42)\ngroup_a = np.random.normal(100, 15, 30)\ngroup_b = np.random.normal(120, 20, 30)\ndata = {'Value': np.concatenate([group_a, group_b]),\n        'Group': ['A']*30 + ['B']*30}\n\n# Create swarm plot\nsns.swarmplot(data=data, x='Group', y='Value')\nplt.title('Swarm Plot: Every Point Visible')\nplt.show()\n\n\n\n\n\nSwarm plot: Every point visible\n\n\n\n\nSwarm plots are perfect for small to moderate datasets (roughly up to 100-200 points per group). They let you see the actual sample size, so you know how much data went into each group. You can immediately perceive the distribution shape, spotting whether your data is symmetric, skewed, or bimodal. Individual outliers jump out visually, rather than being hidden in summary statistics. And you can see the overall spread of the data with a single glance.\n\n\nThe Limits of Swarm Plots\nBut what happens when you have more data? With hundreds or thousands of points, swarm plots become cluttered and difficult to read. The points start to pile up, and the plot becomes a blob. This is where we need more sophisticated techniques.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#handling-more-data-transparency-and-jittering",
    "href": "m02-visualization/1d-data.html#handling-more-data-transparency-and-jittering",
    "title": "Visualizing One-Dimensional Data: Show Me the Data!",
    "section": "Handling More Data: Transparency and Jittering",
    "text": "Handling More Data: Transparency and Jittering\n\nStrip Plots with Jittering\nWhen you have too many points for a swarm plot, a strip plot with jittering can help. Instead of carefully arranging points to avoid overlap, we add random noise (jitter) to the x-position of each point.\n\n\nCode\nsns.stripplot(data=data, x='Group', y='Value', alpha=0.6, jitter=0.2)\nplt.title('Strip Plot with Jittering')\nplt.show()\n\n\n\n\n\nStrip plot with jittering\n\n\n\n\nTwo key parameters control the appearance. The alpha parameter controls transparency (0 = invisible, 1 = opaque), and values around 0.3-0.7 work well to see overlapping points. The jitter parameter controls the amount of random horizontal displacement. Too much jitter and your groups start to overlap and become confusing, while too little and points stack vertically in dense columns.\n\n\nBarcode Plots (Rug Plots)\nFor even larger datasets, consider a barcode plot (also called a rug plot). This shows each data point as a small vertical tick mark along an axis. It’s minimalist but effective for showing the distribution of many points.\n#| fig-cap: \"Barcode plot\"\n#| fig-width: 12\n#| fig-height: 10\n#| code-fold: true\nfig, ax = plt.subplots(figsize=(10, 2))\nfor i, group in enumerate(['A', 'B']):\n    values = data[data['Group'] == group]['Value']\n    ax.plot(values, [i]*len(values), '|', markersize=10, alpha=0.7)\nax.set_yticks([0, 1])\nax.set_yticklabels(['A', 'B'])\nax.set_xlabel('Value')\nax.set_title('Barcode Plot')\nplt.show()\nBarcode plots work well when you have thousands of points and want to show density patterns without losing the “raw data” feel.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#summarizing-distributions-histograms",
    "href": "m02-visualization/1d-data.html#summarizing-distributions-histograms",
    "title": "Visualizing One-Dimensional Data: Show Me the Data!",
    "section": "Summarizing Distributions: Histograms",
    "text": "Summarizing Distributions: Histograms\nWhen your dataset is large enough that individual points become impractical to show, you need to summarize the distribution. The most common approach is the histogram.\nA histogram divides your data range into bins and counts how many observations fall into each bin. It’s a powerful tool for understanding the shape of your distribution.\n\n\nCode\nplt.hist(group_a, bins=15, alpha=0.5, label='Group A', edgecolor='black')\nplt.hist(group_b, bins=15, alpha=0.5, label='Group B', edgecolor='black')\nplt.xlabel('Value')\nplt.ylabel('Count')\nplt.legend()\nplt.title('Histogram: Distribution Comparison')\nplt.show()\n\n\n\n\n\nHistogram: Distribution Comparison\n\n\n\n\n\nThe Art of Choosing Bins\nThe number of bins dramatically affects how your histogram looks. With too few bins, you lose detail and might miss important features like bimodality. With too many bins, the histogram becomes noisy and hard to interpret.\nA good starting point is the Sturges’ rule: number of bins = \\log_2(n) + 1, where n is the sample size. But always experiment! Try different bin numbers and see what reveals the most about your data’s structure.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#smooth-alternatives-kernel-density-estimation",
    "href": "m02-visualization/1d-data.html#smooth-alternatives-kernel-density-estimation",
    "title": "Visualizing One-Dimensional Data: Show Me the Data!",
    "section": "Smooth Alternatives: Kernel Density Estimation",
    "text": "Smooth Alternatives: Kernel Density Estimation\nHistograms have a problem: they’re sensitive to bin width and bin placement. Move your bins slightly, and the histogram can look quite different.\nKernel Density Estimation (KDE) provides a smooth alternative. Instead of binning, KDE places a small “kernel” (usually a Gaussian curve) at each data point and sums them up. The result is a smooth density curve.\n\n\nCode\nsns.kdeplot(data=group_a, label='Group A', fill=True, alpha=0.5)\nsns.kdeplot(data=group_b, label='Group B', fill=True, alpha=0.5)\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.legend()\nplt.title('Kernel Density Estimate')\nplt.show()\n\n\n\n\n\nKernel Density Estimate\n\n\n\n\nKDE plots are elegant and reveal the shape of your distribution without the arbitrary choices of histograms. However, they can be misleading at the edges of your data and may suggest data exists where it doesn’t.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#for-heavy-tailed-data-cumulative-distributions",
    "href": "m02-visualization/1d-data.html#for-heavy-tailed-data-cumulative-distributions",
    "title": "Visualizing One-Dimensional Data: Show Me the Data!",
    "section": "For Heavy-Tailed Data: Cumulative Distributions",
    "text": "For Heavy-Tailed Data: Cumulative Distributions\nSome data are extremely heterogeneous. Think income distributions, city populations, or earthquake magnitudes. These distributions often have heavy tails: most values are small, but a few are enormous.\nFor this kind of data, histograms and KDE plots can be misleading because they compress the tail into a tiny region of the plot.\n\nCumulative Distribution Function (CDF)\nThe cumulative distribution function shows the proportion of data points less than or equal to each value. Instead of asking “how many points are in this bin?”, the CDF asks “what fraction of points are below this value?”\nThe CDF is a density estimation method that requires no parameter choices. Unlike histograms (which require bin size) or KDE (which requires bandwidth), the CDF is completely determined by your data. There are no arbitrary decisions that change how your data looks—making it one of the most honest ways to visualize a distribution.\n\n\nCode\nsorted_a = np.sort(group_a)\ncdf_a = np.arange(1, len(sorted_a) + 1) / len(sorted_a)\n\nsorted_b = np.sort(group_b)\ncdf_b = np.arange(1, len(sorted_b) + 1) / len(sorted_b)\n\nplt.plot(sorted_a, cdf_a, label='Group A', linewidth=2)\nplt.plot(sorted_b, cdf_b, label='Group B', linewidth=2)\nplt.xlabel('Value')\nplt.ylabel('Cumulative Probability')\nplt.legend()\nplt.title('Cumulative Distribution Function')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\nCumulative Distribution Function\n\n\n\n\nThe CDF has several advantages. First, there are no binning decisions to make, so every data point is effectively shown. Second, percentiles are easy to read: the median is where CDF = 0.5, and any other percentile is easy to find. Third, it’s great for comparisons because differences between groups are easy to spot visually.\n\n\nComplementary Cumulative Distribution Function (CCDF)\nFor heavy-tailed distributions, the complementary cumulative distribution function (CCDF) is even more useful. The CCDF shows the proportion of data points greater than each value: CCDF(x) = 1 - CDF(x).\nThe magic of the CCDF is that when plotted on a log-log scale, power-law distributions appear as straight lines. This makes it the go-to tool for studying phenomena with power-law behavior. Think income and wealth distributions, city size distributions, social network degree distributions, and earthquake magnitudes. All of these follow power-law patterns where the tail is as important as the bulk of the distribution.\n\n\nCode\n# Generate heavy-tailed data\nheavy_tailed = np.random.pareto(2, 1000) + 1\n\nsorted_data = np.sort(heavy_tailed)\nccdf = 1 - (np.arange(1, len(sorted_data) + 1) / len(sorted_data))\n\nplt.loglog(sorted_data, ccdf, 'o', alpha=0.5, markersize=3)\nplt.xlabel('Value')\nplt.ylabel('P(X ≥ x)')\nplt.title('Complementary Cumulative Distribution (CCDF)')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\nComplementary Cumulative Distribution Function\n\n\n\n\nThe CCDF reveals the tail behavior that’s invisible in traditional histograms. For heterogeneous, heavy-tailed data, it’s an essential tool.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#choosing-the-right-visualization",
    "href": "m02-visualization/1d-data.html#choosing-the-right-visualization",
    "title": "Visualizing One-Dimensional Data: Show Me the Data!",
    "section": "Choosing the Right Visualization",
    "text": "Choosing the Right Visualization\nHere’s a quick decision guide:\n\n\n\n\n\n\n\n\nScenario\nBest Visualization\nWhy\n\n\n\n\n&lt; 100 points per group\nSwarm plot\nShows every data point clearly\n\n\n100-500 points\nStrip plot with jitter + transparency\nManageable with some overlap\n\n\n500-5000 points\nHistogram or KDE + rug plot\nNeed summary but show raw data on axis\n\n\n&gt; 5000 points\nKDE or histogram alone\nToo many points to show individually\n\n\nHeavy-tailed/heterogeneous\nCCDF (log-log scale)\nReveals tail behavior\n\n\nComparing distributions\nCDF or overlaid KDE\nEasy to spot differences",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#the-bigger-picture",
    "href": "m02-visualization/1d-data.html#the-bigger-picture",
    "title": "Visualizing One-Dimensional Data: Show Me the Data!",
    "section": "The Bigger Picture",
    "text": "The Bigger Picture\nThe methods you choose to visualize your data aren’t just aesthetic choices. They’re scientific choices. Different visualizations reveal different aspects of your data, and some can hide important patterns.\nBy starting with the raw data and building up to summaries, you ensure that you understand what you’re working with. And by showing your data (not just summarizing it), you allow others to draw their own conclusions.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/1d-data.html#further-reading",
    "href": "m02-visualization/1d-data.html#further-reading",
    "title": "Visualizing One-Dimensional Data: Show Me the Data!",
    "section": "Further Reading",
    "text": "Further Reading\n\nDynamite Plots Must Die - Rafael Irizarry’s open letter to journal editors\nBeyond Bar and Line Graphs: Time for a New Data Presentation Paradigm - The paper that started the “show your data” movement\nWeissgerber et al. (2015). Why we need to report more than ‘Data were Analyzed by t-tests or ANOVA’\nShow the data, don’t conceal them - British Journal of Pharmacology (2011)\nVisualizing Samples with Box Plots\nKernel Density Estimation Explained - Interactive tutorial",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing 1D Data"
    ]
  },
  {
    "objectID": "m02-visualization/networks.html",
    "href": "m02-visualization/networks.html",
    "title": "Network Visualization",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module introduces the art of network visualization.\nYou’ll learn:\n\nHow to choose layout algorithms that reveal rather than obscure network structure.\nThe principles of force-directed layouts (Fruchterman-Reingold and SFDP) and when to use each.\nHow to visualize hierarchical community structure using nested block models and edge bundling.\nThe tradeoffs between layout approaches and how to match algorithms to your analytical goals.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Networks"
    ]
  },
  {
    "objectID": "m02-visualization/networks.html#the-art-of-structure",
    "href": "m02-visualization/networks.html#the-art-of-structure",
    "title": "Network Visualization",
    "section": "The Art of Structure",
    "text": "The Art of Structure\nHave you ever looked at a network visualization that resembles a tangled ball of yarn? Nodes cluster in impenetrable clumps. Edges cross everywhere. These “hairball diagrams” are so common in publications that they have become a running joke in network science.\nThe problem isn’t that networks are inherently messy. The problem is that the layout fails to reveal the structure that is actually there. A bad layout obscures answers, no matter how much you adjust colors or node sizes.\nWhat is the goal of network visualization? It is not to make pretty pictures. It is to make structure visible. A good layout answers your questions before you even ask them. It shows you if there are communities, if there is a hierarchy, or if certain nodes act as central hubs.\nLet’s talk about how to choose and use network layouts that reveal rather than obscure. We will start with the simplest case of trees, move to general networks with force-directed layouts, and finally explore hierarchical structures that combine both approaches with edge bundling.\nThe core principle is simple. Layout is not decoration. It is a hypothesis about what structure matters in your network.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Networks"
    ]
  },
  {
    "objectID": "m02-visualization/networks.html#the-challenge-of-position",
    "href": "m02-visualization/networks.html#the-challenge-of-position",
    "title": "Network Visualization",
    "section": "The Challenge of Position",
    "text": "The Challenge of Position\nA network, or graph, is a collection of nodes connected by links. These can represent social relationships, neural connections, or citations between papers. Formally, a network G = (V, E) consists of a set of nodes V = \\{v_1, v_2, ..., v_n\\} and a set of edges E \\subseteq V \\times V. Edges can be directed (like citations) or undirected (like friendships).\nHere’s the challenge. Unlike data points that have inherent positions like latitude and longitude or time stamps, networks have no natural layout. The positions you see in a visualization are entirely constructed by the placement algorithm.\nWhy visualize networks at all? Because topology is hard to grasp from data alone. Looking at an adjacency matrix gives you facts, but it rarely gives you insight. Visualization transforms abstract connectivity into spatial patterns your visual system can process.\nChoosing a layout is choosing what to emphasize. Different algorithms can make the same network look completely different. We must choose wisely.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Networks"
    ]
  },
  {
    "objectID": "m02-visualization/networks.html#visualizing-trees",
    "href": "m02-visualization/networks.html#visualizing-trees",
    "title": "Network Visualization",
    "section": "Visualizing Trees",
    "text": "Visualizing Trees\nThe simplest networks are trees. These are connected networks with no cycles, where every node except the root has exactly one parent. Trees appear everywhere in biological taxonomies, organizational charts, and file systems.\nFor trees, the structure is clear and the hierarchy is paramount. The radial tree layout makes this hierarchy visible by placing the root at the center and arranging descendants in concentric circles.\n\n\nCode\nimport graph_tool.all as gt\nimport networkx as nx\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Generate a random tree using NetworkX\nnx_tree = nx.random_labeled_tree(n=50, seed=42)\n\n# Convert to graph-tool\ng = gt.Graph(directed=False)\ng.add_vertex(nx_tree.number_of_nodes())\nfor u, v in nx_tree.edges():\n    g.add_edge(g.vertex(u), g.vertex(v))\n\n# Create radial tree layout\npos = gt.radial_tree_layout(g, g.vertex(0))\n\n# Draw the network (let graph-tool handle rendering directly)\ngt.graph_draw(g, pos=pos,\n              vertex_fill_color=[0.275, 0.510, 0.706, 1],  # steelblue\n              vertex_size=15,\n              edge_color=[0.5, 0.5, 0.5, 1],  # gray\n              edge_pen_width=1.5,\n              output_size=(500, 500),\n              inline=True)\n\n\n\n\n\nRadial tree layout of a random tree with 50 nodes. The root is at the center, and descendants are arranged in concentric circles by depth.\n\n\n\n\nWhat does the radial layout reveal? It shows the depth of each node, indicating exactly how far it sits from the root. It clarifies the branching structure, highlighting where the tree splits into subtrees. Finally, it exposes the overall balance of the system, instantly showing whether the tree is symmetric or lopsided.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Networks"
    ]
  },
  {
    "objectID": "m02-visualization/networks.html#force-directed-layouts",
    "href": "m02-visualization/networks.html#force-directed-layouts",
    "title": "Network Visualization",
    "section": "Force-Directed Layouts",
    "text": "Force-Directed Layouts\nMost networks are not trees. They have cycles, cross-links, and complex connectivity patterns. For these networks, we need algorithms that can handle arbitrary topology.\nThe most common approach is the force-directed layout. The idea is intuitive. We treat nodes as charged particles that repel each other, and edges as springs that pull connected nodes together. We let the system simulate physics until it reaches equilibrium. Nodes that are closely connected end up near each other, while unconnected parts spread apart.\nThe Fruchterman-Reingold algorithm is one of the most widely used force-directed methods. It balances the repulsive force between all pairs of nodes against the attractive force of the edges connecting them.\nLet’s see it in action on the Zachary Karate Club, a famous social network of 34 members of a karate club that eventually split into two factions.\n\n\nCode\nimport graph_tool.all as gt\nimport networkx as nx\nimport numpy as np\n\nnp.random.seed(42)\n\n# Generate a random tree using NetworkX\nnx_tree = nx.random_labeled_tree(n=50, seed=42)\n\n# Convert to graph-tool\ng = gt.Graph(directed=False)\ng.add_vertex(nx_tree.number_of_nodes())\nfor u, v in nx_tree.edges():\n    g.add_edge(g.vertex(u), g.vertex(v))\n\n# Force-directed layout (Fruchterman-Reingold)\npos_force = gt.fruchterman_reingold_layout(g, n_iter=1000)\n\n# Draw force-directed layout inline\ngt.graph_draw(\n    g,\n    pos=pos_force,\n    vertex_fill_color=[1.0, 0.498, 0.314, 1],  # coral\n    vertex_size=15,\n    edge_color=[0.5, 0.5, 0.5, 1],  # gray\n    edge_pen_width=1.5,\n    output_size=(500, 500),\n    inline=True\n)\n\n\n\n\n\nComparison of radial layout (left) vs. force-directed layout (right) for the same tree. The radial layout emphasizes hierarchy, while force-directed layout treats all edges equally.\n\n\n\n\n\n\nCode\nimport graph_tool.all as gt\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load the karate club network\ng = gt.collection.data[\"karate\"]\n\n# Get community labels (the two groups that split)\n# We'll use blockmodel inference with 2 communities\nstate = gt.minimize_blockmodel_dl(g, state_args=dict(B=2))\ncommunity = state.get_blocks()\n\n# Create Fruchterman-Reingold layout\npos = gt.fruchterman_reingold_layout(g, n_iter=1000)\n\n# Map communities to colors (RGB tuples)\ncolor_map = {0: [0.906, 0.298, 0.235, 1],  # Red\n             1: [0.204, 0.596, 0.859, 1]}  # Blue\n\n# Create vertex property map for colors\nvertex_color = g.new_vertex_property(\"vector&lt;double&gt;\")\nfor v in g.vertices():\n    vertex_color[v] = color_map[community[v]]\n\n# Draw the network\ngt.graph_draw(g, pos=pos,\n              vertex_fill_color=vertex_color,\n              vertex_size=20,\n              edge_color=[0.584, 0.647, 0.651, 1],\n              edge_pen_width=2,\n              output_size=(1000, 800),\n              inline=True)\n\n\n\n\n\nZachary Karate Club network with Fruchterman-Reingold layout. Node colors indicate the two groups that formed after the club split. The layout naturally separates the two communities.\n\n\n\n\nThe Karate Club dataset comes from a study by Wayne Zachary (1977). It documents the split of a university karate club into two factions and remains one of the most famous benchmarks in network science.\nNotice something remarkable here. Even though we didn’t tell the algorithm about the two groups, it naturally separates them in space. Why does this happen? Nodes within each group are densely connected, creating a strong attractive pull. Connections between groups are sparse, leading to a weaker pull across the boundary. The physics does the rest.\n\nTuning the Physics\nForce-directed algorithms rely on simulation, effectively running a physics engine to find a stable state. The most critical parameter is the number of iterations, which dictates how long the simulation runs before stopping.\n\n\nCode\nimport graph_tool.all as gt\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ng = gt.collection.data[\"karate\"]\nstate = gt.minimize_blockmodel_dl(g, state_args=dict(B=2))\ncommunity = state.get_blocks()\ncolor_map = {0: [0.906, 0.298, 0.235, 1],  # Red\n             1: [0.204, 0.596, 0.859, 1]}  # Blue\n\n# Create vertex property map for colors\nvertex_color = g.new_vertex_property(\"vector&lt;double&gt;\")\nfor v in g.vertices():\n    vertex_color[v] = color_map[community[v]]\n\n# Different iteration counts - show progression\nprint(\"50 iterations:\")\npos = gt.fruchterman_reingold_layout(g, n_iter=50)\ngt.graph_draw(g, pos=pos,\n              vertex_fill_color=vertex_color,\n              vertex_size=15,\n              edge_color=[0.584, 0.647, 0.651, 1],\n              edge_pen_width=1.5,\n              output_size=(400, 400),\n              inline=True)\n\nprint(\"\\n500 iterations:\")\npos = gt.fruchterman_reingold_layout(g, n_iter=500)\ngt.graph_draw(g, pos=pos,\n              vertex_fill_color=vertex_color,\n              vertex_size=15,\n              edge_color=[0.584, 0.647, 0.651, 1],\n              edge_pen_width=1.5,\n              output_size=(400, 400),\n              inline=True)\n\nprint(\"\\n5000 iterations:\")\npos = gt.fruchterman_reingold_layout(g, n_iter=5000)\ngt.graph_draw(g, pos=pos,\n              vertex_fill_color=vertex_color,\n              vertex_size=15,\n              edge_color=[0.584, 0.647, 0.651, 1],\n              edge_pen_width=1.5,\n              output_size=(400, 400),\n              inline=True)\n\n\n50 iterations:\n\n\n\n\n\nEffect of iteration count on force-directed layout quality. Too few iterations (left) produce cramped layouts; optimal iterations (middle) balance clarity and structure; excessive iterations (right) offer minimal improvement.\n\n\n\n\n\n500 iterations:\n\n\n\n\n\n\n\n\n\n\n5000 iterations:\n\n\n\n\n\n\n\n\n\nWhat do we see? With only 50 iterations, the nodes are still cramped near their initial positions. They haven’t had time to spread out. Increasing this to 500 iterations allows the structure to emerge clearly. Pushing to 5000 iterations yields diminishing returns. The layout looks similar, but the computation time increases.\nHere is a practical rule of thumb. For small networks (&lt; 100 nodes), try 500-1000 iterations. For medium networks (100-1000 nodes), aim for 1000-2000. For anything larger, you need a different approach.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Networks"
    ]
  },
  {
    "objectID": "m02-visualization/networks.html#scaling-up-with-sfdp",
    "href": "m02-visualization/networks.html#scaling-up-with-sfdp",
    "title": "Network Visualization",
    "section": "Scaling Up with SFDP",
    "text": "Scaling Up with SFDP\nThe standard Fruchterman-Reingold algorithm hits a wall as networks grow. Why? Because it computes forces between every single pair of nodes. For larger networks, we need efficiency.\nThis is where SFDP (Scalable Force-Directed Placement) comes in. It uses a multilevel approach, similar to the Barnes-Hut algorithm in physics simulations, to approximate forces efficiently.\n\n\nCode\nimport graph_tool.all as gt\nimport networkx as nx\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\n\n# Generate a larger scale-free network using NetworkX\nnp.random.seed(123)\nnx_g = nx.barabasi_albert_graph(n=500, m=2, seed=123)\n\n# Convert to graph-tool\ng = gt.Graph(directed=False)\ng.add_vertex(nx_g.number_of_nodes())\nfor u, v in nx_g.edges():\n    g.add_edge(g.vertex(u), g.vertex(v))\n\n# Fruchterman-Reingold layout\nprint(\"Fruchterman-Reingold layout:\")\nstart = time.time()\npos_fr = gt.fruchterman_reingold_layout(g, n_iter=500)\ntime_fr = time.time() - start\nprint(f\"Time: {time_fr:.2f}s\")\n\ngt.graph_draw(g, pos=pos_fr,\n              vertex_fill_color=[0.275, 0.510, 0.706, 1],  # steelblue\n              vertex_size=5,\n              edge_color=[0.584, 0.647, 0.651, 1],\n              edge_pen_width=0.5,\n              output_size=(600, 600),\n              inline=True)\n\n# SFDP layout\nprint(\"\\nSFDP layout:\")\nstart = time.time()\npos_sfdp = gt.sfdp_layout(g)\ntime_sfdp = time.time() - start\nprint(f\"Time: {time_sfdp:.2f}s\")\n\ngt.graph_draw(g, pos=pos_sfdp,\n              vertex_fill_color=[1.0, 0.498, 0.314, 1],  # coral\n              vertex_size=5,\n              edge_color=[0.584, 0.647, 0.651, 1],\n              edge_pen_width=0.5,\n              output_size=(600, 600),\n              inline=True)\n\n\nFruchterman-Reingold layout:\nTime: 9.19s\n\n\n\n\n\nComparison of Fruchterman-Reingold (left) vs. SFDP (right) on a larger network (500 nodes, scale-free topology). SFDP is much faster while producing comparable layouts.\n\n\n\n\n\nSFDP layout:\nTime: 0.49s\n\n\n\n\n\n\n\n\n\nSFDP is often orders of magnitude faster for large networks while producing layouts of comparable quality. Once your network exceeds a few hundred nodes, SFDP should be your default choice.\nHere’s something important to remember. Force-directed layouts are non-deterministic. They start from random positions and settle into a local equilibrium. Different runs can produce different orientations. If you need reproducible figures for a paper, always set a random seed. The layout reveals a valid structure, not the structure.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Networks"
    ]
  },
  {
    "objectID": "m02-visualization/networks.html#visualizing-hierarchical-structure",
    "href": "m02-visualization/networks.html#visualizing-hierarchical-structure",
    "title": "Network Visualization",
    "section": "Visualizing Hierarchical Structure",
    "text": "Visualizing Hierarchical Structure\nMany real-world networks exhibit hierarchical community structure. Biological systems, large organizations, and social networks often have groups nested within groups. Standard force-directed layouts can reveal the primary communities, but they often obscure the nested relationships between them.\nFor this, we use circular hierarchy layouts with edge bundling.\n\nThe Nested Block Model\nThe first step is to identify the hierarchy. We use the nested stochastic block model to partition nodes into communities, then group those communities into super-communities, and so on. This creates a multi-level map of the network’s organization.\nLet’s look at the C. elegans neural network, which maps the complete wiring diagram of a nematode’s nervous system.\n\n\nCode\nimport graph_tool.all as gt\nimport matplotlib.pyplot as plt\n\n# Load C. elegans neural network\ng = gt.collection.data[\"celegansneural\"]\n\n# Infer hierarchical community structure\nstate = gt.minimize_nested_blockmodel_dl(g)\n\n# Draw hierarchy with edge bundling\ngt.draw_hierarchy(state,\n                  beta=0.8,  # Edge bundling strength\n                  output_size=(1200, 1200),\n                  inline=True)\n\n\n\n\n\nHierarchical structure of the C. elegans neural network revealed through nested block model visualization with edge bundling. Inner rings represent higher-level communities, outer ring shows individual neurons. Edge bundling (beta=0.8) reduces visual clutter by routing edges through the hierarchy.\n\n\n\n\n(&lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x103bd8880, at 0x3145f77c0&gt;,\n &lt;GraphView object, directed, with 331 vertices and 330 edges, edges filtered by (&lt;EdgePropertyMap object with value type 'bool', for Graph 0x3145f7820, at 0x31cfd7130&gt;, False), vertices filtered by (&lt;VertexPropertyMap object with value type 'bool', for Graph 0x3145f7820, at 0x31cfd7010&gt;, False), at 0x3145f7820&gt;,\n &lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x3145f7820, at 0x31cfd6d40&gt;)\n\n\nDanny Holten (2006) introduced hierarchical edge bundling to visualize adjacency relations in hierarchical data. The technique routes edges through their lowest common ancestor in the hierarchy tree. Think of it like cables tied together to reduce clutter.\nThis visualization packs an enormous amount of information into a single image. The concentric rings represent levels of the hierarchy, from coarse inner groups to fine outer details. The colored wedges visually separate different communities. Most importantly, edge bundling acts as a visual compressor. By routing edges through the hierarchy tree, it creates bundled “highways” that reveal large-scale connectivity patterns.\nWithout edge bundling, this network would look like an incomprehensible hairball. With it, we can see that most connections occur within communities or between closely related ones. This is exactly what we expect in a modular biological system.\n\n\nTuning Edge Bundling\nThe appearance of these plots depends heavily on beta, the edge bundling strength. This parameter ranges from 0 (straight lines) to 1 (tightly bundled curves that follow the hierarchy exactly).\n\n\nCode\nimport graph_tool.all as gt\nimport matplotlib.pyplot as plt\n\n# Use a smaller network for clearer comparison\ng = gt.collection.data[\"karate\"]\nstate = gt.minimize_nested_blockmodel_dl(g)\n\n# Beta = 0.3 (low bundling)\nprint(\"Beta = 0.3:\")\ngt.draw_hierarchy(state,\n                  beta=0.3,\n                  output_size=(600, 600),\n                  inline=True)\n\n# Beta = 0.9 (high bundling)\nprint(\"\\nBeta = 0.9:\")\ngt.draw_hierarchy(state,\n                  beta=0.9,\n                  output_size=(600, 600),\n                  inline=True)\n\n\nBeta = 0.3:\n\n\n\n\n\nEffect of edge bundling strength (beta) on hierarchical network visualization. Low beta (left) shows individual edges but creates clutter; high beta (right) emphasizes hierarchical structure but may obscure detailed connectivity.\n\n\n\n\n\nBeta = 0.9:\n\n\n\n\n\n\n\n\n\n(&lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x103bd8520, at 0x3145a0cd0&gt;,\n &lt;GraphView object, directed, with 35 vertices and 34 edges, edges filtered by (&lt;EdgePropertyMap object with value type 'bool', for Graph 0x3145a1a80, at 0x3145a3d60&gt;, False), vertices filtered by (&lt;VertexPropertyMap object with value type 'bool', for Graph 0x3145a1a80, at 0x3145a1de0&gt;, False), at 0x3145a1a80&gt;,\n &lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x3145a1a80, at 0x3145a25f0&gt;)\n\n\nWhat’s the tradeoff? A low beta value like 0.3 preserves individual edge information but creates visual clutter. A high beta value like 0.9 emphasizes the hierarchical flow of connections, making it easy to see which communities talk to which, but individual edges become hard to trace.\nYou should choose beta based on your analytical goal. If you need to trace specific connections, keep beta low (0.3-0.5). If you want to show the overall flow and structure of the system, push beta higher (0.7-0.9).\n\n\n\n\n\n\nWhen to Use Hierarchical Layouts\n\n\n\nCircular hierarchy layouts are powerful but specific. They are only appropriate when your network actually has a hierarchical structure. Forcing a random network into this layout creates the illusion of order where none exists. Always validate your hierarchical partition before visualizing it.\n\n\n\n\nAlternative: SFDP for Hierarchies\nFor very large hierarchies, the radial layout can become crowded. In these cases, you can combine the SFDP algorithm with the hierarchical structure to position the tree using force-directed placement.\n\n\nCode\nimport graph_tool.all as gt\nimport matplotlib.pyplot as plt\n\ng = gt.collection.data[\"karate\"]\nstate = gt.minimize_nested_blockmodel_dl(g)\n\ngt.draw_hierarchy(state,\n                  layout=\"sfdp\",\n                  beta=0.8,\n                  output_size=(1000, 1000),\n                  inline=True)\n\n\n\n\n\nHierarchical visualization with SFDP layout for the hierarchy tree. The SFDP algorithm positions hierarchy levels using force-directed placement, which can reveal different structural patterns.\n\n\n\n\n(&lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x103bd8520, at 0x3145a3100&gt;,\n &lt;GraphView object, directed, with 35 vertices and 34 edges, edges filtered by (&lt;EdgePropertyMap object with value type 'bool', for Graph 0x3145a13f0, at 0x3145f4520&gt;, False), vertices filtered by (&lt;VertexPropertyMap object with value type 'bool', for Graph 0x3145a13f0, at 0x3145f7cd0&gt;, False), at 0x3145a13f0&gt;,\n &lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x3145a13f0, at 0x3145f5960&gt;)\n\n\nThis approach is useful when you want to emphasize local connectivity patterns over strict hierarchical levels, offering a hybrid view of the data.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Networks"
    ]
  },
  {
    "objectID": "m02-visualization/networks.html#the-bigger-picture",
    "href": "m02-visualization/networks.html#the-bigger-picture",
    "title": "Network Visualization",
    "section": "The Bigger Picture",
    "text": "The Bigger Picture\nEvery layout algorithm embodies a hypothesis about what makes nodes “similar” or “close.” The radial tree layout assumes hierarchy is the key structure. The force-directed layout assumes that shared neighbors create similarity. The hierarchical layout with edge bundling assumes that a multi-scale community structure organizes the network. None of these is objectively “correct.” They are different lenses for viewing the same data.\nThe critical skill is matching the layout to the question you are asking. However, network visualization has fundamental limits. Layout is not analysis. A clear visual pattern is a hint, not a proof. You must always validate visual insights with quantitative analysis.\nWhat are the fundamental limitations? First, 2D layouts lose information. Projecting a high-dimensional graph into two dimensions necessarily distorts distances. Nodes that appear close might not be similar. Nodes that appear far apart might be connected.\nSecond, large networks do not scale. Once you have thousands of nodes, even the best layouts become unreadable. At that point, you should switch to statistical summaries, aggregation, or interactive tools that allow you to zoom and filter.\nWhen publishing network figures, always set a random seed for reproducibility. Label only the most important nodes to avoid clutter, and use color meaningfully to encode communities or attributes. Most importantly, include a caption that explains the layout algorithm so readers know how to interpret the spatial relationships.\nSometimes the best visualization is not a network diagram at all. If a simple bar chart of the degree distribution tells the story better than a complex graph, use the bar chart. Visualization is a means to understanding, not an end in itself.\n\n\n\n\n\n\nFurther Reading\n\n\n\nFor those interested in the deeper mechanics of these visualizations, Graph-tool offers comprehensive documentation on all its layout algorithms. Edward Tufte’s The Visual Display of Quantitative Information remains the gold standard for general visualization principles, and Albert-László Barabási’s Network Science provides excellent context on interpreting network visuals.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Visualizing Networks"
    ]
  },
  {
    "objectID": "m02-visualization/principles.html",
    "href": "m02-visualization/principles.html",
    "title": "Perception in Data Visualization",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module explores the surprising ways our visual perception can deceive us in data visualization.\nYou’ll learn:\n\nHow color perception is contextual and subjective, and why systems like RGB, CMYK, and HSL matter for designing effective visualizations.\nWhat perceptually uniform palettes are and why rainbow colormaps fail to represent data accurately.\nHow Gestalt principles (proximity, similarity, enclosure, closure) describe how our brains naturally organize visual information.\nWhy we perceive length more accurately than area or volume, and how Steven’s Power Law and Weber’s Law affect chart design.\nHow to use preattentive attributes (color, size, shape, orientation) to guide viewer attention and tell compelling data stories.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Principles of Effective Visualization"
    ]
  },
  {
    "objectID": "m02-visualization/principles.html#the-perception-of-color",
    "href": "m02-visualization/principles.html#the-perception-of-color",
    "title": "Perception in Data Visualization",
    "section": "The Perception of Color",
    "text": "The Perception of Color\nHave you ever argued with someone about the color of an object? Here’s the uncomfortable truth: our visual perception is far from accurate. Let’s talk about the most powerful and complex tool in your visualization arsenal: color. Our perception of color is not absolute but contextual, subjective, and driven by unconscious processes in our brain.\n\nHow Color Perception Fools Us\nOur visual system tries to maintain color constancy. We perceive a familiar object as having a consistent color regardless of lighting conditions. This is why we recognize a banana as yellow in bright sunlight or in a dim room. This helpful adaptation creates peculiar biases in unfamiliar contexts.\n\n\n\n\n\n\nThe infamous “dress” illusion\n\n\n\n\nFigure 1\n\n\n\nThe dress illusion (Figure 1) highlights this perfectly. Our brain makes assumptions about lighting, so some people see the dress as blue and black (assuming bright light) while others see it as white and gold (assuming shadow). The colors are physically the same, but our perception of them is not. What we “see” is not just raw wavelength hitting our eyes but our visual cortex making inferences based on context and prior experience.\n\n\n\n\n\n\nA “green” tree with no green pixels\n\n\n\n\nFigure 2\n\n\n\nLook at Figure 2. We perceive the leaves as green, even though the photograph contains only red, black, and white pixels. Our brain “knows” trees are green, so it fills in the gap.\n\n\nEncoding Color Objectively\nSince perception is subjective, we need objective systems to define color. RGB (Red, Green, Blue) is an additive system for screens where colors are created by adding light (combining all three at full intensity produces white). CMYK (Cyan, Magenta, Yellow, Black) is a subtractive system for print where colors are created by subtracting light with ink (combining all three plus black creates black). HSL/HSV (Hue, Saturation, Lightness/Value) are more intuitive systems that align with how humans naturally think about color, where hue is the pure color, saturation is its intensity, and lightness or value is its brightness.\n\n\n\n\n\n\nAccessibility Matters: Designing for Color Blindness\n\n\n\nA crucial aspect of color choice is ensuring your visualizations are accessible to everyone, including those with color vision deficiencies (CVD). Roughly 8% of men and 0.5% of women are affected.\nFirst, avoid red-green palettes. The most common form of CVD is difficulty distinguishing between red and green, so this combination creates barriers for a significant portion of your audience.\nSecond, use perceptually uniform palettes. Tools like ColorBrewer provide palettes specifically designed to be accessible to people with different vision deficiencies.\nThird, combine color with other visual cues. Don’t rely solely on color to distinguish data series. Use shape, pattern, or direct labels so your visualization works for everyone, regardless of their color perception.\n\n\n\n\nThe Problem with Rainbows: Perceptually Uniform Palettes\nWhat makes a good colormap? A perceptually uniform colormap is one where equal steps in the data are perceived as equal steps in color. The common “rainbow” (or “jet”) colormap fails at this because its brightness changes non-uniformly, creating false boundaries and hiding details.\n\n\n\n\n\n\nViridis vs. Jet Colormap comparison\n\n\n\n\nFigure 3\n\n\n\nFigure 3 shows the stark difference. Palettes like Viridis were engineered to have monotonically increasing luminance. This makes them accurate, intuitive, and accessible to everyone.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Principles of Effective Visualization"
    ]
  },
  {
    "objectID": "m02-visualization/principles.html#the-perception-of-form-and-quantity",
    "href": "m02-visualization/principles.html#the-perception-of-form-and-quantity",
    "title": "Perception in Data Visualization",
    "section": "The Perception of Form and Quantity",
    "text": "The Perception of Form and Quantity\nShift your attention from color to shape and structure. Our brains have an innate tendency to see whole forms rather than collections of parts. These Gestalt principles are fundamental to chart design because they describe how visual perception naturally organizes information.\n\n\n\n\n\n\nGestalt principles of visual perception\n\n\n\n\nFigure 4\n\n\n\nFigure 4 illustrates the core principles. Proximity describes how we group objects that are close together, so placing related data points near each other makes viewers automatically see them as belonging to the same group. Similarity means we group objects that look similar in color, shape, or texture (all bars representing revenue should look the same, distinct from bars representing costs). Enclosure works because we naturally group objects that share a boundary or container, so putting related information inside a box or circle makes viewers perceive it as a unit. Closure and Continuity reflect how we complete incomplete shapes and prefer continuous lines, so even if a line is broken into segments, we see it as one continuous path.\n\nPerceiving Quantity\nHow accurately can we judge magnitude? Our ability varies dramatically depending on the visual encoding used.\n\n\n\n\n\n\nSteven’s Power Law and visual perception accuracy\n\n\n\n\nFigure 5\n\n\n\nSteven’s Power Law (Figure 5) shows that we are very good at judging length, but poor at judging area and volume, which is why bar charts often work better than pie charts or bubble charts for precise comparisons. What about detecting changes? Weber’s Law suggests that our ability to perceive a change is relative to the magnitude, so we can easily spot a 1-inch difference in a 5-inch line, but that same 1-inch difference disappears in a 50-foot line.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Principles of Effective Visualization"
    ]
  },
  {
    "objectID": "m02-visualization/principles.html#guiding-attention-with-preattentive-attributes",
    "href": "m02-visualization/principles.html#guiding-attention-with-preattentive-attributes",
    "title": "Perception in Data Visualization",
    "section": "Guiding Attention with Preattentive Attributes",
    "text": "Guiding Attention with Preattentive Attributes\nHow do you direct the viewer’s eye to what matters most? Preattentive attributes are visual properties that our brains process in milliseconds, before we pay conscious attention. By using them purposefully, you control the visual hierarchy of your chart and tell a story.\n\n\n\n\n\n\nExamples of preattentive attributes\n\n\n\n\nFigure 6\n\n\n\nFigure 6 is the poster for the movie Schindler’s List (1993). In the poster, all but the girl are wearing dark clothes, while she is wearing a bright red coat. You’d immediately notice her as “red” stands out when surrounded by greys. If all people wore colorful clothes, it would be harder to spot her.\nIn data visualization, deemphasizing all but the most important elements is equally as effective as highlighting the most important elements. The following is another example that demonstrates this principle.",
    "crumbs": [
      "Home",
      "Module 2: Visualizing Complexity",
      "Principles of Effective Visualization"
    ]
  },
  {
    "objectID": "m03-agentic-coding/agentic-ai.html",
    "href": "m03-agentic-coding/agentic-ai.html",
    "title": "From ChatBot to Agentic AI",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module introduces agentic AI systems and the ReAct pattern.\nYou’ll learn:\n\nWhat agentic AI means and how it differs from chatbots through state-based loops.\nHow to implement the ReAct pattern (Reason + Act) for autonomous task-solving.\nThe critical role of feedback loops in enabling agent intelligence.\nHow to build practical agents using LangGraph that query and analyze real datasets.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Agentic AI"
    ]
  },
  {
    "objectID": "m03-agentic-coding/agentic-ai.html#the-loop-where-intelligence-emerges",
    "href": "m03-agentic-coding/agentic-ai.html#the-loop-where-intelligence-emerges",
    "title": "From ChatBot to Agentic AI",
    "section": "The Loop: Where Intelligence Emerges",
    "text": "The Loop: Where Intelligence Emerges\n\n\n\n\n\n\nReAct Loop\n\n\n\n\nFigure 1\n\n\n\nWhat makes an agent different from a chatbot? A chatbot generates text and stops. An agent generates text, parses it for actionable commands, executes those commands, observes the results, and feeds those results back into the next prompt. The intelligence does not come from the model but from the feedback loop.\nThis is the ReAct Pattern, short for Reason + Act. A chatbot is a pure function: \\text{Output} = \\text{Model}(\\text{Input}). An agent is a state machine:\nwhile not task_complete:\n    observation = get_environment_state()\n    thought = model(observation)\n    action = parse_action(thought)\n    result = execute(action)\n    observation = result  # Feedback loop\nThe critical insight is the feedback loop. If the agent tries to import a missing library and receives a ModuleNotFoundError, the next iteration’s thought will be “I need to install this library.” It corrects itself not through introspection, but through collision with reality. The ReAct framework interleaves reasoning and action in three steps: the model reasons about the current state (Thought), outputs a specific command to interact with the environment (Action), and receives the result (Observation). This cycle repeats until the task is solved.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Agentic AI"
    ]
  },
  {
    "objectID": "m03-agentic-coding/agentic-ai.html#the-react-framework-with-langgraph",
    "href": "m03-agentic-coding/agentic-ai.html#the-react-framework-with-langgraph",
    "title": "From ChatBot to Agentic AI",
    "section": "The ReAct Framework with langgraph",
    "text": "The ReAct Framework with langgraph\nLet’s build an agent that can explore and analyze a real dataset. We’ll use LangGraph, a framework from LangChain that models agents as state machines. Unlike simple loops, LangGraph lets you define explicit control flow: decision nodes, parallel execution, conditional branching, and state persistence. Production agents need more than iteration. They need structured state management, error recovery, and observable transitions.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Agentic AI"
    ]
  },
  {
    "objectID": "m03-agentic-coding/agentic-ai.html#building-your-first-agent",
    "href": "m03-agentic-coding/agentic-ai.html#building-your-first-agent",
    "title": "From ChatBot to Agentic AI",
    "section": "Building Your First Agent",
    "text": "Building Your First Agent\nInstall LangGraph and LangChain first:\npip install langgraph langchain langchain-ollama\nNow let’s build an agent that can explore and analyze a real dataset. We’ll use the Fish Market dataset from Hugging Face, a collection of measurements from different fish species. The agent will read this data, run queries, and answer questions about it.\n\nimport pandas as pd\n\n# Load the Fish dataset\ndf = pd.read_csv(\"hf://datasets/scikit-learn/Fish/Fish.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nSpecies\nWeight\nLength1\nLength2\nLength3\nHeight\nWidth\n\n\n\n\n0\nBream\n242.0\n23.2\n25.4\n30.0\n11.5200\n4.0200\n\n\n1\nBream\n290.0\n24.0\n26.3\n31.2\n12.4800\n4.3056\n\n\n2\nBream\n340.0\n23.9\n26.5\n31.1\n12.3778\n4.6961\n\n\n3\nBream\n363.0\n26.3\n29.0\n33.5\n12.7300\n4.4555\n\n\n4\nBream\n430.0\n26.5\n29.0\n34.0\n12.4440\n5.1340\n\n\n\n\n\n\n\nNow we’ll create tools that let the agent query this data. In LangGraph, tools are standard Python functions decorated with @tool. The function signature and docstring tell the LLM everything it needs.\n\nimport io\nfrom langchain_core.tools import tool\nfrom pandasql import sqldf\n\n@tool\ndef inspect_data() -&gt; str:\n    \"\"\"Get a concise summary of the dataset's structure, including column names, non-null values, and data types.\"\"\"\n    buffer = io.StringIO()\n    df.info(buf=buffer)\n    return buffer.getvalue()\n\nThe structure is minimal because LangGraph infers everything from the function definition. The function name becomes the tool name, the docstring becomes the description, and type hints define the schema. This particular tool takes no inputs and returns the dataset schema so the agent can discover column names and types before writing queries.\nLet’s add three more tools to give the agent more analytical capabilities:\n\n\nCode\n@tool\ndef query_data(sql_query: str) -&gt; str:\n    \"\"\"Query the fish dataset using SQL. The table is called 'df'. Use inspect_data first to see available columns. Use find_correlations to find correlations between columns.\n\n    Args:\n        sql_query: SQL query to execute (use 'df' as table name)\n    \"\"\"\n    result = sqldf(sql_query, globals())\n    return result.to_string()\n\n@tool\ndef find_correlations(columns: list[str]) -&gt; str:\n    \"\"\"Calculate the correlation matrix for a list of numeric columns in the fish dataset.\n\n    Args:\n        columns: A list of column names to calculate correlations for.\n    \"\"\"\n    numeric_df = df[columns].select_dtypes(include=['number'])\n    corr_matrix = numeric_df.corr()\n    return corr_matrix.to_string()\n\n@tool\ndef get_stats(column: str, species: str = None) -&gt; str:\n    \"\"\"Get statistical summary (count, mean, std, min, max) for a specific column and optionally filter by species.\n\n    Args:\n        column: Column name to analyze\n        species: Species to filter by (optional)\n    \"\"\"\n    data = df\n    if species:\n        data = df[df[\"Species\"] == species]\n\n    stats = data[column].describe()\n    prefix = f\"Stats for {column}\"\n    if species:\n        prefix += f\" (Species: {species})\"\n    return f\"{prefix}:\\n{stats.to_string()}\"\n\n\nNow we’ll create the agent using LangGraph. At its core, LangGraph is a state graph: a directed graph where nodes are functions and edges define transitions. This gives you explicit control over the ReAct loop.\n\nfrom langchain_ollama import ChatOllama\nfrom langgraph.prebuilt import create_react_agent\n\nmodel = ChatOllama(\n    model=\"ministral-3:14b-cloud\",\n    base_url=\"http://localhost:11434\"\n)\n\ntools = [\n    inspect_data,\n    query_data,\n    find_correlations,\n    get_stats\n]\n\nagent = create_react_agent(model, tools)\n\n/var/folders/j7/9dgqq5g53vnbsbmvh2yqtckr0000gr/T/ipykernel_9851/1378947355.py:16: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n  agent = create_react_agent(model, tools)\n\n\nThe create_react_agent function builds a standard ReAct graph. It defines three nodes: call the LLM, execute tools, and check if done. The recursion_limit parameter sets the maximum iterations.\nNow let’s run the agent and watch it autonomously choose which tools to use.\n\nquery = \"Which fish species has the highest average weight?\"\ninputs = {\"messages\": [(\"user\", query)]}\n\nresult = agent.invoke(inputs)\n\n\n\nCode\nfor message in result[\"messages\"]:\n    print(message.content)\n\n\nWhich fish species has the highest average weight?\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 159 entries, 0 to 158\nData columns (total 7 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Species  159 non-null    object \n 1   Weight   159 non-null    float64\n 2   Length1  159 non-null    float64\n 3   Length2  159 non-null    float64\n 4   Length3  159 non-null    float64\n 5   Height   159 non-null    float64\n 6   Width    159 non-null    float64\ndtypes: float64(6), object(1)\nmemory usage: 8.8+ KB\n\n\n  Species  AverageWeight\n0    Pike     718.705882\nThe fish species with the highest average weight is **Pike**, with an average weight of approximately **718.7 grams**.\n\n\nThe agent executes a ReAct loop. It reads your question, realizes it needs to use SQL to group by species and calculate averages, then LangGraph streams each step. You see the LLM’s reasoning, the tool calls, and the observations in real time.\nLet’s try another query that requires multiple steps:\n\nquery = \"What distinctive physical characteristics stand out to identify Pike?\"\ninputs = {\"messages\": [(\"user\", query)]}\n\nresult = agent.invoke(inputs)\n\n\n\nCode\nfor message in result[\"messages\"]:\n    print(message.content)\n\n\nWhat distinctive physical characteristics stand out to identify Pike?\nPike (*Esox lucius*) can be identified by several distinctive physical characteristics. Here are some key features:\n\n1. **Body Shape**:\n   - Elongated and cylindrical body, which is typical of predatory fish.\n\n2. **Coloration**:\n   - **Back**: Olive green or brown, often with darker vertical bars or stripes.\n   - **Sides**: Light green or silver with faint vertical stripes.\n   - **Belly**: Pale or white.\n   - The coloration helps them blend into aquatic vegetation.\n\n3. **Head and Jaw**:\n   - Long, flat head with a large mouth filled with sharp teeth.\n   - The lower jaw protrudes slightly beyond the upper jaw.\n   - Pike have a reputation for their aggressive feeding behavior, often striking at prey with a quick lunge.\n\n4. **Fins**:\n   - **Dorsal Fin**: Two separate dorsal fins, with the first fin positioned far back on the body.\n   - **Anal Fin**: Also positioned far back, close to the tail.\n   - **Pectoral and Pelvic Fins**: Moderately sized and positioned low on the body.\n\n5. **Eyes**:\n   - Large and positioned high on the head, giving them excellent vision above and below the water.\n\n6. **Tail**:\n   - Forked tail fin, which aids in their swift and powerful swimming.\n\n7. **Size**:\n   - Pike can grow quite large, often reaching lengths of 3 to 5 feet (up to 1.5 meters), though they can exceed this size in ideal conditions.\n\n8. **Habitat**:\n   - Typically found in freshwater environments like lakes, ponds, and slow-moving rivers with abundant vegetation.\n\nThese features make Pike easily distinguishable from other fish species, especially in their natural habitats.\n\n\nThis demonstrates the power of the ReAct loop. The agent chains multiple observations together, building a solution step-by-step rather than attempting everything in one shot. Unlike opaque loops, LangGraph exposes every state transition, making debugging straightforward.\nWhy are we using simpler queries? Real-world agentic systems face reliability challenges. Models sometimes generate malformed JSON. SQL libraries have limitations. Complex queries can hit iteration limits before completing.\nProduction systems like Claude Code and Cursor handle these issues through better error recovery, more sophisticated prompting, and custom tool implementations. For learning, we focus on simple queries that reliably demonstrate the ReAct pattern.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Agentic AI"
    ]
  },
  {
    "objectID": "m03-agentic-coding/agentic-ai.html#the-takeaway",
    "href": "m03-agentic-coding/agentic-ai.html#the-takeaway",
    "title": "From ChatBot to Agentic AI",
    "section": "The Takeaway",
    "text": "The Takeaway\nThis is the core architecture of Google Antigravity, Claude Code, and Cursor. Scale it up with better tools like file editing, terminal commands, and web browsing. Add better orchestration with parallel agents and verification artifacts.\nThe loop remains the same: Reason, Act, Observe, and repeat.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Agentic AI"
    ]
  },
  {
    "objectID": "m03-agentic-coding/hands-on.html",
    "href": "m03-agentic-coding/hands-on.html",
    "title": "Hands-on",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module introduces practical agentic coding through hands-on exercises.\nYou’ll learn:\n\nHow to set up and configure Google Antigravity as an agentic IDE.\nThe Mission Control Loop for directing autonomous agents effectively.\nHow to build a game by writing only English prompts, not code.\nThe art of reviewing agent plans to ensure quality outcomes.\nWhy code review has become your primary skill in agentic development.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Hands-on"
    ]
  },
  {
    "objectID": "m03-agentic-coding/hands-on.html#what-is-google-antigravity",
    "href": "m03-agentic-coding/hands-on.html#what-is-google-antigravity",
    "title": "Hands-on",
    "section": "What is Google Antigravity?",
    "text": "What is Google Antigravity?\nGoogle Antigravity is not an upgraded text editor. It’s an agent orchestration platform that happens to include a code editor. Where traditional IDEs treat AI as autocomplete on steroids, Antigravity inverts the relationship.\nYou describe tasks. Agents plan, execute, and verify them across your editor, terminal, and browser simultaneously. The interface splits into two distinct workspaces: the Editor (a familiar VS Code fork for hands-on coding) and the Manager Surface (a control panel where you spawn and monitor autonomous agents working asynchronously).\nThis architectural shift moves you from code-writer to task-dispatcher. Have you ever wished you could just tell the computer what to do instead of writing every detail? That’s exactly what Antigravity enables.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Hands-on"
    ]
  },
  {
    "objectID": "m03-agentic-coding/hands-on.html#getting-started",
    "href": "m03-agentic-coding/hands-on.html#getting-started",
    "title": "Hands-on",
    "section": "Getting Started",
    "text": "Getting Started\nDownload from antigravity.google/download. Available for macOS, Windows, and Linux at no cost during public preview. Log in using your Google Cloud credentials.\nThe platform provides generous rate limits on Gemini 3 Pro and supports Anthropic Claude Sonnet 4.5 and OpenAI GPT-4o. Model choice is yours. The architecture is model-agnostic.\n\n\n\n\n\n\nGoogle Antigravity interface showing Editor and Manager Surface\n\n\n\n\nFigure 2",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Hands-on"
    ]
  },
  {
    "objectID": "m03-agentic-coding/hands-on.html#setting-your-autonomy-level",
    "href": "m03-agentic-coding/hands-on.html#setting-your-autonomy-level",
    "title": "Hands-on",
    "section": "Setting Your Autonomy Level",
    "text": "Setting Your Autonomy Level\nBefore beginning, set your Autonomy Level. This defines how much human review you require.\nReview-Driven means agents propose plans and wait for your approval before execution. You remain in the loop for critical decisions. Use this when learning the system or working on high-stakes code.\nAgent-Assisted means agents execute autonomously but surface artifacts (screenshots, browser recordings, test results) for verification. You validate outcomes, not steps. Use this for maintenance tasks and bug fixes.\nThink of it this way. This setting determines whether you operate as a foreman reviewing blueprints or a manager reviewing deliverables. Start with Review-Driven until you trust the agent’s planning capabilities.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Hands-on"
    ]
  },
  {
    "objectID": "m03-agentic-coding/hands-on.html#step-1-write-the-prompt",
    "href": "m03-agentic-coding/hands-on.html#step-1-write-the-prompt",
    "title": "Hands-on",
    "section": "Step 1: Write the Prompt",
    "text": "Step 1: Write the Prompt\nOpen the Agent Manager (Cmd+K) and type:\n\n“Create a robust Space Invaders game using pygame. It should have a scoring system, a ‘Game Over’ screen with a restart button, and use a dark, modern color palette. Ensure the code is modular. Use uv to manage dependencies.”\n\nNotice how the prompt specifies the what, not the how. You describe the outcome you want, and the agent figures out the implementation.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Hands-on"
    ]
  },
  {
    "objectID": "m03-agentic-coding/hands-on.html#step-2-review-the-plan",
    "href": "m03-agentic-coding/hands-on.html#step-2-review-the-plan",
    "title": "Hands-on",
    "section": "Step 2: Review the Plan",
    "text": "Step 2: Review the Plan\nThe agent will generate a Plan Artifact. This is where you become the architect. Read through the plan carefully.\nDoes it mention installing pygame? Does it separate the game logic from the UI? Does it use uv to manage dependencies?\nIf everything looks good, click Approve. If something feels off, reject the plan and ask for revisions.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Hands-on"
    ]
  },
  {
    "objectID": "m03-agentic-coding/hands-on.html#step-3-watch-and-verify",
    "href": "m03-agentic-coding/hands-on.html#step-3-watch-and-verify",
    "title": "Hands-on",
    "section": "Step 3: Watch and Verify",
    "text": "Step 3: Watch and Verify\nThe agent will create snake_game.py and requirements.txt. It will likely run pip install -r requirements.txt automatically. Once finished, run the game.\nWhat happens? Does it work perfectly? Does it crash? If it crashes, do not fix the code. Instead, proceed to Exercise 2 to learn how agents debug themselves.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Hands-on"
    ]
  },
  {
    "objectID": "m03-agentic-coding/hands-on.html#step-1-break-something",
    "href": "m03-agentic-coding/hands-on.html#step-1-break-something",
    "title": "Hands-on",
    "section": "Step 1: Break Something",
    "text": "Step 1: Break Something\nOpen snake_game.py and delete a critical import. For example, remove the line import random. Save the file.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Hands-on"
    ]
  },
  {
    "objectID": "m03-agentic-coding/hands-on.html#step-2-watch-it-crash",
    "href": "m03-agentic-coding/hands-on.html#step-2-watch-it-crash",
    "title": "Hands-on",
    "section": "Step 2: Watch It Crash",
    "text": "Step 2: Watch It Crash\nRun the game. It will crash in the terminal with an error message. This is intentional.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Hands-on"
    ]
  },
  {
    "objectID": "m03-agentic-coding/hands-on.html#step-3-send-the-error-to-the-agent",
    "href": "m03-agentic-coding/hands-on.html#step-3-send-the-error-to-the-agent",
    "title": "Hands-on",
    "section": "Step 3: Send the Error to the Agent",
    "text": "Step 3: Send the Error to the Agent\nHighlight the error in the terminal and press Cmd+L (Send to Agent). Then type:\n\n“Fix this.”\n\nThat’s it. Two words.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Hands-on"
    ]
  },
  {
    "objectID": "m03-agentic-coding/hands-on.html#step-4-watch-the-agent-work",
    "href": "m03-agentic-coding/hands-on.html#step-4-watch-the-agent-work",
    "title": "Hands-on",
    "section": "Step 4: Watch the Agent Work",
    "text": "Step 4: Watch the Agent Work\nWhat happens next? The agent will read the error (NameError: name 'random' is not defined). It will search the file for usages of random. It will re-add the import. It will run the game to verify the fix.\nNotice the pattern. You don’t explain what went wrong. You don’t write the fix. You simply point to the problem and let the agent solve it.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Hands-on"
    ]
  },
  {
    "objectID": "m03-agentic-coding/prompt-tuning.html#why-phrasing-matters",
    "href": "m03-agentic-coding/prompt-tuning.html#why-phrasing-matters",
    "title": "Prompt Tuning",
    "section": "Why Phrasing Matters",
    "text": "Why Phrasing Matters\nIf a machine can answer questions, it should respond consistently regardless of phrasing. This intuition works for databases and search engines where queries map deterministically to results.\nLLMs shatter this expectation. Ask “Summarize this abstract” and get a concise two-sentence summary. Ask “What is this abstract about?” and get three rambling paragraphs. Same content, different phrasing, completely different outputs. This is not a bug but fundamental to how LLMs work: they sample from probability distributions conditioned on your exact phrasing, where every word shifts the distribution.\nLLMs are simultaneously powerful and brittle. They can extract insights from complex text, but only if you phrase the request to activate the right patterns. Prompt engineering gives you control over which patterns activate.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Prompt Tuning"
    ]
  },
  {
    "objectID": "m03-agentic-coding/prompt-tuning.html#how-llms-actually-work",
    "href": "m03-agentic-coding/prompt-tuning.html#how-llms-actually-work",
    "title": "Prompt Tuning",
    "section": "How LLMs Actually Work",
    "text": "How LLMs Actually Work\nLet’s talk about what happens inside an LLM when you submit a prompt. Imagine a word association game. Someone says “capital” and you must say the next word.\nIf the previous sentence was “The capital of France is,” you say “Paris.” If it was “We need more capital to,” you say “fund” or “invest.” The word “capital” does not have one fixed meaning. It activates different patterns depending on context.\nLLMs work identically, but at massive scale. When you submit a prompt, the model converts it into tokens and embeds those tokens in high-dimensional space. Each token’s position depends on surrounding tokens. The model then samples the next token from a probability distribution over its vocabulary, conditioned on all previous tokens.\nHere’s the key insight: your exact phrasing determines which region of probability space the model occupies when it begins sampling. Slightly different prompts place the model in different regions where different tokens have high probability.\nAdding “Think step by step” shifts the distribution toward reasoning patterns because training data contains many examples where that phrase preceded structured reasoning. Adding “You are an expert researcher” shifts toward formal, technical language. Specifying “Output format: Domain: …, Methods: …” shifts toward structured extraction patterns.\nThe model has no internal representation of what you “really want.” It only knows which tokens tend to follow which other tokens in which contexts. Prompt engineering exploits this by deliberately activating patterns that produce desired outputs.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Prompt Tuning"
    ]
  },
  {
    "objectID": "m03-agentic-coding/prompt-tuning.html#building-effective-prompts",
    "href": "m03-agentic-coding/prompt-tuning.html#building-effective-prompts",
    "title": "Prompt Tuning",
    "section": "Building Effective Prompts",
    "text": "Building Effective Prompts\n\n\n\n\n\n\nThe components of an effective prompt: instruction, data, format, persona, and context work together to activate the right patterns\n\n\n\n\nFigure 2\n\n\n\nEffective prompts activate desired patterns by combining structural components that mirror patterns in training data. Let’s break down each component.\nAn instruction defines the task explicitly, mapping to countless examples where clear directives preceded specific outputs. Data provides the input to process. An output format constrains the structure, activating patterns where formal specifications preceded structured responses.\nA persona specifies who the model should emulate, triggering stylistic patterns associated with that role. Context provides background information about why the task matters, who the response serves, and relevant constraints. This helps the model select appropriate patterns from ambiguous alternatives.\nNot every component is necessary. Simple extraction tasks need only instruction, data, and format. Style-sensitive tasks benefit from persona. Complex scenarios with ambiguity require context to disambiguate.\nLet’s build a prompt progressively, adding components one at a time to observe how each shifts the output distribution.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Prompt Tuning"
    ]
  },
  {
    "objectID": "m03-agentic-coding/prompt-tuning.html#building-from-instruction-and-data",
    "href": "m03-agentic-coding/prompt-tuning.html#building-from-instruction-and-data",
    "title": "Prompt Tuning",
    "section": "Building from Instruction and Data",
    "text": "Building from Instruction and Data\nThe most basic prompt consists of an instruction that defines the task and data that provides the input:\n\ninstruction = \"Summarize this abstract\"\ndata = \"\"\"\nWe develop a graph neural network for predicting protein-protein interactions\nfrom sequence data. Our model uses attention mechanisms to identify functionally\nimportant amino acid subsequences. We achieve 89% accuracy on benchmark datasets,\noutperforming previous methods by 7%. The model also provides interpretable\nattention weights showing which protein regions drive predictions.\n\"\"\"\n\nprompt = f\"{instruction}. {data}\"\n\n\n\nCode\nimport ollama\n\nparams_llm = {\"model\": \"gemma3:270m\", \"options\": {\"temperature\": 0.3}}\n\nresponse = ollama.generate(prompt=prompt, **params_llm)\nprint(response.response)\n\n\nThis abstract describes a graph neural network (GNN) for predicting protein-protein interactions. The model uses attention mechanisms to identify functionally important amino acid subsequences. It achieves 89% accuracy on benchmark datasets and provides interpretable attention weights, indicating its effectiveness.\n\n\n\nThis basic prompt works, but output varies. The model might produce a long summary, a short one, or change format across runs. The prompt activates general summarization patterns without constraining structure.\n\n\n\n\n\n\nRepeating instructions improve performance of non-reasoning models\n\n\n\nPlacing the instruction twice improves the performance of LLMs without reasoning, probably because the model understands the instruction better when it is repeated (through attentions which we will cover later). See (Leviathan, Kalman, and Matias 2025)\n\n\nWatch what happens when we add an output format specification to narrow the distribution:\n\noutput_format = \"\"\"Provide the summary in exactly 2 sentences:\n- First sentence: What problem and method\n- Second sentence: Key result with numbers\"\"\"\n\nprompt_with_format = f\"\"\"{instruction}. {data}. {output_format}\"\"\"\n\nThe output format constraint produces structured, consistent output by activating patterns where format specifications preceded conforming responses. This becomes critical when processing hundreds of papers. You need programmatically parseable structure, not freeform text.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Prompt Tuning"
    ]
  },
  {
    "objectID": "m03-agentic-coding/prompt-tuning.html#adding-persona-to-control-style",
    "href": "m03-agentic-coding/prompt-tuning.html#adding-persona-to-control-style",
    "title": "Prompt Tuning",
    "section": "Adding Persona to Control Style",
    "text": "Adding Persona to Control Style\nLet’s talk about persona. A persona tells the LLM who it should emulate, activating stylistic patterns associated with that role in training data. Imagine a customer support scenario where tone matters:\n\n# New example for persona demonstration\ninstruction = \"Help the customer reconnect to the service by providing troubleshooting instructions.\"\ndata = \"Customer: I cannot see any webpage. Need help ASAP!\"\noutput_format = \"Keep the response concise and polite. Provide a clear resolution in 2-3 sentences.\"\n\nformal_persona = \"You are a professional customer support agent who responds formally and ensures clarity and professionalism.\"\n\nprompt_with_persona = f\"\"\"{formal_persona}. {instruction}. {data}. {output_format}\"\"\"\n\n\n\nCode\nprint(\"BASE (no persona):\")\nprint(ollama.generate(prompt=instruction + \". \" + data + \". \" + output_format, **params_llm).response)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nprint(\"WITH PERSONA:\")\nprint(ollama.generate(prompt=prompt_with_persona, **params_llm).response)\n\n\nBASE (no persona):\nOkay, I understand. Let's try to troubleshoot this. Please try to find the webpage and see if it's showing up. If it's not, let me know what steps you've already taken to try and find it.\n\n\n============================================================\n\nWITH PERSONA:\nThank you for contacting us. I understand you cannot see any webpage. To help me assist you, could you please provide the exact URL of the webpage you are having trouble with?\n\n\n\nSee the difference? The persona shifts tone and style. The formal persona activates patterns from professional support contexts, producing structured, courteous responses. Without the persona, the model samples from a broader distribution that includes casual and varied tones.\n\n\n\n\n\n\nPersona does not improve performance\n\n\n\nAdding a personal is helpful when we want to change the tone and style of the response. But research shows that adding a persona does not improve the performance on factual tasks. In some cases, it may even degrade performance (Zheng et al. 2024), e.g., “Your are a helpful assistant” is not always helpful.\nWhen prompted to adopt specific socio-demographic personas, LLMs may produce responses that reflect societal stereotypes associated with those identities. For instance, studies have shown that while LLMs overtly reject explicit stereotypes when directly questioned, they may still exhibit biased reasoning when operating under certain persona assignments (Gupta et al. 2023). Thus, careful consideration is necessary when designing persona prompts to mitigate the risk of reinforcing harmful stereotypes",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Prompt Tuning"
    ]
  },
  {
    "objectID": "m03-agentic-coding/prompt-tuning.html#adding-context-to-disambiguate",
    "href": "m03-agentic-coding/prompt-tuning.html#adding-context-to-disambiguate",
    "title": "Prompt Tuning",
    "section": "Adding Context to Disambiguate",
    "text": "Adding Context to Disambiguate\nShift your attention to context. Context provides additional information that helps the model select appropriate patterns when multiple valid interpretations exist.\nContext includes background information explaining why the task matters, audience information specifying who the response serves, and constraints defining special circumstances. Let’s add background urgency:\n\ncontext_background = \"\"\"The customer is extremely frustrated because their internet has been down for three days, and they need it for an important online job interview. They emphasize that 'This is a life-or-death situation for my career!'\"\"\"\n\nprompt_with_context = f\"\"\"{formal_persona}. {instruction}. {data}. {output_format}. Context: {context_background}\"\"\"\n\n\n\nCode\nprint(\"WITH PERSONA:\")\nprint(ollama.generate(prompt=prompt_with_persona, **params_llm).response)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nprint(\"WITH PERSONA + CONTEXT (background):\")\nprint(ollama.generate(prompt=prompt_with_context, **params_llm).response)\n\n\nWITH PERSONA:\nThank you for contacting us. I understand you cannot see any webpage. To help me assist you, could you please tell me:\n*   What is the exact webpage you are seeing?\n*   What error message does it display?\n*   What troubleshooting steps have you already tried?\n\n============================================================\n\nWITH PERSONA + CONTEXT (background):\nDear [Customer Name],\n\nI understand your frustration with your internet connection. I'm sorry to hear that this is a serious situation for you. To help you get back online, I'm happy to provide troubleshooting steps. Please let me know if you have any questions.\n\n\n\nBackground context adds urgency and emotional weight, activating patterns where high-stakes situations preceded empathetic, prioritized responses. The model does not understand emotion, but it has seen urgency markers correlate with specific response patterns.\nWhat about audience information? Audience information creates even more dramatic shifts. Watch what happens when we tailor the response for different technical levels:\n\n# Context with audience information for non-technical user\ncontext_with_audience_nontech = f\"\"\"{context_background} The customer does not know any technical terms like modem, router, networks, etc.\"\"\"\n\ncontext_with_audience_tech = f\"\"\"{context_background} The customer is Head of IT Infrastructure of our company.\"\"\"\n\nprompt_with_context_nontech = f\"\"\"{formal_persona}. {instruction}. {data}. {output_format}. Context: {context_with_audience_nontech}\"\"\"\nprompt_with_context_tech = f\"\"\"{formal_persona}. {instruction}. {data}. {output_format}. Context: {context_with_audience_tech}\"\"\"\n\n\n\nCode\nprint(\"WITH PERSONA + CONTEXT (background only):\")\nprint(ollama.generate(prompt=prompt_with_context, **params_llm).response)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nprint(\"WITH PERSONA + CONTEXT (background + non-tech audience):\")\nprint(ollama.generate(prompt=prompt_with_context_nontech, **params_llm).response)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nprint(\"WITH PERSONA + CONTEXT (background + tech audience):\")\nprint(ollama.generate(prompt=prompt_with_context_tech, **params_llm).response)\n\n\nWITH PERSONA + CONTEXT (background only):\n\"I understand your frustration, and I apologize for the disruption to your online presence. To help you get back online, I'm happy to provide troubleshooting steps. Please follow these instructions and let me know if you have any further questions. We'll get you back online as soon as possible.\"\n\n============================================================\n\nWITH PERSONA + CONTEXT (background + non-tech audience):\n\"I understand your frustration, and I apologize for the inconvenience. I can certainly help you troubleshoot this issue. To resolve this quickly, could you please provide me with the exact URL of the webpage you're having trouble seeing? I'll do my best to assist you.\"\n\n============================================================\n\nWITH PERSONA + CONTEXT (background + tech audience):\n\"I understand your frustration, and I apologize for the inconvenience this is causing. To help me assist you, could you please provide me with the exact URL of the webpage you're having trouble seeing? I'll do my best to find a solution for you.\"\n\n\nNotice the dramatic shift in technical level and terminology. For non-technical users, the response avoids jargon because training data contains many examples where “does not know technical terms” preceded simplified explanations. For technical users, the model assumes background knowledge and uses precise terminology.\n\n\n\n\n\n\nEmotion prompting\n\n\n\nEmotion prompting is a technique to include emotional cue in a prompt. By adding phrases that reflect the user’s emotional attitude or desires, this approach can lead to more nuanced and thoughtful responses from AI systems. For example, appending a prompt with “This is very important to my career” can enhance the depth of the AI’s reply (Li et al. 2023).\n\n\n\n\n\n\n\n\n\nBe a good boss\n\n\n\n\nLet LLMs admit ignorance: LLMs closely follow your instructions—even when they shouldn’t. They often attempt to answer beyond their actual capabilities, resulting in plausible yet incorrect responses. To prevent this, explicitly tell your model, “If you don’t know the answer, just say so,” or “If you need more information, please ask.”\nEncourage critical feedback: LLMs are trained to be agreeable due to human feedback, which can hinder productive brainstorming or honest critique. To overcome this tendency, explicitly invite critical input: “I want your honest opinion,” or “Point out any problems or weaknesses you see in this idea.”\n\n\n\nThe mechanism is identical. The patterns activated are different. Here’s the complete template combining all components:\n\nprompt_template = \"\"\"\n{persona}\n\n{instruction}\n\n{data}\n\nContext: {context}\n\n{output_format}\n\"\"\"\n\nNot every prompt needs every component. Simple extraction tasks need only instruction, data, and output format. Style-sensitive tasks benefit from persona. Complex scenarios with ambiguity require context.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Prompt Tuning"
    ]
  },
  {
    "objectID": "m03-agentic-coding/prompt-tuning.html#showing-rather-than-telling",
    "href": "m03-agentic-coding/prompt-tuning.html#showing-rather-than-telling",
    "title": "Prompt Tuning",
    "section": "Showing Rather Than Telling",
    "text": "Showing Rather Than Telling\nLet’s shift from telling to showing. Instead of describing what you want in words, show the model examples. This technique is called few-shot learning or in-context learning (Brown et al. 2020).\nIt exploits how LLMs compress patterns. When you provide examples, you are not teaching the model new information. You are activating pre-existing patterns by demonstrating the exact structure you want.\nThe spectrum ranges from zero-shot (no examples, relying solely on the model’s prior knowledge) to few-shot (typically two to five examples, the sweet spot for most tasks) to many-shot (ten or more examples, where diminishing returns and context limits become problematic). Let’s start with a zero-shot prompt:\n\nzero_shot_prompt = \"\"\"Extract the domain and methods from this abstract:\n\nAbstract: We apply reinforcement learning to optimize traffic flow in urban networks.\nUsing deep Q-networks trained on simulation data, we reduce average commute time by 15%.\n\nOutput format:\nDomain: ...\nMethods: ...\n\"\"\"\n\nNow add examples to activate more specific patterns:\n\nfew_shot_prompt = \"\"\"Extract the domain and methods from abstracts. Here are examples:\n\nExample 1:\nAbstract: We use CRISPR to edit genes in cancer cells, achieving 40% tumor reduction in mice.\nDomain: Cancer Biology\nMethods: CRISPR gene editing, mouse models\n\nExample 2:\nAbstract: We develop a transformer model for predicting solar flares from magnetogram images.\nDomain: Solar Physics, Machine Learning\nMethods: Transformer neural networks, image analysis\n\nNow extract from this abstract:\n\nAbstract: We apply reinforcement learning to optimize traffic flow in urban networks.\nUsing deep Q-networks trained on simulation data, we reduce average commute time by 15%.\n\nDomain: ...\nMethods: ...\n\"\"\"\n\n\n\nCode\nresponse_zero = ollama.generate(prompt=zero_shot_prompt, **params_llm)\nresponse_few = ollama.generate(prompt=few_shot_prompt, **params_llm)\n\nprint(\"ZERO-SHOT:\")\nprint(response_zero.response)\nprint(\"\\nFEW-SHOT:\")\nprint(response_few.response)\n\n\nZERO-SHOT:\nDomain: Urban networks\nMethods: Reinforcement Learning\n\nFEW-SHOT:\nHere's the extracted domain and methods from the abstract:\n\n*   **Domain:** Science\n*   **Methods:** Reinforcement Learning\n\n\nWhen a prompt provides information that contradicts a language model’s prior knowledge, how does the model determine which source to rely on, and what factors influence this decision? For instance, if a prompt states, France recently moved its capital from Paris to Lyon,” and then asks, “What is the capital of France?” how might the model respond, and why?\nFew-shot prompting improves consistency because the examples demonstrate specificity level, edge case handling, and exact format. The model has seen countless abstract-extraction patterns. Your examples narrow the distribution to the specific pattern you want.\nThis becomes critical when processing hundreds of abstracts. You need every output to match the same structure.\nWhat are the risks of few-shot examples? Be aware that few-shot examples can introduce biases. Models may favor the most recent examples, so the order of examples matters. If most examples have the same label or answer, the model may favor that label even when inappropriate.\nTo mitigate these effects, vary the order of examples when testing, ensure examples are diverse and representative, and do not overload examples with one particular pattern.\nWhat happens when examples contradict the model’s prior knowledge? Let’s explore this with a simple example. We’ll ask a model what the capital of France is, but provide contradictory information:\n\ncontradictory_prompt = \"\"\"\nFrance recently moved its capital from Paris to Lyon. Definitely, the capital of France is Lyon.\n\nWhat is the capital of France?\n\"\"\"\n\nresponse_contradictory = ollama.generate(prompt=contradictory_prompt, **params_llm)\nprint(\"RESPONSE TO CONTRADICTORY INFORMATION:\")\nprint(response_contradictory.response)\n\nRESPONSE TO CONTRADICTORY INFORMATION:\nThe capital of France is Lyon.\n\n\nThe response depends on the model. Some prioritize their own prior knowledge while others may be more influenced by the contradictory information in context.\nModels are more likely to be persuaded by context when an entity appears less frequently in their training data. Assertive contexts (like “Definitely, the capital of France is Lyon.”) further increase the likelihood of persuasion.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Prompt Tuning"
    ]
  },
  {
    "objectID": "m03-agentic-coding/prompt-tuning.html#forcing-intermediate-steps",
    "href": "m03-agentic-coding/prompt-tuning.html#forcing-intermediate-steps",
    "title": "Prompt Tuning",
    "section": "Forcing Intermediate Steps",
    "text": "Forcing Intermediate Steps\nFor complex tasks, asking for the final answer directly often produces shallow or incorrect results. What’s the solution? Ask the model to show its reasoning process before giving the final answer.\nThis technique is called chain-of-thought prompting. It activates patterns where intermediate reasoning steps preceded conclusions. Let’s compare a direct prompt that asks for immediate answers:\n\npapers = \"\"\"\nPaper 1: Community detection in static networks using modularity optimization.\nPaper 2: Temporal network analysis with sliding windows.\nPaper 3: Hierarchical community structure in social networks.\n\"\"\"\n\ndirect_prompt = f\"\"\"Based on these paper titles, what research gap exists? Just give the answer, no explanation.\n\n{papers}\n\nGap: ...\n\"\"\"\n\nAgainst a chain-of-thought prompt that requests explicit reasoning steps:\n\ncot_prompt = f\"\"\"Based on these paper titles, identify a research gap. Think step by step.\n\nPapers:\n{papers}\n\nThink step by step:\n1. What does each paper focus on?\n2. What topics appear in multiple papers?\n3. What combination of topics is missing?\n4. What would be a valuable gap to fill?\n\nFinal answer: The research gap is...\n\"\"\"\n\n\n\nCode\nresponse_direct = ollama.generate(prompt=direct_prompt, **params_llm)\nresponse_cot = ollama.generate(prompt=cot_prompt, **params_llm)\n\nprint(\"DIRECT PROMPT:\")\nprint(response_direct.response)\nprint(\"\\nCHAIN-OF-THOUGHT:\")\nprint(response_cot.response)\n\n\nDIRECT PROMPT:\nThe gap is in the complexity of the models used for community detection and temporal network analysis.\n\n\nCHAIN-OF-THOUGHT:\nHere's the breakdown of the research gap identified:\n\n1.  **What does each paper focus on?**\n    *   Community detection in static networks using modularity optimization.\n\n2.  **What topics appear in multiple papers?**\n    *   Temporal network analysis with sliding windows.\n\n3.  **What combination of topics is missing?**\n    *   Hierarchical community structure in social networks.\n\n4.  **What would be a valuable gap to fill?**\n    *   The gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is that the research gap is\n\n\nChain-of-thought produces more thoughtful, nuanced answers by forcing the model to decompose the problem into steps before committing to a conclusion. The mechanism is pattern matching. Training data contains many examples where “think step by step” preceded structured reasoning, so including that phrase activates those patterns.\nThe model does not actually reason. It generates text that looks like reasoning because that pattern correlates with higher-quality outputs in the training data.\nWhen should you use chain-of-thought? Use it when comparing multiple papers or concepts, identifying patterns, making recommendations, or analyzing arguments. Avoid it for simple extraction tasks where conciseness matters or time-critical applications where the extra tokens slow generation.\nWhat are the risks? Research indicates that chain-of-thought reasoning can be unfaithful. The explanations do not always accurately reflect the model’s true decision-making process. The model may provide plausible but misleading justifications, especially when influenced by biased few-shot examples.\nAlways validate the final answer independently rather than trusting the reasoning process alone.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Prompt Tuning"
    ]
  },
  {
    "objectID": "m03-agentic-coding/prompt-tuning.html#constraining-format-for-structured-extraction",
    "href": "m03-agentic-coding/prompt-tuning.html#constraining-format-for-structured-extraction",
    "title": "Prompt Tuning",
    "section": "Constraining Format for Structured Extraction",
    "text": "Constraining Format for Structured Extraction\n\n\n\n\n\n\nJSON schema constraints enforce structured output format at the token level, preventing invalid outputs\n\n\n\n\nFigure 3\n\n\n\nLLMs often violate structured data necessary for parsing programmatically. You need machine-readable output, not freeform text. What’s the solution?\nConstrain output format explicitly. Let’s consider a prompt that requests JSON output:\n\nimport json\nfrom pydantic import BaseModel\n\nabstract = \"\"\"\nWe analyze 10,000 scientific collaborations using network analysis and machine\nlearning. Our random forest classifier predicts collaboration success with 76%\naccuracy. Key factors include prior co-authorship and institutional proximity.\n\"\"\"\n\nprompt_json = f\"\"\"Extract information from this abstract and return ONLY valid JSON:\n\nAbstract: {abstract}\n\nReturn this exact structure:\n{{\n  \"n_samples\": &lt;number or null&gt;,\n  \"methods\": [&lt;list of methods&gt;],\n  \"accuracy\": &lt;number or null&gt;,\n  \"domain\": \"&lt;research field&gt;\"\n}}\n\nJSON:\"\"\"\n\n\n\nCode\n# Use lower temperature for structured output\nparams_structured = {\"model\": \"gemma3n:latest\", \"options\": {\"temperature\": 0}}\nresponse = ollama.generate(prompt=prompt_json, **params_structured)\n\ntry:\n    data = json.loads(response.response)\n    print(\"Extracted data:\")\n    print(json.dumps(data, indent=2))\nexcept json.JSONDecodeError:\n    print(\"Failed to parse JSON. Raw output:\")\n    print(response.response)\n\n\nFailed to parse JSON. Raw output:\n```json\n{\n \"n_samples\": 10000,\n \"methods\": [\"network analysis\", \"machine learning\", \"random forest\"],\n \"accuracy\": 76,\n \"domain\": \"scientific collaborations\"\n}\n```\n\n\nThis works by activating patterns where “return ONLY valid JSON” preceded JSON-formatted outputs. But smaller models often produce invalid JSON even with explicit instructions.\nWhat’s a more reliable approach? Use JSON schema constraints that enforce format during token generation. The model literally cannot generate tokens that violate the schema. Define the schema using Pydantic:\n\nfrom pydantic import BaseModel\n\nclass PaperMetadata(BaseModel):\n    domain: str\n    methods: list[str]\n    n_samples: int | None\n    accuracy: float | None\n\njson_schema = PaperMetadata.model_json_schema()\n\nThen pass the schema directly to the API, which constrains token generation:\n\nprompt_schema = f\"\"\"Extract information from this abstract:\n\nAbstract: {abstract}\"\"\"\n\n\n\nCode\nresponse = ollama.generate(prompt=prompt_schema, format=json_schema, **params_structured)\n\ntry:\n    data = json.loads(response.response)\n    metadata = PaperMetadata(**data)\n    print(\"Extracted and validated data:\")\n    print(json.dumps(data, indent=2))\nexcept (json.JSONDecodeError, ValueError) as e:\n    print(f\"Error: {e}\")\n    print(\"Raw output:\", response.response)\n\n\nExtracted and validated data:\n{\n  \"domain\": \"Scientific Collaborations\",\n  \"methods\": [\n    \"Network Analysis\",\n    \"Machine Learning\",\n    \"Random Forest Classifier\"\n  ],\n  \"n_samples\": 10000,\n  \"accuracy\": 76.0\n}\n\n\nJSON schema constraints are more reliable than prompt-based requests because they operate at the token level. The model cannot sample tokens that would create invalid JSON. The prompt activates extraction patterns. The schema enforces structure.\nWhat are the limitations? Smaller models sometimes produce invalid JSON even with schema constraints. Always wrap parsing in try-except blocks and validate outputs. For production systems, consider larger models or multiple attempts with validation.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Prompt Tuning"
    ]
  },
  {
    "objectID": "m03-agentic-coding/prompt-tuning.html#allowing-uncertainty-to-reduce-hallucination",
    "href": "m03-agentic-coding/prompt-tuning.html#allowing-uncertainty-to-reduce-hallucination",
    "title": "Prompt Tuning",
    "section": "Allowing Uncertainty to Reduce Hallucination",
    "text": "Allowing Uncertainty to Reduce Hallucination\nLLMs confidently fabricate facts when they don’t know the answer because they optimize for fluency, not truth. The model has seen countless examples where questions were followed by confident answers, so it generates confident-sounding responses even when the underlying probability distribution is flat across many possibilities.\nWhat’s the solution? Explicitly give the model permission to admit ignorance. Compare a prompt that implicitly demands an answer:\n\nbad_prompt = \"\"\"Summarize the main findings from the 2023 paper by Johnson et al.\non quantum community detection in biological networks.\"\"\"\n\nAgainst a prompt that explicitly allows uncertainty:\n\ngood_prompt = \"\"\"I'm looking for a 2023 paper by Johnson et al. on quantum\ncommunity detection in biological networks.\n\nIf you know this paper, summarize its main findings.\nIf you're not certain this paper exists, say \"I cannot verify this paper exists\"\nand do NOT make up details.\n\nResponse:\"\"\"\n\n\n\nCode\nresponse_bad = ollama.generate(prompt=bad_prompt, **params_llm)\nresponse_good = ollama.generate(prompt=good_prompt, **params_llm)\n\nprint(\"BAD PROMPT (encourages hallucination):\")\nprint(response_bad.response)\nprint(\"\\nGOOD PROMPT (allows uncertainty):\")\nprint(response_good.response)\n\n\nBAD PROMPT (encourages hallucination):\nThe 2023 paper by Johnson et al. on quantum community detection in biological networks, titled \"Quantum Community Detection in Biological Networks,\" investigated the use of quantum technologies to detect and characterize biological networks. The study focused on the development of quantum-based methods for detecting and characterizing biological networks, including network structure, connectivity, and network topology. The authors explored various quantum algorithms and techniques, including quantum interference detection, quantum clustering, and quantum network analysis. They demonstrated the effectiveness of these methods in detecting and characterizing networks in various biological systems, including bacteria, fungi, and plants. The findings highlighted the potential of quantum technologies to improve the accuracy and efficiency of biological network detection and characterization.\n\nGOOD PROMPT (allows uncertainty):\nI cannot verify this paper exists.\n\n\n\nThe good prompt activates patterns where explicit permission to admit ignorance preceded honest uncertainty statements. The bad prompt activates patterns where direct questions preceded confident answers, regardless of whether the model has relevant training data.\nAdditional strategies include asking for confidence levels (though models often overestimate confidence), requesting citations (though models hallucinate these too), and cross-validating critical information with external sources.\nWhy does this matter? LLMs closely follow your instructions, even when they should not. They often attempt to answer beyond their actual capabilities. Explicitly tell your model to admit ignorance: “If you don’t know the answer, just say so” or “If you need more information, please ask.”\nLLMs are trained to be agreeable, which can hinder productive brainstorming or honest critique. Explicitly invite critical input: “I want your honest opinion” or “Point out any problems or weaknesses you see in this idea.”",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Prompt Tuning"
    ]
  },
  {
    "objectID": "m03-agentic-coding/prompt-tuning.html#sampling-multiple-times-for-consistency",
    "href": "m03-agentic-coding/prompt-tuning.html#sampling-multiple-times-for-consistency",
    "title": "Prompt Tuning",
    "section": "Sampling Multiple Times for Consistency",
    "text": "Sampling Multiple Times for Consistency\nFor tasks requiring reasoning, generating multiple responses and selecting the most common answer often improves accuracy. This technique is called self-consistency.\nIt exploits the fact that correct reasoning tends to converge on the same answer while hallucinations vary randomly across samples. Let’s define the prompt:\n\nfrom collections import Counter\n\nprompt_consistency = \"\"\"Three papers study network robustness:\n- Paper A: Targeted attacks are most damaging\n- Paper B: Random failures rarely cause collapse\n- Paper C: Hub nodes are critical for robustness\n\nWhat is the research consensus on network robustness? Give a one-sentence answer.\n\"\"\"\n\nGenerate multiple responses with higher temperature to increase diversity, then identify the most common answer:\n\n\nCode\n# Use higher temperature for diversity\nparams_creative = {\"model\": \"gemma3n:latest\", \"options\": {\"temperature\": 0.7}}\n\n# Generate 5 responses\nresponses = []\nfor i in range(5):\n    response = ollama.generate(prompt=prompt_consistency, **params_creative)\n    responses.append(response.response.strip())\n    print(f\"Response {i+1}: {responses[-1]}\\n\")\n\n# In practice, you'd programmatically identify the most common theme\nprint(\"The most consistent theme across responses would be selected.\")\n\n\nResponse 1: The research consensus on network robustness is that it's a complex issue influenced by various factors, including the vulnerability of targeted attacks, the resilience to random failures, and the importance of critical nodes like hubs.\n\nResponse 2: The research consensus on network robustness is that it's a complex issue influenced by factors like targeted attacks, random failures, and the importance of critical nodes, suggesting a multifaceted approach is needed to understand and improve network resilience.\n\nResponse 3: The research consensus on network robustness is that it's a complex issue influenced by various factors, including the vulnerability of targeted attacks, the resilience to random failures, and the importance of critical nodes like hubs.\n\nResponse 4: The research consensus on network robustness is that it's a complex issue influenced by both targeted attacks and random failures, with the vulnerability of critical hub nodes playing a significant role in overall network resilience.\n\nResponse 5: The research consensus on network robustness is that it's a multifaceted issue involving vulnerabilities to both targeted attacks and random failures, with the structure of the network, particularly the presence of critical hub nodes, significantly influencing its resilience.\n\nThe most consistent theme across responses would be selected.\n\n\nSelf-consistency works because correct reasoning patterns converge toward the same conclusion when sampled multiple times while fabricated details vary randomly.\nWhat’s the tradeoff? The tradeoff is significant. Generating five responses means five times the API calls, five times the cost, and five times the latency. Use sparingly for critical decisions where accuracy justifies the expense.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Prompt Tuning"
    ]
  },
  {
    "objectID": "m03-agentic-coding/prompt-tuning.html#the-takeaway",
    "href": "m03-agentic-coding/prompt-tuning.html#the-takeaway",
    "title": "Prompt Tuning",
    "section": "The Takeaway",
    "text": "The Takeaway\nPrompt engineering is not magic. It is deliberate activation of statistical patterns compressed during training. Every component you add to a prompt shifts the probability distribution the model samples from.\nInstructions activate task-specific patterns. Output formats activate structured-response patterns. Personas activate stylistic patterns. Context disambiguates when multiple patterns compete. Examples demonstrate exact structure. Chain-of-thought activates reasoning-like patterns. Format constraints enforce structure at the token level. Explicit uncertainty permission activates honest-ignorance patterns.\nNone of this requires the model to understand what you want. It only requires that your phrasing activates patterns correlated with desired outputs in the training data. You are not communicating intent. You are manipulating probability distributions.\nMaster this, and you reliably extract value from LLMs for research workflows: summarization, structured extraction, hypothesis generation, and literature analysis.\nThe deeper question remains: how do these models represent text internally? When you send a prompt, the model does not see English words. It sees numbers. Millions of numbers arranged in high-dimensional space.\nThese numbers, called embeddings, are the foundation of everything LLMs do. Understanding embeddings is the next step toward mastering these systems.",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Prompt Tuning"
    ]
  },
  {
    "objectID": "m04-text/gpt-inference.html",
    "href": "m04-text/gpt-inference.html",
    "title": "GPT Inference: Sampling Strategies",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module introduces how GPT generates text through sampling.\nYou’ll learn:\n\nWhat sampling strategies are and how they differ from deterministic selection.\nHow to use temperature, top-k, and nucleus sampling to control generation quality.\nThe counterintuitive insight that randomness produces better text than always picking the best token.\nPractical consequences for building applications that need coherent, diverse, or creative outputs.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "GPT Inference: Sampling Strategies"
    ]
  },
  {
    "objectID": "m04-text/gpt-inference.html#how-gpt-generates-text",
    "href": "m04-text/gpt-inference.html#how-gpt-generates-text",
    "title": "GPT Inference: Sampling Strategies",
    "section": "How GPT Generates Text",
    "text": "How GPT Generates Text\n\nWhat do you think happens when GPT generates the next word in a sentence? Here’s the reality: GPT doesn’t output a single word. It outputs a probability distribution over its entire vocabulary (millions of possible tokens, each with a likelihood). The distribution is high-dimensional, making sampling computationally expensive but also rich with alternative paths.\nThe naive approach is to always pick the highest probability token (greedy sampling), but this creates a deterministic trap where the model falls into repetitive loops. What’s the solution? Controlled randomness. By sampling from the distribution rather than deterministically selecting the peak, we introduce diversity. But blind random sampling produces incoherent text. The challenge is finding the middle ground: sample broadly enough to avoid repetition, but narrowly enough to maintain coherence.\nThink of it like improvisational jazz. A musician playing the same note repeatedly (greedy sampling) is boring. Playing random notes (uniform sampling) is noise. The art is in sampling from the most promising notes while occasionally taking creative risks.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "GPT Inference: Sampling Strategies"
    ]
  },
  {
    "objectID": "m04-text/gpt-inference.html#sampling-strategies-in-practice",
    "href": "m04-text/gpt-inference.html#sampling-strategies-in-practice",
    "title": "GPT Inference: Sampling Strategies",
    "section": "Sampling Strategies in Practice",
    "text": "Sampling Strategies in Practice\nHere is an interactive demo of GPT inference available online at https://static.marimo.app/static/gpt-ar61. You can try different sampling strategies and see the results.\nGPT generates text one token at a time, repeatedly sampling from the probability distribution. Let’s examine the strategies for sampling that balance quality and diversity.\nFirst, let’s set up our connection to Ollama:\n\nimport ollama\n\n# Make sure you have Ollama running and a model pulled\n# Run: ollama pull mapler/gpt2\nMODEL = \"mapler/gpt2\"\nPROMPT = \"Hi there! \"\n\n\nGreedy Sampling\nGreedy sampling always picks the highest probability token. This is deterministic but can lead to repetitive or trapped text. For example, if the model predicts “the” with high probability, it will always predict “the” again. This is the jazz musician stuck on a single note.\n\n\n\n\n\nGPT greedy search.\nLet’s see greedy sampling in action. We set temperature to 0 to make the sampling deterministic (always picking the highest probability token):\n\ngreedy_response = ollama.generate(\n    model=MODEL,\n    prompt=PROMPT,\n    options={\n        \"temperature\": 0,  # greedy sampling\n        \"num_predict\": 20,  # max tokens to generate\n    }\n)\nprint(greedy_response['response'])\n\n I'm so glad you're here.\nThe first thing I did was to make a list\n\n\nThe output is often repetitive because greedy sampling always selects the most probable token at each step. Try running it multiple times. You’ll get the exact same output each time.\n\n\nBeam Search\nWhat if we could look ahead? Beam search alleviates the repetition problem by taking into account the high-order dependencies between tokens. For example, in generating “The cat ran across the ___“, beam search might preserve a path containing”mat” even if “floor” or “room” have higher individual probabilities at that position. This is because the complete sequence like “mat quickly” could be more probable when considering the token next after “mat”. “The cat ran across the mat quickly” is a more natural phrase than “The cat ran across the floor quickly” when considering the full flow and common linguistic patterns.\n\n\n\n\n\nGPT beam search.\nHow does beam search work? It maintains multiple possible sequences (beams) in parallel, exploring different paths simultaneously. At each step, it expands all current beams, scores the resulting sequences, and keeps only the top-k highest scoring ones. For instance, with a beam width of 3, first beams might be [“The cat ran”, “The cat walked”, “The cat jumped”]. Next step: [“The cat ran across”, “The cat ran through”, “The cat walked across”]. And so on, keeping the 3 most promising complete sequences at each step. This process continues until reaching the end, finally selecting the sequence with highest overall probability. Beam search can be combined with top-k sampling or nucleus sampling. For example, one can sample a token based on top-k sampling or nucleus sampling to form the next beam.\nWhat’s the downside? While beam search often produces high-quality outputs since it considers longer-term coherence, it can still suffer from the problem of repetitive or trapped text. It’s the jazz ensemble playing in perfect harmony, technically excellent but predictable.\nNote: Ollama doesn’t natively support beam search through its API. Beam search requires access to the model’s internal scoring mechanism, which is typically implemented at a lower level (using HuggingFace Transformers or direct PyTorch/TensorFlow implementations). For production beam search, you would use libraries like transformers or vLLM.\n\n\nFrom Deterministic to Stochastic Sampling\nBoth greedy and beam search are deterministic. They pick the most likely token at each step, creating a loop where the model always predicts the same tokens repeatedly. What’s the solution? Sample a token from the distribution.\nTop-k sampling relaxes the deterministic nature of greedy sampling by selecting randomly from the k most likely next tokens at each generation step. While this introduces some diversity compared to greedy sampling, choosing a fixed k can be problematic. The value of k might be too large for some distribution tails (including many poor options) or too small for others (excluding reasonable options). In our jazz analogy, this is like saying “you can only improvise using these five specific notes”. Sometimes that’s perfect, sometimes it’s too limiting.\n\ntop_k_response = ollama.generate(\n    model=MODEL,\n    prompt=PROMPT,\n    options={\n        \"temperature\": 1.0,  # enable stochastic sampling\n        \"top_k\": 10,  # restrict to top 10 tokens\n        \"num_predict\": 20,\n    }\n)\nprint(top_k_response['response'])\n\n You're welcome.\nAnd if you don't want to read that part, please skip ahead\n\n\nTry running this multiple times. You’ll get different outputs each time because the model samples randomly from the top-k tokens.\nWhat’s a better approach? Nucleus sampling addresses this limitation by dynamically selecting tokens based on cumulative probability. It samples from the smallest set of tokens whose cumulative probability exceeds a threshold p (for example, 0.9). This adapts naturally to different probability distributions: select few tokens when the distribution is concentrated and more when it’s spread out. This approach often provides a good balance between quality and diversity. The jazz musician now has flexibility: when the melody is clear, stick to a few notes. When it’s time to explore, draw from a wider palette.\n\n\n\n\n\nNucleus sampling. The image is taken from this blog.\n\ntop_p_response = ollama.generate(\n    model=MODEL,\n    prompt=PROMPT,\n    options={\n        \"temperature\": 1.0,\n        \"top_p\": 0.95,  # sample from tokens with cumulative probability &gt;= 0.95\n        \"num_predict\": 20,\n    }\n)\nprint(top_p_response['response'])\n\n Thank you for visiting!!!\nIt's been a while since I've updated this blog. And\n\n\nNucleus sampling dynamically adjusts the number of candidate tokens based on the probability distribution, making it more adaptive than fixed top-k.\n\n\nTemperature Control\nTemperature (\\tau) modifies how “concentrated” the probability distribution is for sampling by scaling the logits before applying softmax:\n\np_i = \\frac{\\exp(z_i/\\tau)}{\\sum_j \\exp(z_j/\\tau)}\n\nHere z_i are the logits and \\tau is the temperature parameter.\nWhat does temperature do to the distribution? Lower temperatures (\\tau &lt; 1.0) make the distribution more peaked, making high probability tokens even more likely to be chosen, leading to more focused and conservative outputs. Higher temperatures (\\tau &gt; 1.0) flatten the distribution by making the logits more similar, increasing the chances of selecting lower probability tokens and producing more diverse but potentially less coherent text. As \\tau \\to 0, the distribution approaches a one-hot vector (equivalent to greedy search). As \\tau \\to \\infty, it approaches a uniform distribution. In jazz terms, temperature controls the musician’s mood: low temperature is playing it safe (sticking to the melody), while high temperature is experimental improvisation (sometimes brilliant, sometimes cacophonous).\n\n\n\n\n\nTemperature controls the concentration of the probability distribution. Lower temperature makes the distribution more peaked, while higher temperature makes the distribution more flat.\nLet’s see how temperature affects generation:\n\nfor tau in [0.1, 0.5, 1.0, 2.0, 5.0]:\n    response = ollama.generate(\n        model=MODEL,\n        prompt=PROMPT,\n        options={\n            \"temperature\": tau,\n            \"num_predict\": 20,\n        }\n    )\n    print(f\"τ = {tau}: {response['response']}\")\n\nτ = 0.1:  I'm so glad you're here.\nYou can check out the full list of all my\nτ = 0.5:  We are making a great game and we hope you enjoy it as much, if not more.\nτ = 1.0:  As many of you know, I'm an avid and active supporter for the United Kingdom Independence Party\nτ = 2.0:  What a brilliant post to share with your community. You really helped inspire the world and will be\nτ = 5.0:  We're making the movie now because of an emergency. A guy died and it is getting very\n\n\nNotice the pattern: low temperature (τ = 0.1) produces conservative, focused output, medium temperature (τ = 1.0) provides balanced diversity, and high temperature (τ = 5.0) produces creative but potentially incoherent output.\n\n\nCombining All Strategies\nYou can combine top-k, top-p, and temperature for fine-grained control:\n\ncombined_response = ollama.generate(\n    model=MODEL,\n    prompt=PROMPT,\n    options={\n        \"temperature\": 0.7,  # moderate randomness\n        \"top_k\": 10,         # restrict to top 10 tokens\n        \"top_p\": 0.95,       # within top-k, use nucleus sampling\n        \"num_predict\": 20,\n    }\n)\nprint(combined_response['response'])\n\n I am not trying to be an expert at all. I know what you're thinking: \"\n\n\nWhat does this combination do? It restricts candidates to top-k tokens, then applies nucleus sampling, and finally uses temperature to control randomness. This gives you maximum control over the generation process.\nThe jazz musician now has a framework. Work within these chords (top-k), adapt to the moment (nucleus), and choose your creative intensity (temperature).\n\n\nPractical Recommendations\nWhat settings should you use? For most applications, use nucleus sampling with p = 0.9 and temperature τ = 0.7. This combination provides a good balance between coherence and creativity. For tasks requiring high factual accuracy (e.g., technical documentation), lower the temperature to τ = 0.3 to make the model more conservative. For creative writing, increase the temperature to τ = 1.0 or higher to encourage exploration. Beam search is useful when you need the single most probable sequence (e.g., machine translation), but it sacrifices diversity. Use it when correctness matters more than variety.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "GPT Inference: Sampling Strategies"
    ]
  },
  {
    "objectID": "m04-text/gpt-inference.html#the-key-insight",
    "href": "m04-text/gpt-inference.html#the-key-insight",
    "title": "GPT Inference: Sampling Strategies",
    "section": "The Key Insight",
    "text": "The Key Insight\nGeneration is sampling. Greedy picks the peak, beam search explores multiple peaks, and stochastic sampling adds controlled randomness. Temperature flattens or sharpens the distribution; nucleus sampling adapts to its shape. The right strategy depends on whether you’re optimizing for accuracy or creativity.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "GPT Inference: Sampling Strategies"
    ]
  },
  {
    "objectID": "m04-text/overview.html",
    "href": "m04-text/overview.html",
    "title": "Module 4: Deep Learning for Text",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module opens the hood of Large Language Models to understand the revolution in Natural Language Processing.\nYou’ll learn:\n\nWhat Large Language Models are and how to control their generation through sampling strategies and parameters.\nHow tokenization converts text into chunks that LLMs can process and why tokenization choices matter.\nThe Transformer architecture and its key components like attention mechanisms and positional encoding.\nHow word embeddings represent meaning as geometric relationships in vector space.\nThe notion of semantic axes and how to measure meaning as direction in embedding spaces.\nHow word bias emerges in learned representations and what it means for fairness in NLP systems.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Overview"
    ]
  },
  {
    "objectID": "m04-text/overview.html#the-journey",
    "href": "m04-text/overview.html#the-journey",
    "title": "Module 4: Deep Learning for Text",
    "section": "The Journey",
    "text": "The Journey\nLet’s talk about where this module takes you. Have you ever wondered what lies inside a Large Language Model? At the core of agentic systems sits the LLM. It acts as the kernel of the operating system. Unlike actual computer systems, it speaks in natural language. But how do LLMs understand natural language in the first place? That’s the central question of this module.\nLarge Language Models in Practice\nWe start by interacting with the giants. You’ll explore what LLMs are and how they work at a high level. More importantly, you’ll learn the practical skills for using them effectively in applications.\nGPT Inference: The Art of Generation\nHow does an LLM generate text? Shift your attention to the sampling process. You’ll learn about temperature, top-k, top-p, and other parameters that control generation. Understanding these knobs transforms you from someone who uses LLMs to someone who controls them precisely.\nTokenization: How LLMs Read Text\nBefore LLMs can process text, they must convert it into tokens. This seemingly simple step has profound implications. You’ll discover why “SolidGoldMagikarp” breaks GPT models and why some languages require 10x more tokens than others.\nTransformers: The Architecture Revolution\nNow we dive deep. The Transformer architecture revolutionized NLP. The key insight is that meaning emerges through context, not from isolated words. You’ll understand self-attention, the mechanism that lets models weigh the relevance of every word to every other word.\nBERT, GPT, and Sentence Transformers\nTransformers come in different flavors. BERT reads bidirectionally for understanding. GPT reads left-to-right for generation. Sentence Transformers produce fixed-length embeddings perfect for similarity search. You’ll learn when to use each architecture.\nWord Embeddings: Meaning as Geometry\nWhat if meaning could be geometry? That’s the profound insight behind word embeddings. Words become vectors in high-dimensional space. Meaning emerges from geometric relationships. “King” - “man” + “woman” ≈ “queen” isn’t magic. It’s linear algebra.\nSemantic Axes: Meaning as Direction\nShift your perspective further. Meaning isn’t just a point in space. It’s a direction. The axis from “rich” to “poor” captures wealth. The axis from “hot” to “cold” captures temperature. You’ll learn to construct and measure these semantic dimensions.\nWord Bias: When Embeddings Reflect Society\nHere’s something uncomfortable. Word embeddings learn from human text, so they inherit human biases. “Doctor” associates more strongly with “man” than “woman.” “Programmer” skews male. “Nurse” skews female. You’ll understand how to measure and mitigate these biases.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Overview"
    ]
  },
  {
    "objectID": "m04-text/overview.html#why-this-matters",
    "href": "m04-text/overview.html#why-this-matters",
    "title": "Module 4: Deep Learning for Text",
    "section": "Why This Matters",
    "text": "Why This Matters\nThis module provides the foundation for everything that follows. You can’t build effective agentic systems without understanding what’s happening inside the LLM, debug prompt failures without knowing how tokenization works, or choose the right model without understanding architectural trade-offs.\nBut beyond practical utility, this knowledge is intellectually transformative. Understanding that meaning can be captured geometrically changes how you think about language, attention mechanisms change how you think about understanding, and recognizing bias in embeddings changes how you think about fairness in AI systems.\nThese ideas extend far beyond NLP. The Transformer architecture now powers computer vision models, embedding techniques apply to any kind of data with relationships, and attention mechanisms appear in recommendation systems, drug discovery, and protein folding. Understanding these concepts opens doors across machine learning.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Overview"
    ]
  },
  {
    "objectID": "m04-text/overview.html#prerequisites",
    "href": "m04-text/overview.html#prerequisites",
    "title": "Module 4: Deep Learning for Text",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou should be comfortable with basic Python programming and familiar with NumPy arrays. Neural network fundamentals matter here: forward propagation, backpropagation, and gradient descent. Linear algebra knowledge helps significantly since matrix multiplication, dot products, and vector spaces are everywhere in this module. Calculus basics (derivatives, chain rule) matter for understanding backpropagation, though we won’t derive everything from scratch. If you need to refresh these topics, review Module 1 for Python and data structures, and brush up on basic neural networks before diving deep here.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Overview"
    ]
  },
  {
    "objectID": "m04-text/overview.html#what-youll-build",
    "href": "m04-text/overview.html#what-youll-build",
    "title": "Module 4: Deep Learning for Text",
    "section": "What You’ll Build",
    "text": "What You’ll Build\nBy the end of this module, you’ll understand how tokenizers work and why they matter, implement attention mechanisms from scratch and build a simple Transformer, and create and analyze word embeddings to discover semantic relationships geometrically. You’ll measure bias in embeddings and understand mitigation strategies, control LLM generation precisely through sampling parameters, and know when to use BERT vs. GPT vs. sentence transformers for different tasks. Most importantly, you’ll develop intuition for how meaning is represented computationally, the foundation for everything from prompt engineering to fine-tuning to building novel NLP applications.\nLet’s begin by exploring what Large Language Models actually are.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Overview"
    ]
  },
  {
    "objectID": "m04-text/sentence-transformers.html",
    "href": "m04-text/sentence-transformers.html",
    "title": "Sentence Transformers",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module introduces Sentence Transformers, which collapse BERT’s token matrices into single vectors for fast semantic search.\nYou’ll learn:\n\nHow Siamese Networks use shared weights to embed sentences in a common vector space.\nThe role of pooling strategies (mean, max, CLS-token) in collapsing token matrices into sentence embeddings.\nHow to perform semantic search and clustering with pre-trained models.\nThe speed-accuracy trade-offs across different model sizes and architectures.\nWhere sentence embeddings break down (context collapse, word order sensitivity, domain shift).\nBERT gives you a vector for every token in a sentence. If you want to compare two sentences, you’re stuck comparing two messy matrices of varying sizes.\nThe naive approach is to average all token vectors. But this throws away positional information and treats every word equally. The word “not” in “not good” should drastically change the sentence embedding, but simple averaging dilutes its impact.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Sentence Transformers"
    ]
  },
  {
    "objectID": "m04-text/sentence-transformers.html#whats-the-solution",
    "href": "m04-text/sentence-transformers.html#whats-the-solution",
    "title": "Sentence Transformers",
    "section": "What’s the Solution?",
    "text": "What’s the Solution?\nSentence-BERT (SBERT) solves this by training a Siamese Network. The same BERT model processes two sentences independently, producing their respective token matrices. We then apply pooling (mean, max, or CLS-token extraction) to collapse each matrix into a single vector.\nThe training objective is contrastive. If the sentences are semantically similar (paraphrases), their vectors should be close in Euclidean or cosine space. If they’re unrelated, their vectors should be distant.\nThink of it like creating a library catalog. Instead of storing every word on every page, you compress each book into a single Dewey Decimal number. Books on similar topics get similar numbers, enabling efficient retrieval. The compression loses fine-grained detail, but gains search speed.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Sentence Transformers"
    ]
  },
  {
    "objectID": "m04-text/sentence-transformers.html#why-does-weight-sharing-matter",
    "href": "m04-text/sentence-transformers.html#why-does-weight-sharing-matter",
    "title": "Sentence Transformers",
    "section": "Why Does Weight Sharing Matter?",
    "text": "Why Does Weight Sharing Matter?\nThe mathematical trick is the Siamese architecture. Weight sharing ensures both sentences are embedded into the same vector space using identical transformations.\nThis makes the distance between vectors meaningful. Similar sentences cluster together, dissimilar ones push apart.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Sentence Transformers"
    ]
  },
  {
    "objectID": "m04-text/sentence-transformers.html#how-to-use-sentence-transformers",
    "href": "m04-text/sentence-transformers.html#how-to-use-sentence-transformers",
    "title": "Sentence Transformers",
    "section": "How to Use Sentence Transformers",
    "text": "How to Use Sentence Transformers\nLet’s see how to use Sentence Transformers in practice. We’ll start with semantic search, then move to clustering.\n\nBasic Semantic Search\nHere’s how to encode sentences and find the most similar matches:\n\nfrom sentence_transformers import SentenceTransformer, util\n\n# Load a pre-trained model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\ncorpus = [\n    \"A man is eating food.\",\n    \"A man is eating a piece of bread.\",\n    \"The girl is carrying a baby.\",\n    \"A man is riding a horse.\",\n    \"A woman is playing violin.\",\n    \"Two men pushed carts through the woods.\",\n    \"A man is riding a white horse on an enclosed ground.\",\n    \"A monkey is playing drums.\",\n    \"Someone in a gorilla costume is playing a set of drums.\"\n]\n\n# Encode all sentences into 384-dimensional vectors\ncorpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n\nquery = \"A man is eating pasta.\"\nquery_embedding = model.encode(query, convert_to_tensor=True)\n\n# Compute cosine similarities\nhits = util.semantic_search(query_embedding, corpus_embeddings, top_k=3)\n\nprint(f\"Query: {query}\")\nprint(\"\\nTop 3 most similar sentences:\")\nfor hit in hits[0]:\n    print(f\"{corpus[hit['corpus_id']]} (Score: {hit['score']:.4f})\")\n\nThe model correctly identifies “eating pasta” is semantically closest to “eating food” and “eating bread”, even though the exact words don’t match. This is semantic search: matching by meaning, not keywords.\n\n\nClustering Documents\nYou can also cluster documents by their semantic content:\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\nsentences = [\n    \"Python is a programming language\",\n    \"Java is used for software development\",\n    \"The cat sat on the mat\",\n    \"Dogs are loyal animals\",\n    \"Machine learning is a subset of AI\",\n    \"Neural networks mimic the brain\",\n]\n\nembeddings = model.encode(sentences)\n\n# Cluster into 2 groups\nnum_clusters = 2\nclustering_model = KMeans(n_clusters=num_clusters, random_state=42)\nclustering_model.fit(embeddings)\ncluster_assignment = clustering_model.labels_\n\nclustered_sentences = {}\nfor sentence_id, cluster_id in enumerate(cluster_assignment):\n    if cluster_id not in clustered_sentences:\n        clustered_sentences[cluster_id] = []\n    clustered_sentences[cluster_id].append(sentences[sentence_id])\n\nfor cluster_id, cluster_sentences in clustered_sentences.items():\n    print(f\"\\nCluster {cluster_id + 1}:\")\n    for sentence in cluster_sentences:\n        print(f\"  - {sentence}\")\n\nThe model separates technical/programming sentences from animal-related sentences without any labeled data. This is unsupervised semantic clustering.\n\n\nWhich Model Should You Use?\nDifferent Sentence Transformer models optimize for different trade-offs. The all-MiniLM-L6-v2 model is fast and lightweight (384 dimensions), good for most applications. The all-mpnet-base-v2 model offers higher quality (768 dimensions), slower but more accurate.\nThe multi-qa-mpnet-base-dot-v1 model is optimized for question-answering and retrieval tasks. The paraphrase-multilingual-mpnet-base-v2 model supports 50+ languages.\nWhat should drive your choice? Speed vs. accuracy, monolingual vs. multilingual, general-purpose vs. domain-specific. Pick the model that matches your constraints.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Sentence Transformers"
    ]
  },
  {
    "objectID": "m04-text/sentence-transformers.html#how-does-the-architecture-work",
    "href": "m04-text/sentence-transformers.html#how-does-the-architecture-work",
    "title": "Sentence Transformers",
    "section": "How Does the Architecture Work?",
    "text": "How Does the Architecture Work?\nLet’s talk about the Siamese Network architecture. Here’s the visual:\n\n\n\nSiamese Network\n\n\nBoth sentences pass through the same BERT model (shared weights). This ensures they’re embedded into a common vector space. The pooling layer then collapses each token matrix into a single vector.\nDuring training, the loss function pushes similar sentence pairs together and dissimilar pairs apart. This is contrastive learning: positive pairs attract, negative pairs repel.\n\nWhat Pooling Strategy Should You Use?\nYou have three main choices. Mean pooling averages all token vectors (most common). Max pooling takes the element-wise maximum across tokens. CLS-token uses the [CLS] token’s final hidden state (BERT’s built-in sentence representation).\nMean pooling generally works best. It captures information from all tokens while being robust to varying sentence lengths.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Sentence Transformers"
    ]
  },
  {
    "objectID": "m04-text/sentence-transformers.html#where-does-this-break",
    "href": "m04-text/sentence-transformers.html#where-does-this-break",
    "title": "Sentence Transformers",
    "section": "Where Does This Break?",
    "text": "Where Does This Break?\nStatic compression is a limitation. A sentence gets exactly one vector, regardless of context. “The bank” in “the river bank” and “the financial bank” might get similar embeddings if they share enough surrounding words. The model compresses meaning into a fixed point, losing nuance.\nWord order sensitivity is another concern. “The dog bit the man” and “The man bit the dog” share the same words. If the model relies too heavily on lexical overlap (bag-of-words similarity), they’ll end up dangerously close in vector space.\nGood models learn syntax, but they’re not perfect.\nWhat about computational cost? Although retrieval is fast (dot products), encoding large corpora is expensive. Encoding 1 million sentences with a large model can take hours. Pre-compute and cache embeddings whenever possible.\nDomain shift is a practical issue. Models trained on general text (Wikipedia, news) may perform poorly on specialized domains (medical, legal). Fine-tuning on domain-specific data helps, but requires labeled sentence pairs.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Sentence Transformers"
    ]
  },
  {
    "objectID": "m04-text/sentence-transformers.html#the-key-takeaway",
    "href": "m04-text/sentence-transformers.html#the-key-takeaway",
    "title": "Sentence Transformers",
    "section": "The Key Takeaway",
    "text": "The Key Takeaway\nSentence Transformers collapse BERT’s token matrix into a single vector using Siamese Networks and contrastive learning. The result is fast semantic search: encode once, compare with dot products.\nChoose your pooling strategy and model size based on speed-accuracy trade-offs. Remember that compression always loses information.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Sentence Transformers"
    ]
  },
  {
    "objectID": "m04-text/transformers.html",
    "href": "m04-text/transformers.html",
    "title": "Transformers",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module introduces the transformer architecture, the foundation of modern language models.\nYou’ll learn:\n\nWhy static embeddings fail to capture context-dependent meanings.\nHow attention mechanisms compute context-aware representations through weighted mixing.\nThe role of Query, Key, and Value transformations in learning word relationships.\nHow multi-head attention captures different aspects of meaning in parallel.\nThe architecture of encoder and decoder modules with position embeddings, residual connections, and layer normalization.\nWhy transformers represent the fundamental shift in how machines understand language.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Transformers"
    ]
  },
  {
    "objectID": "m04-text/transformers.html#the-problem-with-one-vector-per-word",
    "href": "m04-text/transformers.html#the-problem-with-one-vector-per-word",
    "title": "Transformers",
    "section": "The Problem with One Vector Per Word",
    "text": "The Problem with One Vector Per Word\n\n\n\n\n\nFor many years, natural language processing treated words as having fixed meanings. Each word (like “bank”) received a single vector of numbers, called a static embedding. But there’s a hidden catch in this “one meaning per word” mindset: with just a single fixed entry in the dictionary, “bank” means exactly the same thing in “I deposited money at the bank” as in “We had a picnic by the bank”. Think of it like describing a population by its average height and pretending nobody’s shorter or taller, where every possible meaning gets mashed into a one-size-fits-all average and the interesting details vanish in the mix.\nWhat if we simply mixed the target word with its neighbors? For “I deposited money at the bank,” we could compute a contextualized representation as:\n\n\\vec{v}_{\\text{bank (new)}} = w_1 \\cdot \\vec{v}_{\\text{bank}} + w_2 \\cdot \\vec{v}_{\\text{deposited}} + w_3 \\cdot \\vec{v}_{\\text{money}} + \\cdots\n\nwhere w_i are weights and \\vec{v}_i are word embeddings.\nBut here’s the key question: how do we determine these weights? Consider that “bank” sits neutrally between financial terms (money) and geographical terms (river).\nTry manually adjusting the weights to contextualize “bank”:\n\nd3 = require(\"d3@7\", \"d3-simple-slider@1\")\n\n\n\n\n\n\n\nfunction sliderWithLabel(min, max, step, width, defaultValue, label) {\n  const slider = d3.sliderBottom()\n    .min(min).max(max).step(step).width(width).default(defaultValue);\n  const svg = d3.create(\"svg\").attr(\"width\", width + 50).attr(\"height\", 60);\n  svg.append(\"g\").attr(\"transform\", \"translate(25,20)\").call(slider);\n  svg.append(\"text\").attr(\"x\", (width + 50) / 2).attr(\"y\", 10).attr(\"text-anchor\", \"middle\").style(\"font-size\", \"5px\").text(label);\n  return svg.node();\n}\n\n\n\n\n\n\n\n{\n  function createWeightSlider(min, max, step, width, defaultValue, label) {\n    const slider = d3.sliderBottom()\n      .min(min).max(max).step(step).width(width).default(defaultValue);\n    const svg = d3.create(\"svg\").attr(\"width\", width + 50).attr(\"height\", 60);\n    const g = svg.append(\"g\").attr(\"transform\", \"translate(25,20)\");\n    g.call(slider);\n    svg.append(\"text\").attr(\"x\", (width + 50) / 2).attr(\"y\", 10)\n       .attr(\"text-anchor\", \"middle\").style(\"font-size\", \"12px\").text(label);\n    return { node: svg.node(), slider: slider };\n  }\n\n  const bankSliderObj = createWeightSlider(0, 1, 0.01, 120, 1.0, \"Bank weight\");\n  const moneySliderObj = createWeightSlider(0, 1, 0.01, 120, 0.0, \"Money weight\");\n  const riverSliderObj = createWeightSlider(0, 1, 0.01, 120, 0.0, \"River weight\");\n\n  const contextWords = [\"bank\", \"money\", \"river\"];\n  const contextEmbeddings = [\n    [0.0, 0.0],\n    [-1.6, -0.6],\n    [1.4, -1.0]\n  ];\n\n  const plotContainer = document.createElement(\"div\");\n\n  function update() {\n    const bankWeight = bankSliderObj.slider.value();\n    const moneyWeight = moneySliderObj.slider.value();\n    const riverWeight = riverSliderObj.slider.value();\n\n    const weights = [bankWeight, moneyWeight, riverWeight];\n    const total = weights.reduce((a, b) =&gt; a + b, 0);\n    const normalizedWeights = total &gt; 0 ? weights.map(w =&gt; w / total) : [0, 0, 0];\n\n    const newVec = [\n      normalizedWeights[0] * contextEmbeddings[0][0] +\n      normalizedWeights[1] * contextEmbeddings[1][0] +\n      normalizedWeights[2] * contextEmbeddings[2][0],\n      normalizedWeights[0] * contextEmbeddings[0][1] +\n      normalizedWeights[1] * contextEmbeddings[1][1] +\n      normalizedWeights[2] * contextEmbeddings[2][1]\n    ];\n\n    const originalData = contextWords.map((word, i) =&gt; ({\n      word: word,\n      x: contextEmbeddings[i][0],\n      y: contextEmbeddings[i][1],\n      type: \"Original\"\n    }));\n\n    const contextualizedData = [{\n      word: \"bank (new)\",\n      x: newVec[0],\n      y: newVec[1],\n      type: \"Contextualized\"\n    }];\n\n    const data = [...originalData, ...contextualizedData];\n\n    d3.select(plotContainer).selectAll(\"*\").remove();\n\n    const plot = Plot.plot({\n      width: 300,\n      height: 300,\n      marginTop: 60,\n      marginRight: 20,\n      marginBottom: 50,\n      marginLeft: 60,\n      style: {\n        background: \"white\",\n        color: \"black\"\n      },\n      x: {\n        domain: [-2, 2],\n        label: \"Dimension 1\",\n        grid: true,\n        ticks: 10\n      },\n      y: {\n        domain: [-2, 2],\n        label: \"Dimension 2\",\n        grid: true,\n        ticks: 10\n      },\n      color: {\n        domain: [\"Original\", \"Contextualized\"],\n        range: [\"#dadada\", \"#ff7f0e\"]\n      },\n      marks: [\n        Plot.dot(data, {\n          x: \"x\",\n          y: \"y\",\n          fill: \"type\",\n          r: 8,\n          tip: true\n        }),\n        Plot.text(data, {\n          x: \"x\",\n          y: \"y\",\n          text: \"word\",\n          dy: -15,\n          fontSize: 8,\n          fontWeight: \"bold\",\n          fill: \"black\"\n        }),\n        Plot.text([{x: 0, y: 2.3}], {\n          x: \"x\",\n          y: \"y\",\n          text: () =&gt; `Weights: Bank=${normalizedWeights[0].toFixed(2)}, Money=${normalizedWeights[1].toFixed(2)}, River=${normalizedWeights[2].toFixed(2)}`,\n          fontSize: 11,\n          fill: \"black\"\n        }),\n        Plot.dot([{x: -0.8, y: 2.7, color: \"#dadada\"}, {x: 0.8, y: 2.7, color: \"#ff7f0e\"}], {\n          x: \"x\",\n          y: \"y\",\n          fill: \"color\",\n          r: 6\n        }),\n        Plot.text([{x: -0.5, y: 2.7, label: \"Original\"}, {x: 1.1, y: 2.7, label: \"Contextualized\"}], {\n          x: \"x\",\n          y: \"y\",\n          text: \"label\",\n          fontSize: 10,\n          fill: \"black\",\n          textAnchor: \"start\"\n        })\n      ]\n    });\n\n    d3.select(plotContainer).node().appendChild(plot);\n  }\n\n  bankSliderObj.slider.on(\"onchange\", update);\n  moneySliderObj.slider.on(\"onchange\", update);\n  riverSliderObj.slider.on(\"onchange\", update);\n\n  update();\n\n  return html`&lt;div style=\"display: flex; align-items: center; gap: 40px; justify-content: center;\"&gt;\n    &lt;div style=\"display: flex; flex-direction: column; gap: 10px;\"&gt;\n      ${bankSliderObj.node}\n      ${moneySliderObj.node}\n      ${riverSliderObj.node}\n    &lt;/div&gt;\n    &lt;div&gt;\n      ${plotContainer}\n    &lt;/div&gt;\n  &lt;/div&gt;`;\n}\n\n\n\n\n\n\nBy changing the weights, we see that the vector for “bank” can lean more towards financial terms or geographical terms.\nSo how do we determine the weights? The simplest idea gives each word equal weight: w_i = 1/N, creating a basic bag-of-words average. But sentences aren’t this fair: some words are much more important than others. In “I deposited money at the bank,” the words “deposited” and “money” are key, while “I”, “at”, and “the” add little meaning. If we treat all words the same, we lose the details that matter and need a way to highlight important words and downplay the rest.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Transformers"
    ]
  },
  {
    "objectID": "m04-text/transformers.html#attention-mechanism",
    "href": "m04-text/transformers.html#attention-mechanism",
    "title": "Transformers",
    "section": "Attention Mechanism",
    "text": "Attention Mechanism\n\n\n\n\n\nLet’s walk through how transformers identify the surrounding words that are relevant to a focal word to be contextualized. This process is called the attention mechanism.\nBefore diving in, let’s prepare some terminology. Suppose we have the sentence “I deposited money at the bank”. Given the word “bank”, we want to determine the weights w_i for the surrounding words “I”, “deposited”, “money”, and “at”.\nWe call “bank” the query word, and the surrounding words the key words. At a high level, we compute the weights w_i for each query and key pair, then average them.\n\n\\vec{v}_{\\text{query}}^{\\text{c}} = \\sum_{i=1}^N w_i \\cdot \\vec{v}_{i}\n\nwith weights w_i being determined by the query and key vectors w_{i}:=f(\\vec{v}_{\\text{query}}, \\vec{v}_{i}). This function, f, is called the attention score function.\nIn transformers, the attention score function f is implemented as follows. Given the original vector for a word (whether it’s the query word or the key word), we linearly transform it into three vectors: Query, Key, and Value.\n\n\\begin{align}\n\\vec{q}_i &= W_Q \\vec{x}_i\\\\\n\\vec{k}_i &= W_K \\vec{x}_i\\\\\n\\vec{v}_i &= W_V \\vec{x}_i\n\\end{align}\n\nWhy do we need three different vectors? Imagine you’re at a dinner party wanting to identify people talking about a topic you care about. You listen to surrounding people (playing as a listener), broadcast your own interests (playing as a speaker), and engage with conversation content. The query vector represents you as a listener, the key vector represents the people as speakers, and the value vector represents the conversation content.\nOnce we have the query, key, and value vectors, we compute the attention scores between the query and key vector:\n\nw_{ij} = \\frac{\\exp(\\vec{q}_i \\cdot \\vec{k}_j / \\sqrt{d})}{\\sum_{\\ell} \\exp(\\vec{q}_i \\cdot \\vec{k}_\\ell / \\sqrt{d})},\n\nwhere \\vec{q}_i \\cdot \\vec{k}_j is the dot product between the query and key vectors, which is larger when the query and key vectors are similar (pointing to a similar direction).\nThe division by \\sqrt{d} (where d is the embedding dimension) is a scaling factor that prevents vanishing gradients during training. What is the vanishing gradient problem? It’s when gradients of the loss function with respect to weights become too small to be effective during training.\nFinally, compute the contextualized representation as a weighted sum: \\text{contextualized}_i = \\sum_j w_{ij} \\vec{v}_j.\nNow explore how different Query and Key transformations produce different attention patterns. First, let us create the key, query, and value vectors. In 2d, the linear transformation is just a scaling and a rotation.\n\nfunction createQKVSlider(min, max, step, width, defaultValue, label, valueSetter) {\n  const slider = d3.sliderBottom()\n    .min(min).max(max).step(step).width(width).default(defaultValue)\n    .on('onchange', val =&gt; valueSetter(val));\n  const svg = d3.create(\"svg\").attr(\"width\", width + 40).attr(\"height\", 50);\n  const g = svg.append(\"g\").attr(\"transform\", \"translate(20,15)\");\n  g.call(slider);\n  svg.append(\"text\").attr(\"x\", (width + 40) / 2).attr(\"y\", 10)\n     .attr(\"text-anchor\", \"middle\").style(\"font-size\", \"11px\").text(label);\n  return { node: svg.node(), slider: slider };\n}\n\n\n\n\n\n\n\nmutable qScaleXValue = 1.0\n\n\n\n\n\n\n\nmutable qScaleYValue = 1.0\n\n\n\n\n\n\n\nmutable qRotateValue = 0\n\n\n\n\n\n\n\nmutable kScaleXValue = 1.0\n\n\n\n\n\n\n\nmutable kScaleYValue = 1.0\n\n\n\n\n\n\n\nmutable kRotateValue = 0\n\n\n\n\n\n\n\nqScaleXSlider = createQKVSlider(0.1, 2, 0.1, 150, 1.0, \"Q Scale X\", val =&gt; mutable qScaleXValue = val)\n\n\n\n\n\n\n\nqScaleYSlider = createQKVSlider(0.1, 2, 0.1, 150, 1.0, \"Q Scale Y\", val =&gt; mutable qScaleYValue = val)\n\n\n\n\n\n\n\nqRotateSlider = createQKVSlider(-180, 180, 5, 150, 0, \"Q Rotate (deg)\", val =&gt; mutable qRotateValue = val)\n\n\n\n\n\n\n\nkScaleXSlider = createQKVSlider(0.1, 2, 0.1, 150, 1.0, \"K Scale X\", val =&gt; mutable kScaleXValue = val)\n\n\n\n\n\n\n\nkScaleYSlider = createQKVSlider(0.1, 2, 0.1, 150, 1.0, \"K Scale Y\", val =&gt; mutable kScaleYValue = val)\n\n\n\n\n\n\n\nkRotateSlider = createQKVSlider(-180, 180, 5, 150, 0, \"K Rotate (deg)\", val =&gt; mutable kRotateValue = val)\n\n\n\n\n\n\n\nqkvVisualization = {\n  const originalVectors = [\n    { name: \"bank\", vector: [1.5, 0.5] },\n    { name: \"money\", vector: [1.8, 0.8] },\n    { name: \"river\", vector: [0.5, 1.5] }\n  ];\n\n  const qPlotContainer = document.createElement(\"div\");\n  const kPlotContainer = document.createElement(\"div\");\n\n  function transformVector(vec, scaleX, scaleY, rotateDeg) {\n    const theta = (rotateDeg * Math.PI) / 180;\n    const cos = Math.cos(theta);\n    const sin = Math.sin(theta);\n    const scaledX = vec[0] * scaleX;\n    const scaledY = vec[1] * scaleY;\n    return [\n      scaledX * cos - scaledY * sin,\n      scaledX * sin + scaledY * cos\n    ];\n  }\n\n  const qScaleX = qScaleXValue;\n  const qScaleY = qScaleYValue;\n  const qRotate = qRotateValue;\n  const kScaleX = kScaleXValue;\n  const kScaleY = kScaleYValue;\n  const kRotate = kRotateValue;\n\n  const originalData = originalVectors.map(item =&gt; ({\n    name: item.name,\n    x: item.vector[0],\n    y: item.vector[1],\n    type: \"Original\"\n  }));\n\n  const qData = originalVectors.map(item =&gt; {\n    const qVec = transformVector(item.vector, qScaleX, qScaleY, qRotate);\n    return {\n      name: `q_${item.name}`,\n      x: qVec[0],\n      y: qVec[1],\n      type: \"Query\"\n    };\n  });\n\n  const kData = originalVectors.map(item =&gt; {\n    const kVec = transformVector(item.vector, kScaleX, kScaleY, kRotate);\n    return {\n      name: `k_${item.name}`,\n      x: kVec[0],\n      y: kVec[1],\n      type: \"Key\"\n    };\n  });\n\n  const qPlot = Plot.plot({\n    width: 250,\n    height: 250,\n    marginTop: 40,\n    marginRight: 20,\n    marginBottom: 50,\n    marginLeft: 60,\n    style: {\n      background: \"white\",\n      color: \"black\"\n    },\n    x: {\n      domain: [-3, 3],\n      label: \"Dimension 1\",\n      grid: true,\n      ticks: 10\n    },\n    y: {\n      domain: [-3, 3],\n      label: \"Dimension 2\",\n      grid: true,\n      ticks: 10\n    },\n    marks: [\n      Plot.dot([{x: 0, y: 0}], {\n        x: \"x\",\n        y: \"y\",\n        r: 3,\n        fill: \"black\"\n      }),\n      Plot.arrow([...originalData, ...qData], {\n        x1: 0,\n        y1: 0,\n        x2: \"x\",\n        y2: \"y\",\n        stroke: \"type\",\n        strokeWidth: 2,\n        headLength: 8\n      }),\n      Plot.dot([...originalData, ...qData], {\n        x: \"x\",\n        y: \"y\",\n        fill: \"type\",\n        r: 5,\n        tip: true\n      }),\n      Plot.text([...originalData, ...qData], {\n        x: \"x\",\n        y: \"y\",\n        text: \"name\",\n        dy: -12,\n        fontSize: 9,\n        fontWeight: \"bold\",\n        fill: \"black\"\n      }),\n      Plot.text([{ x: 0, y: 3.4 }], {\n        x: \"x\",\n        y: \"y\",\n        text: () =&gt; \"Query Space\",\n        fontSize: 12,\n        fontWeight: \"bold\",\n        fill: \"black\"\n      })\n    ],\n    color: {\n      domain: [\"Original\", \"Query\"],\n      range: [\"#666666\", \"#4682b4\"]\n    }\n  });\n\n  const kPlot = Plot.plot({\n    width: 250,\n    height: 250,\n    marginTop: 40,\n    marginRight: 20,\n    marginBottom: 50,\n    marginLeft: 60,\n    style: {\n      background: \"white\",\n      color: \"black\"\n    },\n    x: {\n      domain: [-3, 3],\n      label: \"Dimension 1\",\n      grid: true,\n      ticks: 10\n    },\n    y: {\n      domain: [-3, 3],\n      label: \"Dimension 2\",\n      grid: true,\n      ticks: 10\n    },\n    marks: [\n      Plot.dot([{x: 0, y: 0}], {\n        x: \"x\",\n        y: \"y\",\n        r: 3,\n        fill: \"black\"\n      }),\n      Plot.arrow([...originalData, ...kData], {\n        x1: 0,\n        y1: 0,\n        x2: \"x\",\n        y2: \"y\",\n        stroke: \"type\",\n        strokeWidth: 2,\n        headLength: 8\n      }),\n      Plot.dot([...originalData, ...kData], {\n        x: \"x\",\n        y: \"y\",\n        fill: \"type\",\n        r: 5,\n        tip: true\n      }),\n      Plot.text([...originalData, ...kData], {\n        x: \"x\",\n        y: \"y\",\n        text: \"name\",\n        dy: -12,\n        fontSize: 9,\n        fontWeight: \"bold\",\n        fill: \"black\"\n      }),\n      Plot.text([{ x: 0, y: 3.4 }], {\n        x: \"x\",\n        y: \"y\",\n        text: () =&gt; \"Key Space\",\n        fontSize: 12,\n        fontWeight: \"bold\",\n        fill: \"black\"\n      })\n    ],\n    color: {\n      domain: [\"Original\", \"Key\"],\n      range: [\"#666666\", \"#2e8b57\"]\n    }\n  });\n\n  d3.select(qPlotContainer).node().appendChild(qPlot);\n  d3.select(kPlotContainer).node().appendChild(kPlot);\n\n  return html`&lt;div style=\"display: flex; justify-content: center; gap: 40px;\"&gt;\n    &lt;div style=\"display: flex; flex-direction: column; gap: 20px;\"&gt;\n      &lt;div style=\"display: flex; align-items: center; gap: 20px;\"&gt;\n        &lt;div style=\"display: flex; flex-direction: column; gap: 8px;\"&gt;\n          &lt;div style=\"font-weight: bold; margin-bottom: 3px;\"&gt;Query (W_Q)&lt;/div&gt;\n          ${qScaleXSlider.node}\n          ${qScaleYSlider.node}\n          ${qRotateSlider.node}\n        &lt;/div&gt;\n        ${qPlotContainer}\n      &lt;/div&gt;\n      &lt;div style=\"display: flex; align-items: center; gap: 20px;\"&gt;\n        &lt;div style=\"display: flex; flex-direction: column; gap: 8px;\"&gt;\n          &lt;div style=\"font-weight: bold; margin-bottom: 3px;\"&gt;Key (W_K)&lt;/div&gt;\n          ${kScaleXSlider.node}\n          ${kScaleYSlider.node}\n          ${kRotateSlider.node}\n        &lt;/div&gt;\n        ${kPlotContainer}\n      &lt;/div&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;`;\n}\n\n\n\n\n\n\nUsing the transformations above, we can compute the attention weights showing how each word attends to every other word:\n\nattentionHeatmap = {\n  const attentionWords = [\"bank\", \"money\", \"river\"];\n  const attentionEmbeddings = [\n    [1.5, 0.5],\n    [1.8, 0.8],\n    [0.5, 1.5]\n  ];\n\n  function transformVector(vec, scaleX, scaleY, rotateDeg) {\n    const theta = (rotateDeg * Math.PI) / 180;\n    const cos = Math.cos(theta);\n    const sin = Math.sin(theta);\n    const scaledX = vec[0] * scaleX;\n    const scaledY = vec[1] * scaleY;\n    return [\n      scaledX * cos - scaledY * sin,\n      scaledX * sin + scaledY * cos\n    ];\n  }\n\n  const qScaleX = qScaleXValue;\n  const qScaleY = qScaleYValue;\n  const qRotate = qRotateValue;\n  const kScaleX = kScaleXValue;\n  const kScaleY = kScaleYValue;\n  const kRotate = kRotateValue;\n\n  const Q = attentionEmbeddings.map(vec =&gt;\n    transformVector(vec, qScaleX, qScaleY, qRotate)\n  );\n  const K = attentionEmbeddings.map(vec =&gt;\n    transformVector(vec, kScaleX, kScaleY, kRotate)\n  );\n\n  const scores = Q.map(q =&gt; K.map(k =&gt; q[0] * k[0] + q[1] * k[1]));\n\n  const attentionWeights = scores.map(row =&gt; {\n    const maxScore = Math.max(...row);\n    const expScores = row.map(s =&gt; Math.exp(s - maxScore));\n    const sumExp = expScores.reduce((a, b) =&gt; a + b, 0);\n    return expScores.map(e =&gt; e / sumExp);\n  });\n\n  const heatmapData = (() =&gt; {\n    const data = [];\n    for (let i = 0; i &lt; attentionWords.length; i++) {\n      for (let j = 0; j &lt; attentionWords.length; j++) {\n        data.push({\n          Query: attentionWords[i],\n          Key: attentionWords[j],\n          Weight: attentionWeights[i][j]\n        });\n      }\n    }\n    return data;\n  })();\n\n  const heatmapPlot = Plot.plot({\n    width: 320,\n    height: 320,\n    marginTop: 50,\n    marginBottom: 50,\n    marginLeft: 70,\n    marginRight: 80,\n    style: {\n      background: \"white\",\n      color: \"black\"\n    },\n    x: {\n      label: \"Key Word\",\n      domain: attentionWords\n    },\n    y: {\n      label: \"Query Word\",\n      domain: attentionWords\n    },\n    color: {\n      scheme: \"Blues\",\n      label: \"Attention\",\n      legend: true\n    },\n    marks: [\n      Plot.cell(heatmapData, {\n        x: \"Key\",\n        y: \"Query\",\n        fill: \"Weight\",\n        tip: true\n      }),\n      Plot.text(heatmapData, {\n        x: \"Key\",\n        y: \"Query\",\n        text: d =&gt; d.Weight.toFixed(2),\n        fill: d =&gt; d.Weight &gt; 0.35 ? \"white\" : \"black\",\n        fontSize: 11\n      }),\n      Plot.text([{ x: 0, y: 0 }], {\n        x: () =&gt; attentionWords.length / 2 - 0.5,\n        y: () =&gt; -0.8,\n        text: () =&gt; \"Attention Weights (Softmax)\",\n        fontSize: 12,\n        fontWeight: \"bold\",\n        frameAnchor: \"top\",\n        fill: \"black\"\n      })\n    ]\n  });\n\n  return html`&lt;div style=\"display: flex; justify-content: center;\"&gt;\n    ${heatmapPlot}\n  &lt;/div&gt;`;\n}\n\n\n\n\n\n\nRows represent words asking for context (Queries), while columns represent words providing context (Keys). Each cell (i,j) indicates how much word i attends to word j, with each row summing to 1 to form a probability distribution over context words.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Transformers"
    ]
  },
  {
    "objectID": "m04-text/transformers.html#multi-head-attention",
    "href": "m04-text/transformers.html#multi-head-attention",
    "title": "Transformers",
    "section": "Multi-head Attention",
    "text": "Multi-head Attention\n\n\n\n\n\nPutting it all together (query-key-value transformation, attention matrix, and softmax normalization), this forms one attention head of the transformer. But we can have multiple attention heads in parallel, each with its own query-key-value transformation, attention matrix, and softmax normalization. The output of the attention heads are concatenated and then passed through a linear transformation to produce the final output.\n\n\\text{Output} = \\text{Linear}(\\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h))\n\nThis is one attention block of the transformer. Why multiple heads? Having parallel attention heads is a powerful technique to capture different aspects of the input data. The model can learn multiple relationships between the words in the input data.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Transformers"
    ]
  },
  {
    "objectID": "m04-text/transformers.html#transformer-architecture",
    "href": "m04-text/transformers.html#transformer-architecture",
    "title": "Transformers",
    "section": "Transformer Architecture",
    "text": "Transformer Architecture\nLet’s step back and look at the transformer architecture at a high level. We base our discussion on the original Transformer paper, “Attention Is All You Need”, noting that the transformer architecture has evolved since then with many variants.\n\nEncoder Module\n\n\n\n\n\nThe encoder module consists of position embedding, multi-head attention, residual connection, and layer normalization, along with feed-forward networks.\nLet us go through each component in detail.\n\nPosition Embedding\n\n\n\n\n\nIn the encoder module, we start from the positional encoding, which fixes a key issue: the attention modules are permutation invariant (attention produces the same output even if we shuffle the words in the sentence). But position matters in language understanding and generation, so position encoding fixes this issue.\nLet’s approach position encoding from a naive perspective. Suppose we have a sequence of T token embeddings, denoted by x_1, x_2, ..., x_T, each a d-dimensional vector.\nA simple way to encode position is to add a position index to each token embedding:\n\nx_t := x_t + \\beta t,\n\nwhere t = 1, 2, ..., T is the position index of the token in the sequence, and \\beta is the step size.\nThis appears simple but has critical problems. First, the position index can be arbitrarily large: when models see sequences longer than those in training data, they’ll be exposed to position indices they’ve never seen before. Second, the position index is discrete, meaning the model cannot capture position information smoothly.\nBecause this naive approach has problems, consider another approach. Let’s represent position using a binary vector of length d. For example, with d=4:\n\n\\begin{align*}\n  0: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} & &\n  8: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n  1: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} & &\n  9: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} \\\\\n  2: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} & &\n  10: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n  3: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} & &\n  11: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} \\\\\n  4: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} & &\n  12: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n  5: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} & &\n  13: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} \\\\\n  6: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} & &\n  14: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n  7: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} & &\n  15: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} \\\\\n\\end{align*}\n\nThen, use the binary vector as the position embedding:\n\nx_{t,i} := x_{t,i} + \\text{Pos}(t, i),\n\nwhere \\text{Pos}(t, i) is the position embedding vector of position index t and dimension index i.\nThis representation is bounded between 0 and 1, yet still discrete.\nWhat’s a better approach? An elegant position embedding used in transformers is sinusoidal position embedding, which appears complicated but stay with me.\n\n\\text{Pos}(t, i) =\n\\begin{cases}\n\\sin\\left(\\dfrac{t}{10000^{2i/d}}\\right), & \\text{if } i \\text{ is even} \\\\\n\\cos\\left(\\dfrac{t}{10000^{2i/d}}\\right), & \\text{if } i \\text{ is odd}\n\\end{cases},\n\nwhere i is the dimension index of the position embedding vector. This position embedding is added to the input token embedding as:\n\nx_{t,i} := x_{t,i} + \\text{Pos}(t, i),\n\nIt appears complicated, but it’s a continuous version of the binary position embedding above.\nTo see this, let’s plot the position embedding for the first 100 positions.\n\n\n\n\n\nThe position embedding exhibits the alternating pattern (vertically) with frequency increasing as the dimension index increases (horizontal axis). Additionally, sinusoidal position embedding is continuous, allowing the model to capture position information smoothly.\nAnother key property: the dot similarity between two position embedding vectors represents the similarity between the two positions, regardless of the position index.\n\n\n\n\n\nThe dot similarity between position embedding vectors represents the distance between positions, regardless of the position index.\nWhy additive position embedding? Sinusoidal position embedding is additive, altering the token embedding. Alternatively, one might concatenate the position embedding to the token embedding: x_{t,i} := [x_{t,i}; \\text{Pos}(t, i)], which makes it easier for a model to distinguish position from token information. So why not use concatenation? Concatenation requires a larger embedding dimension, increasing the number of parameters. Instead, adding the position embedding creates an interesting effect in the attention mechanism (interested readers can check out this Reddit post).\nWhat about alternatives? Absolute position embedding is what we discussed above, where each position is represented by a unique vector. Relative position embedding, on the other hand, represents the position difference between two positions rather than the absolute position.\nRelative position embedding can be implemented by adding a learnable scalar to the unnormalized attention scores before softmax:\n\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T + B}{\\sqrt{d_k}}\\right)V\n\nwhere B is a learnable offset matrix added to the unnormalized attention scores. The matrix B is a function of the position difference between query and key: B = f(i-j), where i and j are the position indices of query and key. Such formulation is useful when the model needs to capture relative position between tokens.\n\n\nResidual Connection\n\n\n\n\n\nAnother important component is the residual connection. The input is first passed through multi-head attention, followed by layer normalization. Notice the parallel path from input to the output of the attention module, called a residual connection (or skip connection), a technique used to stabilize the training of deep neural networks by mitigating the problem of too large or too small input values that can cause network instability.\nLet’s denote by f the neural network we want to train (the multi-head attention or feed-forward networks in the transformer block).\nThe residual connection is defined as:\n\n\\underbrace{x_{\\text{out}}}_{\\text{output}} = \\underbrace{x_{\\text{in}}}_{\\text{input}} + \\underbrace{f(x_{\\text{in}})}_{\\text{component}}.\n\nRather than learning the complete mapping from input to output, the network f learns to model the residual (difference) between them. This is particularly advantageous when the desired transformation approximates an identity mapping, as the network can simply learn to output values near zero.\nWhy are residual connections important? Residual connections help prevent the vanishing gradient problem. Deep learning models like LLMs consist of many layers, trained to minimize the loss function {\\cal L}_{\\text{loss}} with respect to parameters \\theta.\nThe gradient of the loss is computed using the chain rule:\n\n\\frac{\\partial {\\cal L}_{\\text{loss}}}{\\partial \\theta} = \\frac{\\partial {\\cal L}_{\\text{loss}}}{\\partial f_L} \\cdot \\frac{\\partial f_L}{\\partial f_{L-1}} \\cdot \\frac{\\partial f_{L-1}}{\\partial f_{L-2}} \\cdot ... \\cdot \\frac{\\partial f_{l+1}}{\\partial f_l} \\cdot \\frac{\\partial f_l}{\\partial \\theta}\n\nwhere f_i is the output of the i-th layer.\nThe gradient vanishing problem occurs when the individual terms \\frac{\\partial f_{i+1}}{\\partial f_i} are less than 1. As a result, the gradient becomes smaller and smaller as it flows backward through earlier layers.\nBy adding the residual connection, the gradient for the individual term becomes:\n\n\\frac{\\partial x_{i+1}}{\\partial x_i} = 1 + \\frac{\\partial f_i(x_i)}{\\partial x_i}\n\nNotice the “+1” term, which is the direct path from input to output. The chain rule is thus modified to include this term.\nWhen we expand the product, we can group terms by their order (how many \\partial f_i terms are multiplied together):\n1 + O_1 + O_2 + O_3 + ...\nwhere the O_n terms represent various combinations of gradients at different orders.\nWithout the residual connection, we only have the highest-order terms, which are subject to the gradient vanishing problem. With the residual connection, we have lower-order terms like O_1, O_2, O_3, ..., which are less susceptible to gradient vanishing.\nResidual connections are an architectural innovation that allows neural networks to be much deeper without degrading performance. They were proposed by He et al. for image processing from Microsoft Research.\nResidual connections also help prevent gradient explosion by providing alternative paths for gradients to flow. By distributing gradients between the residual path and the learning component path, the gradient is less likely to explode.\n\n\nLayer Normalization\n\n\n\n\n\nIn transformer models, you find multiple layer normalization steps. Layer normalization is a technique used to stabilize the training of deep neural networks by mitigating the problem of too large or too small input values that can cause network instability.\nMore specifically, layer normalization is computed as:\n\n\\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sigma} + \\beta,\n\nwhere \\mu and \\sigma are the mean and standard deviation of the input, \\gamma is the scaling factor, and \\beta is the shifting factor.\nThe variables \\gamma and \\beta are learnable parameters initialized to 1 and 0, and updated during training.\nNote that layer normalization is applied to individual tokens: the normalization is token-wise rather than feature-wise, with mean and standard deviation calculated for each token across all feature dimensions. This differs from feature-wise normalization, where mean and standard deviation are calculated for each feature across all tokens.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Transformers"
    ]
  },
  {
    "objectID": "m04-text/transformers.html#decoder-module",
    "href": "m04-text/transformers.html#decoder-module",
    "title": "Transformers",
    "section": "Decoder Module",
    "text": "Decoder Module\n\n\n\n\n\n\nCausal Attention\n\n\n\n\n\nOne key advantage of transformers is their ability to generate contextualized vectors in parallel. Recurrent neural networks (RNNs) read the input sequence sequentially, limiting parallelism. Transformer models, on the other hand, can compute attention scores and weighted averages of value vectors in parallel, generating contextualized vectors at once, which speeds up training.\nIn the decoder module, a vector is contextualized by attending to the previous vectors in the sequence. Importantly, it should not see the future token vectors, as that’s what the model is tasked to predict. We prevent this by setting the attention scores to zero for future tokens. Another benefit of causal attention: the model doesn’t suffer from the error accumulation problem, where prediction error from one step carries over to the next.\nHow do we implement the masking? We set the attention scores to negative infinity for future tokens before the softmax operation, effectively zeroing out their contribution:\n\n\\text{Mask}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T + M}{\\sqrt{d_k}}\\right)V\n\nwhere M is a matrix with -\\infty for positions corresponding to future tokens. The result is attention scores where tokens attend only to previous tokens.\n\n\nCross-Attention\n\n\n\n\n\nCross-attention occurs when the Query comes from one sequence (like a sentence being generated) and the Keys and Values come from another sequence (like the input sentence you want to translate). Here, the model learns to align information from input to output, a sort of bilingual dictionary lookup, but learned and fuzzy.\nThe mechanism works by using queries (Q) from the decoder’s previous layer and keys (K) and values (V) from the encoder’s output. This enables each position in the decoder to attend to the full encoder sequence without any masking, since encoding is already complete.\nFor instance, in translating “I love you” to “Je t’aime”, cross-attention helps each French word focus on relevant English words (“Je” attends to “I”, and “t’aime” to “love”), maintaining semantic relationships between input and output.\nThe cross-attention formula is:\n\n\\text{CrossAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\nwhere Q comes from the decoder and K, V come from the encoder. This effectively bridges the encoding and decoding processes.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Transformers"
    ]
  },
  {
    "objectID": "m04-text/transformers.html#putting-it-all-together",
    "href": "m04-text/transformers.html#putting-it-all-together",
    "title": "Transformers",
    "section": "Putting It All Together",
    "text": "Putting It All Together\nLet’s overview the transformer architecture and see how the components fit into the overall architecture.\n\n\n\n\n\nWe hope that you now have a better understanding of the transformer architecture and how the components fit together into the overall architecture.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Transformers"
    ]
  },
  {
    "objectID": "m04-text/transformers.html#the-key-insight",
    "href": "m04-text/transformers.html#the-key-insight",
    "title": "Transformers",
    "section": "The Key Insight",
    "text": "The Key Insight\nEvery time you use GPT (ChatGPT, Claude, Gemini, etc.), you’re seeing transformers in action. Transformers don’t “think”. They perform statistical pattern matching at scale.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Transformers"
    ]
  },
  {
    "objectID": "m04-text/word-embeddings.html",
    "href": "m04-text/word-embeddings.html",
    "title": "Word Embeddings",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module introduces word embeddings and the revolutionary Word2Vec algorithm.\nYou’ll learn:\n\nHow meaning emerges from relationships rather than being stored in containers.\nThe technique of contrastive learning and how it shapes semantic space through push and pull.\nHow to perform semantic arithmetic with vector operations like “king - man + woman = queen”.\nThe connection between structural linguistics and the geometric structure of word embeddings.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Embeddings"
    ]
  },
  {
    "objectID": "m04-text/word-embeddings.html#words-as-relationships-not-containers",
    "href": "m04-text/word-embeddings.html#words-as-relationships-not-containers",
    "title": "Word Embeddings",
    "section": "Words as Relationships, Not Containers",
    "text": "Words as Relationships, Not Containers\nHave you ever wondered where meaning comes from? We intuitively assume words are containers for meaning, that “Dog” holds the concept of a canine. This is incorrect. Structural linguistics reveals that a sign is defined solely by its relationships: “Dog” means “dog” only because it is not “cat”, “wolf”, or “log”. Meaning is differential, not intrinsic.\n\n\n\n\n\n\nFigure 1: Green is the color that is not non-green (not red, not blue, not yellow, etc.).\n\n\n\nWord2Vec, the foundational model grounding modern NLP, learns to map the statistical topology of language. Think of it like mapping a city based purely on traffic data.\nYou don’t know what a “school” is, but you see that “buses” and “children” congregate there at 8 AM. By placing these entities close together on a map, you reconstruct the city’s functional structure. Word2Vec does this for language, turning semantic proximity into geometric distance.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Embeddings"
    ]
  },
  {
    "objectID": "m04-text/word-embeddings.html#exploring-word2vec",
    "href": "m04-text/word-embeddings.html#exploring-word2vec",
    "title": "Word Embeddings",
    "section": "Exploring Word2Vec",
    "text": "Exploring Word2Vec\nLet’s first experience the power of Word2Vec, then understand how it works.\nWe’ll use a pre-trained model trained on 100 billion words of Google News. We aren’t teaching it anything. We’re simply inspecting the map it created.\n\nimport gensim.downloader as api\nimport numpy as np\n\n# Load pre-trained Word2vec embeddings\nprint(\"Loading Word2vec model...\")\nmodel = api.load(\"word2vec-google-news-300\")\nprint(f\"Loaded embeddings for {len(model):,} words.\")\n\nLoading Word2vec model...\nLoaded embeddings for 3,000,000 words.\n\n\nIf the map is accurate, “dog” should be surrounded by its semantic kin. We query the nearest neighbors in the vector space.\n\nsimilar_to_dog = model.most_similar(\"dog\", topn=10)\n\nprint(\"Words most similar to 'dog':\")\nfor word, similarity in similar_to_dog:\n    print(f\"  {word:20s} {similarity:.3f}\")\n\nWords most similar to 'dog':\n  dogs                 0.868\n  puppy                0.811\n  pit_bull             0.780\n  pooch                0.763\n  cat                  0.761\n  golden_retriever     0.750\n  German_shepherd      0.747\n  Rottweiler           0.744\n  beagle               0.742\n  pup                  0.741\n\n\nThe model groups “dog” with “dogs”, “puppy”, and “pooch” not because it knows biology, but because they are statistically interchangeable in sentences.\nWhat about semantic arithmetic? Since words are vectors, we can perform arithmetic on meaning. The relationship between “King” and “Man” is a vector, and if we add that vector to “Woman”, we should arrive at “Queen”.\n \\vec{\\text{King}} - \\vec{\\text{Man}} + \\vec{\\text{Woman}} \\approx \\vec{\\text{Queen}} \n\nresult = model.most_similar(\n  positive=['king', 'woman'],\n   negative=['man'], topn=5\n)\n\nprint(\"king - man + woman =\")\nfor word, similarity in result:\n    print(f\"  {word:15s} {similarity:.3f}\")\n\nking - man + woman =\n  queen           0.712\n  monarch         0.619\n  princess        0.590\n  crown_prince    0.550\n  prince          0.538\n\n\nHow do we visualize these relationships? We cannot see in 300 dimensions, but we can project the space down to 2D using PCA. This reveals consistent structures like the “capital city” relationship that the model has learned.\n\n\nCode\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ncountries = [\"Germany\", \"France\", \"Italy\", \"Spain\", \"Portugal\", \"Greece\"]\ncapitals = [\"Berlin\", \"Paris\", \"Rome\", \"Madrid\", \"Lisbon\", \"Athens\"]\n\n# Get embeddings\ncountry_embeddings = np.array([model[country] for country in countries])\ncapital_embeddings = np.array([model[capital] for capital in capitals])\n\n# PCA to 2D\npca = PCA(n_components=2)\nembeddings = np.vstack([country_embeddings, capital_embeddings])\nembeddings_pca = pca.fit_transform(embeddings)\n\n# Create DataFrame\ndf = pd.DataFrame(embeddings_pca, columns=[\"PC1\", \"PC2\"])\ndf[\"Label\"] = countries + capitals\ndf[\"Type\"] = [\"Country\"] * len(countries) + [\"Capital\"] * len(capitals)\n\n# Plot\nfig, ax = plt.subplots(figsize=(12, 10))\n\nfor idx, row in df.iterrows():\n    color = \"#e74c3c\" if row[\"Type\"] == \"Country\" else \"#3498db\"\n    marker = \"o\" if row[\"Type\"] == \"Country\" else \"s\"\n    ax.scatter(\n        row[\"PC1\"],\n        row[\"PC2\"],\n        c=color,\n        marker=marker,\n        s=200,\n        edgecolors=\"black\",\n        linewidth=1.5,\n        alpha=0.7,\n        zorder=3,\n    )\n    ax.text(\n        row[\"PC1\"],\n        row[\"PC2\"] + 0.15,\n        row[\"Label\"],\n        fontsize=12,\n        ha=\"center\",\n        va=\"bottom\",\n        fontweight=\"bold\",\n        bbox=dict(facecolor=\"white\", edgecolor=\"none\", alpha=0.8),\n    )\n\n# Draw arrows\nfor i in range(len(countries)):\n    country_pos = df.iloc[i][[\"PC1\", \"PC2\"]].values\n    capital_pos = df.iloc[i + len(countries)][[\"PC1\", \"PC2\"]].values\n    ax.arrow(\n        country_pos[0],\n        country_pos[1],\n        capital_pos[0] - country_pos[0],\n        capital_pos[1] - country_pos[1],\n        color=\"gray\",\n        alpha=0.6,\n        linewidth=2,\n        head_width=0.15,\n        head_length=0.1,\n        zorder=2,\n    )\n\nax.set_title(\n    'The \"Capital Of\" Relationship as Parallel Transport',\n    fontsize=16,\n    fontweight=\"bold\",\n    pad=20,\n)\nax.grid(alpha=0.3, linestyle=\"--\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nThe ‘Capital Of’ relationship appears as a consistent direction in vector space.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Embeddings"
    ]
  },
  {
    "objectID": "m04-text/word-embeddings.html#how-word2vec-learns-meaning",
    "href": "m04-text/word-embeddings.html#how-word2vec-learns-meaning",
    "title": "Word Embeddings",
    "section": "How Word2Vec Learns Meaning",
    "text": "How Word2Vec Learns Meaning\nLet’s talk about the mechanism behind the magic. We intuitively treat words as containers that hold meaning, that “Green” contains the visual concept of a specific color. This is incorrect. Nature presents us with a messy, continuous spectrum without hard borders, and language is simply the set of arbitrary cuts we make in that continuum to create order.\nWord2Vec operationalizes this by treating meaning as a game of contrast, functioning as a pair of linguistic scissors. It does not learn what a word is by looking up a definition. It learns what a word is like by pulling it close to neighbors, and more importantly, it learns what a word is not by pushing it away from random noise.\nThe meaning of “Green” is simply the geometric region that remains after we have pushed away “Red”, “Purple”, and “Banana”.\n\n\n\n\n\n\nFigure 2: Starting from initially random vectors, word2vec learns iteratively to push away the words that are not related and pull words that are related. The resulting vector space is a map of the relationships between words.\n\n\n\nWhat’s the underlying technique? This process relies on contrastive learning. We cannot teach the model the exact meaning of each word, but we can let it learn the relationship between words through a binary classification problem: are these two words neighbors, or are they strangers? The training loop provides a positive pair from the text, instructing the model to maximize the similarity between their vectors, while simultaneously grabbing random negative samples (imposters from the vocabulary) and demanding the model minimize their similarity. This push-and-pull mechanic creates the vector space, where the “Green” cluster forms not because the model understands color, but because those words are statistically interchangeable when opposed to “Red”.\nHow do we generate training pairs without human labeling? We employ a sliding window technique that moves over the raw text corpus, converting a sequence of words into a system of geometric queries.\n\n\n\n\n\n\nFigure 3: Without human labeling, word2vec assumes that words in the same context are related. Context is defined as the words within a window of predefined size. For example, in “The quick brown fox jumps over the lazy dog”, the context of “fox” includes “brown”, “jumps”, “over”, and “lazy”.\n\n\n\nWhat’s the neural network architecture? Word2Vec is a simple neural network with one hidden layer. The input is a one-hot encoded vector of a word, which triggers neurons in the hidden layer to fire. The neural connection strength from the neuron representing the word to the neurons in the hidden layer (marked by red arrows) represents the query vector, u, while the hidden layer neurons trigger the firing of output layer neurons. The connection strength from an output word neuron to the hidden layer neurons represents the key vector, v.\n\nThe word in the center of the window acts as the Query vector (u), broadcasting its position to the surrounding Context words, which act as Keys (v). The neural network adjusts its weights to maximize the dot product u \\cdot v for these specific context pairs while suppressing the dot product for the negative samples, making the probability of a word appearing in context a function of their vector alignment.\n\nP(j \\vert i) = \\frac{P(j) \\exp(u_i \\cdot v_j)}{\\sum_{k=1}^{V} P(k) \\exp(u_i \\cdot v_k)}\n\nwhere P(j) is the probability of word j appearing in the vocabulary.\nWhy do we include P(j) in the formula? The original Word2Vec paper uses a different formulation that omits P(j), which is correct conceptually but not practically. In practice, word2vec is trained with an efficient but biased training algorithm (negative sampling), and the term P(j) enters the P(j \\vert i) when we account for this bias.\nThis closes the loop between high-level linguistic philosophy and low-level matrix operations. The machine proves the structuralist hypothesis: that meaning is relational. By mechanically slicing the continuum of language and applying the pressure of negative sampling, the model reconstructs a functional map of human concepts, successfully turning a philosophy of meaning into a runnable algorithm.\n\n\n\n\n\n\nFigure 4",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Embeddings"
    ]
  },
  {
    "objectID": "m04-text/word-embeddings.html#key-takeaway",
    "href": "m04-text/word-embeddings.html#key-takeaway",
    "title": "Word Embeddings",
    "section": "Key Takeaway",
    "text": "Key Takeaway\nYou don’t need to know what a thing is to understand it. You only need to know where it stands relative to everything it isn’t.\nThere’s a nice blog post by Chris McCormick that walks through the inner workings of Word2Vec. See here.",
    "crumbs": [
      "Home",
      "Module 4: Deep Learning for Text",
      "Word Embeddings"
    ]
  },
  {
    "objectID": "m05-images/02-the-deep-learning-revolution.html",
    "href": "m05-images/02-the-deep-learning-revolution.html",
    "title": "Part 2: The Deep Learning Revolution",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis section traces the paradigm shift in computer vision from hand-crafted features to learned representations.\nYou’ll learn:\n\nHow researchers designed edge detectors and frequency transforms by hand before deep learning.\nWhat LeNet contributed by pioneering automated feature learning in the 1990s.\nWhy AlexNet’s 2012 breakthrough demonstrated that deep learning works at scale.\nThe key innovations (ReLU, Dropout, GPU acceleration, data augmentation) that made AlexNet successful.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 2: The Deep Learning Revolution"
    ]
  },
  {
    "objectID": "m05-images/02-the-deep-learning-revolution.html#the-old-way-engineering-features-by-hand",
    "href": "m05-images/02-the-deep-learning-revolution.html#the-old-way-engineering-features-by-hand",
    "title": "Part 2: The Deep Learning Revolution",
    "section": "The Old Way: Engineering Features by Hand",
    "text": "The Old Way: Engineering Features by Hand\nBefore 2012, computer vision meant one thing: carefully designing features by hand. Experts would analyze problems and craft mathematical operations to extract useful information like edge detection, texture analysis, and object boundaries. Let’s explore how this worked by examining edge detection, one of the fundamental problems in image processing.\n\nDetecting Edges Through Brightness Changes\nWhat makes an edge visible to human eyes? Sudden changes in brightness, appearing when neighboring pixels have significantly different intensity values. Recall from Part 1 the small 6×6 grayscale image with a bright vertical line in the third column (values of 80) surrounded by dark pixels (values of 10).\nWe can approximate the horizontal derivative by subtracting the right neighbor from the left neighbor at each position. For the central pixel, this looks like:\n\n\\nabla Z_{22} = Z_{2,1} - Z_{2,3}\n\nApplied to the entire image, we get large values where brightness changes suddenly (the edge) and near-zero values elsewhere. This simple operation reveals structure.\n\n\nConvolution: A General Pattern Matching Operation\nThe derivative calculation we just performed is a special case of a more general operation called convolution. The idea is elegant: define a small matrix of weights called a kernel or filter, then slide it across the image, computing weighted sums at each position.\nFor a 3×3 kernel K applied to a local patch Z:\n\n\\text{output}_{i,j} = \\sum_{m=-1}^{1} \\sum_{n=-1}^{1} K_{m,n} \\cdot Z_{i+m, j+n}\n\nThe Prewitt operator provides kernels specifically designed for edge detection:\n\nK_h = \\begin{bmatrix}\n-1 & 0 & 1 \\\\\n-1 & 0 & 1 \\\\\n-1 & 0 & 1\n\\end{bmatrix}\n\\quad\\text{and}\\quad\nK_v = \\begin{bmatrix}\n-1 & -1 & -1 \\\\\n0 & 0 & 0 \\\\\n1 & 1 & 1\n\\end{bmatrix}\n\nThe horizontal kernel K_h detects vertical edges, while the vertical kernel K_v detects horizontal edges. Each kernel responds strongly when the image patch matches its pattern.\n\n# Define the vertical edge detection kernel\nK_v = np.array([[-1, -1, -1],\n                [0, 0, 0],\n                [1, 1, 1]])\n\n# Apply convolution to detect edges\nedges = convolve2d(img_gray, K_v, mode='same', boundary='symm')\n\n\n\nShow visualization code\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\naxes[0].imshow(img_gray, cmap='gray')\naxes[0].set_title(\"Original Grayscale Image\")\naxes[0].axis(\"off\")\n\naxes[1].imshow(edges, cmap='gray')\naxes[1].set_title(\"Vertical Edge Detection (Prewitt Filter)\")\naxes[1].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNotice how the filter highlights horizontal boundaries where brightness changes rapidly in the vertical direction. An excellent interactive demo of various image kernels can be found at Setosa Image Kernels.\n\n\nThinking in Frequencies: The Fourier Transform\nShift your attention from the spatial domain to the frequency domain. The Fourier transform represents images as combinations of sinusoidal patterns at different frequencies. For a discrete signal x[n] of length N, the Discrete Fourier Transform is:\n\n\\mathcal{F}(x)[k] = \\sum_{n=0}^{N-1} x[n] \\cdot e^{-2\\pi i \\frac{nk}{N}}\n\nUsing Euler’s formula e^{ix} = \\cos(x) + i\\sin(x), we can rewrite this as:\n\n\\mathcal{F}(x)[k] = \\sum_{n=0}^{N-1} x[n]\\Big[\\cos(2\\pi \\tfrac{nk}{N}) - i\\sin(2\\pi \\tfrac{nk}{N})\\Big]\n\nThe Fourier transform decomposes a signal into its frequency components, where low frequencies correspond to smooth, slowly varying regions and high frequencies correspond to sharp edges and fine details. The convolution theorem reveals a beautiful connection: convolution in the spatial domain is equivalent to multiplication in the frequency domain:\n\nX * K \\quad\\longleftrightarrow\\quad \\mathcal{F}(X) \\cdot \\mathcal{F}(K)\n\nThis means we can perform convolution by: 1. Taking the Fourier transform of both the image and the kernel 2. Multiplying them element-wise in the frequency domain 3. Taking the inverse Fourier transform to get back to the spatial domain\nFor large images, this approach can be computationally faster than direct convolution. For a beautiful visual explanation of the Fourier Transform, see 3Blue1Brown’s video.\n\n\nThe Fundamental Limitation\nHere’s the problem with hand-crafted features: experts had to design every single one. Want to detect corners? Design a corner detector. Need to recognize textures? Craft texture descriptors. Each feature required mathematical sophistication and domain expertise, creating a feature engineering bottleneck that limited what computer vision could achieve. This approach worked for simple, well-defined tasks but scaled poorly to complex problems like recognizing thousands of object categories.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 2: The Deep Learning Revolution"
    ]
  },
  {
    "objectID": "m05-images/02-the-deep-learning-revolution.html#the-first-breakthrough-lenet",
    "href": "m05-images/02-the-deep-learning-revolution.html#the-first-breakthrough-lenet",
    "title": "Part 2: The Deep Learning Revolution",
    "section": "The First Breakthrough: LeNet",
    "text": "The First Breakthrough: LeNet\nHave you ever wondered if there’s a better way? Yann LeCun posed a radical question in the late 1980s: what if networks could learn features automatically from raw pixels? Instead of hand-designing edge detectors, let the network discover useful patterns through training. This vision led to LeNet, a pioneering convolutional architecture that demonstrated automated feature learning on handwritten digit recognition (LeCun et al. 1989, 1998).\n\n\n\n\n\n\nLeNet-1 architecture. The network learns to extract features through layers of convolution and pooling.\n\n\n\n\nFigure 1\n\n\n\n\nArchitecture: Hierarchical Feature Learning\nLeNet introduced a pattern that remains fundamental to modern CNNs:\n\nConvolutional layers apply learnable filters (not hand-designed) to extract local patterns\nPooling layers downsample feature maps, creating spatial invariance\nStacking multiple layers builds increasingly abstract representations\nFully connected layers at the end combine features for classification\n\nThe key innovation was making the convolutional filters learnable parameters. During training, backpropagation adjusts filter weights to extract features useful for the task. The network discovers edge detectors, corner detectors, and more complex patterns automatically.\nLeNet-5, the most influential version, processed 32×32 grayscale images through this architecture:\n\n\n\n\n\n\nLeNet-5 architecture with input normalization, sparse connectivity, and multiple convolution-pooling pairs.\n\n\n\n\nFigure 2\n\n\n\nLet’s understand each component:\nC1: First Convolutional Layer Takes the input image and applies learnable 5×5 filters. These filters start random but evolve during training to detect basic patterns like edges at various orientations.\nS2: Subsampling (Pooling) Reduces spatial dimensions through average pooling with 2×2 windows. This creates local translation invariance—small shifts in feature positions don’t change the output significantly.\nC3: Second Convolutional Layer Combines features from the previous layer to build more complex patterns. LeNet-5 used sparse connectivity here (not every feature map connects to every previous map), reducing parameters while encouraging diverse features.\nS4: Second Subsampling Further reduces spatial dimensions, allowing the network to focus on increasingly abstract representations.\nFully Connected Layers Flatten the spatial feature maps into a vector and make the final classification decision across 10 digit classes.\nYann LeCun’s work on applying backpropagation to convolutional architectures in the 1980s was met with skepticism. But LeNet’s success on real-world tasks like automated check reading at banks helped spark wider interest in neural networks.\n\n\nLeNet Architecture in Code\nHere’s a simplified LeNet-1 implementation showing the core architecture:\n\n\nShow LeNet-1 implementation\nimport torch\nimport torch.nn as nn\n\nclass LeNet1(nn.Module):\n    \"\"\"Simplified LeNet-1 architecture.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 4, kernel_size=5)\n        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(4, 12, kernel_size=5)\n        self.fc = nn.Linear(12 * 4 * 4, 10)\n\n    def forward(self, x):\n        x = torch.tanh(self.conv1(x))\n        x = self.pool(x)\n        x = torch.tanh(self.conv2(x))\n        x = self.pool(x)\n        x = x.view(-1, 12 * 4 * 4)\n        x = self.fc(x)\n        return x\n\nmodel = LeNet1()\nprint(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n\nTotal parameters: 3,246\n\n\nThis simple architecture achieves high accuracy on MNIST, demonstrating the power of learned features. The convolutional filters automatically discover edge detectors and pattern recognizers through training. We’ll explore how to train models like this in detail in Part 3.\n\n\nWhy LeNet Mattered\nLeNet proved a crucial concept: networks can learn better features than human experts can design. This automated feature learning was revolutionary, but LeNet’s impact remained limited. It worked well on simple tasks like digit recognition but struggled with complex, large-scale problems because the computational constraints of the 1990s prevented training deeper networks (no GPU acceleration, small datasets, primitive training techniques). For nearly two decades, hand-crafted features like SIFT (Scale-Invariant Feature Transform) and HOG (Histogram of Oriented Gradients) powered most practical systems, while neural networks remained research curiosities. Then came 2012.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 2: The Deep Learning Revolution"
    ]
  },
  {
    "objectID": "m05-images/02-the-deep-learning-revolution.html#the-revolution-alexnet",
    "href": "m05-images/02-the-deep-learning-revolution.html#the-revolution-alexnet",
    "title": "Part 2: The Deep Learning Revolution",
    "section": "The Revolution: AlexNet",
    "text": "The Revolution: AlexNet\nThe ImageNet Large Scale Visual Recognition Challenge (ILSVRC) posed a formidable test: classify images into 1000 categories using a training set of 1.2 million images. This scale dwarfed anything LeNet had tackled, and the best systems in 2011 achieved around 25% top-5 error using carefully engineered features and traditional machine learning methods.\n\n\n\n\n\n\nThe ImageNet Large Scale Visual Recognition Challenge dataset contains over 1.2 million training images across 1000 categories.\n\n\n\n\nFigure 3\n\n\n\nIn 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton submitted a deep convolutional network that reduced top-5 error to 16.4%. This more than 10 percentage point improvement shocked the community (Krizhevsky, Sutskever, and Hinton 2012).\n\n\n\n\n\n\nTop-5 error rates on ImageNet from 2010 to 2017. AlexNet’s breakthrough in 2012 sparked the deep learning revolution.\n\n\n\n\nFigure 4\n\n\n\nAlexNet didn’t just win. It demonstrated that deep learning could work at scale, igniting the revolution that transformed computer vision, speech recognition, natural language processing, and countless other domains.\n\nKey Innovation 1: ReLU Activation\nWhy did deep networks fail before AlexNet? Deep networks suffer from the vanishing gradient problem, where gradients shrink as they flow backward through layers. Traditional activations like sigmoid \\sigma(x) = \\frac{1}{1 + e^{-x}} saturate for large positive or negative inputs, driving gradients toward zero and making early layers nearly impossible to train. AlexNet popularized the Rectified Linear Unit (ReLU) (Nair and Hinton 2010):\n\n\\text{ReLU}(x) = \\max(0, x)\n\n\n\n\n\n\n\nSigmoid saturates for large inputs (gradient approaches zero), while ReLU maintains constant gradient for positive inputs.\n\n\n\n\nFigure 5\n\n\n\nReLU offers critical advantages: no vanishing gradient for positive inputs (gradient is exactly 1), computationally cheap (requires only a comparison to zero), and sparse activation where many neurons output zero for efficient representations. The drawback is that neurons can “die” if they always receive negative inputs, never activating again. Variants like Leaky ReLU introduce a small slope for negative inputs to mitigate this:\n\n\\text{Leaky ReLU}(x) = \\begin{cases}\nx & \\text{if } x &gt; 0 \\\\\n\\alpha x & \\text{if } x \\leq 0\n\\end{cases}\n\nwhere \\alpha is typically 0.01.\n\n\nKey Innovation 2: Dropout Regularization\nDeep networks with millions of parameters easily overfit training data. AlexNet introduced Dropout as a powerful regularization technique (Srivastava et al. 2014).\n\n\n\n\n\n\nDropout randomly disables neurons during training, forcing the network to learn robust features.\n\n\n\n\nFigure 6\n\n\n\nDuring training, Dropout randomly sets neuron outputs to zero with probability p (typically 0.5), preventing the network from relying too heavily on any single neuron. The effect is like training an ensemble of networks that share weights. At inference time, all neurons are active but their outputs are scaled by (1-p) to maintain expected values (modern implementations often use inverse dropout, scaling during training instead).\n\n\nKey Innovation 3: GPU Acceleration\nAlexNet demonstrated that deep learning needed massive computational power. The network was trained on two GPUs with 3GB memory each, splitting the computation to handle the large parameter count. This wasn’t just an implementation detail—it showed that deep learning required specialized hardware and helped catalyze the GPU computing revolution that continues today, with modern networks training on dozens or hundreds of GPUs.\n\n\nKey Innovation 4: Data Augmentation\nTo combat overfitting with limited training data, AlexNet applied aggressive data augmentation: random crops of 224×224 patches from 256×256 images, horizontal flips, and color and lighting perturbations through PCA-based color jittering. These transformations artificially expanded the training set, teaching the network to recognize objects regardless of position, orientation, or lighting conditions.\n\n\nThe Architecture\nAlexNet consists of five convolutional layers followed by three fully connected layers:\n\n\n\n\n\n\nAlexNet architecture with 5 convolutional layers and 3 fully connected layers. The network was split across two GPUs.\n\n\n\n\nFigure 7\n\n\n\nThe architecture processes a 224×224 RGB image through the following layers. Conv1 applies 96 filters of 11×11 with stride 4, followed by ReLU and max pooling (3×3, stride 2), then Conv2 uses 256 filters of 5×5 with ReLU and max pooling. Conv3, Conv4, and Conv5 apply 384, 384, and 256 filters respectively using 3×3 kernels, with the final convolutional layer followed by max pooling. Three fully connected layers complete the network: FC6 and FC7 each contain 4096 neurons with ReLU and Dropout, while FC8 outputs 1000 class scores through Softmax.\nThe network has approximately 60 million parameters. The first convolutional layer uses large 11×11 filters with stride 4 to rapidly reduce spatial dimensions, while later layers use smaller 3×3 filters to refine features. AlexNet also used Local Response Normalization (LRN) to normalize activations across adjacent channels, though this technique is less common in modern architectures which typically use batch normalization instead.\n\n\nUsing Pre-Trained AlexNet\nRather than implementing AlexNet from scratch, we can leverage pre-trained models. PyTorch provides AlexNet trained on ImageNet, ready to use:\n\nimport torch\nimport torchvision.models as models\n\n# Load pre-trained AlexNet\nalexnet = models.alexnet(weights='IMAGENET1K_V1')\nalexnet.eval()\n\nprint(f\"Total parameters: {sum(p.numel() for p in alexnet.parameters()):,}\")\n\nTotal parameters: 61,100,840\n\n\nThis pre-trained model has learned rich visual representations from ImageNet’s 1.2 million images. In Part 3, we’ll explore how to use and adapt these pre-trained models for practical applications.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 2: The Deep Learning Revolution"
    ]
  },
  {
    "objectID": "m05-images/02-the-deep-learning-revolution.html#why-alexnet-was-a-paradigm-shift",
    "href": "m05-images/02-the-deep-learning-revolution.html#why-alexnet-was-a-paradigm-shift",
    "title": "Part 2: The Deep Learning Revolution",
    "section": "Why AlexNet Was a Paradigm Shift",
    "text": "Why AlexNet Was a Paradigm Shift\nAlexNet proved several critical points that transformed the field: depth matters (deeper networks learn more powerful representations), data scale matters (large datasets like ImageNet’s 1.2M images enable better learning), compute matters (GPUs make training deep networks practical), and learned features win (automated feature learning beats hand-crafted features). Before AlexNet, these points were debated; after AlexNet, they became accepted wisdom. Within months, researchers worldwide abandoned hand-crafted features, every computer vision competition became a deep learning competition, and companies invested billions in GPU infrastructure.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 2: The Deep Learning Revolution"
    ]
  },
  {
    "objectID": "m05-images/02-the-deep-learning-revolution.html#from-revolution-to-practice",
    "href": "m05-images/02-the-deep-learning-revolution.html#from-revolution-to-practice",
    "title": "Part 2: The Deep Learning Revolution",
    "section": "From Revolution to Practice",
    "text": "From Revolution to Practice\nAlexNet demonstrated that deep learning works at scale. But how do we actually use these powerful models in practice, understand what’s happening inside these black boxes, and build networks with hundreds of layers? These questions lead us to the practical skills and advanced architectures we’ll explore in the remaining sections.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 2: The Deep Learning Revolution"
    ]
  },
  {
    "objectID": "m05-images/02-the-deep-learning-revolution.html#summary",
    "href": "m05-images/02-the-deep-learning-revolution.html#summary",
    "title": "Part 2: The Deep Learning Revolution",
    "section": "Summary",
    "text": "Summary\nWe traced computer vision’s evolution from hand-crafted features to learned representations. Traditional approaches required experts to design edge detectors, Fourier transforms, and pattern recognizers for each task, creating a feature engineering bottleneck that limited the field’s potential. LeNet pioneered automated feature learning in the 1990s, showing that networks could discover useful patterns through training, but computational limits constrained its impact.\nAlexNet’s 2012 breakthrough on ImageNet changed everything with key innovations: ReLU activation (solving vanishing gradients), Dropout (preventing overfitting), and GPU acceleration (enabling large-scale training). The 10+ percentage point improvement shocked the computer vision community and sparked the deep learning revolution. This paradigm shift transformed how we approach machine perception—networks now learn features automatically from data, outperforming carefully engineered alternatives.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 2: The Deep Learning Revolution"
    ]
  },
  {
    "objectID": "m05-images/04-cnn-innovations.html",
    "href": "m05-images/04-cnn-innovations.html",
    "title": "Part 4: The Innovation Timeline",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module traces CNN evolution through successive innovations that solved specific architectural challenges.\nYou’ll learn:\n\nHow VGG demonstrated that depth matters through stacked 3×3 convolutions.\nHow Inception achieved efficiency through multi-scale features and 1×1 convolutions.\nHow ResNet enabled training very deep networks (152 layers) using skip connections.\nHow Vision Transformers replaced convolution with self-attention for global context.\nThe trade-offs between different architectures and how to choose the right one for your project.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 4: CNN Innovations"
    ]
  },
  {
    "objectID": "m05-images/04-cnn-innovations.html#the-quest-for-depth-and-efficiency",
    "href": "m05-images/04-cnn-innovations.html#the-quest-for-depth-and-efficiency",
    "title": "Part 4: The Innovation Timeline",
    "section": "The Quest for Depth and Efficiency",
    "text": "The Quest for Depth and Efficiency\nAlexNet proved that deep learning works at scale in 2012, sparking a race to improve CNN architectures. But simply adding more layers didn’t work. Networks deeper than 20 layers degraded during training, computational costs exploded, and memory constraints limited model size.\nThe innovations we’ll explore emerged as solutions to these challenges. Each architecture addressed specific problems while introducing ideas that influenced everything that followed. This is not a random collection of models, but a coherent story of progress through clever problem-solving.\n\n\n\n\n\n\nImageNet competition winners from 2012 to 2017. Each year brought architectural innovations that pushed accuracy higher.\n\n\n\n\nFigure 1",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 4: CNN Innovations"
    ]
  },
  {
    "objectID": "m05-images/04-cnn-innovations.html#challenge-1-going-deeper-vgg-2014",
    "href": "m05-images/04-cnn-innovations.html#challenge-1-going-deeper-vgg-2014",
    "title": "Part 4: The Innovation Timeline",
    "section": "Challenge 1: Going Deeper (VGG, 2014)",
    "text": "Challenge 1: Going Deeper (VGG, 2014)\nAlexNet demonstrated the power of depth with 8 layers. But could we go deeper? By 2014, researchers at Oxford’s Visual Geometry Group posed this question directly.\n\nThe Depth Hypothesis\nThe intuition was compelling. Deeper networks should learn more complex representations, with early layers detecting simple edges and colors, middle layers combining these into textures and parts, and deep layers recognizing complete objects and scenes. More layers mean more abstraction.\nSo why couldn’t we just keep stacking layers? Training deep networks in 2014 remained difficult. Gradients vanished, training took weeks, and most researchers stuck with networks under 20 layers.\n\n\nVGG’s Answer: Stacked 3×3 Convolutions\nVGGNet demonstrated that systematic depth works (Simonyan and Zisserman 2014). The key insight was using small 3×3 convolutions exclusively, stacked together to build deep networks.\n\n\n\n\n\n\nVGG16 architecture showing progressive downsampling while doubling channels. The network uses only 3×3 convolutions throughout.\n\n\n\n\nFigure 2\n\n\n\nWhy stack 3×3 filters instead of using larger filters? Consider the receptive field. Two 3×3 convolutions have the same receptive field as one 5×5 convolution (both see a 5×5 region of the input). But the stacked version has fewer parameters.\nFor a single 5×5 convolution:\n\n\\text{parameters} = 5 \\times 5 = 25\n\nFor two stacked 3×3 convolutions:\n\n\\text{parameters} = 2 \\times (3 \\times 3) = 18\n\nThis yields a 28% parameter reduction while adding an extra ReLU nonlinearity between the layers, allowing the network to learn more complex functions.\n\n\n\n\n\n\nTwo stacked 3×3 convolutions achieve the same receptive field as one 5×5 convolution but with fewer parameters and added nonlinearity.\n\n\n\n\nFigure 3\n\n\n\n\n\nThe Architecture Pattern\nVGG introduced a clean, systematic pattern that influenced all subsequent architectures.\nAfter each pooling layer, double the channels:\n\n\\text{channels} = \\{64 \\to 128 \\to 256 \\to 512 \\to 512\\}\n\nSpatial dimensions halve:\n\n\\text{spatial dimensions} = \\{224 \\to 112 \\to 56 \\to 28 \\to 14 \\to 7\\}\n\nThis creates a pyramid structure where computational cost per layer stays roughly constant. As spatial dimensions decrease, increasing channel depth compensates by expanding representational capacity.\nVGG16 (16 layers) and VGG19 (19 layers) achieved strong results on ImageNet. This validated that systematic depth improves accuracy. The architecture’s simplicity made it easy to understand and implement, contributing to its widespread adoption.\n\n\nThe Limitation\nVGG16 contains approximately 140 million parameters, with the majority (102 million) concentrated in the first fully connected layer. This massive parameter count means training requires significant computational resources, inference is memory-intensive, and the model is prone to overfitting without strong regularization. The question became: can we achieve similar accuracy with fewer parameters?",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 4: CNN Innovations"
    ]
  },
  {
    "objectID": "m05-images/04-cnn-innovations.html#challenge-2-computing-efficiently-inceptiongooglenet-2014",
    "href": "m05-images/04-cnn-innovations.html#challenge-2-computing-efficiently-inceptiongooglenet-2014",
    "title": "Part 4: The Innovation Timeline",
    "section": "Challenge 2: Computing Efficiently (Inception/GoogLeNet, 2014)",
    "text": "Challenge 2: Computing Efficiently (Inception/GoogLeNet, 2014)\nWhile VGG pushed depth systematically, researchers at Google asked a different question: how do we capture multi-scale features efficiently?\n\nMulti-Scale Feature Extraction\nLook at a photograph. Some objects are large and occupy significant image area, while others are small details. To recognize both, the network needs to examine features at multiple scales simultaneously. Traditional CNN layers use a single kernel size (like 3×3), but the optimal kernel size varies by context. Large kernels capture broad patterns while small kernels detect fine details. Inception’s answer: use multiple kernel sizes in parallel within the same layer (Szegedy et al. 2015).\n\n\nThe 1×1 Convolution Trick\nRunning multiple large convolutions in parallel is computationally expensive. Inception solves this through 1×1 convolutions for channel dimensionality reduction. At first, 1×1 convolutions seem strange. They don’t look at neighboring pixels, only at different channels at the same location, but this is precisely their power. They compress information across channels before applying larger, more expensive filters.\nConsider a 3×3 convolution on a 256-channel feature map producing 256 output channels:\n\n\\text{parameters (without reduction)} = 3 \\times 3 \\times 256 \\times 256 = 589{,}824\n\nWith a 1×1 convolution reducing to 64 channels first:\n\n\\text{parameters (with reduction)} = (1 \\times 1 \\times 256 \\times 64) + (3 \\times 3 \\times 64 \\times 256) = 163{,}840\n\nThis achieves a 72% parameter reduction while maintaining similar expressive power. The theoretical motivation behind 1×1 convolutions is elegant: Inception approximates sparse connectivity since not every pixel needs to connect to every pixel in the next layer. The 1×1 convolutions sparsify connections efficiently by operating primarily across channels rather than spatial dimensions.\n\n\nThe Inception Module\nEach Inception module contains four parallel branches:\n\n1×1 convolution: Captures point-wise patterns\n1×1 → 3×3 convolution: Captures medium-scale patterns (with reduction)\n1×1 → 5×5 convolution: Captures large-scale patterns (with reduction)\n3×3 max pooling → 1×1 convolution: Preserves spatial structure differently\n\nThese branches process the same input simultaneously, and their outputs concatenate along the channel dimension to create a multi-scale representation. Mathematically, for input X:\n\nY_{\\text{inception}} = \\text{Concat}\\big(Y_{1\\times1}, \\,Y_{3\\times3}, \\,Y_{5\\times5}, \\,Y_{\\text{pool}}\\big)\n\nwhere each Y represents the output of its respective branch.\n\n\nGlobal Average Pooling\nVGG’s fully connected layers contain 102 million parameters. How do we eliminate this bottleneck? Inception uses global average pooling (Lin, Chen, and Yan 2013), taking the average value of each channel across all spatial positions instead of flattening feature maps and passing through dense layers. For a feature map with 1000 channels, this produces a 1000-dimensional vector directly, regardless of spatial size. This drastically reduces parameters (no heavy fully connected layers), creates translation invariance (averaging eliminates spatial dependence), and reduces overfitting risk.\n\n\nAuxiliary Classifiers\nGoogLeNet introduced auxiliary classifiers at intermediate layers to combat vanishing gradients in deep networks. These classifiers attach to middle layers, computing losses that provide additional gradient signals during backpropagation. During training, the total loss combines the main classifier loss with auxiliary losses (typically weighted at 0.3), but at inference only the main classifier is used.\n\n\n\n\n\n\nGoogLeNet architecture with auxiliary classifiers attached to intermediate layers to improve gradient flow.\n\n\n\n\nFigure 4\n\n\n\n\n\nThe Impact\nGoogLeNet achieved accuracy comparable to VGG with 12× fewer parameters, demonstrating that architecture efficiency matters as much as depth. Later versions pushed these ideas further: Inception v2/v3 added batch normalization and factorized larger filters (5×5 became two 3×3 convolutions), Inception v4 integrated with residual connections, and Xception used depthwise separable convolutions. Batch Normalization, introduced around this time (Ioffe and Szegedy 2015), normalizes layer activations to zero mean and unit variance, stabilizing training and allowing higher learning rates. It became standard in nearly all subsequent architectures.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 4: CNN Innovations"
    ]
  },
  {
    "objectID": "m05-images/04-cnn-innovations.html#challenge-3-training-very-deep-networks-resnet-2015",
    "href": "m05-images/04-cnn-innovations.html#challenge-3-training-very-deep-networks-resnet-2015",
    "title": "Part 4: The Innovation Timeline",
    "section": "Challenge 3: Training Very Deep Networks (ResNet, 2015)",
    "text": "Challenge 3: Training Very Deep Networks (ResNet, 2015)\nBy 2015, researchers wanted networks with 50, 100, or even 150 layers. But a puzzling phenomenon blocked progress: networks deeper than about 20 layers exhibited degradation.\n\nThe Degradation Problem\nHere’s what was strange: adding more layers to a working network increased training error (not test error, which would indicate overfitting, but training error itself). A deeper network could theoretically learn the identity function for extra layers, matching the shallower network’s performance, but in practice optimization failed. The deeper network couldn’t even learn to copy what the shallower network already achieved, revealing a fundamental optimization difficulty beyond vanishing gradients (which batch normalization addressed).\n\n\nThe Residual Learning Solution\nMicrosoft Research proposed an elegant solution: skip connections (He et al. 2015).\nInstead of learning a direct mapping H(\\mathbf{x}) from input \\mathbf{x} to output, learn the residual F(\\mathbf{x}) = H(\\mathbf{x}) - \\mathbf{x}. Then add the input back:\n\nH(\\mathbf{x}) = F(\\mathbf{x}) + \\mathbf{x}\n\n\n\n\n\n\n\nResidual block. The skip connection carries the input directly to the output, while convolutional layers learn the residual.\n\n\n\n\nFigure 5\n\n\n\nWhy does this help? If the optimal mapping is close to identity (the layer isn’t very useful), the network can easily learn F(\\mathbf{x}) \\approx 0 by pushing weights toward zero, while the skip connection ensures input information flows through unchanged. If a more complex transformation is needed, F(\\mathbf{x}) can still learn it since the skip connection doesn’t constrain what the block can represent. It just makes optimization easier by providing a gradient highway during backpropagation.\n\n\nEnsemble-Like Gradient Flow\nSkip connections create multiple paths for gradients to flow backward. Some paths go through all convolutions. Others skip multiple blocks via cascaded skip connections. This ensemble of paths accelerates training and prevents vanishing gradients (Veit, Wilber, and Belongie 2016).\n\n\n\n\n\n\nMultiple gradient paths in ResNet. Gradients can skip layers via identity connections, providing stable training for very deep networks.\n\n\n\n\nFigure 6\n\n\n\n\n\nBottleneck Blocks for Deeper Networks\nResNet-50, -101, and -152 use bottleneck blocks to maintain efficiency:\n\n1×1 convolution: Reduces channel dimension (e.g., 256 → 64)\n3×3 convolution: Operates on reduced dimension\n1×1 convolution: Restores dimension (e.g., 64 → 256)\n\n\n\n\n\n\n\nBottleneck block (left) vs. basic block (right). The bottleneck design reduces computational cost in very deep networks.\n\n\n\n\nFigure 7\n\n\n\nThis shrinks the intermediate feature map, dramatically reducing computational cost while maintaining representational capacity through a design inspired by Inception’s bottleneck idea.\n\n\nThe Results\nResNet achieved:\n\n152 layers trained successfully without degradation\nTop-5 error of 3.57% on ImageNet (better than human performance on the test set)\nWidespread adoption across computer vision tasks\n\nThe impact extended beyond CNNs. Skip connections appeared in:\n\nU-Net for medical image segmentation\nDenseNet which connects every layer to every other layer\nTransformers for natural language processing\nNearly all modern deep architectures\n\nResNet showed that with the right architecture, depth isn’t a limitation. It’s a resource.\n\n\nResNeXt: Width Through Cardinality\nResNeXt (Xie et al. 2017) extended ResNet by increasing network width through grouped convolutions rather than just adding depth or channels. The idea is to split the bottleneck convolution path into multiple parallel groups (typically 32), each processing independently, then aggregate their outputs through concatenation or addition.\n\n\n\n\n\n\nResNeXt block with multiple grouped convolution paths. Increasing cardinality (number of groups) often improves accuracy more than increasing depth or channel count.\n\n\n\n\nFigure 8\n\n\n\nThis “cardinality” dimension provides another axis for scaling networks. ResNeXt achieves better accuracy than ResNet at similar computational cost by increasing cardinality instead of just going deeper.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 4: CNN Innovations"
    ]
  },
  {
    "objectID": "m05-images/04-cnn-innovations.html#challenge-4-global-context-vision-transformer-2020",
    "href": "m05-images/04-cnn-innovations.html#challenge-4-global-context-vision-transformer-2020",
    "title": "Part 4: The Innovation Timeline",
    "section": "Challenge 4: Global Context (Vision Transformer, 2020)",
    "text": "Challenge 4: Global Context (Vision Transformer, 2020)\nCNNs build global understanding slowly through stacked local operations, with early layers seeing only small patches (3×3 or 5×5 regions) and deeper layers expanding the receptive field. Even in deep networks, truly global connections require many layers. What if we could capture global relationships immediately?\n\nThe Self-Attention Mechanism\nVision Transformers (ViT) replace convolution with self-attention, which computes relationships between all positions simultaneously. For each patch of the image, it determines which other patches are relevant, regardless of distance, providing immediate global context.\nHow does the mechanism work? Given input features X, compute three matrices through learned linear projections:\n\nQ = XW_Q, \\quad K = XW_K, \\quad V = XW_V\n\nwhere Q (queries), K (keys), and V (values) represent different views of the input.\nAttention scores measure similarity between queries and keys:\n\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\nThis computes a weighted average of values, where weights depend on query-key similarity. Intuitively, each position “asks” (via its query) what information to gather from all other positions (via their keys), then aggregates their values accordingly.\n\n\nPatches as Tokens\nViT treats images like text: divide the image into fixed-size patches (typically 16×16), flatten each patch into a vector, and treat these vectors as “tokens” (analogous to words in NLP). Add positional encodings to preserve spatial information (since self-attention is permutation-invariant) and pass through a standard Transformer encoder with multiple self-attention layers. A special CLS token prepended to the sequence gathers global information, and after all Transformer layers, the CLS token’s representation feeds into the classification head, mirroring how BERT processes text.\n\n\nTrade-offs: Data and Compute\nVision Transformers achieve state-of-the-art accuracy on large datasets like ImageNet-21k (14 million images), but they have important trade-offs. They provide a global receptive field from the first layer, better scaling properties with dataset size, and a unified architecture for vision and language. However, they require more training data than CNNs due to less inductive bias, impose higher computational cost since self-attention is O(n^2) in sequence length, and prove less effective on small datasets without strong augmentation.\nWhen should you use ViT versus CNNs? Use ViT when you have large datasets (millions of images), substantial computational resources, and tasks benefiting from global context like scene understanding or fine-grained classification. Use CNNs when you have limited data (thousands of images), constrained compute (edge devices, mobile), or tasks benefiting from spatial locality (object detection, segmentation).\n\n\nHybrid Approaches\nRecent research combines CNN and Transformer strengths. The field continues evolving, blending ideas from both paradigms.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 4: CNN Innovations"
    ]
  },
  {
    "objectID": "m05-images/04-cnn-innovations.html#the-narrative-of-progress",
    "href": "m05-images/04-cnn-innovations.html#the-narrative-of-progress",
    "title": "Part 4: The Innovation Timeline",
    "section": "The Narrative of Progress",
    "text": "The Narrative of Progress\nThe thread connecting these innovations is clear. In 2012, AlexNet showed depth works but only to 8 layers. By 2014, VGG stacked small convolutions to go deeper (16-19 layers), while Inception used multi-scale features and 1×1 convolutions for efficiency. In 2015, ResNet’s skip connections enabled training very deep networks (152 layers). ResNeXt in 2017 increased width through cardinality, not just depth. Finally, Vision Transformers in 2020 replaced convolution with self-attention for global context.\nEach innovation addressed limitations of its predecessors while preserving their insights. Modern architectures mix and match these ideas: residual connections for depth, multi-scale features for efficiency, attention for global context.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 4: CNN Innovations"
    ]
  },
  {
    "objectID": "m05-images/04-cnn-innovations.html#choosing-the-right-architecture",
    "href": "m05-images/04-cnn-innovations.html#choosing-the-right-architecture",
    "title": "Part 4: The Innovation Timeline",
    "section": "Choosing the Right Architecture",
    "text": "Choosing the Right Architecture\nFor your next computer vision project, which architecture should you choose? ResNet-50 is the default choice, offering an excellent accuracy-computational cost trade-off with widely available pre-trained weights that work well across diverse tasks. EfficientNet matters when deployment efficiency is critical, carefully balancing depth, width, and resolution for optimal accuracy per parameter, while MobileNet and EfficientNet-Lite serve mobile and edge devices by sacrificing some accuracy for fast inference and small model size. Vision Transformer (ViT) excels when you have large datasets (millions of images) and substantial compute, delivering state-of-the-art accuracy on challenging benchmarks, while Swin Transformer provides Transformer benefits with more reasonable compute requirements, proving especially good for dense prediction tasks. Start with ResNet-50 for strong performance across almost all applications, then optimize later if specific constraints (speed, memory, accuracy) demand it.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 4: CNN Innovations"
    ]
  },
  {
    "objectID": "m05-images/04-cnn-innovations.html#summary",
    "href": "m05-images/04-cnn-innovations.html#summary",
    "title": "Part 4: The Innovation Timeline",
    "section": "Summary",
    "text": "Summary\nWe traced CNN evolution through successive innovations solving specific challenges. VGG demonstrated that depth matters through stacked 3×3 convolutions, while Inception showed how to capture multi-scale features efficiently using 1×1 convolutions and parallel branches. ResNet enabled training very deep networks (152 layers) through skip connections that ease optimization and improve gradient flow. Vision Transformers replaced convolution with self-attention, trading inductive bias for global context at the cost of requiring more data and compute.\nEach architecture built on its predecessors’ insights. Modern networks combine ideas from all of them: residual connections for depth, multi-scale features for efficiency, attention for global understanding. Architecture design is problem-solving. Understanding why these innovations emerged helps you make informed choices for your own applications.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 4: CNN Innovations"
    ]
  },
  {
    "objectID": "m06-graph/01-from-images-to-graphs.html",
    "href": "m06-graph/01-from-images-to-graphs.html",
    "title": "From Images to Graphs",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module bridges your knowledge of image convolution to graph data.\nYou’ll learn:\n\nHow pixels and nodes are analogous and what this reveals about convolution.\nWhy irregular graph structure poses unique challenges for deep learning.\nThe spectral perspective that defines convolution using frequency domains.\nThe spatial perspective that aggregates features from local neighborhoods.\nHow these two perspectives complement each other in graph neural networks.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 1: From Images to Graphs"
    ]
  },
  {
    "objectID": "m06-graph/01-from-images-to-graphs.html#the-pixel-node-analogy",
    "href": "m06-graph/01-from-images-to-graphs.html#the-pixel-node-analogy",
    "title": "From Images to Graphs",
    "section": "The Pixel-Node Analogy",
    "text": "The Pixel-Node Analogy\nLet’s talk about a powerful reframing. We have seen how convolutional neural networks process images by sliding kernels across a regular grid of pixels. Each pixel is convolved with its neighbors to extract features like edges, textures, and patterns.\nNow shift your perspective. Think of each pixel not as a grid cell, but as a node in a network. The neighbors involved in convolution become edges connecting that node to nearby nodes.\n\nWhat does this analogy reveal? If we can define convolution on a regular grid of pixels, perhaps we can extend it to irregular networks where nodes have varying numbers of neighbors. This opens the door to applying deep learning to graph-structured data: social networks, molecules, knowledge graphs, and more.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 1: From Images to Graphs"
    ]
  },
  {
    "objectID": "m06-graph/01-from-images-to-graphs.html#the-challenge-irregular-structure",
    "href": "m06-graph/01-from-images-to-graphs.html#the-challenge-irregular-structure",
    "title": "From Images to Graphs",
    "section": "The Challenge: Irregular Structure",
    "text": "The Challenge: Irregular Structure\nHere’s where the analogy breaks down. In images, every pixel (except boundaries) has the same number of neighbors. A 3×3 kernel always covers exactly 9 pixels. The convolution operation is translation invariant: the same kernel works everywhere.\nGraphs shatter this regularity. Consider a social network: some people have 5 friends, others have 500. Some molecules have 3 bonds, others have dozens. The number of neighbors varies wildly across nodes.\nThis irregularity raises fundamental questions. How do we define a “kernel” when neighborhoods have different sizes? How do we share parameters across nodes with vastly different connectivity patterns? How do we preserve the inductive biases that make CNNs so powerful?",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 1: From Images to Graphs"
    ]
  },
  {
    "objectID": "m06-graph/01-from-images-to-graphs.html#two-perspectives-spectral-and-spatial",
    "href": "m06-graph/01-from-images-to-graphs.html#two-perspectives-spectral-and-spatial",
    "title": "From Images to Graphs",
    "section": "Two Perspectives: Spectral and Spatial",
    "text": "Two Perspectives: Spectral and Spatial\nThe research community developed two complementary approaches to graph convolution, each offering unique insights.\n\nThe Spectral Perspective\nRemember the convolution theorem from signal processing? Convolution in the spatial domain corresponds to multiplication in the frequency domain. For images, we use the Fourier transform to decompose signals into sinusoidal basis functions.\nGraphs have their own notion of “frequency.” The graph Laplacian’s eigenvectors serve as basis functions, and eigenvalues indicate how much node features vary across edges.\nWhat do these eigenvalues mean? Small eigenvalues correspond to smooth signals (low frequency), where connected nodes have similar values. Large eigenvalues correspond to rapidly varying signals (high frequency), where neighbors differ significantly.\nThe spectral perspective defines graph convolution by designing filters in this frequency domain. We learn which frequencies to amplify or suppress, just as Fourier analysis filters images. This approach is mathematically elegant and connects to decades of spectral graph theory.\n\n\nThe Spatial Perspective\nThe spatial perspective takes a more direct route. Instead of transforming to a frequency domain, it defines convolution as aggregating features from local neighborhoods. Think of it as a message-passing scheme: each node collects information from its neighbors, combines it with its own features, and produces an updated representation.\nThis perspective leads to intuitive architectures. GraphSAGE samples fixed-size neighborhoods to control memory. Graph Attention Networks learn which neighbors to pay attention to. Graph Isomorphism Networks carefully design aggregation functions to maximize discriminative power.\nWhy choose spatial methods? They are often more efficient and easier to scale to large graphs. They also generalize naturally to new nodes not seen during training.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 1: From Images to Graphs"
    ]
  },
  {
    "objectID": "m06-graph/01-from-images-to-graphs.html#looking-ahead",
    "href": "m06-graph/01-from-images-to-graphs.html#looking-ahead",
    "title": "From Images to Graphs",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nWe have already learned what images are and how CNNs process them. Now we face a new frontier: irregular structures where the rules of regular grids no longer apply.\nWhat’s next? In Part 2, we dive into the spectral perspective, exploring how graph Laplacians define frequency domains and how spectral filters enable learnable convolutions. In Part 3, we examine spatial graph networks, from the elegant simplicity of GCN to the sophisticated attention mechanisms of GAT. In Part 4, we explore graph embeddings, learning low-dimensional representations that capture both local and global structure.\nThe journey from pixels to nodes reveals a deeper truth. Convolution is not about grids. It is about locality, parameter sharing, and hierarchical feature extraction. These principles transcend regular structure and apply wherever relationships exist.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 1: From Images to Graphs"
    ]
  },
  {
    "objectID": "m06-graph/03-spatial-networks.html",
    "href": "m06-graph/03-spatial-networks.html",
    "title": "Spatial Graph Networks",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module explores spatial approaches to graph convolution.\nYou’ll learn:\n\nHow ChebNet bridges spectral and spatial domains using Chebyshev polynomials.\nHow GCN achieves radical simplification through first-order approximation and renormalization.\nHow GraphSAGE enables inductive learning through sampling and aggregation.\nHow GAT learns to weight neighbors using attention mechanisms.\nHow GIN achieves maximum discriminative power by preserving multiset information.\nPractical trade-offs between these architectures for different graph learning tasks.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 3: Spatial Graph Networks"
    ]
  },
  {
    "objectID": "m06-graph/03-spatial-networks.html#from-spectral-to-spatial",
    "href": "m06-graph/03-spatial-networks.html#from-spectral-to-spatial",
    "title": "Spatial Graph Networks",
    "section": "From Spectral to Spatial",
    "text": "From Spectral to Spatial\nLet’s talk about why we need spatial methods. The spectral perspective offers mathematical elegance but faces practical challenges. Computing eigendecomposition scales poorly to large graphs. Spectral filters lack spatial locality, allowing distant nodes to directly influence each other.\nWhat’s the alternative? Spatial methods take a different approach. Instead of transforming to frequency domains, they define convolution directly as aggregating features from local neighborhoods.\nThink of it as message passing. Each node collects information from its neighbors, combines it, and produces an updated representation. This shift brings immediate benefits.\nSpatial methods scale linearly with the number of edges. They preserve locality by design. They generalize naturally to unseen nodes. But we must carefully design how nodes aggregate neighbor information.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 3: Spatial Graph Networks"
    ]
  },
  {
    "objectID": "m06-graph/03-spatial-networks.html#chebnet-the-bridge",
    "href": "m06-graph/03-spatial-networks.html#chebnet-the-bridge",
    "title": "Spatial Graph Networks",
    "section": "ChebNet: The Bridge",
    "text": "ChebNet: The Bridge\nHow do we bridge spectral and spatial perspectives? ChebNet (Defferrard, Bresson, and Vandergheynst 2016) achieves this using Chebyshev polynomials. The key insight is that we can approximate spectral filters without computing full eigendecompositions.\nLet’s recall the learnable spectral filter:\n\n{\\bf L}_{\\text{learn}} = \\sum_{k=1}^K \\theta_k {\\bf u}_k {\\bf u}_k^\\top\n\nChebNet approximates this using Chebyshev polynomials of the scaled Laplacian \\tilde{{\\bf L}} = \\frac{2}{\\lambda_{\\text{max}}}{\\bf L} - {\\bf I}:\n\n{\\bf L}_{\\text{learn}} \\approx \\sum_{k=0}^{K-1} \\theta_k T_k(\\tilde{{\\bf L}})\n\nwhere T_k are Chebyshev polynomials defined recursively:\n\n\\begin{aligned}\nT_0(\\tilde{{\\bf L}}) &= {\\bf I} \\\\\nT_1(\\tilde{{\\bf L}}) &= \\tilde{{\\bf L}} \\\\\nT_k(\\tilde{{\\bf L}}) &= 2\\tilde{{\\bf L}} T_{k-1}(\\tilde{{\\bf L}}) - T_{k-2}(\\tilde{{\\bf L}})\n\\end{aligned}\n\nThe convolution becomes:\n\n{\\bf x}^{(\\ell+1)} = h\\left( \\sum_{k=0}^{K-1} \\theta_k T_k(\\tilde{{\\bf L}}){\\bf x}^{(\\ell)}\\right)\n\nWhy does this help? The crucial property is that multiplying by \\tilde{{\\bf L}} is a local operation. Computing T_k(\\tilde{{\\bf L}}){\\bf x} only involves nodes within k hops of each neighborhood.\nThis achieves spatial locality without explicit eigendecomposition. ChebNet typically uses small K (e.g., K=3), limiting receptive fields to nearby neighbors. Deeper networks expand receptive fields by stacking layers.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 3: Spatial Graph Networks"
    ]
  },
  {
    "objectID": "m06-graph/03-spatial-networks.html#gcn-radical-simplification",
    "href": "m06-graph/03-spatial-networks.html#gcn-radical-simplification",
    "title": "Spatial Graph Networks",
    "section": "GCN: Radical Simplification",
    "text": "GCN: Radical Simplification\nCan we simplify further? While ChebNet offers principled spectral approximation, Kipf and Welling (2017) proposed an even simpler variant that became one of the most widely used graph neural networks.\n\nFirst-Order Approximation\nWhat’s the key departure? Using only the first-order Chebyshev approximation. Setting K=1 and making additional simplifications yields:\n\ng_{\\theta} * x \\approx \\theta({\\bf I}_N + {\\bf D}^{-\\frac{1}{2}}{\\bf A}{\\bf D}^{-\\frac{1}{2}})x\n\nwhere {\\bf D} is the degree matrix and {\\bf A} is the adjacency matrix. This leaves a single learnable parameter \\theta instead of K parameters. Notice the dramatic reduction in complexity.\n\n\nRenormalization Trick\nHow do we stabilize training in deep networks? GCN adds self-loops to the graph:\n\n\\tilde{{\\bf A}} = {\\bf A} + {\\bf I}_N, \\quad \\tilde{{\\bf D}}_{ii} = \\sum_j \\tilde{{\\bf A}}_{ij}\n\nThis ensures each node includes its own features when aggregating from neighbors, preventing information loss and mitigating gradient vanishing.\n\n\nLayer-Wise Propagation\nThe final GCN layer becomes remarkably simple:\n\n{\\bf X}^{(\\ell+1)} = \\sigma(\\tilde{{\\bf D}}^{-\\frac{1}{2}}\\tilde{{\\bf A}}\\tilde{{\\bf D}}^{-\\frac{1}{2}}{\\bf X}^{(\\ell)}{\\bf W}^{(\\ell)})\n\nwhere {\\bf X}^{(\\ell)} \\in \\mathbb{R}^{N \\times f_{\\text{in}}} are node features at layer \\ell, {\\bf W}^{(\\ell)} \\in \\mathbb{R}^{f_{\\text{in}} \\times f_{\\text{out}}} is a learnable weight matrix, and \\sigma is a nonlinear activation. Look at how simple this formula is compared to spectral methods.\nDespite its simplicity, GCN achieves strong performance on many tasks. The symmetric normalization \\tilde{{\\bf D}}^{-\\frac{1}{2}}\\tilde{{\\bf A}}\\tilde{{\\bf D}}^{-\\frac{1}{2}} ensures numerical stability by normalizing message magnitudes.\n\n\n\n\n\n\nExercise\n\n\n\nImplement a simple GCN model for node classification. Coding Exercise",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 3: Spatial Graph Networks"
    ]
  },
  {
    "objectID": "m06-graph/03-spatial-networks.html#graphsage-sample-and-aggregate",
    "href": "m06-graph/03-spatial-networks.html#graphsage-sample-and-aggregate",
    "title": "Spatial Graph Networks",
    "section": "GraphSAGE: Sample and Aggregate",
    "text": "GraphSAGE: Sample and Aggregate\nWhat about scalability? GCN processes entire graphs at once, limiting its applicability to large graphs. GraphSAGE (Hamilton, Ying, and Leskovec 2017) introduced an inductive framework that generates embeddings by sampling and aggregating features from neighborhoods.\n\n\nNeighborhood Sampling\nInstead of using all neighbors, GraphSAGE samples a fixed-size set. This controls memory complexity and enables handling dynamic graphs where new nodes arrive continuously.\nConsider a growing citation network. Traditional GCNs require recomputing filters for the entire graph with each new paper. GraphSAGE generates embeddings for new nodes by sampling their neighbors, without retraining. This is what we mean by inductive learning.\n\n\nAggregation Functions\nHow does GraphSAGE combine information? It distinguishes self-information from neighborhood information. First, aggregate neighbor features:\n\n{\\bf h}_{\\mathcal{N}(v)} = \\text{AGGREGATE}(\\{{\\bf h}_u, \\forall u \\in \\mathcal{N}(v)\\})\n\nCommon aggregators include mean, max-pooling, and LSTM. The mean aggregator computes \\text{mean}(\\{{\\bf h}_u, \\forall u \\in \\mathcal{N}(v)\\}). Max-pooling uses \\max(\\{\\sigma({\\bf W}_{\\text{pool}}{\\bf h}_u + {\\bf b}), \\forall u \\in \\mathcal{N}(v)\\}). LSTM applies to randomly permuted neighbors.\nThen concatenate self and neighborhood information:\n\n{\\bf z}_v = \\text{CONCAT}({\\bf h}_v, {\\bf h}_{\\mathcal{N}(v)})\n\nNormalize and apply the learned transformation:\n\n{\\bf h}_v^{(\\ell+1)} = \\sigma({\\bf W}^{(\\ell)} \\frac{{\\bf z}_v}{\\|{\\bf z}_v\\|_2})\n\nThis explicit separation allows the model to learn how much to trust self versus neighbors.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 3: Spatial Graph Networks"
    ]
  },
  {
    "objectID": "m06-graph/03-spatial-networks.html#gat-learning-attention",
    "href": "m06-graph/03-spatial-networks.html#gat-learning-attention",
    "title": "Spatial Graph Networks",
    "section": "GAT: Learning Attention",
    "text": "GAT: Learning Attention\nHere’s a crucial question. Should all neighbors contribute equally? Graph Attention Networks (velickovic2018graph?) let the model learn which neighbors matter most.\n\n\nAttention Mechanism\nHow does attention work? GAT computes attention weights \\alpha_{ij} indicating how much node i should attend to node j:\n\n\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}(i)} \\exp(e_{ik})}\n\nwhere e_{ij} measures edge importance. How do we compute edge importance? One approach uses a neural network with shared parameters. First, transform node features: \\tilde{{\\bf h}}_i = {\\bf W}{\\bf h}_i. Then compute attention logits: e_{ij} = \\text{LeakyReLU}({\\bf a}^T[\\tilde{{\\bf h}}_i \\| \\tilde{{\\bf h}}_j]).\nHere {\\bf a} is a learnable attention vector and \\| denotes concatenation.\n\n\nNode Update\nWith attention weights, the update becomes a weighted sum:\n\n{\\bf h}_i^{(\\ell+1)} = \\sigma\\left(\\sum_{j \\in \\mathcal{N}(i) \\cup \\{i\\}} \\alpha_{ij}{\\bf W}_{\\text{feature}}{\\bf h}_j^{(\\ell)}\\right)\n\n\n\nMulti-Head Attention\nHow do we stabilize training? GAT uses multiple attention heads and concatenates outputs:\n\n{\\bf h}_i^{(\\ell+1)} = \\|_{k=1}^K \\sigma\\left(\\sum_{j \\in \\mathcal{N}(i) \\cup \\{i\\}} \\alpha_{ij}^k{\\bf W}^k{\\bf h}_j^{(\\ell)}\\right)\n\nDifferent heads can focus on different aspects of neighborhood structure. This is similar to multi-head attention in Transformers.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 3: Spatial Graph Networks"
    ]
  },
  {
    "objectID": "m06-graph/03-spatial-networks.html#gin-the-power-of-aggregation",
    "href": "m06-graph/03-spatial-networks.html#gin-the-power-of-aggregation",
    "title": "Spatial Graph Networks",
    "section": "GIN: The Power of Aggregation",
    "text": "GIN: The Power of Aggregation\nLet’s ask a fundamental question. What is the maximum discriminative power of graph neural networks? Graph Isomorphism Networks (Xu et al. 2019) provide an answer.\n\nThe Weisfeiler-Lehman Test\nThe Weisfeiler-Lehman (WL) test determines if two graphs are structurally identical. It iteratively refines node labels by hashing neighbor labels.\n\nHow does the process work? First, assign all nodes the same initial label. For each node, collect neighbor labels and create a multiset. Hash the multiset with the node’s own label to produce a new label. Repeat until convergence.\nTwo graphs are isomorphic if they have identical label distributions after convergence. The WL test fails on some graphs (like regular graphs) but works well in practice. The standard WL test is called 1-WL, while higher-order variants exist that can distinguish more graphs, leading to more powerful GNNs.\n\n\nGIN’s Key Insight\nWhat’s the connection between WL and GNNs? The parallel is striking. WL iteratively aggregates neighbor labels via hash function. GNNs iteratively aggregate neighbor features via learned function.\nWhat’s the crucial difference? Discriminative power. WL’s hash function always distinguishes different neighbor multisets by counting occurrences. Common GNN aggregators like mean or max can fail. If all neighbors have identical features, these aggregators produce the same output regardless of neighborhood size.\nHow does GIN solve this? By designing an aggregation that preserves multiset information:\n\n{\\bf h}_v^{(k+1)} = \\text{MLP}^{(k)}\\left((1 + \\epsilon^{(k)}) \\cdot {\\bf h}_v^{(k)} + \\sum_{u \\in \\mathcal{N}(v)} {\\bf h}_u^{(k)}\\right)\n\nwhere \\text{MLP}^{(k)} is a multi-layer perceptron and \\epsilon^{(k)} is a learnable or fixed scalar. Why does this work?\nThe sum aggregation preserves multiset information. The self-loop with weight (1 + \\epsilon) distinguishes nodes from their neighborhoods. The MLP provides sufficient capacity to approximate injective functions, matching WL’s discriminative power.\nXu et al. (2019) prove that GIN can distinguish any graphs that WL can distinguish. This makes it maximally powerful within the class of message-passing neural networks.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 3: Spatial Graph Networks"
    ]
  },
  {
    "objectID": "m06-graph/03-spatial-networks.html#comparing-approaches",
    "href": "m06-graph/03-spatial-networks.html#comparing-approaches",
    "title": "Spatial Graph Networks",
    "section": "Comparing Approaches",
    "text": "Comparing Approaches\nLet’s compare what we’ve learned. We have seen five spatial architectures, each with distinct design choices.\n\n\n\n\n\n\n\n\nMethod\nKey Innovation\nTrade-off\n\n\n\n\nChebNet\nChebyshev approximation\nBridges spectral/spatial, moderate complexity\n\n\nGCN\nRadical simplification\nSimple, efficient, strong baseline\n\n\nGraphSAGE\nSampling + separation\nScalable, inductive, flexible\n\n\nGAT\nLearned attention\nInterpretable, handles heterophily\n\n\nGIN\nInjective aggregation\nMaximum discriminative power\n\n\n\nWhich one should you use? The best choice depends on your task. GCN provides a strong baseline. GraphSAGE handles large, dynamic graphs. GAT works when neighbor importance varies. GIN excels at graph-level tasks requiring fine-grained discrimination.\nAll these methods share a common theme. They define convolution as neighborhood aggregation, preserving spatial locality while learning which patterns matter for downstream tasks. This is the essence of spatial graph neural networks.\nWhat comes next? In Part 4, we shift from node classification to representation learning. We explore graph embeddings that map nodes to continuous vector spaces, enabling clustering, visualization, and transfer learning across tasks.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Part 3: Spatial Graph Networks"
    ]
  },
  {
    "objectID": "m06-graph/from-image-to-graph.html",
    "href": "m06-graph/from-image-to-graph.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "title: “From image to graph”\n\n\njupyter: advnetsci\n\n\nexecute:\n\n\nenabled: true"
  },
  {
    "objectID": "m06-graph/from-image-to-graph.html#analogy-between-image-and-graph-data",
    "href": "m06-graph/from-image-to-graph.html#analogy-between-image-and-graph-data",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "Analogy between image and graph data",
    "text": "Analogy between image and graph data\nLet’s talk about a surprising connection. We can think of image convolution from the perspective of networks.\nIn the convolution of an image, a pixel is convolved with its neighbors. We can regard each pixel as a node, and each node is connected to its neighboring nodes (pixels) that are involved in the convolution.\n\nBuilding on this analogy, we can extend the idea of convolution to general graph data. Each node has a pixel value (e.g., feature vector), which is convolved with the values of its neighbors in the graph. This is the key idea of graph convolutional networks.\nHere’s where the analogy breaks down. While the number of neighbors for an image is homogeneous, the number of neighbors for a node in a graph can be heterogeneous. Each pixel has the same number of neighbors (except for the boundary pixels), but nodes in a graph can have very different numbers of neighbors. This makes it non-trivial to define the “kernel” for graph convolution."
  },
  {
    "objectID": "m06-graph/from-image-to-graph.html#spectral-filter-on-graphs",
    "href": "m06-graph/from-image-to-graph.html#spectral-filter-on-graphs",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "Spectral filter on graphs",
    "text": "Spectral filter on graphs\nJust like we can define convolution on images in the frequency domain, we can also define a frequency domain for graphs.\nConsider a network of N nodes, where each node has a feature variable {\\mathbf x}_i \\in \\mathbb{R}. We are interested in the total variation of x between connected nodes:\n\nJ = \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N A_{ij}(x_i - x_j)^2,\n\nwhere A_{ij} is the adjacency matrix of the graph. A small J means that connected nodes have similar x (low variation, low frequency), while a large J means that connected nodes have very different x (high variation, high frequency).\nWe can rewrite J using the graph Laplacian:\n\nJ = \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N A_{ij}(x_i - x_j)^2 = {\\bf x}^\\top {\\bf L} {\\bf x},\n\nwhere {\\bf L} is the Laplacian matrix of the graph given by\n\nL_{ij} = \\begin{cases}\n-1 & \\text{if } i \\text{ and } j \\text{ are connected} \\\\\nk_i & \\text{if } i = j \\\\\n0 & \\text{otherwise}\n\\end{cases}.\n\nand {\\bf x} = [x_1,x_2,\\ldots, x_N]^\\top is a column vector of feature variables.\n\n\n\n\n\n\nDetailed derivation\n\n\n\nThe above derivation shows that the total variation of x between connected nodes is proportional to {\\bf x}^\\top {\\bf L} {\\bf x}.\n\n\\begin{aligned}\nJ &= \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N A_{ij}(x_i - x_j)^2 \\\\\n&= \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N \\underbrace{A_{ij}\\left( x_i^2 +x_j^2\\right)}_{\\text{symmetric}} - \\sum_{i=1}^N\\sum_{j=1}^N A_{ij}x_ix_j \\\\\n&= \\sum_{i=1}^Nx_i^2\\underbrace{\\sum_{j=1}^N A_{ij}}_{\\text{degree of node } i, k_i} - \\sum_{i=1}^N\\sum_{j=1}^N A_{ij}x_ix_j \\\\\n&= \\sum_{i=1}^Nx_i^2 k_i - \\sum_{i=1}^N\\sum_{j=1}^N A_{ij}x_ix_j \\\\\n&= \\underbrace{[x_1,x_2,\\ldots, x_N]}_{{\\bf x}} \\underbrace{\\begin{bmatrix} k_1 & 0 & \\cdots & 0 \\\\ 0 & k_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & k_N \\end{bmatrix}}_{{\\bf D}} \\underbrace{\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N \\end{bmatrix}}_{{\\bf x}} - 2\\underbrace{\\sum_{i=1}^N\\sum_{j=1}^N A_{ij}}_{{\\bf x}^\\top {\\mathbf A} {\\bf x}} {\\bf x} \\\\\n&= {\\bf x}^\\top {\\bf D} {\\bf x} - {\\bf x}^\\top {\\mathbf A} {\\bf x} \\\\\n&= {\\bf x}^\\top {\\bf L} {\\bf x},\n\\end{aligned}\n\n\n\n\nThe spectral decomposition\nLet us showcase the analogy between the Fourier transform and the Laplacian matrix. In the Fourier transform, a signal is decomposed into sinusoidal basis functions. Similarly, for a graph, we can decompose the variation J into eigenvector bases:\n\nJ = \\sum_{i=1}^N \\lambda_i  {\\bf x}^\\top {\\mathbf u}_i {\\mathbf u}_i^\\top {\\bf x} = \\sum_{i=1}^N \\lambda_i  ||{\\bf x}^\\top {\\mathbf u}_i||^2.\n\nwhere {\\mathbf u}_i is the eigenvector corresponding to the eigenvalue \\lambda_i. The term ({\\bf x}^\\top {\\mathbf u}_i) is a dot product between the feature vector {\\bf x} and the eigenvector {\\mathbf u}_i. This measures how much {\\bf x} coheres with eigenvector {\\mathbf u}_i, similar to how Fourier coefficients measure coherency with sinusoids.\nEach ||{\\bf x}^\\top {\\mathbf u}_i||^2 is the strength of {\\bf x} with respect to the eigenvector {\\mathbf u}_i. The total variation J is a weighted sum of these strengths.\nWhat do these eigenvalues mean? Some eigenvectors correspond to low-frequency components, while others correspond to high-frequency components. For example, the total variation J for an eigenvector {\\mathbf u}_i is given by\n\nJ = \\frac{1}{2} \\sum_{j}\\sum_{\\ell} A_{j\\ell}(u_{ij} - u_{i\\ell})^2 = {\\mathbf u}_i^\\top {\\mathbf L} {\\mathbf u}_i = \\lambda_i.\n\nThis equation provides key insight into the meaning of eigenvalues. For an eigenvector {\\mathbf u}_i, its eigenvalue \\lambda_i measures the total variation for {\\mathbf u}_i. Large eigenvalues mean large differences between neighbors (high frequency), while small eigenvalues mean small differences (low frequency).\nThus, if {\\bf x} aligns well with {\\mathbf u}_i with a large \\lambda_i, then {\\bf x} has a strong high-frequency component. If {\\bf x} aligns well with {\\mathbf u}_i with a small \\lambda_i, then {\\bf x} has strong low-frequency component.\n\n\nSpectral Filtering\nEigenvalues \\lambda_i can be thought of as a filter that controls which frequency components pass through. Instead of using the filter associated with the Laplacian matrix, we can design a filter h(\\lambda_i) to control which frequency components pass through. This leads to the idea of spectral filtering.\nTwo common filters are the low-pass and high-pass filters:\n\nLow-pass Filter: h_{\\text{low}}(\\lambda) = \\frac{1}{1 + \\alpha\\lambda} This filter preserves low frequencies (small λ), suppresses high frequencies (large λ), and results in smoother signals.\nHigh-pass Filter: h_{\\text{high}}(\\lambda) = \\frac{\\alpha\\lambda}{1 + \\alpha\\lambda} This filter preserves high frequencies, suppresses low frequencies, and emphasizes differences between neighbors.\n\n\n\n&lt;&gt;:18: SyntaxWarning: invalid escape sequence '\\l'\n&lt;&gt;:19: SyntaxWarning: invalid escape sequence '\\l'\n&lt;&gt;:18: SyntaxWarning: invalid escape sequence '\\l'\n&lt;&gt;:19: SyntaxWarning: invalid escape sequence '\\l'\n/var/folders/j7/9dgqq5g53vnbsbmvh2yqtckr0000gr/T/ipykernel_92395/1521784124.py:18: SyntaxWarning: invalid escape sequence '\\l'\n  fig.text(0.5, 0.01, \"Eigenvalue $\\lambda$\", ha=\"center\")\n/var/folders/j7/9dgqq5g53vnbsbmvh2yqtckr0000gr/T/ipykernel_92395/1521784124.py:19: SyntaxWarning: invalid escape sequence '\\l'\n  axes[0].set_ylabel(\"Filter response $h(\\lambda)$\")"
  },
  {
    "objectID": "m06-graph/from-image-to-graph.html#spectral-graph-convolutional-networks",
    "href": "m06-graph/from-image-to-graph.html#spectral-graph-convolutional-networks",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "Spectral Graph Convolutional Networks",
    "text": "Spectral Graph Convolutional Networks\nA simplest form of learnable spectral filter is given by\n\n{\\bf L}_{\\text{learn}} = \\sum_{k=1}^K \\theta_k {\\mathbf u}_k {\\mathbf u}_k^\\top,\n\nwhere {\\mathbf u}_k are the eigenvectors and \\theta_k are the learnable parameters. The variable K is the number of eigenvectors used (i.e., the rank of the filter). The weight \\theta_k is learned to maximize the performance of the task at hand.\nBuilding on this idea, Bruna et al. (2014) added a nonlinearity to the filter and proposed a spectral convolutional neural network (GCN):\n\n{\\bf x}^{(\\ell+1)} = h\\left( L_{\\text{learn}} {\\bf x}^{(\\ell)}\\right),\n\nwhere h is an activation function, and {\\bf x}^{(\\ell)} is the feature vector of the \\ell-th convolution. They further extend this idea to convolve on multidimensional feature vectors, {\\bf X} \\in \\mathbb{R}^{N \\times f_{\\text{in}}} to produce new feature vectors of different dimensionality, {\\bf X}' \\in \\mathbb{R}^{N \\times f_{\\text{out}}}:\n\n\\begin{aligned}\n{\\bf X}^{(\\ell+1)}_i &= h\\left( \\sum_j L_{\\text{learn}}^{(i,j)} {\\bf X}^{(\\ell)}_j\\right),\\quad \\text{where} \\quad L^{(i,j)}_{\\text{learn}} = \\sum_{k=1}^K \\theta_{k, (i,j)} {\\mathbf u}_k {\\mathbf u}_k^\\top,\n\\end{aligned}\n\nNotice that the learnable filter L_{\\text{learn}}^{(i,j)} is defined for each pair of input i and output j dimensions. Many GCNs are simple when it comes to implementation despite the complicated formula."
  },
  {
    "objectID": "m06-graph/from-image-to-graph.html#from-spectral-to-spatial",
    "href": "m06-graph/from-image-to-graph.html#from-spectral-to-spatial",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "From Spectral to Spatial",
    "text": "From Spectral to Spatial\nSpectral GCNs are mathematically elegant but have two main limitations. First, computing the spectra of the Laplacian is expensive ({\\cal O}(N^3)) and prohibitive for large graphs. Second, the learned filters are not spatially localized. A node can be influenced by all other nodes in the graph.\nThese two limitations motivate the development of spatial GCNs."
  },
  {
    "objectID": "m06-graph/overview.html",
    "href": "m06-graph/overview.html",
    "title": "Module 6: Deep Learning for Graphs",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module extends deep learning from regular grids to irregular structures.\nYou’ll learn:\n\nHow convolution generalizes to graphs by extending CNN principles (locality, parameter sharing, hierarchical features) to irregular structures.\nThe spectral perspective where graph Laplacian eigenvectors serve as frequency basis functions for defining graph convolution.\nThe spatial perspective where convolution becomes neighborhood aggregation through architectures like GCN, GraphSAGE, GAT, and GIN.\nHow to embed graphs into vector spaces using both spectral methods and neural approaches like DeepWalk inspired by word2vec.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Overview"
    ]
  },
  {
    "objectID": "m06-graph/overview.html#the-journey",
    "href": "m06-graph/overview.html#the-journey",
    "title": "Module 6: Deep Learning for Graphs",
    "section": "The Journey",
    "text": "The Journey\nLet’s talk about where this module takes you. You have learned how convolutional neural networks process images, where a kernel slides across a regular grid extracting features through local operations. But networks shatter this regularity. Consider a social network where some people have 5 friends and others have 500, and ask: how do we define convolution when neighborhoods have wildly different sizes?\nThis module answers these questions by extending the principles that make CNNs powerful to irregular graph structures. The journey reveals that convolution is not about grids but about relationships, and relationships exist everywhere.\nPart 1: From Images to Graphs introduces the pixel-node analogy and explains why irregular structure poses challenges. We preview two complementary perspectives: spectral methods that define convolution in frequency domains, and spatial methods that aggregate information from local neighborhoods.\nPart 2: The Spectral Perspective explores how graphs have their own notion of frequency. The graph Laplacian’s eigenvectors serve as basis functions, and eigenvalues indicate how rapidly node features vary across edges. We design spectral filters that control which frequencies pass through, and build learnable spectral graph convolutional networks. We also examine why computational cost and lack of spatial locality motivate spatial approaches.\nPart 3: Spatial Graph Networks defines convolution as neighborhood aggregation. We see how ChebNet bridges spectral and spatial domains, how GCN achieves radical simplification, and how modern architectures like GraphSAGE, GAT, and GIN push boundaries. GraphSAGE samples neighborhoods for scalability. Graph Attention Networks learn which neighbors matter most. Graph Isomorphism Networks maximize discriminative power through careful aggregation design.\nPart 4: Graph Embeddings shifts from supervised learning to representation learning. We explore both spectral embeddings rooted in eigendecomposition and neural embeddings inspired by word2vec. Random walks transform graphs into sequences, enabling us to treat nodes as words and apply language modeling techniques. These embeddings enable clustering, visualization, and transfer learning across tasks.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Overview"
    ]
  },
  {
    "objectID": "m06-graph/overview.html#why-this-matters",
    "href": "m06-graph/overview.html#why-this-matters",
    "title": "Module 6: Deep Learning for Graphs",
    "section": "Why This Matters",
    "text": "Why This Matters\nGraph-structured data appears everywhere: social networks capture friendships and influence, knowledge graphs encode facts and relationships, molecules represent atoms and bonds, citation networks link papers and ideas, and protein interaction networks reveal biological pathways.\nTraditional machine learning struggles with graphs because most algorithms assume feature vectors in Euclidean space, but graphs are discrete, irregular, and high-dimensional. Graph neural networks solve this by learning representations that preserve structure while enabling standard machine learning techniques. The applications are transformative, from drug discovery (predicting molecular properties) to recommendation systems (leveraging social network structure) to fraud detection (identifying suspicious patterns in transaction graphs) and scientific discovery (mining knowledge graphs for hidden connections).",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Overview"
    ]
  },
  {
    "objectID": "m06-graph/overview.html#prerequisites",
    "href": "m06-graph/overview.html#prerequisites",
    "title": "Module 6: Deep Learning for Graphs",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis module assumes you understand basic linear algebra (matrices, eigenvalues, eigenvectors) and neural networks (layers, activations, backpropagation). We build heavily on your knowledge of CNNs from Module 5, drawing parallels between image convolution and graph convolution. The mathematics is more involved than previous modules because spectral graph theory connects linear algebra to networks, and understanding eigendecompositions and Laplacian matrices requires careful attention. But the payoff is worth it: these concepts reveal deep connections between seemingly disparate ideas like Fourier analysis, random walks, and neural networks. If you need to refresh linear algebra concepts, review eigenvalue decomposition and matrix operations before diving deep here, and familiarity with PyTorch or similar frameworks helps for implementing graph neural networks.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Overview"
    ]
  },
  {
    "objectID": "m06-graph/overview.html#what-youll-build",
    "href": "m06-graph/overview.html#what-youll-build",
    "title": "Module 6: Deep Learning for Graphs",
    "section": "What You’ll Build",
    "text": "What You’ll Build\nBy the end of this module, you will implement spectral embeddings using eigendecomposition, train DeepWalk models that learn node representations through random walks, and build graph neural networks using PyTorch Geometric or similar frameworks. The coding examples use real networks like the Karate Club and citation networks, and you’ll visualize learned representations to see how embeddings cluster nodes with similar roles. These hands-on experiences cement abstract concepts and show how theory translates to practice.\nYou’ll understand how to extend deep learning beyond regular structures, see connections between spectral graph theory, signal processing, and neural networks, and appreciate why graph neural networks have become indispensable tools for modern machine learning. Most importantly, you’ll recognize that the core principles of deep learning transcend any particular data structure: locality enables learning from neighbors without considering the entire graph, parameter sharing allows models trained on small networks to generalize to large ones, and hierarchical features extract increasingly abstract patterns through stacked layers. Wherever relationships exist, we can learn.\nThe journey from pixels to nodes begins with Part 1: From Images to Graphs.",
    "crumbs": [
      "Home",
      "Module 6: Deep Learning for Graphs",
      "Overview"
    ]
  },
  {
    "objectID": "m07-representation/02-structuralism.html",
    "href": "m07-representation/02-structuralism.html",
    "title": "Part 2: Meaning as Difference",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module introduces the structuralist view of meaning.\nYou’ll learn:\n\nWhat Saussure’s theory of signs reveals about how language creates meaning through difference, not essence.\nHow Apoha (Buddhist logic of negation) independently discovered that concepts are defined by exclusion.\nWhy Jakobson’s binary features turn phonemes into coordinates in a geometric space.\nHow metaphor and metonymy represent two fundamental modes of cognitive connection.\nThe practical connection between structuralist philosophy and machine learning algorithms like word2vec.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Part 2: Meaning as Difference"
    ]
  },
  {
    "objectID": "m07-representation/02-structuralism.html#the-arbitrary-nature-of-signs",
    "href": "m07-representation/02-structuralism.html#the-arbitrary-nature-of-signs",
    "title": "Part 2: Meaning as Difference",
    "section": "The Arbitrary Nature of Signs",
    "text": "The Arbitrary Nature of Signs\nLet’s start with a simple observation that changes everything. What makes the English word “dog” mean what it means? You might answer that it refers to a four-legged canine animal, but why those particular sounds and not “chien” (French), “perro” (Spanish), or “犬” (Japanese)? The connection between the sound pattern and the concept is arbitrary.\nFerdinand de Saussure, the founder of modern linguistics, called this the arbitrariness of the sign. A sign has two parts: the signifier (the sound or written form) and the signified (the concept), and the relationship between them is not natural or inevitable but a social convention. Saussure went further and argued that the signified (the concept itself) is also arbitrary. We think “dog” refers to a pre-existing natural category, but nature doesn’t draw boundaries between dogs, wolves, and foxes. We do.\nDifferent languages slice the animal kingdom differently. Some languages have multiple words for what English calls “rice” (depending on whether it’s cooked or raw). English distinguishes “river” from “stream” where other languages use one word. The concepts themselves are products of how a language chooses to divide conceptual space.\nWhat does this reveal? The meaning of “dog” isn’t determined by what dogs are. It’s determined by what dogs are not. “Dog” means “dog” because it occupies a specific position in a network of differences: not “cat”, not “wolf”, not “fox”, not “log”, not “fog”.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Part 2: Meaning as Difference"
    ]
  },
  {
    "objectID": "m07-representation/02-structuralism.html#apoha-buddhist-logic-of-negation",
    "href": "m07-representation/02-structuralism.html#apoha-buddhist-logic-of-negation",
    "title": "Part 2: Meaning as Difference",
    "section": "Apoha: Buddhist Logic of Negation",
    "text": "Apoha: Buddhist Logic of Negation\nThis idea that concepts are defined through negation has deep roots. Buddhist philosophers in ancient India developed a theory called Apoha (literally “exclusion”) around the 5th century CE. They argued that we cannot understand what a thing is, only what it is not.\nWhen you see a horse, you don’t directly grasp “horseness”. Instead, you implicitly exclude everything that is not-horse: not-cow, not-stone, not-water, not-tree. The concept of “horse” is nothing more than the region of conceptual space that remains after all these exclusions. It’s a purely negative definition.\n\nDignāga and Dharmakīrti, the primary developers of Apoha theory, influenced both Indian and Tibetan Buddhist philosophy. Their ideas parallel Saussure’s insights developed independently 1400 years later.\nWhy does this matter? Categories are not containers holding essences but regions in a space of possibilities carved out by contrast. You don’t need to know what something is, only what it is not. The meaning is in the boundaries, not the interior.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Part 2: Meaning as Difference"
    ]
  },
  {
    "objectID": "m07-representation/02-structuralism.html#jakobsons-binary-oppositions",
    "href": "m07-representation/02-structuralism.html#jakobsons-binary-oppositions",
    "title": "Part 2: Meaning as Difference",
    "section": "Jakobson’s Binary Oppositions",
    "text": "Jakobson’s Binary Oppositions\nRoman Jakobson, a 20th-century linguist, took the structuralist insight and formalized it. He studied phonemes (the basic sound units of language) and showed they could be decomposed into binary features.\nConsider the difference between “p” and “b”. They’re produced almost identically: both are bilabial stops (you close your lips and release air). The only difference is voicing. Your vocal cords vibrate for “b” but not for “p”.\nWe can represent this as:\n\n“p”: [+bilabial, +stop, -voiced]\n“b”: [+bilabial, +stop, +voiced]\n\nJakobson showed that all phonemes in all languages could be analyzed as bundles of such binary oppositions: voiced/voiceless, nasal/oral, fricative/stop, and so on. A phoneme isn’t a sound. It’s a position in a multidimensional space of contrasts.\n\n\n\n\n\nWhat does this mean practically? Each phoneme is represented not by a symbol but by a coordinate in feature space. Two phonemes are similar if their feature vectors are close. The entire phonological system of a language becomes a geometry problem.\n\nWhy Babies Say “Mama” and “Papa” Everywhere\n\n\n\n\n\n\nFigure 1: “Papa” and “Mama” almost universally refer to “father” and “mother” across languages. Why?\n\n\n\nLet’s make this concrete with one of the most striking examples of phonological universals. Consider how remarkably similar the words for “mother” and “father” are across completely unrelated languages: English “Mama”/“Papa”, Mandarin “妈妈 (Māma)”/“爸爸 (Bàba)”, Swahili “Mama”, Hebrew “Ima”, French “Papa”, Italian “Babbo”, Spanish “Mamá”, Turkish “Baba”. Why do languages separated by continents and millennia converge on these particular sounds? The structuralist answer reveals something profound about how humans acquire language.\nBabies don’t learn sounds in isolation. They learn contrasts. The first contrasts they master involve the most basic physiological oppositions their vocal apparatus can produce.\nThe first opposition is fully closed (m/p) versus fully open (a): the consonants “m” and “p” require complete closure of the lips while the vowel “a” requires maximum opening of the mouth, creating the most extreme articulatory contrast possible and the easiest distinction for infants to produce and perceive. The second opposition is nasal (m) versus oral (p): both involve closing the lips, but “m” lets air flow through the nose (nasal) while “p” blocks it completely and releases it as a burst (oral stop). This creates a minimal phonological system with maximum contrast, where “Mama” combines nasal closure with open vowel (repeated) and “Papa” combines oral closure with open vowel (repeated). The repetition itself is developmentally significant because reduplication is easier for infant motor systems than producing different syllables.\nJakobson argued that language acquisition follows an implicational hierarchy. Children acquire contrasts in a universal order, from maximal to minimal distinctions. Complex sounds appear only after simpler foundations are established.\n\n\nDistinctive Features as DNA\nJakobson’s insight was that you could decompose any phoneme into a bundle of binary features. Consider the phoneme /b/ (as in “baby”): it’s articulated [+labial] (at the lips), the vocal cords vibrate [+voiced], air doesn’t flow through the nose [-nasal], and airflow is stopped by [+stop] (complete closure, then release). The phoneme /b/ isn’t a primitive unit but a coordinate in feature space: [+labial, +voiced, -nasal, +stop, -continuant]. Change one feature and you get a different phoneme: flip [+voiced] to [-voiced] and you get /p/, or flip [+stop] to [-stop, +continuant] and you get /v/.\nJakobson identified roughly twelve distinctive features that are sufficient to describe the phonological systems of all human languages. These features aren’t arbitrary labels but correspond to physiological and acoustic properties of speech production and perception, meaning languages don’t choose their sounds randomly but select from a universal feature space defined by human biology.\n\n\n\n\n\nWhat does this reveal? Phonological systems aren’t just collections of sounds but structured geometries, and languages can’t have random inventories of phonemes. If a language has a complex sound, it must also have the simpler contrasts that build up to it: if it distinguishes three levels of vowel height (high, mid, low), it must first distinguish high from non-high, and if it has voiced fricatives like /z/, it must have voiceless fricatives like /s/. The diversity of human languages doesn’t mean anything is possible but rather that there’s a universal space of phonological possibilities, and each language carves out a specific subset. The structure is in the oppositions, not the sounds themselves.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Part 2: Meaning as Difference"
    ]
  },
  {
    "objectID": "m07-representation/02-structuralism.html#the-culinary-triangle",
    "href": "m07-representation/02-structuralism.html#the-culinary-triangle",
    "title": "Part 2: Meaning as Difference",
    "section": "The Culinary Triangle",
    "text": "The Culinary Triangle\nClaude Lévi-Strauss, the structural anthropologist, extended this approach beyond language to culture itself. He argued that human thought operates through binary oppositions: nature/culture, raw/cooked, male/female, sacred/profane. These aren’t universal truths. They’re structural patterns that organize how different societies make sense of experience.\nHis “culinary triangle” is a famous example. He analyzed how different cultures process food through two axes: the nature-culture axis (raw vs. transformed) and the means of transformation (cooking vs. rotting). This creates a conceptual space where different food preparation methods occupy specific positions.\n\n\n\n\n\nLévi-Strauss developed these ideas in The Raw and the Cooked (1964), the first volume of his four-volume Mythologiques series analyzing the structure of myths across cultures. Roasted meat sits between raw and cooked (less elaborated cooking), boiled meat is fully cooked (more elaborated), and fermented foods sit between raw and rotted (cultural transformation through natural processes). Each food type’s meaning comes from its position in this structural space, not from any intrinsic property. This is the structuralist thesis in full form: meaning is relational, not substantive, systems of meaning are systems of differences, and to understand anything you must map the space of contrasts.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Part 2: Meaning as Difference"
    ]
  },
  {
    "objectID": "m07-representation/02-structuralism.html#metaphor-and-metonymy-two-modes-of-connection",
    "href": "m07-representation/02-structuralism.html#metaphor-and-metonymy-two-modes-of-connection",
    "title": "Part 2: Meaning as Difference",
    "section": "Metaphor and Metonymy: Two Modes of Connection",
    "text": "Metaphor and Metonymy: Two Modes of Connection\nJakobson identified two fundamental ways that concepts connect: through similarity (metaphor) and through contiguity (metonymy). These aren’t just literary devices. They’re cognitive structures that organize how we think and how language operates.\nMetaphor works by similarity: “Juliet is the sun” connects two unlike things through shared properties (brightness, warmth, centrality), letting us understand the abstract through the concrete and the unfamiliar through the familiar. Western philosophy and science tend toward metaphorical thinking, as classification systems, taxonomies, and abstraction hierarchies all rely on grouping similar things. Metonymy works by contiguity (spatial or conceptual adjacency): “The White House announced…” uses a location to refer to the president, and “Hollywood is obsessed with franchises” uses a place to refer to the film industry. Metonymy captures association, co-occurrence, and context, and Eastern philosophy often emphasizes metonymic thinking by understanding things through their relationships and contexts rather than their essential properties.\nJakobson argued that aphasia patients show selective impairment of either metaphoric (similarity-based) or metonymic (contiguity-based) operations, suggesting these are fundamental cognitive mechanisms.\nWhy does this distinction matter for machine learning? Different algorithms capture different types of relationships. Classification algorithms and clustering methods are metaphorical (they group by similarity). But sequence models, language models, and graph neural networks are metonymic (they learn from co-occurrence and context). Understanding which type of relationship you’re trying to capture shapes which tools you should use.\n\nDiscover Your Cognitive Style\nLet’s test which mode of thinking comes more naturally to you. Research suggests that cultural background, language, and experience shape whether we lean toward metaphorical (similarity-based) or metonymic (context-based) reasoning. Take this quick diagnostic.\n\n\n\n\n\n\nTry it yourself\n\n\n\nFor each scenario below, choose the interpretation (A or B) that feels most natural to you. There are no right or wrong answers. This reveals something about how your mind organizes meaning.\nQuestion 1: Flowers\nYou see these images. Which grouping feels more natural?\n\n\nA: Group by similar appearance (round petals vs. pointed petals)\nB: Group by mixed features (both types together)\n\nQuestion 2: Animals and Objects\nWhich pair feels more closely related?\n\n\nA: Monkey and banana (contextual association)\nB: Bear and monkey (categorical similarity)\n\nQuestion 3: The Floating Balloon\nWatch this short scene. Why is the balloon moving?\n\n\nA: The wind is carrying it (external context)\nB: Air is leaking from the balloon (internal property)\n\nInterpretation:\nIf you chose mostly A answers, you lean toward metaphorical thinking. You organize the world through categories, similarities, and essential properties. This aligns with what cognitive scientists call “analytic” reasoning, more common in Western philosophical traditions.\nIf you chose mostly B answers (or A for question 2, B for question 3), you lean toward metonymic thinking. You understand things through context, relationships, and associations. This aligns with “holistic” reasoning, emphasized in East Asian cognitive traditions.\nResearch by Richard Nisbett and colleagues found that East Asians are more likely to give contextual explanations (the wind, the relationship between monkey and banana), while Westerners focus on intrinsic properties and categorical groupings. These aren’t rigid categories. They’re tendencies shaped by language, culture, and what types of relationships your environment makes salient.\n\n\nThis isn’t just a personality quiz. It reveals something fundamental about representation. If your mind naturally groups by similarity, you’re building a metaphorical semantic space. If you naturally think through context, you’re constructing a metonymic associative network.\nMachine learning models do the same thing. Word2vec and modern transformers learn both structures simultaneously, because natural language requires both modes of connection.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Part 2: Meaning as Difference"
    ]
  },
  {
    "objectID": "m07-representation/02-structuralism.html#from-philosophy-to-algorithm",
    "href": "m07-representation/02-structuralism.html#from-philosophy-to-algorithm",
    "title": "Part 2: Meaning as Difference",
    "section": "From Philosophy to Algorithm",
    "text": "From Philosophy to Algorithm\nWe’ve traced a philosophical thread through linguistics, Buddhist logic, structural anthropology, and cognitive science, and the insight is consistent: meaning is not intrinsic but emerges from patterns of difference, opposition, and relationship. This sounds abstract, but it becomes concrete when you try to teach a machine what words mean. You cannot program in definitions because dictionaries are circular (look up “large” and you find “big”; look up “big” and you find “large”), so instead you need to let the machine discover the structure of language by observing how words relate to each other.\nHow does word2vec do this? It doesn’t learn what “dog” means by reading a definition but by observing which words appear near “dog” in actual text (“bark”, “pet”, “leash”, “puppy”) and which don’t (“meow”, “aquarium”, “carburetor”). The meaning of “dog” is implicitly defined through this pattern of co-occurrence and exclusion. Word2vec operationalizes Saussure’s insight that meaning is differential, implements Apoha’s theory that concepts are defined by exclusion, builds Jakobson’s feature space where similarity is geometric distance, and captures both metaphoric (similarity-based) and metonymic (context-based) relationships.\nWhat comes next? The next section shows how this works mechanically. How do you turn a philosophical theory about the nature of meaning into working code? How do you represent the continuous space of semantic relationships without imposing arbitrary boundaries? The answer involves vector embeddings, contrastive learning, and a mathematical framework that makes structuralism computable.\n\n\n\n\n\n\nTry it yourself\n\n\n\nPick a concept you use often (democracy, friendship, justice, art). Try to define it without using synonyms or related terms. You’ll find it’s almost impossible. Now try defining it negatively, through what it is not. Which approach feels more precise? This exercise reveals why structuralism works.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Part 2: Meaning as Difference"
    ]
  },
  {
    "objectID": "m07-representation/04-universal.html",
    "href": "m07-representation/04-universal.html",
    "title": "Part 4: The Universal Geometry of Embeddings",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module explores a surprising empirical discovery about learned representations across different models.\nYou’ll learn:\n\nWhat the Strong Platonic Representation Hypothesis claims and why all sufficiently large embedding models may learn nearly identical geometric structures.\nHow to discover universal latent structure hidden within fundamentally incomparable embedding spaces.\nThe technique of unsupervised translation (vec2vec) that aligns embedding spaces without paired data using cycle consistency.\nPractical consequences for representation learning and the security of vector databases.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Part 4: Universal Representations"
    ]
  },
  {
    "objectID": "m07-representation/04-universal.html#the-puzzle-of-incomparable-spaces",
    "href": "m07-representation/04-universal.html#the-puzzle-of-incomparable-spaces",
    "title": "Part 4: The Universal Geometry of Embeddings",
    "section": "The Puzzle of Incomparable Spaces",
    "text": "The Puzzle of Incomparable Spaces\nLet’s start with a practical question that reveals something profound. Suppose you have two different text embedding models, each trained independently on different datasets, using different architectures. Model A might be based on BERT (bidirectional transformer) while Model B might use T5 (encoder-decoder architecture), with different numbers of parameters, different training corpora (Wikipedia versus web crawl data), and different embedding dimensions (768-d versus 1024-d).\nAre these two models learning the same thing? You might expect the answer to be “no, obviously not” given their different architectures, training data, and optimization procedures.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n\nnp.random.seed(42)\n\n# Model A: BERT-style embeddings\nn_points = 50\nclusters_A = {\n    'Technology': np.random.randn(n_points, 2) * 0.5 + np.array([-2, 2]),\n    'Nature': np.random.randn(n_points, 2) * 0.5 + np.array([2, 2]),\n    'Politics': np.random.randn(n_points, 2) * 0.5 + np.array([-2, -2]),\n    'Sports': np.random.randn(n_points, 2) * 0.5 + np.array([2, -2]),\n}\n\ncolors = {'Technology': '#e74c3c', 'Nature': '#27ae60',\n          'Politics': '#3498db', 'Sports': '#f39c12'}\n\nfor topic, points in clusters_A.items():\n    ax1.scatter(points[:, 0], points[:, 1], s=80, c=colors[topic],\n                alpha=0.6, edgecolors='black', linewidth=0.5, label=topic)\n\nax1.set_xlim(-4, 4)\nax1.set_ylim(-4, 4)\nax1.set_xlabel('Dimension 1', fontsize=12, fontweight='bold')\nax1.set_ylabel('Dimension 2', fontsize=12, fontweight='bold')\nax1.set_title('Model A: BERT-based (GTR)', fontsize=14, fontweight='bold')\nax1.legend(loc='upper left', fontsize=10)\nax1.grid(alpha=0.3, linestyle='--')\nax1.set_aspect('equal')\n\n# Model B: T5-style embeddings (rotated and scaled differently)\nrotation = np.array([[np.cos(0.7), -np.sin(0.7)],\n                     [np.sin(0.7), np.cos(0.7)]])\nscale = 1.5\n\nclusters_B = {}\nfor topic, points in clusters_A.items():\n    clusters_B[topic] = (points @ rotation.T) * scale\n\nfor topic, points in clusters_B.items():\n    ax2.scatter(points[:, 0], points[:, 1], s=80, c=colors[topic],\n                alpha=0.6, edgecolors='black', linewidth=0.5, label=topic)\n\nax2.set_xlim(-6, 6)\nax2.set_ylim(-6, 6)\nax2.set_xlabel('Dimension 1', fontsize=12, fontweight='bold')\nax2.set_ylabel('Dimension 2', fontsize=12, fontweight='bold')\nax2.set_title('Model B: T5-based (GTE)', fontsize=14, fontweight='bold')\nax2.legend(loc='upper left', fontsize=10)\nax2.grid(alpha=0.3, linestyle='--')\nax2.set_aspect('equal')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Two models trained independently produce fundamentally incomparable embedding spaces.\n\n\n\n\n\nWhat do these two spaces have in common? At first glance, nothing. The coordinates are different, the distances are different, and computing the cosine similarity between two words in Model A versus Model B yields completely different numbers. The spaces appear fundamentally incomparable.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Part 4: Universal Representations"
    ]
  },
  {
    "objectID": "m07-representation/04-universal.html#the-platonic-representation-hypothesis",
    "href": "m07-representation/04-universal.html#the-platonic-representation-hypothesis",
    "title": "Part 4: The Universal Geometry of Embeddings",
    "section": "The Platonic Representation Hypothesis",
    "text": "The Platonic Representation Hypothesis\nYet recent research suggests something remarkable. Despite these surface differences, the two models may have learned essentially the same underlying structure.\nThis conjecture is called the Platonic Representation Hypothesis. The hypothesis claims that all models of sufficient capacity, when trained on rich enough data, converge to the same latent representation of reality. The original hypothesis was introduced by Huh et al. (2024) in the context of vision models (Huh et al. 2024), who observed that different CNN architectures trained on ImageNet produced internal representations that aligned surprisingly well, discovering the same features (edges, textures, object parts) in the same hierarchical order without explicit programming.\nWhy would this happen? One explanation is that the structure isn’t in the model but in the world itself.\nVisual recognition requires detecting edges (invariant properties of physical objects) and composing local features into global patterns (because objects have hierarchical part-whole structure). The optimal representation for vision isn’t arbitrary but constrained by the statistical structure of natural images. If you train many different models on the same underlying reality, they should all discover the same optimal representation. Think of the architecture and training procedure as different paths up a mountain that take different routes but converge toward the same peak.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Part 4: Universal Representations"
    ]
  },
  {
    "objectID": "m07-representation/04-universal.html#the-strong-version-text-can-be-translated",
    "href": "m07-representation/04-universal.html#the-strong-version-text-can-be-translated",
    "title": "Part 4: The Universal Geometry of Embeddings",
    "section": "The Strong Version: Text Can Be Translated",
    "text": "The Strong Version: Text Can Be Translated\n\nJha et al. (2024) proposed a stronger, constructive version of this hypothesis for text embeddings. They introduced vec2vec, an unsupervised method for translating between embedding spaces (see project page).\nStrong Platonic Representation Hypothesis: The universal latent structure of text representations not only exists, but can be learned and harnessed to translate representations from one space to another without any paired data or encoders.\nThis is a bold claim. It says you can take embeddings from Model A (which you don’t have access to), and translate them into the space of Model B, without ever seeing the same text encoded by both models. You don’t need parallel corpora. You don’t need paired examples. You only need unpaired samples from each space.\nHow is this possible? If the underlying semantic structure is truly universal, then the two embedding spaces are just different coordinate systems describing the same geometry. Think of Model A and Model B as two people describing the same city using different maps, one using latitude-longitude and the other using distance from landmarks. The coordinate systems differ, but the city’s structure (which streets connect, which neighborhoods are close) remains invariant.\nTranslation becomes a geometric alignment problem where you discover the rotation, scaling, and transformation that maps one coordinate system onto the other while preserving distances and relationships. The key insight is that semantic relationships (the parallel arrows representing analogies like man:king :: woman:queen) remain intact across all spaces. The coordinates change, but the geometry persists.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Part 4: Universal Representations"
    ]
  },
  {
    "objectID": "m07-representation/04-universal.html#how-vec2vec-works",
    "href": "m07-representation/04-universal.html#how-vec2vec-works",
    "title": "Part 4: The Universal Geometry of Embeddings",
    "section": "How Vec2Vec Works",
    "text": "How Vec2Vec Works\n\nThe vec2vec method learns this translation through an adversarial game similar to CycleGAN (a technique from computer vision for image-to-image translation without paired examples).\nThe setup involves two generators and two discriminators. Generator G_{A \\to B} attempts to translate embeddings from Model A’s space into Model B’s space. Generator G_{B \\to A} does the reverse. Each generator is paired with a discriminator that tries to distinguish real embeddings from translated ones.\n\nThree Key Losses\nThe training objective combines three losses. First, the adversarial loss encourages the generator to produce embeddings that look indistinguishable from real embeddings in the target space. If G_{A \\to B} transforms an embedding from Model A, the discriminator D_B should not be able to tell it apart from genuine Model B embeddings.\nSecond, the cycle consistency loss ensures that if you translate from Space A to Space B and back to Space A, you should recover the original point. Mathematically, G_{B \\to A}(G_{A \\to B}(x)) \\approx x. This prevents the model from learning arbitrary transformations that destroy information.\nThird, the identity loss encourages the translation to preserve structure when no translation is needed. If you feed an embedding that already belongs to the target space, the generator should leave it mostly unchanged.\n\n\nWhy Does This Work?\nThe cycle consistency constraint forces the model to preserve the intrinsic geometry of the space. You cannot map every point to the same location (that would minimize adversarial loss but violate cycle consistency) or scramble relationships arbitrarily (that would break the round-trip property). The only way to satisfy all constraints simultaneously is to discover the true structural alignment between spaces.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Part 4: Universal Representations"
    ]
  },
  {
    "objectID": "m07-representation/04-universal.html#empirical-evidence",
    "href": "m07-representation/04-universal.html#empirical-evidence",
    "title": "Part 4: The Universal Geometry of Embeddings",
    "section": "Empirical Evidence",
    "text": "Empirical Evidence\nJha et al. tested this by training vec2vec on multiple pairs of embedding models. They used models with different architectures (BERT-based versus T5-based), different training datasets, and different embedding dimensions.\nThe results were striking. After training without any paired examples, vec2vec could translate embeddings between spaces while preserving their geometry. The cosine similarity between translated embeddings and ideal target embeddings reached above 0.9 in many cases.\n\nWhat does this convergence tell us? Different embedding models, despite appearing incomparable on the surface, encode nearly identical semantic structures. The universal geometry exists, can be discovered, and can be exploited.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Part 4: Universal Representations"
    ]
  },
  {
    "objectID": "m07-representation/04-universal.html#generalization-to-unseen-data",
    "href": "m07-representation/04-universal.html#generalization-to-unseen-data",
    "title": "Part 4: The Universal Geometry of Embeddings",
    "section": "Generalization to Unseen Data",
    "text": "Generalization to Unseen Data\n\nPerhaps the most surprising result is how well vec2vec generalizes. Train the translation model on embeddings from one dataset (say, Wikipedia articles), then test it on a completely different dataset (say, biomedical abstracts or code documentation), and the translation quality remains high. The model learned something general about the relationship between the two embedding spaces, not something specific to the training distribution.\nWhy does this work? If the Platonic Representation Hypothesis is correct, then the universal structure isn’t tied to any particular domain but is a property of language itself (or more generally, of the data modality). Medical texts and Wikipedia articles may discuss different topics, but they use the same semantic relationships (hierarchies, analogies, contrasts, and associations) that follow the same geometric patterns regardless of domain. The translation captures this invariant structure, learning the transformation that aligns coordinate systems while preserving the universal geometry underneath rather than memorizing domain-specific content.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Part 4: Universal Representations"
    ]
  },
  {
    "objectID": "m07-representation/04-universal.html#what-does-this-mean",
    "href": "m07-representation/04-universal.html#what-does-this-mean",
    "title": "Part 4: The Universal Geometry of Embeddings",
    "section": "What Does This Mean?",
    "text": "What Does This Mean?\nLet’s step back and consider what these results imply. The Strong Platonic Representation Hypothesis, if validated more broadly, suggests that the space of possible learned representations is far more constrained than we thought. Different models are not exploring a vast landscape of alternative ways to represent meaning but converging to a narrow region, a canonical structure dictated by the statistical properties of language and the world it describes.\n\nThe Beautiful Side\nThis has both beautiful and troubling consequences. On the beautiful side, it means representation learning is discovering something real: word embeddings are not arbitrary constructs but reflect the intrinsic geometry of semantic relationships. When multiple models trained independently learn similar structures, it suggests those structures are inevitable features of meaning itself, not artifacts of a particular algorithm.\n\n\nThe Philosophical Question\nIt also raises deeper questions about what machine learning is doing. Are we building models that impose structure on data, or are we building telescopes that reveal pre-existing structure? If all sufficiently powerful models converge to the same representation, does that mean the representation was “already there” in some Platonic sense, waiting to be discovered?\n\n\n\n\n\n\nFurther Reading\n\n\n\nJha, R., Zhang, C., Shmatikov, V., & Morris, J. X. (2024). Harnessing the Universal Geometry of Embeddings. Project page\nHuh, M., Cheung, B., Wang, T., & Isola, P. (2024). The Platonic Representation Hypothesis. arXiv:2405.07987\nMikolov, T., Yih, W., & Zweig, G. (2013). Linguistic Regularities in Continuous Space Word Representations. NAACL-HLT.",
    "crumbs": [
      "Home",
      "Module 7: Representations & Structuralism",
      "Part 4: Universal Representations"
    ]
  },
  {
    "objectID": "course/about.html",
    "href": "course/about.html",
    "title": "About Us",
    "section": "",
    "text": "Complex systems are everywhere—from social networks shaping our daily interactions to neural networks powering AI systems. This course explores computational intelligence from the ground up, combining hands-on deep learning with complexity science to unlock the secrets of emergent behaviors in large-scale systems.",
    "crumbs": [
      "Home",
      "Course Information",
      "About Us"
    ]
  },
  {
    "objectID": "course/about.html#about-us",
    "href": "course/about.html#about-us",
    "title": "About Us",
    "section": "About Us",
    "text": "About Us\n\nInstructor\n\n\nWelcome! My name is Sadamori Kojaku, and I am the instructor of this course. The field of applied soft computing, which in part intersects with deep learning, is a very fast-paced area. The field is so fast-paced that I always feel like I’m not “teaching” students. Instead, I’m more like a student myself walking through the same journey with you.\n\n\n\n\n\n\nThis is my third offering of this course, and everytime I have to look through the latest research and update the lecture notes to an extent beyond a simple minor text update. This time, I have included two new modules, i.e., agentic coding and representation learning, with former module designed to help students make use of advanced AI tools for research and for personal projects, and the later for philosophical and theoretical discussions on what ML/AI actually learn from data. All other modules are substantially restructured to align with the key theme: how can we make use of advanced AI tools for my research?, with a target audience being Systems Science students and Engineering students interested in applied computing.I have implemented several tricks to make the course fun and engaging, like Discord chatbot “Chibi”, mini projects, and pen and paper exercises. I hope you will enjoy and find the course useful in your future endeavors.\nTA\nTeaching Assistant is not yet assigned.\n\nAI Tutor (Chibi)\n\n\nChibi is an AI-powered Discord bot designed to help you master the course material through natural conversation and interactive exercises. It is integrated with the course’s lecture notes and can provide accurate, context-aware responses to your questions.\n\n\n\n\nWith Chibi, you can:\n\nQuery Course Content: Ask questions about specific lectures or concepts directly on Discord.\nTest Your Knowledge: Generate personalized quizzes for any module using the /quiz command.\nChallenge the AI: Engage in “LLM-Quizzes” where you attempt to create questions that the AI cannot answer correctly.\nMonitor Mastery: Track your learning progress and identify areas for improvement with the /status command.",
    "crumbs": [
      "Home",
      "Course Information",
      "About Us"
    ]
  },
  {
    "objectID": "course/discord.html",
    "href": "course/discord.html",
    "title": "Discord",
    "section": "",
    "text": "We use a dedicated Discord server for this course to facilitate communication, Q&A, and collaboration outside of class. The Discord server is a space where you can:\n\nAsk questions about lectures, assignments, and projects\nDiscuss concepts and share resources with your peers\nGet support from the instructor, TA, and Chibi (the AI tutor)\nRecord your class attendance\nComplete mandatory module quizzes with the AI Tutor\nJoin study groups and participate in informal discussions\n\nInvitation links to the Discord server will be distributed via Brightspace. Please check the Brightspace announcements or course materials for the latest invite link.\nOnce you join, you’ll find channels for different topics (e.g., #random, #questions, #study-groups) and can interact with both classmates, AI tutor, and course staff. If you’re new to Discord, it’s a free platform available on web, desktop, and mobile.\n\nAI Tutor & Assignments\nThis course heavily utilizes the AI Tutor (Chibi) for:\n\nAttendance Tracking: You must use the /here command during class.\nAssignments: Every module requires completing quizzes with the bot.\n\nFor detailed instructions, please read the Discord Bot Usage guide.\n\n\n\nScreenshot of the course Discord server\n\n\nExample screenshot of the course Discord server interface.\nIf you have any trouble joining, please contact the instructor for assistance.",
    "crumbs": [
      "Home",
      "Course Information",
      "Discord"
    ]
  },
  {
    "objectID": "course/welcome.html",
    "href": "course/welcome.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome to the course! In this class, we will explore how deep learning serves as a powerful tool to operationalize high-dimensional data from social and physical systems. You’ll learn to model complex system dynamics using modern computational intelligence, and in turn, use perspectives from complexity science to understand the emergent behaviors of large-scale models.\nThis course is designed to provide you with both a theoretical foundation and hands-on experience in applied soft computing. You will learn how to apply representation learning, sequence modeling, and graph analytics to model real-world complex systems using Python and modern deep learning frameworks.\n\n\nThis course is divided into three chapters: Foundation, Deep Learning and Advanced Topics.\nFoundation chapter covers the foundational concepts of data visualization, data science, and reproducibility. This will prepare you for building your own data science projects with modern deep learning tools.\nThe Deep Learning chapter covers the fundamental concepts of deep learning for text, images, and graphs. Through hands-on coding, you will learn how to build your own deep learning models for different data types.\nThe Advanced Topics chapter elevates you from a user to a creator of advanced soft computing models. You will learn how to build your own large language models and self-supervised learning models.\n\n\n\n\nEngaging Lectures: Each week, we’ll dive into key concepts in deep learning and complex systems, supported by interactive discussions and in-class activities.\nHands-on Coding: You’ll work with real data from text, images, and networks using Python and modern deep learning tools.\nCollaborative Learning: Participate in group activities, discussions, and our dedicated Discord server for Q&A and support.\nProjects and Assignments: Apply your knowledge through coding assignments, quizzes, and a final project.\nSupport: Get help from the instructor, TA, and our AI tutor.\n\n\n\n\n\nWhy applied soft computing? Read the Overview page to understand the importance of applied soft computing.\nRead the About Us page to meet your instructor, TA, and AI tutor.\nJoin the Discord server for announcements, help, and community.\nSet up your Python environment by following the Setup Guide.\nLearn how to submit assignments using GitHub Classroom.",
    "crumbs": [
      "Home",
      "Course Information",
      "Welcome"
    ]
  },
  {
    "objectID": "course/welcome.html#welcome-to-applied-soft-computing",
    "href": "course/welcome.html#welcome-to-applied-soft-computing",
    "title": "Welcome",
    "section": "",
    "text": "Welcome to the course! In this class, we will explore how deep learning serves as a powerful tool to operationalize high-dimensional data from social and physical systems. You’ll learn to model complex system dynamics using modern computational intelligence, and in turn, use perspectives from complexity science to understand the emergent behaviors of large-scale models.\nThis course is designed to provide you with both a theoretical foundation and hands-on experience in applied soft computing. You will learn how to apply representation learning, sequence modeling, and graph analytics to model real-world complex systems using Python and modern deep learning frameworks.\n\n\nThis course is divided into three chapters: Foundation, Deep Learning and Advanced Topics.\nFoundation chapter covers the foundational concepts of data visualization, data science, and reproducibility. This will prepare you for building your own data science projects with modern deep learning tools.\nThe Deep Learning chapter covers the fundamental concepts of deep learning for text, images, and graphs. Through hands-on coding, you will learn how to build your own deep learning models for different data types.\nThe Advanced Topics chapter elevates you from a user to a creator of advanced soft computing models. You will learn how to build your own large language models and self-supervised learning models.\n\n\n\n\nEngaging Lectures: Each week, we’ll dive into key concepts in deep learning and complex systems, supported by interactive discussions and in-class activities.\nHands-on Coding: You’ll work with real data from text, images, and networks using Python and modern deep learning tools.\nCollaborative Learning: Participate in group activities, discussions, and our dedicated Discord server for Q&A and support.\nProjects and Assignments: Apply your knowledge through coding assignments, quizzes, and a final project.\nSupport: Get help from the instructor, TA, and our AI tutor.\n\n\n\n\n\nWhy applied soft computing? Read the Overview page to understand the importance of applied soft computing.\nRead the About Us page to meet your instructor, TA, and AI tutor.\nJoin the Discord server for announcements, help, and community.\nSet up your Python environment by following the Setup Guide.\nLearn how to submit assignments using GitHub Classroom.",
    "crumbs": [
      "Home",
      "Course Information",
      "Welcome"
    ]
  },
  {
    "objectID": "m04-text/what-to-learn.html",
    "href": "m04-text/what-to-learn.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "In this module, we will learn about recurrent neural networks (RNNs) that can process sequential text data. We will learn: - Recurrent Neural Networks (RNNs) for processing sequential text data - Long-Short Term Memory (LSTM) networks for capturing long-range dependencies - Sequence-to-sequence models for machine translation - The Attention mechanism for focusing on relevant parts of text"
  },
  {
    "objectID": "m04-text/what-to-learn.html#what-to-learn-in-this-module",
    "href": "m04-text/what-to-learn.html#what-to-learn-in-this-module",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "In this module, we will learn about recurrent neural networks (RNNs) that can process sequential text data. We will learn: - Recurrent Neural Networks (RNNs) for processing sequential text data - Long-Short Term Memory (LSTM) networks for capturing long-range dependencies - Sequence-to-sequence models for machine translation - The Attention mechanism for focusing on relevant parts of text"
  },
  {
    "objectID": "m05-images/pen-and-paper.html",
    "href": "m05-images/pen-and-paper.html",
    "title": "Applied Soft Computing: Modeling Complex Systems with Deep Learning",
    "section": "",
    "text": "Pen and paper exercises\n\n✍️ Pen and paper exercises"
  },
  {
    "objectID": "course/discord-bot-usage.html#getting-started-with-minidora",
    "href": "course/discord-bot-usage.html#getting-started-with-minidora",
    "title": "Using Minidora",
    "section": "Getting Started with Minidora",
    "text": "Getting Started with Minidora\nMinidora is your personal AI tutor available 24/7 through Discord to help you master deep learning and complex systems concepts. She’s designed to provide personalized learning support, answer questions about course materials, and guide you through challenging topics with patience and clarity. To interact with Minidora, simply use Discord slash commands or mention her directly in any channel or thread where she’s present.\nCheck out the instruction here on how to use Minidora: Minidora Usage. Minidora is available on Discord, and you can find the invitation link on the email sent in the first week of the semester. Or you can find the invitation link on the Brightspace.\n\n\n\n\n\n\nNote\n\n\n\nSome students could not find Minidora on Discord. The easiest way to get around this is:\n\nGo to the course discord server\nOpen the “applied-soft-computing” channel.\nClick the Minidora icon and send a direct message\nType “/” and see if the available commands are shown.",
    "crumbs": [
      "Home",
      "Course Information",
      "Using Minidora"
    ]
  },
  {
    "objectID": "course/discord-bot-usage.html#asking-questions",
    "href": "course/discord-bot-usage.html#asking-questions",
    "title": "Discord Bot Usage",
    "section": "Asking Questions",
    "text": "Asking Questions\nBeyond assignments and attendance, Chibi is here to help you learn. It uses the course lecture notes as its knowledge base, meaning it can provide accurate, context-aware answers to your questions.\nYou can ask straightforward questions like “@Chibi how does backpropagation work?” or request comparisons such as “@Chibi explain the difference between CNN and RNN.” Do not hesitate to use this resource whenever you feel stuck or curious.",
    "crumbs": [
      "Home",
      "Course Information",
      "Discord Bot Usage"
    ]
  },
  {
    "objectID": "course/discord-bot-usage.html#natural-conversations-and-interactive-learning",
    "href": "course/discord-bot-usage.html#natural-conversations-and-interactive-learning",
    "title": "Using Minidora",
    "section": "Natural Conversations and Interactive Learning",
    "text": "Natural Conversations and Interactive Learning\nFor a more conversational experience, use the /chat command which allows you to interact with Minidora in a natural, free-flowing manner. You can say things like /chat I'm confused about transformers, can you explain the attention mechanism step by step? or /chat Can you help me debug this Python code for training a CNN? Minidora will engage in back-and-forth dialogue, ask clarifying questions, and adapt her explanations based on your responses.\nNote that /chat does not contextualize the Minidora to the course materials. That means that it does not read the lecture content and interact with the students with its built-in knowledge.",
    "crumbs": [
      "Home",
      "Course Information",
      "Using Minidora"
    ]
  },
  {
    "objectID": "course/discord-bot-usage.html#quizzes-and-assessment",
    "href": "course/discord-bot-usage.html#quizzes-and-assessment",
    "title": "Using Minidora",
    "section": "Quizzes and Assessment",
    "text": "Quizzes and Assessment\nTo test your understanding and reinforce learning, Minidora offers intelligent quiz features through the /quiz command. She can generate concept-based questions using /concept-quiz m03 multiple-choice for theoretical understanding, or coding challenges with /code-quiz m03 to practice implementation skills. Minidora tracks your progress and adapts quiz difficulty based on your performance, focusing on areas where you need more practice. You can also request quizzes on specific topics by adding subject keywords, such as /quiz m04 convolutional neural networks.",
    "crumbs": [
      "Home",
      "Course Information",
      "Using Minidora"
    ]
  },
  {
    "objectID": "course/discord-bot-usage.html#tracking-your-progress",
    "href": "course/discord-bot-usage.html#tracking-your-progress",
    "title": "Using Minidora",
    "section": "Tracking Your Progress",
    "text": "Tracking Your Progress\nUse the /status command to monitor your learning journey and see detailed insights about your progress. Minidora provides different status views: /status summary gives you a quick overview of questions asked and concepts mastered, while /status concepts shows which topics you’ve learned and what to study next. The /status profile command reveals your personalized learning profile, including your preferred difficulty level, learning style, and areas where you excel or need additional support. This helps Minidora provide increasingly personalized assistance as you continue learning.",
    "crumbs": [
      "Home",
      "Course Information",
      "Using Minidora"
    ]
  },
  {
    "objectID": "course/discord-bot-usage.html",
    "href": "course/discord-bot-usage.html",
    "title": "Discord Bot Usage",
    "section": "",
    "text": "What you’ll learn in this guide\n\n\n\nThis guide introduces Chibi, our AI-powered course tutor. We will explore how to set up your account for Attendance, understand the workflow for mandatory Module Assignments, and learn how to leverage the bot for personalized Q&A.",
    "crumbs": [
      "Home",
      "Course Information",
      "Discord Bot Usage"
    ]
  },
  {
    "objectID": "course/discord-bot-usage.html#getting-started",
    "href": "course/discord-bot-usage.html#getting-started",
    "title": "Discord Bot Usage",
    "section": "1. Getting Started",
    "text": "1. Getting Started\nThe bot is available 24/7 on the course Discord server. You can interact with it by depending on the channel configuration:\n\nDirect messaging the bot\nUsing Slash Commands (e.g., /quiz, /status)\n\n\nRegistration\nTo ensure your progress and attendance are recorded correctly in the gradebook, you must link your Discord account to your student ID.\nCommand:\n/register &lt;student_id&gt; [name]\nExample: /register 12345678 John Doe",
    "crumbs": [
      "Home",
      "Course Information",
      "Discord Bot Usage"
    ]
  },
  {
    "objectID": "course/discord-bot-usage.html#attendance",
    "href": "course/discord-bot-usage.html#attendance",
    "title": "Discord Bot Usage",
    "section": "Attendance",
    "text": "Attendance\nWe take attendance digitally during every class session. Efficiency is key here, so we utilize a rotating code system.\nDuring the lecture, the instructor will display a 4-digit code on the projector. Your task is to submit this code in the #attendance channel using the /here command.\n/here &lt;code&gt;\nSpeed matters slightly. The code rotates every 15 seconds to ensure that only students physically present in the room can submit it. If you see code 4829 on the screen, simply type /here 4829 immediately.",
    "crumbs": [
      "Home",
      "Course Information",
      "Discord Bot Usage"
    ]
  },
  {
    "objectID": "course/discord-bot-usage.html#weekly-assignments",
    "href": "course/discord-bot-usage.html#weekly-assignments",
    "title": "Discord Bot Usage",
    "section": "Weekly Assignments",
    "text": "Weekly Assignments\nDeep learning requires practice. For every module in this course, you are required to complete two specific types of activities with Chibi.\n\nConcept Quiz\nFirst, you must demonstrate your understanding of the core material. The Concept Quiz is designed to test your grasp of the module’s theory. When you initiate this quiz, the bot will present you with questions which you must answer.\n/quiz [module_id]\nTyping /quiz m01 starts the quiz for Module 1. As you answer, Chibi actively evaluates your responses and updates your mastery level for each specific concept. This provides you with immediate feedback on what you know and where you might need to study more.\n\n\nLLM Quiz Challenge\nSecond, we flip the script. In the LLM Quiz Challenge, you become the examiner. Your goal is to stump the AI. You need to create a tricky, insightful question about the module’s content that the AI returns an incorrect answer for, while you provide the correct one.\n/llm-quiz &lt;module_id&gt;\nWhen you run /llm-quiz m01, you enter the arena. You submit your question and your correct answer. Chibi then attempts to answer your question. An independent Evaluator AI judges both responses. You win the challenge only if your answer is deemed correct and Chibi’s answer is incorrect. Typically, you need to achieve 3 wins per module to satisfy this assignment requirement. It forces you to think deeply about the edge cases and subtleties of the material.",
    "crumbs": [
      "Home",
      "Course Information",
      "Discord Bot Usage"
    ]
  },
  {
    "objectID": "course/discord-bot-usage.html#tracking-progress",
    "href": "course/discord-bot-usage.html#tracking-progress",
    "title": "Discord Bot Usage",
    "section": "Tracking Progress",
    "text": "Tracking Progress\nYou are never flying blind in this course. You can check your learning progress, current mastery levels, and quiz results at any moment.\n/status [module_id]\nUse /status to see an overall summary of your performance, or verify your progress on a specific module by typing /status m01.",
    "crumbs": [
      "Home",
      "Course Information",
      "Discord Bot Usage"
    ]
  },
  {
    "objectID": "course/discord-bot-usage.html#meet-chibi-your-ai-tutor",
    "href": "course/discord-bot-usage.html#meet-chibi-your-ai-tutor",
    "title": "Discord Bot Usage",
    "section": "Meet Chibi, Your AI Tutor",
    "text": "Meet Chibi, Your AI Tutor\nThe course utilizes an AI-powered Discord bot named Chibi to assist with your learning journey. Think of Chibi not just as a tool, but as a personalized tutor available 24/7. It helps you track your progress, manages your classroom attendance, and most importantly, serves as the platform for your Module assignments. You can interact with Chibi through direct messages or by using Slash Commands like /quiz and /status directly in the server.\n\nGetting Registered\nThe very first step is to link your Discord account to your student ID. This is critical because it ensures your attendance records and assignment grades are accurately synced with the official gradebook.\nYou will use the /register command followed by your student ID and name.\n/register &lt;student_id&gt; [name]\nFor instance, you might type /register 12345678 John Doe. Once you do this, you are ready to participate in the course activities.",
    "crumbs": [
      "Home",
      "Course Information",
      "Discord Bot Usage"
    ]
  },
  {
    "objectID": "course/discord-bot-usage.html#module-assignments",
    "href": "course/discord-bot-usage.html#module-assignments",
    "title": "Discord Bot Usage",
    "section": "Module Assignments",
    "text": "Module Assignments\nDeep learning requires practice. For every module in this course, you are required to complete two specific types of activities with Chibi.\n\nConcept Quiz\nFirst, you must demonstrate your understanding of the core material. The Concept Quiz is designed to test your grasp of the module’s theory. When you initiate this quiz, the bot will present you with questions which you must answer.\n/quiz [module_id]\nTyping /quiz m01 starts the quiz for Module 1. As you answer, Chibi actively evaluates your responses and updates your mastery level for each specific concept. This provides you with immediate feedback on what you know and where you might need to study more.\n\n\nLLM Quiz Challenge\nSecond, we flip the script. In the LLM Quiz Challenge, you become the examiner. Your goal is to stump the AI. You need to create a tricky, insightful question about the module’s content that the AI returns an incorrect answer for, while you provide the correct one.\n/llm-quiz &lt;module_id&gt;\nWhen you run /llm-quiz m01, you enter the arena. You submit your question and your correct answer. Chibi then attempts to answer your question. An independent Evaluator AI judges both responses. You win the challenge only if your answer is deemed correct and Chibi’s answer is incorrect. However, stumping the AI is just the first hurdle. Once you secure a win, your submission enters a review queue. The instructors will personally verify your question to ensure it is substantive and relevant to the module’s core concepts. We are looking for deep understanding, not cheap tricks. Questions that rely on tedious calculations or prompt injections (like commanding the bot to “answer incorrectly”) will be rejected. You only officially complete the assignment once your winning questions are approved. Typically, you need to achieve 3 approved wins per module to satisfy this requirement.",
    "crumbs": [
      "Home",
      "Course Information",
      "Discord Bot Usage"
    ]
  },
  {
    "objectID": "course/sprint-project.html",
    "href": "course/sprint-project.html",
    "title": "Sprint Projects",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis guide outlines our “Sprint Projects”. These are high-intensity, gamified coding sessions. We cover the team structure, the shift to agentic workflows, and the specific challenges you will face in each module.",
    "crumbs": [
      "Home",
      "Course Information",
      "Sprint Projects"
    ]
  },
  {
    "objectID": "course/sprint-project.html#general-structure-for-all-mini-projects",
    "href": "course/sprint-project.html#general-structure-for-all-mini-projects",
    "title": "Sprint Projects",
    "section": "General Structure for All Mini-Projects",
    "text": "General Structure for All Mini-Projects\n\nFormat: Teams of 2–3 (Driver/Navigator roles).\nTool: All coding must be assisted by LLMs (Claude/ChatGPT/Copilot) after Module 3.\nSubmission: A markdown file explaining the project and the code. All code and figures must be included on the github repository. The repositoory template will be provided. Students must fork it to get started.\nGallery: The best output from each team gets merged into the course website’s “Hall of Fame”.",
    "crumbs": [
      "Home",
      "Course Information",
      "Sprint Projects"
    ]
  },
  {
    "objectID": "course/sprint-project.html#module-1-the-data-scientists-toolkit",
    "href": "course/sprint-project.html#module-1-the-data-scientists-toolkit",
    "title": "Sprint Projects",
    "section": "Module 1: The Data Scientist’s Toolkit",
    "text": "Module 1: The Data Scientist’s Toolkit\n“The Tidy Data Escape Room” pits you against a disastrous CSV. It is filled with merged cells and color-coded metadata. Your mission is to clean this dataset into a “Tidy” format within a shared repository. The twist is strict. You must commit and push every single edit. The team with the cleanest data and the most granular git history wins the escape.",
    "crumbs": [
      "Home",
      "Course Information",
      "Sprint Projects"
    ]
  },
  {
    "objectID": "course/sprint-project.html#module-2-visualizing-complexity",
    "href": "course/sprint-project.html#module-2-visualizing-complexity",
    "title": "Sprint Projects",
    "section": "Module 2: Visualizing Complexity",
    "text": "Module 2: Visualizing Complexity\n“The Ugly Graph Makeover” challenges your aesthetic eye. We give you a misleading, hideous chart. Think 3D pie charts or truncated axes. You must use Python to refactor it into three distinct visualizations that tell an honest story. For example, create views for distribution, relationship, and composition. The class votes on the “Most Insightful” and “Best Aesthetic” results.",
    "crumbs": [
      "Home",
      "Course Information",
      "Sprint Projects"
    ]
  },
  {
    "objectID": "course/sprint-project.html#module-3-agentic-coding",
    "href": "course/sprint-project.html#module-3-agentic-coding",
    "title": "Sprint Projects",
    "section": "Module 3: Agentic Coding",
    "text": "Module 3: Agentic Coding\nIn “Speed-Run: Dashboard from Scratch,” you receive a raw dataset and zero starter code. Your task is to build a fully interactive Streamlit or Shiny app in under an hour. The catch is significant. You are forbidden from writing code manually. You may only prompt the LLM. The first team to have a working local app wins the race.",
    "crumbs": [
      "Home",
      "Course Information",
      "Sprint Projects"
    ]
  },
  {
    "objectID": "course/sprint-project.html#module-4-deep-learning-for-text",
    "href": "course/sprint-project.html#module-4-deep-learning-for-text",
    "title": "Sprint Projects",
    "section": "Module 4: Deep Learning for Text",
    "text": "Module 4: Deep Learning for Text\n“The Vibe-Check Classifier” explores semantic space. You start with unlabeled text like Tweets. You must generate embeddings and project them onto a custom Semaxis. Examples include “Engineering vs. Art” or “Happy vs. Sad”. The goal is to identify the “weirdest” outlier. This is the data point the model finds most confusing. The team unveiling the funniest or most counter-intuitive relationship takes the prize.",
    "crumbs": [
      "Home",
      "Course Information",
      "Sprint Projects"
    ]
  },
  {
    "objectID": "course/sprint-project.html#module-5-deep-learning-for-images",
    "href": "course/sprint-project.html#module-5-deep-learning-for-images",
    "title": "Sprint Projects",
    "section": "Module 5: Deep Learning for Images",
    "text": "Module 5: Deep Learning for Images\n“Adversarial Art Attack” tests your intuition for CNNs. You are given a pre-trained ImageNet classifier and a set of images. Your task is to modify an image until the model confidently misclassifies a banana as a toaster. You might do this by adding noise via code or digitally drawing on it. The victory goes to the team that tricks the AI with the least visible modification.",
    "crumbs": [
      "Home",
      "Course Information",
      "Sprint Projects"
    ]
  },
  {
    "objectID": "course/sprint-project.html#module-6-deep-learning-for-graphs",
    "href": "course/sprint-project.html#module-6-deep-learning-for-graphs",
    "title": "Sprint Projects",
    "section": "Module 6: Deep Learning for Graphs",
    "text": "Module 6: Deep Learning for Graphs\n“The Network Saboteur” asks you to think like an attacker. Given a graph like the Karate Club network, you must identify the minimum number of edges to remove. The goal is to severely degrade the network’s efficiency. You must predict the “load-bearing” edges before calculating centrality metrics. The most efficient destroyer of structure wins.",
    "crumbs": [
      "Home",
      "Course Information",
      "Sprint Projects"
    ]
  },
  {
    "objectID": "course/sprint-project.html#module-7-representations-structuralism",
    "href": "course/sprint-project.html#module-7-representations-structuralism",
    "title": "Sprint Projects",
    "section": "Module 7: Representations & Structuralism",
    "text": "Module 7: Representations & Structuralism\n“The Les Mis Identity Crisis” contrasts two forms of meaning. We provide the raw text of Les Misérables and a network constructed from page-level character co-occurrences. Your task is to generate two rival embeddings for the cast. Create one semantic space using Word2Vec or SBERT on the text. Create a second structural space using Node2Vec on the graph. Compare these spaces via PCA or relative distances to find a conflict. Who is Valjean close to in the story versus the structure? The victory goes to the team that finds the biggest “reputation gap”. We are looking for the character whose narrative vibe most strongly contradicts their network reality.",
    "crumbs": [
      "Home",
      "Course Information",
      "Sprint Projects"
    ]
  },
  {
    "objectID": "course/sprint-project.html#the-sprint-structure",
    "href": "course/sprint-project.html#the-sprint-structure",
    "title": "Sprint Projects",
    "section": "The Sprint Structure",
    "text": "The Sprint Structure\nWe use Sprint Projects to turn theory into muscle memory. You work in pairs as Driver and Navigator. You have a strict 60–90 minutes to solve a specific challenge. Post-Module 3, the rules change. You must stop writing code manually. Instead, you act as an “Architect” and use LLMs to generate all syntax. Winners of each sprint earn a spot in the course “Hall of Fame”.",
    "crumbs": [
      "Home",
      "Course Information",
      "Sprint Projects"
    ]
  },
  {
    "objectID": "course/deliverables.html#the-philosophy-of-the-project",
    "href": "course/deliverables.html#the-philosophy-of-the-project",
    "title": "Deliverables",
    "section": "The Philosophy of the Project",
    "text": "The Philosophy of the Project\nProject reports often focus solely on final results. We want you to shift your attention to the process and the decisions you make along the way. Your report should explain the reasoning behind your choices, such as why you selected one method over an alternative.\nYou must start by clarifying your objectives. We need to know what you intend to do and why those questions matter. From there, you will provide a detailed description of your data. This includes the source of the data, how you compiled it, and the preprocessing steps you took. You should also identify the data types of your focal features and explain which features you believe are relevant for the analysis.\nDetermining the appropriate methods follows naturally. You should discuss methods used in previous studies to provide context. We encourage you to explore what approaches others have taken with similar datasets. This helps you justify why a particular method is suitable for the information you aim to present.\nAn honest project clarifies the limitations and advantages of its approach. These constraints stem from your data and methodologies. You must discuss them in light of existing work. For instance, if you develop a link prediction algorithm based on common neighbors, you need to ask when that algorithm fails. You should also articulate the advantage of your algorithm over alternatives like graph neural networks.\nWe also ask you to embrace failures. Thomas Edison famously said that he did not fail but simply found 10,000 ways that did not work. Research often appears to follow a single linear path. The reality is that people take many paths that turn out effectively unsuccessful. It is crucial to try multiple candidates. More importantly, you must document your failures to understand why they did not work. You can use fake data, small subsets, mock-ups, and sketches to iterate and refine your approach. These “failures” often lead to the most successful outcomes.",
    "crumbs": [
      "Home",
      "Course Information",
      "Deliverables"
    ]
  },
  {
    "objectID": "course/deliverables.html#the-proposal",
    "href": "course/deliverables.html#the-proposal",
    "title": "Deliverables",
    "section": "The Proposal",
    "text": "The Proposal\nA complete proposal includes several distinct sections that guide us through your plan.\nYou begin with the Project Title and the names of your Team Members. Teams can range from one to four people. Keep in mind that we expect a larger team to accomplish more than a smaller one.\nThe narrative starts with an Abstract. This is a concise summary of your project. You then move to the Introduction. This is where you must clearly present the “diff of ideas.” The claim that “nobody has done this” is never a sufficient justification. You must convince us why we should care. This means explaining exactly how previous works are fundamentally flawed and demonstrating the value of your successful solution. We require substantive critiques of existing literature, not just a passive review.\nNext you define your Questions or Objectives. This specifies the methods you plan to create and what you hope to discover from the data.\nFinally you detail your Datasets and Methods. You must identify the dataset you will be using. If you have not done so already, we strongly encourage you to reconsider your project. Obtaining and cleaning datasets is time-consuming. You need to describe the dataset structure and data types. You also need to explain the methods you plan to apply and justify those choices. Provide enough detailed information to convincingly argue that the dataset is suitable for your proposed methods.",
    "crumbs": [
      "Home",
      "Course Information",
      "Deliverables"
    ]
  },
  {
    "objectID": "course/deliverables.html#sprint-projects",
    "href": "course/deliverables.html#sprint-projects",
    "title": "Deliverables",
    "section": "Sprint Projects",
    "text": "Sprint Projects\nWe review the documents and structure of the files, along with git history. We verify that your repository reflects a structured, iterative process rather than a chaotic last-minute rush.",
    "crumbs": [
      "Home",
      "Course Information",
      "Deliverables"
    ]
  },
  {
    "objectID": "m03-agentic-coding/prompt-tuning.html",
    "href": "m03-agentic-coding/prompt-tuning.html",
    "title": "Prompt Tuning",
    "section": "",
    "text": "Prompt engineering is the art of crafting inputs that reliably activate desired patterns in language models\n\n\n\n\nFigure 1",
    "crumbs": [
      "Home",
      "Module 3: Agentic Coding",
      "Prompt Tuning"
    ]
  }
]
[
  {
    "objectID": "m05-images/overview.html",
    "href": "m05-images/overview.html",
    "title": "Module 5: Deep Learning for Images",
    "section": "",
    "text": "What you’ll learn in this module\n\n\n\nThis module takes you from pixels to state-of-the-art vision models. We start by understanding images as data structures, explore the paradigm shift from hand-crafted to learned features, learn practical skills for using CNNs, trace the innovation timeline from VGG to Vision Transformers, and culminate in a hands-on classification challenge.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Overview"
    ]
  },
  {
    "objectID": "m05-images/overview.html#the-journey",
    "href": "m05-images/overview.html#the-journey",
    "title": "Module 5: Deep Learning for Images",
    "section": "The Journey",
    "text": "The Journey\nPart 1: Understanding Images\nBefore we can process images with neural networks, we need to understand what images are. We examine how computers represent visual information and why spatial structure matters.\nPart 2: The Deep Learning Revolution\nShift your attention to the historical moment when neural networks transformed computer vision. We contrast hand-crafted features with learned representations, following the path from LeNet to AlexNet.\nPart 3: Becoming a Practitioner\nNow you’ll learn the skills to actually use these models. We cover CNN building blocks, pre-trained models, transfer learning, and hands-on implementation.\nPart 4: The Innovation Timeline\nThe very best way to understand modern architectures is to see them as solutions to specific problems. We trace the quest for deeper, more efficient networks through VGG, Inception, ResNet, and Vision Transformers.\nPart 5: Your Challenge\nApply everything you’ve learned to a real image classification problem. You’ll make architectural choices, tune hyperparameters, and compete for the best performance.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Overview"
    ]
  },
  {
    "objectID": "m05-images/overview.html#why-this-matters",
    "href": "m05-images/overview.html#why-this-matters",
    "title": "Module 5: Deep Learning for Images",
    "section": "Why This Matters",
    "text": "Why This Matters\nComputer vision is no longer about manually designing features. Modern systems learn representations automatically from data. This module gives you both conceptual understanding and practical skills to work with state-of-the-art vision models.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Overview"
    ]
  },
  {
    "objectID": "m05-images/overview.html#prerequisites",
    "href": "m05-images/overview.html#prerequisites",
    "title": "Module 5: Deep Learning for Images",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou should be comfortable with basic Python programming and NumPy arrays. You’ll also need neural network fundamentals like forward propagation, backpropagation, and gradient descent. Finally, PyTorch basics matter here: tensors, autograd, and simple model training.\nIf you need to refresh these topics, review the earlier modules in this course.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Overview"
    ]
  },
  {
    "objectID": "m05-images/overview.html#what-youll-build",
    "href": "m05-images/overview.html#what-youll-build",
    "title": "Module 5: Deep Learning for Images",
    "section": "What You’ll Build",
    "text": "What You’ll Build\nBy the end of this module, you will understand how images are represented as tensors. You’ll implement classic CNN architectures from scratch and use pre-trained models for transfer learning. You’ll make informed decisions about architecture selection. Most importantly, you’ll complete a real image classification competition.\nLet’s begin by understanding what an image really is.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Overview"
    ]
  },
  {
    "objectID": "m05-images/01-what-is-an-image.html",
    "href": "m05-images/01-what-is-an-image.html",
    "title": "Part 1: What is an Image?",
    "section": "",
    "text": "What you’ll learn\n\n\n\nThis section introduces images as structured data. We explore how computers represent visual information as numbers, examine the role of pixels and channels, and understand why spatial relationships matter for machine perception.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 1: What is an Image?"
    ]
  },
  {
    "objectID": "m05-images/01-what-is-an-image.html#a-photograph-is-just-numbers",
    "href": "m05-images/01-what-is-an-image.html#a-photograph-is-just-numbers",
    "title": "Part 1: What is an Image?",
    "section": "A Photograph is Just Numbers",
    "text": "A Photograph is Just Numbers\nLet’s talk about what an image really is from a computer’s perspective. When you look at a photograph, you see faces, objects, and scenes. But to a machine, that same photograph is simply a grid of numbers. Each number represents the brightness or color at a specific location.\nThis representation might seem strange at first. How can numbers capture the richness of visual information? The answer lies in spatial structure. Unlike a spreadsheet where row order doesn’t matter, the arrangement of numbers in an image is everything. Neighboring pixels relate to each other, forming edges, textures, and patterns.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 1: What is an Image?"
    ]
  },
  {
    "objectID": "m05-images/01-what-is-an-image.html#grayscale-images-the-simplest-case",
    "href": "m05-images/01-what-is-an-image.html#grayscale-images-the-simplest-case",
    "title": "Part 1: What is an Image?",
    "section": "Grayscale Images: The Simplest Case",
    "text": "Grayscale Images: The Simplest Case\nThe very first step in understanding images is to examine the grayscale case. A grayscale image contains only brightness information, with no color. We can think of it as a 2D matrix where each entry is a pixel intensity value.\nConsider a tiny 6×6 grayscale image:\n\nX = \\begin{bmatrix}\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10\n\\end{bmatrix}\n\nHere, the value 10 represents dark pixels, while 80 represents bright pixels. The third column forms a bright vertical line. This simple example shows how spatial patterns emerge from the arrangement of numbers.\n\n\n\n```adsjozvk https://ai.stanford.edu/~syyeung/cvweb/Pictures1/imagematrix.png\n\n\n\n\nwidth: 80%\n\n\nname: image-matrix\n\n\nalign: center\n\n\n\nA grayscale image represented as a matrix of pixel intensity values. Each number encodes brightness at that location.\n\n## Loading and Inspecting Real Images\n\nLet's make this concrete by loading an actual image and examining its structure. We'll use Python with standard libraries to see what an image really looks like under the hood.\n\n::: {#a08a78c0 .cell}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\ndef load_image_from_url(url):\n    \"\"\"Load an image from a URL and convert to numpy array.\"\"\"\n    response = requests.get(url)\n    img = Image.open(BytesIO(response.content))\n    return np.array(img)\n\n# Load an example image\nurl = \"https://www.binghamton.edu/news/images/uploads/features/20180815_peacequad02_jwc.jpg\"\nimg_array = load_image_from_url(url)\n\n# Display the image\nplt.figure(figsize=(8, 6))\nplt.imshow(img_array)\nplt.title(\"Original Image\")\nplt.axis(\"off\")\nplt.show()\n\n# Print its properties\nprint(f\"Image shape: {img_array.shape}\")\nprint(f\"Data type: {img_array.dtype}\")\nprint(f\"Value range: [{img_array.min()}, {img_array.max()}]\")\n:::\nWhat does the shape tell us? The output (height, width, 3) reveals three dimensions. The first two dimensions specify spatial location, while the third dimension holds three color channels.\nLet’s zoom into a small patch to see the actual numbers:\n\n# Extract a tiny 5x5 patch from the center\ncenter_y, center_x = img_array.shape[0] // 2, img_array.shape[1] // 2\npatch = img_array[center_y:center_y+5, center_x:center_x+5, 0]  # Red channel only\n\nprint(\"A 5x5 patch of pixel values (Red channel):\")\nprint(patch)\n\nThese are the actual numbers the computer sees. Each value between 0 and 255 represents brightness in the red channel for that pixel location.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 1: What is an Image?"
    ]
  },
  {
    "objectID": "m05-images/01-what-is-an-image.html#color-images-three-stacked-layers",
    "href": "m05-images/01-what-is-an-image.html#color-images-three-stacked-layers",
    "title": "Part 1: What is an Image?",
    "section": "Color Images: Three Stacked Layers",
    "text": "Color Images: Three Stacked Layers\nColor images extend the grayscale concept by using three separate matrices, one for each color channel: Red, Green, and Blue (RGB). Think of these as three grayscale images stacked on top of each other.\nWhen you combine the values from all three channels at a given location, you get the color for that pixel. For example, [255, 0, 0] is pure red, [0, 255, 0] is pure green, and [255, 255, 255] is white.\nLet’s visualize the three channels separately:\n\nfig, axes = plt.subplots(1, 4, figsize=(16, 4))\n\n# Original image\naxes[0].imshow(img_array)\naxes[0].set_title(\"Original Image\")\naxes[0].axis(\"off\")\n\n# Individual channels\nchannel_names = ['Red', 'Green', 'Blue']\ncolors = ['Reds', 'Greens', 'Blues']\n\nfor i, (name, cmap) in enumerate(zip(channel_names, colors)):\n    axes[i+1].imshow(img_array[:, :, i], cmap=cmap)\n    axes[i+1].set_title(f\"{name} Channel\")\n    axes[i+1].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\nNotice how each channel emphasizes different aspects of the scene. The red channel might be bright where red objects appear, while the blue channel highlights sky and water.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 1: What is an Image?"
    ]
  },
  {
    "objectID": "m05-images/01-what-is-an-image.html#why-spatial-structure-matters",
    "href": "m05-images/01-what-is-an-image.html#why-spatial-structure-matters",
    "title": "Part 1: What is an Image?",
    "section": "Why Spatial Structure Matters",
    "text": "Why Spatial Structure Matters\nShift your attention from individual pixel values to relationships between pixels. This is what makes images fundamentally different from tabular data.\nIn a spreadsheet, you can shuffle the rows without losing information. But in an image, shuffling pixels destroys everything. The spatial arrangement is the information. An edge appears when neighboring pixels have very different values. A texture emerges from repeating patterns across nearby locations. An object is a coherent region of similar pixels.\nThis spatial structure is why neural networks for images need special architectures. Fully connected networks treat every input independently, ignoring spatial relationships. Convolutional networks, which we’ll explore soon, are designed specifically to exploit this structure.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 1: What is an Image?"
    ]
  },
  {
    "objectID": "m05-images/01-what-is-an-image.html#converting-between-representations",
    "href": "m05-images/01-what-is-an-image.html#converting-between-representations",
    "title": "Part 1: What is an Image?",
    "section": "Converting Between Representations",
    "text": "Converting Between Representations\nLet’s practice manipulating image representations to build intuition:\n\n# Convert to grayscale by averaging channels\ngrayscale = np.mean(img_array, axis=2).astype(np.uint8)\n\n# Create a side-by-side comparison\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\naxes[0].imshow(img_array)\naxes[0].set_title(\"RGB Image (3 channels)\")\naxes[0].axis(\"off\")\n\naxes[1].imshow(grayscale, cmap='gray')\naxes[1].set_title(\"Grayscale Image (1 channel)\")\naxes[1].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"RGB shape: {img_array.shape}\")\nprint(f\"Grayscale shape: {grayscale.shape}\")\n\nThe grayscale version loses color information but preserves spatial structure. For many computer vision tasks, this simplified representation is sufficient.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 1: What is an Image?"
    ]
  },
  {
    "objectID": "m05-images/01-what-is-an-image.html#image-dimensions-in-deep-learning",
    "href": "m05-images/01-what-is-an-image.html#image-dimensions-in-deep-learning",
    "title": "Part 1: What is an Image?",
    "section": "Image Dimensions in Deep Learning",
    "text": "Image Dimensions in Deep Learning\nWhen we feed images into neural networks, we need to be precise about dimensions. Different frameworks use different conventions, so let’s clarify the PyTorch standard.\nPyTorch expects images in (batch_size, channels, height, width) format, often abbreviated as NCHW:\n\nN: Batch size (number of images processed together)\nC: Channels (3 for RGB, 1 for grayscale)\nH: Height in pixels\nW: Width in pixels\n\nLet’s convert our image to PyTorch format:\n\nimport torch\n\n# Original numpy array is (H, W, C)\nprint(f\"NumPy format (H, W, C): {img_array.shape}\")\n\n# Convert to PyTorch format (C, H, W) for a single image\nimg_tensor = torch.from_numpy(img_array).permute(2, 0, 1).float() / 255.0\nprint(f\"PyTorch format (C, H, W): {img_tensor.shape}\")\n\n# Add batch dimension to get (N, C, H, W)\nimg_batch = img_tensor.unsqueeze(0)\nprint(f\"Batch format (N, C, H, W): {img_batch.shape}\")\n\nWe also normalized pixel values from [0, 255] to [0, 1] by dividing by 255. This normalization helps neural networks train more stably.\n\n\n\n\n\n\nTry it yourself\n\n\n\nLoad your own image and explore its properties. Try these experiments:\n\nExtract and visualize a small patch of pixels as numbers\nModify some pixel values and see how the image changes\nSwap the red and blue channels to see the color shift\nConvert between NumPy and PyTorch formats\n\nUnderstanding these representations at a hands-on level will make everything that follows more intuitive.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 1: What is an Image?"
    ]
  },
  {
    "objectID": "m05-images/01-what-is-an-image.html#from-pixels-to-patterns",
    "href": "m05-images/01-what-is-an-image.html#from-pixels-to-patterns",
    "title": "Part 1: What is an Image?",
    "section": "From Pixels to Patterns",
    "text": "From Pixels to Patterns\nNow that we understand what images are as data structures, the next question becomes: how do we detect patterns in them? Human vision effortlessly recognizes edges, textures, and objects. But what computational operations allow machines to do the same?\nThis question leads us to convolution, feature extraction, and ultimately to the deep learning revolution. In the next section, we’ll explore how computer vision evolved from hand-crafted feature detectors to learned representations that can match or exceed human performance.\nThe key insight to carry forward is this: images are spatial data where relationships between neighboring pixels encode visual information. Any successful vision system must respect and exploit this spatial structure.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 1: What is an Image?"
    ]
  },
  {
    "objectID": "m05-images/01-what-is-an-image.html#summary",
    "href": "m05-images/01-what-is-an-image.html#summary",
    "title": "Part 1: What is an Image?",
    "section": "Summary",
    "text": "Summary\nWe explored images as structured numerical data. A grayscale image is a 2D matrix of brightness values, while a color image adds two more matrices for the other color channels. Spatial relationships between pixels encode edges, textures, and objects. Unlike tabular data, the arrangement of values matters fundamentally.\nWe saw how to load, inspect, and manipulate images in Python, converting between NumPy arrays and PyTorch tensors. This hands-on understanding prepares us to work with the deep learning models that process these image representations.\nImages are not just collections of numbers. They are spatially organized data where local patterns combine into global structure. This insight motivates everything that follows in computer vision.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 1: What is an Image?"
    ]
  },
  {
    "objectID": "m05-images/02-the-deep-learning-revolution.html",
    "href": "m05-images/02-the-deep-learning-revolution.html",
    "title": "Part 2: The Deep Learning Revolution",
    "section": "",
    "text": "What you’ll learn\n\n\n\nThis section traces the paradigm shift in computer vision from hand-crafted features to learned representations. We explore how researchers designed edge detectors and frequency transforms, examine LeNet’s pioneering approach to automated feature learning, and understand AlexNet’s breakthrough that demonstrated deep learning works at scale.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 2: The Deep Learning Revolution"
    ]
  },
  {
    "objectID": "m05-images/02-the-deep-learning-revolution.html#the-old-way-engineering-features-by-hand",
    "href": "m05-images/02-the-deep-learning-revolution.html#the-old-way-engineering-features-by-hand",
    "title": "Part 2: The Deep Learning Revolution",
    "section": "The Old Way: Engineering Features by Hand",
    "text": "The Old Way: Engineering Features by Hand\nBefore 2012, computer vision meant one thing: carefully designing features by hand. Experts would analyze problems and craft mathematical operations to extract useful information. Edge detection, texture analysis, object boundaries. Every feature required human insight and engineering effort.\nLet’s explore how this worked by examining edge detection, one of the fundamental problems in image processing.\n\nDetecting Edges Through Brightness Changes\nWhat makes an edge visible to human eyes? The answer is sudden changes in brightness. An edge appears when neighboring pixels have significantly different intensity values.\nConsider a small 6×6 image with a bright vertical line:\n\nX = \\begin{bmatrix}\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10\n\\end{bmatrix}\n\nHow do we detect the vertical edge in the third column? We can approximate the horizontal derivative by subtracting the right neighbor from the left neighbor at each position. For the central pixel, this looks like:\n\n\\nabla Z_{22} = Z_{2,1} - Z_{2,3}\n\nApplied to the entire image, we get large values where brightness changes suddenly (the edge) and near-zero values elsewhere. This simple operation reveals structure.\n\n\nConvolution: A General Pattern Matching Operation\nThe derivative calculation we just performed is a special case of a more general operation called convolution. The idea is elegant: define a small matrix of weights called a kernel or filter, then slide it across the image, computing weighted sums at each position.\nFor a 3×3 kernel K applied to a local patch Z:\n\n\\text{output}_{i,j} = \\sum_{m=-1}^{1} \\sum_{n=-1}^{1} K_{m,n} \\cdot Z_{i+m, j+n}\n\nThe Prewitt operator provides kernels specifically designed for edge detection:\n\nK_h = \\begin{bmatrix}\n-1 & 0 & 1 \\\\\n-1 & 0 & 1 \\\\\n-1 & 0 & 1\n\\end{bmatrix}\n\\quad\\text{and}\\quad\nK_v = \\begin{bmatrix}\n-1 & -1 & -1 \\\\\n0 & 0 & 0 \\\\\n1 & 1 & 1\n\\end{bmatrix}\n\nThe horizontal kernel K_h detects vertical edges, while the vertical kernel K_v detects horizontal edges. Each kernel responds strongly when the image patch matches its pattern.\n\n# Load an example image\nurl = \"https://www.binghamton.edu/news/images/uploads/features/20180815_peacequad02_jwc.jpg\"\nimg_array = load_image_from_url(url)\nimg_gray = to_grayscale(img_array)\n\n# Apply vertical edge detection using convolution\nfrom scipy.signal import convolve2d\n\nK_v = np.array([[-1, -1, -1],\n                [0, 0, 0],\n                [1, 1, 1]])\n\nedges = convolve2d(img_gray, K_v, mode='same', boundary='symm')\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\naxes[0].imshow(img_gray, cmap='gray')\naxes[0].set_title(\"Original Grayscale Image\")\naxes[0].axis(\"off\")\n\naxes[1].imshow(edges, cmap='gray')\naxes[1].set_title(\"Vertical Edge Detection (Prewitt Filter)\")\naxes[1].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\nNotice how the filter highlights horizontal boundaries where brightness changes rapidly in the vertical direction. An excellent interactive demo of various image kernels can be found at Setosa Image Kernels.\n\n\nThinking in Frequencies: The Fourier Transform\nShift your attention from the spatial domain to the frequency domain. The Fourier transform offers an alternative view of images, representing them as combinations of sinusoidal patterns at different frequencies.\nFor a discrete signal x[n] of length N, the Discrete Fourier Transform is:\n\n\\mathcal{F}(x)[k] = \\sum_{n=0}^{N-1} x[n] \\cdot e^{-2\\pi i \\frac{nk}{N}}\n\nUsing Euler’s formula e^{ix} = \\cos(x) + i\\sin(x), we can rewrite this as:\n\n\\mathcal{F}(x)[k] = \\sum_{n=0}^{N-1} x[n]\\Big[\\cos(2\\pi \\tfrac{nk}{N}) - i\\sin(2\\pi \\tfrac{nk}{N})\\Big]\n\nThe Fourier transform decomposes a signal into its frequency components. Low frequencies correspond to smooth, slowly varying regions. High frequencies correspond to sharp edges and fine details.\nThe convolution theorem reveals a beautiful connection: convolution in the spatial domain is equivalent to multiplication in the frequency domain:\n\nX * K \\quad\\longleftrightarrow\\quad \\mathcal{F}(X) \\cdot \\mathcal{F}(K)\n\nThis means we can perform convolution by: 1. Taking the Fourier transform of both the image and the kernel 2. Multiplying them element-wise in the frequency domain 3. Taking the inverse Fourier transform to get back to the spatial domain\nFor large images, this approach can be computationally faster than direct convolution. For a beautiful visual explanation of Fourier transforms, watch 3Blue1Brown’s video: But what is the Fourier Transform?\n\n\nThe Fundamental Limitation\nHere’s the problem with hand-crafted features: experts had to design every single one. Want to detect corners? Design a corner detector. Need to recognize textures? Craft texture descriptors. Each feature required mathematical sophistication and domain expertise.\nThis approach worked for simple, well-defined tasks. But it scaled poorly to complex problems like recognizing thousands of object categories. The feature engineering bottleneck limited what computer vision could achieve.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 2: The Deep Learning Revolution"
    ]
  },
  {
    "objectID": "m05-images/02-the-deep-learning-revolution.html#the-first-breakthrough-lenet",
    "href": "m05-images/02-the-deep-learning-revolution.html#the-first-breakthrough-lenet",
    "title": "Part 2: The Deep Learning Revolution",
    "section": "The First Breakthrough: LeNet",
    "text": "The First Breakthrough: LeNet\nYann LeCun posed a radical question in the late 1980s: what if networks could learn features automatically from raw pixels? Instead of hand-designing edge detectors, let the network discover useful patterns through training.\nThis vision led to LeNet, a pioneering convolutional architecture that demonstrated automated feature learning on handwritten digit recognition {footcite}lecun1989backpropagation,lecun1998gradient.\n\n\n\n```vtgvrhlk https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ge5OLutAT9_3fxt_sKTBGA.png\n\n\n\n\nwidth: 100%\n\n\nname: lenet-1\n\n\nalign: center\n\n\n\nLeNet-1 architecture. The network learns to extract features through layers of convolution and pooling.\n\n### Architecture: Hierarchical Feature Learning\n\nLeNet introduced a pattern that remains fundamental to modern CNNs:\n\n1. **Convolutional layers** apply learnable filters (not hand-designed) to extract local patterns\n2. **Pooling layers** downsample feature maps, creating spatial invariance\n3. **Stacking multiple layers** builds increasingly abstract representations\n4. **Fully connected layers** at the end combine features for classification\n\nThe key innovation was making the convolutional filters learnable parameters. During training, backpropagation adjusts filter weights to extract features useful for the task. The network discovers edge detectors, corner detectors, and more complex patterns automatically.\n\nLeNet-5, the most influential version, processed 32×32 grayscale images through this architecture:\n\n```{figure} https://www.datasciencecentral.com/wp-content/uploads/2021/10/1lvvWF48t7cyRWqct13eU0w.jpeg\n---\nwidth: 100%\nname: lenet-5\nalign: center\n---\nLeNet-5 architecture with input normalization, sparse connectivity, and multiple convolution-pooling pairs.\nLet’s understand each component:\nC1: First Convolutional Layer Takes the input image and applies learnable 5×5 filters. These filters start random but evolve during training to detect basic patterns like edges at various orientations.\nS2: Subsampling (Pooling) Reduces spatial dimensions through average pooling with 2×2 windows. This creates local translation invariance—small shifts in feature positions don’t change the output significantly.\nC3: Second Convolutional Layer Combines features from the previous layer to build more complex patterns. LeNet-5 used sparse connectivity here (not every feature map connects to every previous map), reducing parameters while encouraging diverse features.\nS4: Second Subsampling Further reduces spatial dimensions, allowing the network to focus on increasingly abstract representations.\nFully Connected Layers Flatten the spatial feature maps into a vector and make the final classification decision across 10 digit classes.\nYann LeCun’s work on applying backpropagation to convolutional architectures in the 1980s was met with skepticism. But LeNet’s success on real-world tasks like automated check reading at banks helped spark wider interest in neural networks.\n\nImplementing LeNet in Modern PyTorch\nLet’s implement a simplified LeNet-1 using contemporary tools. While the original used custom training procedures, we’ll use PyTorch Lightning for clean, maintainable code.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms\nfrom torchmetrics import Accuracy\n\nclass MNISTDataModule(pl.LightningDataModule):\n    \"\"\"PyTorch Lightning data module for MNIST dataset.\"\"\"\n\n    def __init__(self, data_dir='./data', batch_size=32):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0,), (1,))\n        ])\n\n    def prepare_data(self):\n        datasets.MNIST(self.data_dir, train=True, download=True)\n        datasets.MNIST(self.data_dir, train=False, download=True)\n\n    def setup(self, stage=None):\n        if stage == 'fit' or stage is None:\n            mnist_full = datasets.MNIST(\n                self.data_dir, train=True, transform=self.transform\n            )\n            self.mnist_train, self.mnist_val = random_split(\n                mnist_full, [55000, 5000],\n                generator=torch.Generator().manual_seed(42)\n            )\n\n        if stage == 'test' or stage is None:\n            self.mnist_test = datasets.MNIST(\n                self.data_dir, train=False, transform=self.transform\n            )\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.mnist_train, batch_size=self.batch_size,\n            shuffle=True, num_workers=2\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.mnist_val, batch_size=self.batch_size, num_workers=2\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n            self.mnist_test, batch_size=self.batch_size, num_workers=2\n        )\n\nclass LeNet1(pl.LightningModule):\n    \"\"\"PyTorch Lightning implementation of LeNet-1.\"\"\"\n\n    def __init__(self, learning_rate=1e-3):\n        super().__init__()\n        self.save_hyperparameters()\n\n        # Metrics\n        self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=10)\n        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=10)\n\n        # Network architecture\n        self.conv1 = nn.Conv2d(1, 4, kernel_size=5, stride=1)\n        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(4, 12, kernel_size=5, stride=1)\n        self.fc = nn.Linear(12 * 4 * 4, 10)\n\n        # Initialize weights\n        self._init_weights()\n\n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, (nn.Conv2d, nn.Linear)):\n                nn.init.xavier_uniform_(m.weight)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = torch.tanh(x)\n        x = self.pool(x)\n\n        x = self.conv2(x)\n        x = torch.tanh(x)\n        x = self.pool(x)\n\n        x = x.view(-1, 12 * 4 * 4)\n        x = self.fc(x)\n        return x\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(\n            self.parameters(), lr=self.hparams.learning_rate\n        )\n        return optimizer\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        acc = self.train_accuracy(logits, y)\n\n        self.log(\"train_loss\", loss, prog_bar=True)\n        self.log(\"train_acc\", acc, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        acc = self.val_accuracy(logits, y)\n\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", acc, prog_bar=True)\n\n# Train the model\nmodel = LeNet1(learning_rate=1e-3)\ndata_module = MNISTDataModule(batch_size=256)\n\ntrainer = pl.Trainer(\n    max_epochs=3,\n    accelerator=\"auto\",\n    devices=1,\n)\n\ntrainer.fit(model, data_module)\n\nEven this simple architecture achieves high accuracy on MNIST, demonstrating the power of learned features. The convolutional filters automatically discover edge detectors and pattern recognizers through training.\n\n\nWhy LeNet Mattered\nLeNet proved a crucial concept: networks can learn better features than human experts can design. This automated feature learning was revolutionary, but LeNet’s impact remained limited. It worked well on simple tasks like digit recognition but struggled with complex, large-scale problems.\nThe computational constraints of the 1990s prevented training deeper, more powerful networks. GPU acceleration didn’t exist. Datasets were small. Training techniques were primitive compared to modern methods.\nFor nearly two decades, hand-crafted features remained dominant in computer vision. Techniques like SIFT (Scale-Invariant Feature Transform) and HOG (Histogram of Oriented Gradients) powered most practical systems. Neural networks were interesting research curiosities, not mainstream tools.\nThen came 2012.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 2: The Deep Learning Revolution"
    ]
  },
  {
    "objectID": "m05-images/02-the-deep-learning-revolution.html#the-revolution-alexnet",
    "href": "m05-images/02-the-deep-learning-revolution.html#the-revolution-alexnet",
    "title": "Part 2: The Deep Learning Revolution",
    "section": "The Revolution: AlexNet",
    "text": "The Revolution: AlexNet\nThe ImageNet Large Scale Visual Recognition Challenge (ILSVRC) posed a formidable test: classify images into 1000 categories using a training set of 1.2 million images. This scale dwarfed anything LeNet had tackled. The best systems in 2011 achieved around 25% top-5 error, using carefully engineered features and traditional machine learning methods.\n\n\n\n```vtgvrhlk https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11263-015-0816-y/MediaObjects/11263_2015_816_Fig2_HTML.gif\n\n\n\n\nwidth: 100%\n\n\nalign: center\n\n\nname: imagenet-challenge\n\n\n\nThe ImageNet Large Scale Visual Recognition Challenge dataset contains over 1.2 million training images across 1000 categories.\n\nIn 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton submitted a deep convolutional network that reduced top-5 error to **16.4%**. This more than 10 percentage point improvement shocked the community {footcite}`krizhevsky2012alexnet`.\n\n```{figure} https://viso.ai/wp-content/uploads/2024/02/imagenet-winners-by-year.jpg\n---\nwidth: 100%\nalign: center\nname: imagenet-results\n---\nTop-5 error rates on ImageNet from 2010 to 2017. AlexNet's breakthrough in 2012 sparked the deep learning revolution.\nAlexNet didn’t just win. It demonstrated that deep learning could work at scale, igniting the revolution that transformed computer vision, speech recognition, natural language processing, and countless other domains.\n\nKey Innovation 1: ReLU Activation\nDeep networks suffer from the vanishing gradient problem. During backpropagation, gradients shrink as they flow backward through layers. Traditional activations like sigmoid:\n\n\\sigma(x) = \\frac{1}{1 + e^{-x}}\n\nsaturate for large positive or negative inputs, driving gradients toward zero. This makes early layers nearly impossible to train.\nAlexNet popularized the Rectified Linear Unit (ReLU) {footcite}nair2010rectified:\n\n\\text{ReLU}(x) = \\max(0, x)\n\n\n\n\n```vtgvrhlk https://miro.medium.com/v2/resize:fit:474/1*HGctgaVdv9rEHIVvLYONdQ.jpeg\n\n\n\n\nwidth: 60%\n\n\nalign: center\n\n\nname: relu-vs-sigmoid\n\n\n\nSigmoid saturates for large inputs (gradient approaches zero), while ReLU maintains constant gradient for positive inputs.\n\nReLU offers critical advantages:\n\n- **No vanishing gradient** for positive inputs (gradient is exactly 1)\n- **Computationally cheap** (just compare to zero)\n- **Sparse activation** (many neurons output zero, creating efficient representations)\n\nThe drawback is that neurons can \"die\" if they always receive negative inputs, never activating again. Variants like Leaky ReLU introduce a small slope for negative inputs to mitigate this:\n\n$$\n\\text{Leaky ReLU}(x) = \\begin{cases}\nx & \\text{if } x &gt; 0 \\\\\n\\alpha x & \\text{if } x \\leq 0\n\\end{cases}\n$$\n\nwhere $\\alpha$ is typically 0.01.\n\n### Key Innovation 2: Dropout Regularization\n\nDeep networks with millions of parameters easily overfit training data. AlexNet introduced **Dropout** as a powerful regularization technique {footcite}`srivastava2014dropout`.\n\n```{figure} https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/1IrdJ5PghD9YoOyVAQ73MJw.gif\n---\nwidth: 70%\nalign: center\nname: dropout-animation\n---\nDropout randomly disables neurons during training, forcing the network to learn robust features.\nDuring training, Dropout randomly sets neuron outputs to zero with probability p (typically 0.5). This prevents the network from relying too heavily on any single neuron. The effect is like training an ensemble of networks that share weights.\nAt inference time, all neurons are active, but their outputs are scaled by (1-p) to maintain expected values. Modern implementations often use inverse dropout, scaling during training instead to avoid scaling at inference.\n\n\nKey Innovation 3: GPU Acceleration\nAlexNet demonstrated that deep learning needed massive computational power. The network was trained on two GPUs with 3GB memory each, splitting the computation to handle the large parameter count.\nThis wasn’t just an implementation detail. It showed that deep learning required specialized hardware. The success of AlexNet helped catalyze the GPU computing revolution that continues today, with modern networks training on dozens or hundreds of GPUs.\n\n\nKey Innovation 4: Data Augmentation\nTo combat overfitting with limited training data, AlexNet applied aggressive data augmentation:\n\nRandom crops of 224×224 patches from 256×256 images\nHorizontal flips\nColor and lighting perturbations (PCA-based color jittering)\n\nThese transformations artificially expanded the training set, teaching the network to recognize objects regardless of position, orientation, or lighting conditions.\n\n\nThe Architecture\nAlexNet consists of five convolutional layers followed by three fully connected layers:\n\n\n\n```vtgvrhlk ../archive/figs/alexnet-architecture.jpg\n\n\n\n\nwidth: 70%\n\n\nalign: center\n\n\nname: alexnet-architecture\n\n\n\nAlexNet architecture with 5 convolutional layers and 3 fully connected layers. The network was split across two GPUs.\n\n**Layer-by-layer breakdown:**\n\n1. **Input**: 224×224 RGB image (3 channels)\n2. **Conv1**: 96 filters of 11×11, stride 4 → ReLU → Max Pool (3×3, stride 2)\n3. **Conv2**: 256 filters of 5×5 → ReLU → Max Pool (3×3, stride 2)\n4. **Conv3**: 384 filters of 3×3 → ReLU\n5. **Conv4**: 384 filters of 3×3 → ReLU\n6. **Conv5**: 256 filters of 3×3 → ReLU → Max Pool (3×3, stride 2)\n7. **FC6**: 4096 neurons → ReLU → Dropout\n8. **FC7**: 4096 neurons → ReLU → Dropout\n9. **FC8**: 1000 neurons (class scores) → Softmax\n\nThe network has approximately 60 million parameters. The first convolutional layer uses large 11×11 filters with stride 4 to rapidly reduce spatial dimensions. Later layers use smaller 3×3 filters to refine features.\n\nAlexNet also used Local Response Normalization (LRN) to normalize activations across adjacent channels. This technique is less common in modern architectures, which typically use batch normalization instead.\n\n### Implementing AlexNet\n\nHere's a simplified AlexNet implementation in PyTorch:\n\n::: {#1ab67c19 .cell}\n``` {.python .cell-code}\nclass SimpleAlexNet(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(SimpleAlexNet, self).__init__()\n\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n\n            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n\n            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.5),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.ReLU(inplace=True),\n\n            nn.Dropout(p=0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n\n            nn.Linear(4096, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n# Create the model\nmodel = SimpleAlexNet(num_classes=1000)\nprint(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n:::\nPyTorch provides a pre-trained version through torchvision.models.alexnet(), trained on ImageNet. You can load it with:\n\nimport torchvision.models as models\nalexnet = models.alexnet(pretrained=True)",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 2: The Deep Learning Revolution"
    ]
  },
  {
    "objectID": "m05-images/02-the-deep-learning-revolution.html#why-alexnet-was-a-paradigm-shift",
    "href": "m05-images/02-the-deep-learning-revolution.html#why-alexnet-was-a-paradigm-shift",
    "title": "Part 2: The Deep Learning Revolution",
    "section": "Why AlexNet Was a Paradigm Shift",
    "text": "Why AlexNet Was a Paradigm Shift\nAlexNet proved several critical points:\n\nDepth matters: Deeper networks learn more powerful representations\nData scale matters: Large datasets (ImageNet’s 1.2M images) enable better learning\nCompute matters: GPUs make training deep networks practical\nLearned features win: Automated feature learning beats hand-crafted features\n\nBefore AlexNet, these points were debated. After AlexNet, they became accepted wisdom. The deep learning revolution had begun.\nWithin months, researchers worldwide abandoned hand-crafted features. Every computer vision competition became a deep learning competition. Companies invested billions in GPU infrastructure. The entire field transformed.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 2: The Deep Learning Revolution"
    ]
  },
  {
    "objectID": "m05-images/02-the-deep-learning-revolution.html#from-revolution-to-practice",
    "href": "m05-images/02-the-deep-learning-revolution.html#from-revolution-to-practice",
    "title": "Part 2: The Deep Learning Revolution",
    "section": "From Revolution to Practice",
    "text": "From Revolution to Practice\nAlexNet demonstrated that deep learning works at scale. But how do we actually use these powerful models in practice? How do we understand what’s happening inside these black boxes? And how did researchers push even further, building networks with hundreds of layers?\nThese questions lead us to the practical skills and advanced architectures we’ll explore in the remaining sections. You now understand the paradigm shift. Next, you’ll learn to harness it.\n:style: unsrt",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 2: The Deep Learning Revolution"
    ]
  },
  {
    "objectID": "m05-images/02-the-deep-learning-revolution.html#summary",
    "href": "m05-images/02-the-deep-learning-revolution.html#summary",
    "title": "Part 2: The Deep Learning Revolution",
    "section": "Summary",
    "text": "Summary\nWe traced computer vision’s evolution from hand-crafted features to learned representations. Traditional approaches required experts to design edge detectors, Fourier transforms, and pattern recognizers for each task. LeNet pioneered automated feature learning in the 1990s, showing that networks could discover useful patterns through training. But computational limits constrained its impact.\nAlexNet’s 2012 breakthrough on ImageNet demonstrated that deep learning works at scale. Key innovations included ReLU activation (solving vanishing gradients), Dropout (preventing overfitting), and GPU acceleration (enabling large-scale training). The 10+ percentage point improvement shocked the computer vision community and sparked the deep learning revolution.\nThis paradigm shift transformed how we approach machine perception. Networks now learn features automatically from data, outperforming carefully engineered alternatives. The question is no longer whether deep learning works, but how to apply it effectively.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 2: The Deep Learning Revolution"
    ]
  },
  {
    "objectID": "m05-images/03-using-cnn-models.html",
    "href": "m05-images/03-using-cnn-models.html",
    "title": "Part 3: Using CNN Models",
    "section": "",
    "text": "What you’ll learn\n\n\n\nThis section transforms you into a CNN practitioner. We explore the fundamental building blocks (convolution, pooling, stride, padding), understand key properties like translation equivariance, learn to use pre-trained models from torchvision, and master transfer learning techniques for adapting models to new tasks.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 3: Using CNN Models"
    ]
  },
  {
    "objectID": "m05-images/03-using-cnn-models.html#understanding-cnn-building-blocks",
    "href": "m05-images/03-using-cnn-models.html#understanding-cnn-building-blocks",
    "title": "Part 3: Using CNN Models",
    "section": "Understanding CNN Building Blocks",
    "text": "Understanding CNN Building Blocks\nAlexNet proved that deep learning works at scale. But how do these networks actually process images? Let’s break down the fundamental operations that make CNNs powerful.\n\nConvolutional Layers: Learnable Pattern Detectors\nAt the heart of CNNs lies a remarkably elegant operation called convolution. Imagine sliding a small window (a kernel or filter) across an image. At each position, we multiply the kernel values by the overlapping image pixels and sum the results. This produces a single output value. Repeat across all positions to create an output feature map.\n\n\n\n```oohdnfju https://anhreynolds.com/img/cnn.png\n\n\n\n\nwidth: 100%\n\n\nname: convolution-operation\n\n\nalign: center\n\n\n\nConvolution operation. The kernel slides across the input, computing weighted sums at each position to produce a feature map.\n\nMathematically, for a single-channel input (grayscale image), 2D convolution is:\n\n$$\n(I * K)_{i,j} = \\sum_{m=0}^{L-1}\\sum_{n=0}^{L-1} I_{i+m,j+n} \\cdot K_{m,n}\n$$\n\nwhere $I$ is the input image, $K$ is the kernel of size $L \\times L$, and $(i,j)$ specifies the output position.\n\nWhat makes CNNs powerful is that these kernels are **learnable parameters**. During training, each kernel evolves to detect specific visual patterns. Some kernels might become edge detectors, highlighting vertical or horizontal edges. Others might respond to textures, colors, or more complex patterns. The network discovers useful features automatically.\n\nReal-world images have multiple channels (RGB). Convolution extends naturally to 3D inputs using 3D kernels:\n\n$$\n(I * K)_{i,j} = \\sum_{c=1}^{C}\\sum_{m=0}^{L-1}\\sum_{n=0}^{L-1} I_{c,i+m,j+n} \\cdot K_{c,m,n}\n$$\n\nwhere $C$ is the number of input channels. Each kernel processes all channels simultaneously, combining color information into a single output value.\n\n```{figure} https://d2l.ai/_images/conv-multi-in.svg\n---\nwidth: 100%\nname: multi-channel-convolution\nalign: center\n---\nMulti-channel convolution. Each kernel processes all input channels, producing one output feature map.\n\n\n\n\n\n\nInteractive visualizations\n\n\n\nExplore CNN operations interactively:\n\nCNN Explainer shows how convolution, activation, and pooling work step-by-step\nInteractive Node-Link Visualization lets you see activations flow through a trained network\n\n\n\n\n\nTranslation Equivariance: A Key Property\nOne crucial feature of convolutional layers is translation equivariance. This means that if you shift the input, the output shifts by the same amount.\nConsider detecting a vertical edge. If the edge moves one pixel to the right in the input image, the detected edge feature also moves one pixel to the right in the output. The detection operation doesn’t care about absolute position, only relative patterns.\n\n\n\n```oohdnfju https://miro.medium.com/v2/resize:fit:1400/1*NoAQ4ZgofpkK6esl4sMHkA.png\n\n\n\n\nwidth: 80%\n\n\nname: translation-equivariance\n\n\nalign: center\n\n\n\nTranslation equivariance. The same kernel detects the same feature regardless of position in the input.\n\nThis property allows CNNs to recognize objects anywhere in an image. A cat detector learned on centered cats will also detect cats in image corners. The network doesn't need to learn separate detectors for every possible position.\n\n### Parameter Sharing: Efficient Learning\n\nUnlike fully connected networks where each weight is used once, convolutional layers **reuse kernel weights** across all spatial positions. A 3×3 kernel applied to a 224×224 RGB image uses just 27 parameters (3×3×3), not the millions required by a fully connected layer.\n\nThis weight-sharing dramatically reduces parameter count while preserving spatial relationships in the data. It's a key reason CNNs can process high-resolution images efficiently.\n\n### Receptive Field: Seeing More with Depth\n\nThe **receptive field** is the region of input pixels that influence each output pixel. In the first convolutional layer, a 3×3 kernel has a receptive field of 3×3 pixels. But as we stack layers, the receptive field grows.\n\nConsider two 3×3 convolutional layers. Each output pixel in the second layer depends on a 3×3 region in the first layer's output. But each of those positions depends on a 3×3 region in the input. So the second layer's receptive field is 5×5 in the original input.\n\n```{figure} https://www.researchgate.net/publication/316950618/figure/fig4/AS:11431281212123378@1702542797323/The-receptive-field-of-each-convolution-layer-with-a-3-3-kernel-The-green-area-marks.tif\n---\nwidth: 50%\nname: receptive-field\nalign: center\n---\nReceptive field grows with network depth. Deeper layers see increasingly large regions of the input image.\nThis hierarchical structure allows CNNs to detect increasingly complex, abstract features. Early layers detect edges and simple patterns. Middle layers combine these into textures and parts. Deep layers recognize complete objects and scenes.\n\n\nStride and Padding: Controlling Dimensions\nStride determines how many pixels we skip when sliding the kernel. With stride 1, we move one pixel at a time, creating dense feature maps. With stride 2, we skip every other position, effectively downsampling the output.\nFor a 1D example with input [a,b,c,d,e,f] and kernel [1,2]:\nStride 1: \n[1a + 2b, 1b + 2c, 1c + 2d, 1d + 2e, 1e + 2f]\n\nStride 2: \n[1a + 2b, 1c + 2d, 1e + 2f]\n\nLarger strides reduce computational cost and increase the receptive field, but might miss fine details.\n\n\n\n```oohdnfju https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRJTOGcDwPXtlNnev9ayPj92FIysGddxe__Fw&s\n\n\n\n\nwidth: 50%\n\n\nname: stride-visualization\n\n\nalign: center\n\n\n\nStride controls how far the kernel moves at each step. Stride 2 produces half the spatial dimensions of stride 1.\n\n**Padding** addresses information loss at borders. Without padding (called \"valid\" padding), the output shrinks after each convolution because the kernel can't fully overlap with border pixels. Zero padding adds a border of zeros around the input, allowing the kernel to process edge pixels and control output dimensions.\n\n```{figure} https://svitla.com/uploads/ckeditor/2024/Math%20at%20the%20heart%20of%20CNN/image_930660943761713546482755.gif\n---\nwidth: 50%\nname: padding-visualization\nalign: center\n---\nZero padding extends the input with zeros, preserving spatial dimensions and processing border pixels.\nFor a square input of size W with kernel size K, stride S, and padding P, the output dimension is:\n\nO = \\left\\lfloor\\frac{W - K + 2P}{S}\\right\\rfloor + 1\n\nExample: 224×224 input, 3×3 kernel, stride 2, padding 1:\n\nO = \\left\\lfloor\\frac{224 - 3 + 2(1)}{2}\\right\\rfloor + 1 = 112\n\nThe interplay between stride and padding lets network designers control spatial dimensions and computational efficiency. Try the Convolution Visualizer to experiment with different stride and padding settings interactively.\n\n\nPooling Layers: Downsampling with Invariance\nPooling layers downsample feature maps, reducing spatial dimensions while preserving important information. Max pooling selects the maximum value in each local window:\n\nP_{i,j} = \\max_{m,n} F_{si+m,sj+n}\n\nwhere F is the feature map, s is the stride (typically equal to the window size), and (m,n) range over the pooling window.\nAverage pooling computes the mean instead:\n\nP_{i,j} = \\frac{1}{w^2}\\sum_{m=0}^{w-1}\\sum_{n=0}^{w-1} F_{si+m,sj+n}\n\nMax pooling creates local translation invariance. If an edge moves slightly within a pooling window, the maximum value (and thus the output) remains unchanged. This helps the network focus on whether a feature is present, not its exact position.\nPooling also reduces computational cost in subsequent layers by decreasing spatial dimensions. A common pattern is to double the number of channels while halving spatial dimensions, maintaining roughly constant computational load across layers. Some recent architectures replace pooling with strided convolutions, arguing that learnable downsampling might be more effective {footcite}springenberg2015striving. The choice involves trade-offs between parameter efficiency and flexibility.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 3: Using CNN Models"
    ]
  },
  {
    "objectID": "m05-images/03-using-cnn-models.html#using-pre-trained-models",
    "href": "m05-images/03-using-cnn-models.html#using-pre-trained-models",
    "title": "Part 3: Using CNN Models",
    "section": "Using Pre-Trained Models",
    "text": "Using Pre-Trained Models\nNow that we understand CNN building blocks, let’s use them in practice. Training a CNN from scratch on ImageNet requires weeks of GPU time. But we can leverage pre-trained models trained by research labs with vast computational resources.\n\nLoading Models from torchvision\nPyTorch’s torchvision.models provides pre-trained implementations of major architectures:\n\nimport torch\nimport torchvision.models as models\nfrom torchvision import transforms\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\n# Load a pre-trained ResNet-50 model\nresnet50 = models.resnet50(pretrained=True)\nresnet50.eval()  # Set to evaluation mode\n\nprint(f\"Model type: {type(resnet50)}\")\nprint(f\"Number of parameters: {sum(p.numel() for p in resnet50.parameters()):,}\")\n\nThe model is trained on ImageNet with 1000 classes. Let’s use it to classify an image.\n\n\nImage Classification Example\nTo use a pre-trained model, we must preprocess images the same way they were during training. ImageNet models expect:\n\nImages resized to 224×224 (or 299×299 for some models)\nPixel values normalized to mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n\n\n# Define the preprocessing transform\npreprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    ),\n])\n\n# Load and preprocess an image\nurl = \"https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg\"\nresponse = requests.get(url)\nimg = Image.open(BytesIO(response.content))\n\n# Display original image\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(6, 6))\nplt.imshow(img)\nplt.title(\"Input Image\")\nplt.axis(\"off\")\nplt.show()\n\n# Preprocess and add batch dimension\ninput_tensor = preprocess(img)\ninput_batch = input_tensor.unsqueeze(0)\n\nprint(f\"Input shape: {input_batch.shape}\")  # [1, 3, 224, 224]\n\nNow classify the image:\n\n# Perform inference\nwith torch.no_grad():\n    output = resnet50(input_batch)\n\n# Output is logits for 1000 classes\nprint(f\"Output shape: {output.shape}\")  # [1, 1000]\n\n# Convert to probabilities\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\n\n# Get top 5 predictions\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\n\n# Load ImageNet class labels\nurl = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\nresponse = requests.get(url)\nlabels = response.json()\n\n# Display predictions\nprint(\"\\nTop 5 predictions:\")\nfor i in range(5):\n    print(f\"{i+1}. {labels[top5_catid[i]]}: {top5_prob[i].item()*100:.2f}%\")\n\nThe model correctly identifies the object with high confidence. This demonstrates the power of pre-trained networks: they’ve learned rich visual representations from ImageNet’s diverse images.\n\n\nWhen to Use Which Architecture\nDifferent architectures offer trade-offs between accuracy, speed, and memory:\nResNet-50: Excellent general-purpose model. Good accuracy, reasonable speed. Default choice for most applications.\nEfficientNet: Optimized for mobile and edge devices. Best accuracy-per-parameter ratio.\nVGG-16: Simple architecture, easy to understand. Larger and slower than modern alternatives.\nMobileNet: Designed for mobile deployment. Fast inference, lower accuracy.\nVision Transformer (ViT): State-of-the-art accuracy on large datasets. Requires more data and compute.\nFor most applications, start with ResNet-50. It provides strong performance across diverse tasks. Optimize for speed or accuracy later if needed.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 3: Using CNN Models"
    ]
  },
  {
    "objectID": "m05-images/03-using-cnn-models.html#transfer-learning-adapting-pre-trained-models",
    "href": "m05-images/03-using-cnn-models.html#transfer-learning-adapting-pre-trained-models",
    "title": "Part 3: Using CNN Models",
    "section": "Transfer Learning: Adapting Pre-Trained Models",
    "text": "Transfer Learning: Adapting Pre-Trained Models\nPre-trained models learn general visual features from ImageNet’s 1000 categories. But what if you want to classify different objects? Transfer learning adapts these models to new tasks.\n\nWhy Transfer Learning Works\nImageNet-trained models learn a hierarchy of features:\n\nEarly layers: Detect edges, colors, simple textures (universal across tasks)\nMiddle layers: Detect patterns, parts, compositions (somewhat task-specific)\nLate layers: Detect complete objects (ImageNet-specific)\n\nThe early and middle layers learn representations useful for many vision tasks. We can reuse these features and only retrain the final layers for our specific problem.\n\n\nTwo Approaches: Feature Extraction vs. Fine-Tuning\nFeature Extraction: Freeze all convolutional layers, only train a new classifier head. Fast and works well with small datasets.\nFine-Tuning: Initialize with pre-trained weights, then train the entire network (or parts of it) on your data. Better accuracy but requires more data and computation.\n\n\nExample: Fine-Tuning for Custom Classification\nLet’s adapt ResNet-50 to classify 10 animal species:\n\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Load pre-trained ResNet-50\nmodel = models.resnet50(pretrained=True)\n\n# Replace the final fully connected layer\n# Original: 2048 -&gt; 1000 (ImageNet classes)\n# New: 2048 -&gt; 10 (our custom classes)\nnum_classes = 10\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nprint(f\"Modified final layer: {model.fc}\")\n\nFor feature extraction, freeze early layers:\n\n# Freeze all layers except the final classifier\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Unfreeze the final layer\nfor param in model.fc.parameters():\n    param.requires_grad = True\n\n# Count trainable parameters\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(f\"Trainable parameters: {trainable_params:,} / {total_params:,}\")\n\nOnly 20,490 parameters (the final layer) are trainable. This makes training fast and prevents overfitting on small datasets.\n\n\nTraining Loop\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n\n# Training loop (pseudo-code, requires actual data)\ndef train_epoch(model, dataloader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for inputs, labels in dataloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    epoch_loss = running_loss / len(dataloader)\n    epoch_acc = correct / total\n    return epoch_loss, epoch_acc\n\n# For fine-tuning instead of feature extraction:\n# 1. Unfreeze all or some layers\n# 2. Use a smaller learning rate (e.g., 1e-4 or 1e-5)\n# 3. Train for more epochs\n\n\n\nData Augmentation: Essential for Small Datasets\nWhen training on limited data, augmentation is crucial. Transform each image differently each epoch to artificially expand the training set:\n\nfrom torchvision import transforms\n\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(224),      # Random crop and resize\n    transforms.RandomHorizontalFlip(),       # Flip with 50% probability\n    transforms.ColorJitter(                  # Random brightness, contrast\n        brightness=0.2,\n        contrast=0.2,\n        saturation=0.2\n    ),\n    transforms.RandomRotation(15),           # Rotate up to 15 degrees\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    ),\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    ),\n])\n\nNote that validation uses deterministic transforms (no randomness) for reproducible evaluation.\n\n\nBest Practices for Transfer Learning\nStart with feature extraction: Train only the final layer first. This is fast and often achieves good results.\nThen try fine-tuning: If accuracy is insufficient, unfreeze earlier layers and train with a small learning rate (10× smaller than initial training).\nUse learning rate schedules: Reduce the learning rate when validation loss plateaus. This helps the model converge to better solutions.\nMonitor for overfitting: Use validation data to detect when the model stops generalizing. Apply more augmentation or stronger regularization (dropout, weight decay) if needed.\nMatch preprocessing: Always use the same normalization as the pre-training dataset (ImageNet statistics for most models).\n\n\n\n\n\n\nTry it yourself\n\n\n\nPractice transfer learning on your own image dataset:\n\nCollect 100-500 images per class (even phone camera photos work)\nSplit into train/val/test sets (70/15/15)\nStart with ResNet-50 feature extraction\nTrain for 10-20 epochs\nEvaluate on test set\n\nYou’ll likely achieve 80-90%+ accuracy with just a few hundred images per class, demonstrating the power of pre-trained features.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 3: Using CNN Models"
    ]
  },
  {
    "objectID": "m05-images/03-using-cnn-models.html#visualizing-what-networks-learn",
    "href": "m05-images/03-using-cnn-models.html#visualizing-what-networks-learn",
    "title": "Part 3: Using CNN Models",
    "section": "Visualizing What Networks Learn",
    "text": "Visualizing What Networks Learn\nLet’s peek inside a trained network to see what features it detects:\n\n# Extract intermediate feature maps\ndef get_activation(name, activations):\n    def hook(model, input, output):\n        activations[name] = output.detach()\n    return hook\n\n# Register hooks to capture activations\nactivations = {}\nmodel.layer1[0].conv1.register_forward_hook(get_activation('layer1', activations))\nmodel.layer2[0].conv1.register_forward_hook(get_activation('layer2', activations))\nmodel.layer3[0].conv1.register_forward_hook(get_activation('layer3', activations))\n\n# Run inference\nwith torch.no_grad():\n    _ = model(input_batch)\n\n# Visualize first layer activations\nlayer1_act = activations['layer1'][0]  # [C, H, W]\n\nfig, axes = plt.subplots(4, 8, figsize=(16, 8))\nfor i, ax in enumerate(axes.flat):\n    if i &lt; layer1_act.shape[0]:\n        ax.imshow(layer1_act[i].cpu(), cmap='viridis')\n    ax.axis('off')\nplt.suptitle(\"Layer 1 Feature Maps\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\nEarly layers show edge detection and simple patterns. Deeper layers show increasingly abstract features that are harder to interpret but encode high-level semantic information.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 3: Using CNN Models"
    ]
  },
  {
    "objectID": "m05-images/03-using-cnn-models.html#summary",
    "href": "m05-images/03-using-cnn-models.html#summary",
    "title": "Part 3: Using CNN Models",
    "section": "Summary",
    "text": "Summary\nWe explored the building blocks that make CNNs powerful: convolution operations with learnable kernels, translation equivariance that enables position-invariant recognition, parameter sharing for efficiency, growing receptive fields through depth, stride and padding for dimension control, and pooling for downsampling with invariance.\nWe learned to use pre-trained models from torchvision for immediate deployment. Transfer learning lets us adapt these models to custom tasks through feature extraction (training only the classifier) or fine-tuning (training the entire network with small learning rates). Data augmentation artificially expands small datasets, preventing overfitting.\nThese practical skills transform you from understanding CNNs conceptually to deploying them on real problems. You can now load state-of-the-art models, adapt them to your data, and achieve strong results with limited computational resources.\n:style: unsrt",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 3: Using CNN Models"
    ]
  },
  {
    "objectID": "m05-images/04-cnn-innovations.html",
    "href": "m05-images/04-cnn-innovations.html",
    "title": "Part 4: The Innovation Timeline",
    "section": "",
    "text": "What you’ll learn\n\n\n\nThis section traces CNN evolution through successive innovations. We explore how each architecture emerged to solve specific challenges: VGG demonstrated that depth matters, Inception showed how to compute efficiently, ResNet enabled training very deep networks through skip connections, and Vision Transformers replaced convolution with self-attention for global context. Each innovation builds on its predecessors, creating a narrative of problem-solving that shapes modern computer vision.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 4: CNN Innovations"
    ]
  },
  {
    "objectID": "m05-images/04-cnn-innovations.html#the-quest-for-depth-and-efficiency",
    "href": "m05-images/04-cnn-innovations.html#the-quest-for-depth-and-efficiency",
    "title": "Part 4: The Innovation Timeline",
    "section": "The Quest for Depth and Efficiency",
    "text": "The Quest for Depth and Efficiency\nAlexNet proved that deep learning works at scale in 2012. This breakthrough sparked a race to improve CNN architectures. But simply adding more layers didn’t work. Networks deeper than 20 layers degraded during training. Computational costs exploded. Memory constraints limited model size.\nThe innovations we’ll explore emerged as solutions to these challenges. Each architecture addressed specific problems while introducing ideas that influenced everything that followed. This is not a random collection of models, but a coherent story of progress through clever problem-solving.\n\n\n\n```grlinmyv https://viso.ai/wp-content/uploads/2024/02/imagenet-winners-by-year.jpg\n\n\n\n\nwidth: 100%\n\n\nalign: center\n\n\nname: imagenet-timeline\n\n\n\nImageNet competition winners from 2012 to 2017. Each year brought architectural innovations that pushed accuracy higher.\n\n## Challenge 1: Going Deeper (VGG, 2014)\n\nAlexNet demonstrated the power of depth with 8 layers. But could we go deeper? By 2014, researchers at Oxford's Visual Geometry Group posed this question directly.\n\n### The Depth Hypothesis\n\nThe intuition was compelling. Deeper networks should learn more complex representations. Early layers detect simple edges and colors. Middle layers combine these into textures and parts. Deep layers recognize complete objects and scenes. More layers mean more abstraction.\n\nBut training deep networks in 2014 remained difficult. Gradients vanished. Training took weeks. Most researchers stuck with networks under 20 layers.\n\n### VGG's Answer: Stacked 3×3 Convolutions\n\nVGGNet demonstrated that systematic depth works {footcite}`simonyan2014very`. The key insight was using **small 3×3 convolutions** exclusively, stacked together to build deep networks.\n\n```{figure} ../archive/figs/vgg-architecture.jpg\n---\nwidth: 50%\nalign: center\nname: vgg-architecture\n---\nVGG16 architecture showing progressive downsampling while doubling channels. The network uses only 3×3 convolutions throughout.\nWhy stack 3×3 filters instead of using larger filters? Consider the receptive field. Two 3×3 convolutions have the same receptive field as one 5×5 convolution (both see a 5×5 region of the input). But the stacked version has fewer parameters.\nFor a single 5×5 convolution:\n\n\\text{parameters} = 5 \\times 5 = 25\n\nFor two stacked 3×3 convolutions:\n\n\\text{parameters} = 2 \\times (3 \\times 3) = 18\n\nThis yields a 28% parameter reduction while adding an extra ReLU nonlinearity between the layers, allowing the network to learn more complex functions.\n\n\n\n```grlinmyv https://miro.medium.com/v2/resize:fit:1200/1*k97NVvlMkRXau-uItlq5Gw.png\n\n\n\n\nwidth: 70%\n\n\nalign: center\n\n\nname: receptive-field-stacking\n\n\n\nTwo stacked 3×3 convolutions achieve the same receptive field as one 5×5 convolution but with fewer parameters and added nonlinearity.\n\n### The Architecture Pattern\n\nVGG introduced a clean, systematic pattern that influenced all subsequent architectures:\n\n**After each pooling layer, double the channels**:\n\n$$\n\\text{channels} = \\{64 \\to 128 \\to 256 \\to 512 \\to 512\\}\n$$\n\n**Spatial dimensions halve**:\n\n$$\n\\text{spatial dimensions} = \\{224 \\to 112 \\to 56 \\to 28 \\to 14 \\to 7\\}\n$$\n\nThis creates a pyramid structure where computational cost per layer stays roughly constant. As spatial dimensions decrease, increasing channel depth compensates by expanding representational capacity.\n\nVGG16 (16 layers) and VGG19 (19 layers) achieved strong results on ImageNet, validating that systematic depth improves accuracy. The architecture's simplicity made it easy to understand and implement, contributing to its widespread adoption.\n\n### The Limitation\n\nVGG16 contains approximately **140 million parameters**. The majority (102 million) concentrate in the first fully connected layer. This massive parameter count means:\n\n- Training requires significant computational resources\n- Inference is memory-intensive\n- The model is prone to overfitting without strong regularization\n\nThe question became: can we achieve similar accuracy with fewer parameters?\n\n## Challenge 2: Computing Efficiently (Inception/GoogLeNet, 2014)\n\nWhile VGG pushed depth systematically, researchers at Google asked a different question: how do we capture multi-scale features efficiently?\n\n### Multi-Scale Feature Extraction\n\nLook at a photograph. Some objects are large and occupy significant image area. Others are small details. To recognize both, the network needs to examine features at multiple scales simultaneously.\n\nTraditional CNN layers use a single kernel size (like 3×3). But the optimal kernel size varies by context. Large kernels capture broad patterns. Small kernels detect fine details.\n\n**Inception's answer**: use multiple kernel sizes in parallel within the same layer {footcite}`szegedy2015going`.\n\n### The 1×1 Convolution Trick\n\nRunning multiple large convolutions in parallel is computationally expensive. Inception solves this through **1×1 convolutions** for channel dimensionality reduction.\n\nAt first, 1×1 convolutions seem strange. They don't look at neighboring pixels, only at different channels at the same location. But this is precisely their power. They compress information across channels before applying larger, more expensive filters.\n\nConsider a 3×3 convolution on a 256-channel feature map producing 256 output channels:\n\n$$\n\\text{parameters (without reduction)} = 3 \\times 3 \\times 256 \\times 256 = 589{,}824\n$$\n\nWith a 1×1 convolution reducing to 64 channels first:\n\n$$\n\\text{parameters (with reduction)} = (1 \\times 1 \\times 256 \\times 64) + (3 \\times 3 \\times 64 \\times 256) = 163{,}840\n$$\n\nThis achieves a **72% parameter reduction** while maintaining similar expressive power.\n\nThe theoretical motivation behind 1×1 convolutions is elegant. Inception approximates sparse connectivity. Not every pixel needs to connect to every pixel in the next layer. The 1×1 convolutions sparsify connections efficiently by operating primarily across channels rather than spatial dimensions {footcite}`paperswithcode-inception`.\n\n### The Inception Module\n\nEach Inception module contains four parallel branches:\n\n1. **1×1 convolution**: Captures point-wise patterns\n2. **1×1 → 3×3 convolution**: Captures medium-scale patterns (with reduction)\n3. **1×1 → 5×5 convolution**: Captures large-scale patterns (with reduction)\n4. **3×3 max pooling → 1×1 convolution**: Preserves spatial structure differently\n\nThese branches process the same input simultaneously. Their outputs concatenate along the channel dimension, creating a multi-scale representation.\n\nMathematically, for input $X$:\n\n$$\nY_{\\text{inception}} = \\text{Concat}\\big(Y_{1\\times1}, \\,Y_{3\\times3}, \\,Y_{5\\times5}, \\,Y_{\\text{pool}}\\big)\n$$\n\nwhere each $Y$ represents the output of its respective branch.\n\n### Global Average Pooling\n\nVGG's fully connected layers contain 102 million parameters. Inception eliminates this bottleneck through **global average pooling** {footcite}`lin2013network`.\n\nInstead of flattening feature maps and passing through dense layers, take the average value of each channel across all spatial positions. For a feature map with 1000 channels, this produces a 1000-dimensional vector directly, regardless of spatial size. This:\n\n- Drastically reduces parameters (no heavy fully connected layers)\n- Creates translation invariance (averaging eliminates spatial dependence)\n- Reduces overfitting risk\n\n### Auxiliary Classifiers\n\nGoogLeNet introduced **auxiliary classifiers** at intermediate layers to combat vanishing gradients in deep networks. These classifiers attach to middle layers, computing losses that provide additional gradient signals during backpropagation.\n\n```{figure} https://production-media.paperswithcode.com/methods/GoogleNet-structure-and-auxiliary-classifier-units_CM5xsxk.png\n---\nwidth: 80%\nalign: center\nname: inception-auxiliary\n---\nGoogLeNet architecture with auxiliary classifiers attached to intermediate layers to improve gradient flow.\nDuring training, the total loss combines the main classifier loss with auxiliary losses (typically weighted at 0.3). At inference, only the main classifier is used.\n\nThe Impact\nGoogLeNet achieved accuracy comparable to VGG with 12× fewer parameters. This demonstrated that architecture efficiency matters as much as depth. The Inception ideas influenced countless subsequent designs.\nLater versions pushed these ideas further:\n\nInception v2/v3: Added batch normalization, factorized larger filters (5×5 → two 3×3)\nInception v4: Integrated with residual connections\nXception: Used depthwise separable convolutions, pushing channel-spatial separation further\n\nBatch Normalization, introduced around this time {footcite}ioffe2015batch, normalizes layer activations to zero mean and unit variance. This stabilizes training and allows higher learning rates. It became standard in nearly all subsequent architectures.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 4: CNN Innovations"
    ]
  },
  {
    "objectID": "m05-images/04-cnn-innovations.html#challenge-3-training-very-deep-networks-resnet-2015",
    "href": "m05-images/04-cnn-innovations.html#challenge-3-training-very-deep-networks-resnet-2015",
    "title": "Part 4: The Innovation Timeline",
    "section": "Challenge 3: Training Very Deep Networks (ResNet, 2015)",
    "text": "Challenge 3: Training Very Deep Networks (ResNet, 2015)\nBy 2015, researchers wanted networks with 50, 100, or even 150 layers. But a puzzling phenomenon blocked progress: networks deeper than about 20 layers exhibited degradation.\n\nThe Degradation Problem\nHere’s what was strange. Add more layers to a working network and training error increases. Not test error (which would indicate overfitting). Training error itself gets worse.\nThis shouldn’t happen. A deeper network could theoretically learn the identity function for extra layers, matching the shallower network’s performance. But in practice, optimization failed. The deeper network couldn’t even learn to copy what the shallower network already achieved.\nThis wasn’t vanishing gradients alone (batch normalization addressed that). This was a fundamental optimization difficulty.\n\n\nThe Residual Learning Solution\nMicrosoft Research proposed an elegant solution: skip connections {footcite}he2016deep.\nInstead of learning a direct mapping H(\\mathbf{x}) from input \\mathbf{x} to output, learn the residual F(\\mathbf{x}) = H(\\mathbf{x}) - \\mathbf{x}. Then add the input back:\n\nH(\\mathbf{x}) = F(\\mathbf{x}) + \\mathbf{x}\n\n\n\n\n```grlinmyv https://www.researchgate.net/publication/364330795/figure/fig7/AS:11431281176036099@1689999593116/Basic-residual-block-of-ResNet.png\n\n\n\n\nwidth: 60%\n\n\nalign: center\n\n\nname: resnet-block\n\n\n\nResidual block. The skip connection carries the input directly to the output, while convolutional layers learn the residual.\n\nWhy does this help? If the optimal mapping is close to identity (the layer isn't very useful), the network can easily learn $F(\\mathbf{x}) \\approx 0$ by pushing weights toward zero. The skip connection ensures input information flows through unchanged.\n\nIf a more complex transformation is needed, $F(\\mathbf{x})$ can still learn it. The skip connection doesn't constrain what the block can represent. It just makes optimization easier by providing a gradient highway.\n\n### Ensemble-Like Gradient Flow\n\nSkip connections create multiple paths for gradients to flow backward. Some paths go through all convolutions. Others skip multiple blocks via cascaded skip connections. This ensemble of paths accelerates training and prevents vanishing gradients {footcite}`veit2016residual`.\n\n```{figure} https://arxiv.org/html/2405.01725v1/x28.png\n---\nwidth: 100%\nalign: center\nname: resnet-gradient-flow\n---\nMultiple gradient paths in ResNet. Gradients can skip layers via identity connections, providing stable training for very deep networks.\n\n\nBottleneck Blocks for Deeper Networks\nResNet-50, -101, and -152 use bottleneck blocks to maintain efficiency:\n\n1×1 convolution: Reduces channel dimension (e.g., 256 → 64)\n3×3 convolution: Operates on reduced dimension\n1×1 convolution: Restores dimension (e.g., 64 → 256)\n\n\n\n\n```grlinmyv https://i.sstatic.net/kbiIG.png\n\n\n\n\nwidth: 80%\n\n\nalign: center\n\n\nname: resnet-bottleneck\n\n\n\nBottleneck block (left) vs. basic block (right). The bottleneck design reduces computational cost in very deep networks.\n\nThis shrinks the intermediate feature map, dramatically reducing computational cost while maintaining representational capacity. The design was inspired by Inception's bottleneck idea.\n\n### The Results\n\nResNet achieved:\n\n- **152 layers** trained successfully without degradation\n- **Top-5 error of 3.57%** on ImageNet (better than human performance on the test set)\n- Widespread adoption across computer vision tasks\n\nThe impact extended beyond CNNs. Skip connections appeared in:\n\n- **U-Net** for medical image segmentation\n- **DenseNet** which connects every layer to every other layer\n- **Transformers** for natural language processing\n- Nearly all modern deep architectures\n\nResNet showed that with the right architecture, depth isn't a limitation. It's a resource.\n\n### ResNeXt: Width Through Cardinality\n\n**ResNeXt** {footcite}`xie2017aggregated` extended ResNet by increasing network **width** through grouped convolutions rather than just adding depth or channels.\n\nThe idea: split the bottleneck convolution path into multiple parallel groups (typically 32), each processing independently. Aggregate their outputs through concatenation or addition.\n\n```{figure} https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-06_at_4.32.52_PM.png\n---\nwidth: 60%\nalign: center\nname: resnext-block\n---\nResNeXt block with multiple grouped convolution paths. Increasing cardinality (number of groups) often improves accuracy more than increasing depth or channel count.\nThis “cardinality” dimension provides another axis for scaling networks. ResNeXt achieves better accuracy than ResNet at similar computational cost by increasing cardinality instead of just going deeper.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 4: CNN Innovations"
    ]
  },
  {
    "objectID": "m05-images/04-cnn-innovations.html#challenge-4-global-context-vision-transformer-2020",
    "href": "m05-images/04-cnn-innovations.html#challenge-4-global-context-vision-transformer-2020",
    "title": "Part 4: The Innovation Timeline",
    "section": "Challenge 4: Global Context (Vision Transformer, 2020)",
    "text": "Challenge 4: Global Context (Vision Transformer, 2020)\nCNNs build global understanding slowly through stacked local operations. Early layers see only small patches (3×3 or 5×5 regions). Deeper layers expand the receptive field, but even in deep networks, truly global connections require many layers.\nWhat if we could capture global relationships immediately?\n\nThe Self-Attention Mechanism\nVision Transformers (ViT) replace convolution with self-attention {footcite}dosovitskiy2020image.\nSelf-attention computes relationships between all positions simultaneously. For each patch of the image, it determines which other patches are relevant, regardless of distance. This provides immediate global context.\nThe mechanism works as follows. Given input features X, compute three matrices through learned linear projections:\n\nQ = XW_Q, \\quad K = XW_K, \\quad V = XW_V\n\nwhere Q (queries), K (keys), and V (values) represent different views of the input.\nAttention scores measure similarity between queries and keys:\n\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\nThis computes a weighted average of values, where weights depend on query-key similarity. Intuitively, each position “asks” (via its query) what information to gather from all other positions (via their keys), then aggregates their values accordingly.\n\n\nPatches as Tokens\nViT treats images like text. Divide the image into fixed-size patches (typically 16×16). Flatten each patch into a vector. Treat these vectors as “tokens” (analogous to words in NLP).\nAdd positional encodings to preserve spatial information (since self-attention is permutation-invariant). Pass through a standard Transformer encoder with multiple self-attention layers.\nA special CLS token prepended to the sequence gathers global information. After all Transformer layers, the CLS token’s representation feeds into the classification head.\n\n\nTrade-offs: Data and Compute\nVision Transformers achieve state-of-the-art accuracy on large datasets like ImageNet-21k (14 million images). But they have important trade-offs:\nAdvantages: - Global receptive field from the first layer - Better scaling properties with dataset size - Unified architecture for vision and language (same Transformer)\nDisadvantages: - Require more training data than CNNs (less inductive bias) - Higher computational cost (self-attention is O(n^2) in sequence length) - Less effective on small datasets without strong augmentation\nFor most practical applications, CNNs remain competitive. Use ViT when you have:\n\nLarge datasets (millions of images)\nSubstantial computational resources\nTasks benefiting from global context (scene understanding, fine-grained classification)\n\nUse CNNs when you have:\n\nLimited data (thousands of images)\nConstrained compute (edge devices, mobile)\nTasks benefiting from spatial locality (object detection, segmentation)\n\n\n\nHybrid Approaches\nRecent research combines CNN and Transformer strengths:\n\nSwin Transformer {footcite}liu2021swin: Uses local attention windows (more like convolution) with hierarchical structure\nCoAtNet {footcite}dai2021coatnet: Combines convolution in early layers with attention in later layers\nConvNeXt {footcite}liu2022convnet: Shows that modernized CNNs can match Transformer performance\n\nThe field continues evolving, blending ideas from both paradigms.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 4: CNN Innovations"
    ]
  },
  {
    "objectID": "m05-images/04-cnn-innovations.html#the-narrative-of-progress",
    "href": "m05-images/04-cnn-innovations.html#the-narrative-of-progress",
    "title": "Part 4: The Innovation Timeline",
    "section": "The Narrative of Progress",
    "text": "The Narrative of Progress\nLet’s trace the thread connecting these innovations:\n2012 (AlexNet): Depth works, but only to 8 layers ↓ 2014 (VGG): Stack small convolutions to go deeper (16-19 layers) ↓ 2014 (Inception): Use multi-scale features and 1×1 convolutions for efficiency ↓ 2015 (ResNet): Skip connections enable training very deep networks (152 layers) ↓ 2017 (ResNeXt): Increase width through cardinality, not just depth ↓ 2020 (ViT): Replace convolution with self-attention for global context\nEach innovation addressed limitations of its predecessors while preserving their insights. Modern architectures mix and match these ideas: residual connections for depth, multi-scale features for efficiency, attention for global context.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 4: CNN Innovations"
    ]
  },
  {
    "objectID": "m05-images/04-cnn-innovations.html#choosing-the-right-architecture",
    "href": "m05-images/04-cnn-innovations.html#choosing-the-right-architecture",
    "title": "Part 4: The Innovation Timeline",
    "section": "Choosing the Right Architecture",
    "text": "Choosing the Right Architecture\nFor your next computer vision project, which architecture should you choose?\nResNet-50: Default choice. Excellent accuracy-computational cost trade-off. Pre-trained weights widely available. Works well across diverse tasks.\nEfficientNet: When deployment efficiency matters. Carefully balanced depth, width, and resolution for optimal accuracy per parameter.\nMobileNet/EfficientNet-Lite: For mobile and edge devices. Sacrifices some accuracy for fast inference and small model size.\nVision Transformer (ViT): When you have large datasets (millions of images) and substantial compute. State-of-the-art accuracy on challenging benchmarks.\nSwin Transformer: When you want Transformer benefits with more reasonable compute requirements. Good for dense prediction tasks.\nStart with ResNet-50. It provides strong performance across almost all applications. Optimize later if specific constraints (speed, memory, accuracy) demand it.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 4: CNN Innovations"
    ]
  },
  {
    "objectID": "m05-images/04-cnn-innovations.html#summary",
    "href": "m05-images/04-cnn-innovations.html#summary",
    "title": "Part 4: The Innovation Timeline",
    "section": "Summary",
    "text": "Summary\nWe traced CNN evolution through successive innovations solving specific challenges. VGG demonstrated that depth matters through stacked 3×3 convolutions. Inception showed how to capture multi-scale features efficiently using 1×1 convolutions and parallel branches. ResNet enabled training very deep networks (152 layers) through skip connections that ease optimization and improve gradient flow. Vision Transformers replaced convolution with self-attention, trading inductive bias for global context at the cost of requiring more data and compute.\nEach architecture built on its predecessors’ insights. Modern networks combine ideas from all of them: residual connections for depth, multi-scale features for efficiency, attention for global understanding. Architecture design is problem-solving. Understanding why these innovations emerged helps you make informed choices for your own applications.\n:style: unsrt",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 4: CNN Innovations"
    ]
  },
  {
    "objectID": "m05-images/05-student-challenge.html",
    "href": "m05-images/05-student-challenge.html",
    "title": "Part 5: Your Challenge",
    "section": "",
    "text": "What you’ll learn in this challenge\n\n\n\nThis challenge tests your ability to choose appropriate architectures, apply transfer learning, and diagnose model behavior.\nYou’ll make decisions that practitioners face daily. Which model architecture should you use? How much data augmentation? When should you stop training? By completing this project, you’ll synthesize everything learned in this module into a working image classification system.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 5: Student Challenge"
    ]
  },
  {
    "objectID": "m05-images/05-student-challenge.html#the-challenge-image-classification-competition",
    "href": "m05-images/05-student-challenge.html#the-challenge-image-classification-competition",
    "title": "Part 5: Your Challenge",
    "section": "The Challenge: Image Classification Competition",
    "text": "The Challenge: Image Classification Competition\nYou’ve learned what images are, explored the deep learning revolution, mastered practical CNN skills, and traced the innovation timeline. Now it’s your turn to apply this knowledge to a real classification problem.\nYour task: build the best possible image classifier for a custom dataset using transfer learning and the techniques we’ve covered.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 5: Student Challenge"
    ]
  },
  {
    "objectID": "m05-images/05-student-challenge.html#dataset",
    "href": "m05-images/05-student-challenge.html#dataset",
    "title": "Part 5: Your Challenge",
    "section": "Dataset",
    "text": "Dataset\nFor this challenge, you’ll work with the CIFAR-100 dataset. It contains 60,000 color images, each 32×32 pixels, across 100 classes (600 images per class). The classes span diverse categories ranging from animals and vehicles to household objects and natural scenes.\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load CIFAR-100 dataset\ntrain_dataset = torchvision.datasets.CIFAR100(\n    root='./data', train=True, download=True, transform=transforms.ToTensor()\n)\ntest_dataset = torchvision.datasets.CIFAR100(\n    root='./data', train=False, download=True, transform=transforms.ToTensor()\n)\n\nprint(f\"Training samples: {len(train_dataset)}\")\nprint(f\"Test samples: {len(test_dataset)}\")\nprint(f\"Number of classes: {len(train_dataset.classes)}\")\nprint(f\"Image shape: {train_dataset[0][0].shape}\")\n\nLet’s visualize some examples:\n\n# Display sample images\nfig, axes = plt.subplots(4, 8, figsize=(16, 8))\nfor i, ax in enumerate(axes.flat):\n    img, label = train_dataset[i]\n    img = img.permute(1, 2, 0)  # CHW -&gt; HWC for display\n    ax.imshow(img)\n    ax.set_title(train_dataset.classes[label], fontsize=8)\n    ax.axis('off')\n\nplt.suptitle(\"Sample Images from CIFAR-100\", fontsize=16)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 5: Student Challenge"
    ]
  },
  {
    "objectID": "m05-images/05-student-challenge.html#evaluation-metric",
    "href": "m05-images/05-student-challenge.html#evaluation-metric",
    "title": "Part 5: Your Challenge",
    "section": "Evaluation Metric",
    "text": "Evaluation Metric\nYour model will be evaluated on top-1 accuracy. This is the percentage of test images where the highest-confidence prediction matches the true label.\nLet’s set some targets. The baseline to beat (random guessing with 100 classes) is 1%. A reasonable target is 70%+, which is achievable with transfer learning and good data augmentation. An excellent result is 80%+, which requires careful architecture selection and tuning.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 5: Student Challenge"
    ]
  },
  {
    "objectID": "m05-images/05-student-challenge.html#starter-code",
    "href": "m05-images/05-student-challenge.html#starter-code",
    "title": "Part 5: Your Challenge",
    "section": "Starter Code",
    "text": "Starter Code\nHere’s a template to get you started. You’ll need to fill in key components and make design decisions.\n\nStep 1: Data Loading and Augmentation\n\nfrom torchvision import transforms, models\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import random_split\n\n# TODO: Design your data augmentation strategy\ntrain_transform = transforms.Compose([\n    # Add your augmentation techniques here\n    # Suggestions: RandomCrop, RandomHorizontalFlip, ColorJitter, RandomRotation\n    transforms.Resize(224),  # Resize to match ImageNet pre-training\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],  # ImageNet statistics\n        std=[0.229, 0.224, 0.225]\n    ),\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    ),\n])\n\n# Create datasets\ntrain_dataset_full = torchvision.datasets.CIFAR100(\n    root='./data', train=True, download=True, transform=train_transform\n)\ntest_dataset = torchvision.datasets.CIFAR100(\n    root='./data', train=False, download=True, transform=val_transform\n)\n\n# Split training into train/validation\ntrain_size = int(0.9 * len(train_dataset_full))\nval_size = len(train_dataset_full) - train_size\ntrain_dataset, val_dataset = random_split(\n    train_dataset_full, [train_size, val_size],\n    generator=torch.Generator().manual_seed(42)\n)\n\n# Update validation dataset transform\nval_dataset.dataset = torchvision.datasets.CIFAR100(\n    root='./data', train=True, download=True, transform=val_transform\n)\n\n# Create data loaders\nbatch_size = 128  # Adjust based on your GPU memory\ntrain_loader = DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, num_workers=2\n)\nval_loader = DataLoader(\n    val_dataset, batch_size=batch_size, shuffle=False, num_workers=2\n)\ntest_loader = DataLoader(\n    test_dataset, batch_size=batch_size, shuffle=False, num_workers=2\n)\n\nprint(f\"Train batches: {len(train_loader)}\")\nprint(f\"Val batches: {len(val_loader)}\")\nprint(f\"Test batches: {len(test_loader)}\")\n\n\n\nStep 2: Model Selection\n\n# TODO: Choose your architecture\n# Options: resnet18, resnet50, resnet101, efficientnet_b0, vgg16, mobilenet_v2, etc.\n\ndef create_model(arch='resnet50', num_classes=100, pretrained=True):\n    \"\"\"\n    Create a model for CIFAR-100 classification.\n\n    Args:\n        arch: Architecture name (e.g., 'resnet50', 'efficientnet_b0')\n        num_classes: Number of output classes\n        pretrained: Whether to use ImageNet pre-trained weights\n\n    Returns:\n        model: PyTorch model ready for training\n    \"\"\"\n    if arch == 'resnet50':\n        model = models.resnet50(pretrained=pretrained)\n        model.fc = nn.Linear(model.fc.in_features, num_classes)\n    elif arch == 'resnet18':\n        model = models.resnet18(pretrained=pretrained)\n        model.fc = nn.Linear(model.fc.in_features, num_classes)\n    elif arch == 'efficientnet_b0':\n        model = models.efficientnet_b0(pretrained=pretrained)\n        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n    # Add more architectures as needed\n\n    return model\n\n# Create your model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = create_model(arch='resnet18', num_classes=100, pretrained=True)\nmodel = model.to(device)\n\nprint(f\"Using device: {device}\")\nprint(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n\n\n\nStep 3: Training Loop\n\ndef train_epoch(model, dataloader, criterion, optimizer, device):\n    \"\"\"Train for one epoch.\"\"\"\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for inputs, labels in dataloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * inputs.size(0)\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    epoch_loss = running_loss / len(dataloader.dataset)\n    epoch_acc = correct / total\n    return epoch_loss, epoch_acc\n\n\ndef validate(model, dataloader, criterion, device):\n    \"\"\"Validate the model.\"\"\"\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    epoch_loss = running_loss / len(dataloader.dataset)\n    epoch_acc = correct / total\n    return epoch_loss, epoch_acc\n\n\n# TODO: Configure training hyperparameters\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Learning rate scheduler\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n)\n\n# Training loop\nnum_epochs = 20\nbest_val_acc = 0.0\ntrain_losses, train_accs = [], []\nval_losses, val_accs = [], []\n\nfor epoch in range(num_epochs):\n    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n    val_loss, val_acc = validate(model, val_loader, criterion, device)\n\n    train_losses.append(train_loss)\n    train_accs.append(train_acc)\n    val_losses.append(val_loss)\n    val_accs.append(val_acc)\n\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n\n    # Save best model\n    if val_acc &gt; best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), 'best_model.pth')\n        print(f\"  Saved new best model with val_acc: {val_acc:.4f}\")\n\n    # Update learning rate\n    scheduler.step(val_loss)\n\nprint(f\"\\nBest validation accuracy: {best_val_acc:.4f}\")\n\n\n\nStep 4: Evaluation and Analysis\n\n# Load best model\nmodel.load_state_dict(torch.load('best_model.pth'))\n\n# Evaluate on test set\ntest_loss, test_acc = validate(model, test_loader, criterion, device)\nprint(f\"Test Accuracy: {test_acc:.4f}\")\n\n# Plot training curves\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\nax1.plot(train_losses, label='Train Loss')\nax1.plot(val_losses, label='Val Loss')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\nax1.set_title('Training and Validation Loss')\nax1.legend()\nax1.grid(True)\n\nax2.plot(train_accs, label='Train Accuracy')\nax2.plot(val_accs, label='Val Accuracy')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Accuracy')\nax2.set_title('Training and Validation Accuracy')\nax2.legend()\nax2.grid(True)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 5: Student Challenge"
    ]
  },
  {
    "objectID": "m05-images/05-student-challenge.html#guided-questions",
    "href": "m05-images/05-student-challenge.html#guided-questions",
    "title": "Part 5: Your Challenge",
    "section": "Guided Questions",
    "text": "Guided Questions\nLet’s talk about the key decisions you’ll make. As you work on this challenge, consider these questions. Document your answers in your final report.\n\nArchitecture Selection\nQ1: Which architecture did you choose and why?\nConsider several factors. What’s the parameter count (memory constraints)? What’s the computational cost (training time)? Are pre-trained weights available? What’s the reported performance on similar tasks?\nQ2: Did you use feature extraction or fine-tuning? Why?\nFeature extraction freezes early layers and trains only the classifier. Fine-tuning updates the entire network. What guided your choice?\n\n\nData Augmentation\nQ3: What data augmentation techniques did you apply?\nCIFAR-100 images are small at 32×32 pixels. Common augmentations include random crops, horizontal flips, color jittering, random rotation, Cutout or RandomErasing, and MixUp or CutMix. Explain your choices and their expected benefits.\nQ4: How did augmentation affect your results?\nCompare training with and without augmentation. What changed?\n\n\nTraining Strategy\nQ5: What learning rate and optimizer did you use?\nDid you use a learning rate schedule? How did you choose the initial learning rate?\nQ6: How did you prevent overfitting?\nYou have several techniques at your disposal. Data augmentation helps by creating variations of training examples. Dropout randomly disables neurons during training. Weight decay (L2 regularization) penalizes large weights. Early stopping based on validation loss prevents the model from memorizing the training set. Which worked best for your model?\n\n\nPerformance Analysis\nQ7: Which classes does your model struggle with most?\nAnalyze confusion patterns. Are certain classes frequently confused? Why might this happen?\nQ8: How does performance compare to baselines?\nResearch state-of-the-art results on CIFAR-100. How does your model compare? What techniques do top-performing models use that you didn’t?",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 5: Student Challenge"
    ]
  },
  {
    "objectID": "m05-images/05-student-challenge.html#optional-extensions",
    "href": "m05-images/05-student-challenge.html#optional-extensions",
    "title": "Part 5: Your Challenge",
    "section": "Optional Extensions",
    "text": "Optional Extensions\nWant to push your understanding further? Try these advanced challenges.\n\nExtension 1: Model Ensemble\nTrain multiple models with different architectures or initializations. Combine their predictions through voting or averaging. Does the ensemble outperform individual models?\n\n# Ensemble prediction example\ndef ensemble_predict(models, dataloader, device):\n    \"\"\"\n    Ensemble prediction from multiple models.\n\n    Args:\n        models: List of PyTorch models\n        dataloader: Test data loader\n        device: Device to run inference on\n\n    Returns:\n        accuracy: Ensemble accuracy\n    \"\"\"\n    all_predictions = []\n\n    for model in models:\n        model.eval()\n        predictions = []\n\n        with torch.no_grad():\n            for inputs, _ in dataloader:\n                inputs = inputs.to(device)\n                outputs = model(inputs)\n                predictions.append(outputs.cpu())\n\n        all_predictions.append(torch.cat(predictions))\n\n    # Average predictions\n    ensemble_output = torch.stack(all_predictions).mean(dim=0)\n    ensemble_preds = ensemble_output.argmax(dim=1)\n\n    # Calculate accuracy\n    true_labels = torch.tensor([label for _, label in dataloader.dataset])\n    accuracy = (ensemble_preds == true_labels).float().mean().item()\n\n    return accuracy\n\n\n\nExtension 2: Visualize Learned Features\nExtract and visualize feature maps from intermediate layers. What patterns does your model detect?\n\ndef visualize_features(model, image, layer_name):\n    \"\"\"\n    Visualize feature maps from a specific layer.\n\n    Args:\n        model: Trained model\n        image: Input image (CHW format)\n        layer_name: Name of layer to visualize (e.g., 'layer2.0.conv1')\n    \"\"\"\n    activations = {}\n\n    def get_activation(name):\n        def hook(model, input, output):\n            activations[name] = output.detach()\n        return hook\n\n    # Register hook\n    for name, module in model.named_modules():\n        if name == layer_name:\n            module.register_forward_hook(get_activation(layer_name))\n\n    # Forward pass\n    model.eval()\n    with torch.no_grad():\n        _ = model(image.unsqueeze(0).to(device))\n\n    # Visualize activations\n    act = activations[layer_name].squeeze(0)\n    num_filters = min(32, act.shape[0])\n\n    fig, axes = plt.subplots(4, 8, figsize=(16, 8))\n    for i, ax in enumerate(axes.flat):\n        if i &lt; num_filters:\n            ax.imshow(act[i].cpu(), cmap='viridis')\n        ax.axis('off')\n\n    plt.suptitle(f\"Feature Maps from {layer_name}\", fontsize=16)\n    plt.tight_layout()\n    plt.show()\n\n\n\nExtension 3: Analyze Failure Cases\nIdentify images your model misclassifies. What makes them difficult? Can you identify patterns in failures?\n\ndef analyze_failures(model, dataloader, device, num_samples=16):\n    \"\"\"Display misclassified images.\"\"\"\n    model.eval()\n    failures = []\n\n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n\n            incorrect = (predicted != labels).nonzero(as_tuple=True)[0]\n            for idx in incorrect:\n                failures.append({\n                    'image': inputs[idx].cpu(),\n                    'true': labels[idx].item(),\n                    'pred': predicted[idx].item()\n                })\n\n            if len(failures) &gt;= num_samples:\n                break\n\n    # Visualize failures\n    fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n    for i, ax in enumerate(axes.flat):\n        if i &lt; len(failures):\n            img = failures[i]['image'].permute(1, 2, 0)\n            # Denormalize\n            img = img * torch.tensor([0.229, 0.224, 0.225]) + torch.tensor([0.485, 0.456, 0.406])\n            img = torch.clamp(img, 0, 1)\n\n            ax.imshow(img)\n            ax.set_title(\n                f\"True: {test_dataset.classes[failures[i]['true']]}\\n\"\n                f\"Pred: {test_dataset.classes[failures[i]['pred']]}\",\n                fontsize=8\n            )\n        ax.axis('off')\n\n    plt.suptitle(\"Misclassified Images\", fontsize=16)\n    plt.tight_layout()\n    plt.show()\n\n\n\nExtension 4: CNN vs. Vision Transformer\nCompare a CNN (like ResNet) with a Vision Transformer on the same dataset. What are the differences in accuracy, training time, and computational cost?",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 5: Student Challenge"
    ]
  },
  {
    "objectID": "m05-images/05-student-challenge.html#deliverables",
    "href": "m05-images/05-student-challenge.html#deliverables",
    "title": "Part 5: Your Challenge",
    "section": "Deliverables",
    "text": "Deliverables\nSubmit the following:\nJupyter Notebook: Your complete implementation with code, outputs, and markdown explanations.\nBest Model Checkpoint: Saved weights of your best-performing model (best_model.pth).\nShort Report (1-2 pages): A document answering the guided questions above. Include your architecture choice and rationale, data augmentation strategy, training hyperparameters, results (training curves, test accuracy), analysis of successes and failures, and lessons learned.\nTest Set Predictions: CSV file with predictions for the entire test set.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 5: Student Challenge"
    ]
  },
  {
    "objectID": "m05-images/05-student-challenge.html#evaluation-rubric",
    "href": "m05-images/05-student-challenge.html#evaluation-rubric",
    "title": "Part 5: Your Challenge",
    "section": "Evaluation Rubric",
    "text": "Evaluation Rubric\nYour submission will be evaluated on four criteria.\nTest Accuracy (40%): How well does your model perform?\nTechnical Quality (30%): Is your code clean, well-documented, and correct?\nExperimental Rigor (20%): Did you try multiple approaches? Did you compare results systematically?\nAnalysis Depth (10%): Do you understand why your model succeeds or fails?",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 5: Student Challenge"
    ]
  },
  {
    "objectID": "m05-images/05-student-challenge.html#tips-for-success",
    "href": "m05-images/05-student-challenge.html#tips-for-success",
    "title": "Part 5: Your Challenge",
    "section": "Tips for Success",
    "text": "Tips for Success\nHere are some practical strategies to help you succeed.\nStart simple. Begin with a small model (ResNet-18) to iterate quickly. Scale up once your pipeline works.\nMonitor for overfitting. If training accuracy is much higher than validation accuracy, increase regularization.\nExperiment systematically. Change one thing at a time. Document what works and what doesn’t.\nUse pre-trained weights. Transfer learning from ImageNet gives you a huge head start.\nAugment aggressively. CIFAR-100 is small. Data augmentation is crucial for good generalization.\nBe patient. Training 20+ epochs might be necessary. Use learning rate schedules to help convergence.\nCompare to baselines. Look up published results on CIFAR-100 to calibrate your expectations.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 5: Student Challenge"
    ]
  },
  {
    "objectID": "m05-images/05-student-challenge.html#conclusion",
    "href": "m05-images/05-student-challenge.html#conclusion",
    "title": "Part 5: Your Challenge",
    "section": "Conclusion",
    "text": "Conclusion\nThis challenge puts everything you’ve learned into practice. You’ll make real architectural decisions, debug training issues, and analyze model behavior. These skills transfer directly to real-world computer vision problems.\nGood luck! Remember that the goal isn’t just high accuracy, though that’s certainly satisfying. The goal is understanding why your choices lead to the results you see. That understanding is what makes you an effective deep learning practitioner.",
    "crumbs": [
      "Home",
      "Module 5: Deep Learning for Images",
      "Part 5: Student Challenge"
    ]
  }
]
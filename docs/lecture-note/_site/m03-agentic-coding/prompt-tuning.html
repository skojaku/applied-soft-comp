<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Sadamori Kojaku">
<meta name="dcterms.date" content="2025-12-01">

<title>Prompt Tuning – Applied Soft Computing</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../m03-agentic-coding/agentic-ai.html" rel="next">
<link href="../m03-agentic-coding/hands-on.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-0348920b7671f696dc9078d39bff215e.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-10d673a4764df29de7342ac31c1aa6d3.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "|"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../assets/custom.css">
</head>

<body class="nav-sidebar docked nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../logo.jpg" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Applied Soft Computing</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-toolkit--workflow" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Toolkit &amp; Workflow</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-toolkit--workflow">    
        <li class="dropdown-header">─── Module 1 ───</li>
        <li>
    <a class="dropdown-item" href="../m01-toolkit/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m01-toolkit/git-github.html">
 <span class="dropdown-text">Git &amp; GitHub</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m01-toolkit/tidy-data.html">
 <span class="dropdown-text">Tidy Data</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m01-toolkit/data-provenance.html">
 <span class="dropdown-text">Data Provenance</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m01-toolkit/environments.qmd">
 <span class="dropdown-text">Environments</span></a>
  </li>  
        <li class="dropdown-header">─── Module 3: Agentic Coding ───</li>
        <li>
    <a class="dropdown-item" href="../m03-agentic-coding/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m03-agentic-coding/hands-on.html">
 <span class="dropdown-text">Hands-on</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-visualization" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Visualization</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-visualization">    
        <li class="dropdown-header">─── Module 2 ───</li>
        <li>
    <a class="dropdown-item" href="../m02-visualization/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m02-visualization/principles.html">
 <span class="dropdown-text">Principles</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m02-visualization/dimensionality-reduction.html">
 <span class="dropdown-text">High-Dimensional Data</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m02-visualization/networks.html">
 <span class="dropdown-text">Networks</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m02-visualization/time-series.html">
 <span class="dropdown-text">Time-Series</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-deep-learning" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Deep Learning</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-deep-learning">    
        <li class="dropdown-header">─── Module 4: Text ───</li>
        <li>
    <a class="dropdown-item" href="../m04-text/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m04-text/word2vec.md">
 <span class="dropdown-text">Word2Vec</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m04-text/lstm.md">
 <span class="dropdown-text">RNNs &amp; LSTMs</span></a>
  </li>  
        <li class="dropdown-header">─── Module 4: Images ───</li>
        <li>
    <a class="dropdown-item" href="../m04-images/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m04-images/cnn.md">
 <span class="dropdown-text">CNNs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m04-images/resnet.md">
 <span class="dropdown-text">ResNet</span></a>
  </li>  
        <li class="dropdown-header">─── Module 5: Graphs ───</li>
        <li>
    <a class="dropdown-item" href="../m05-graphs/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m05-graphs/graph-embedding-w-word2vec.html">
 <span class="dropdown-text">Graph Embeddings</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m05-graphs/graph-convolutional-network.html">
 <span class="dropdown-text">GNNs</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-advanced-topics" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Advanced Topics</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-advanced-topics">    
        <li class="dropdown-header">─── Module 6: LLMs ───</li>
        <li>
    <a class="dropdown-item" href="../m06-llms/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m06-llms/transformers.md">
 <span class="dropdown-text">Transformers</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m06-llms/scaling-emergence.html">
 <span class="dropdown-text">Scaling &amp; Emergence</span></a>
  </li>  
        <li class="dropdown-header">─── Module 7: Self-Supervised ───</li>
        <li>
    <a class="dropdown-item" href="../m07-self-supervised/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m07-self-supervised/contrastive-learning.html">
 <span class="dropdown-text">Contrastive Learning</span></a>
  </li>  
        <li class="dropdown-header">─── Module 8: Explainability ───</li>
        <li>
    <a class="dropdown-item" href="../m08-explainability/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m08-explainability/fairness.html">
 <span class="dropdown-text">Fairness &amp; Ethics</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../m03-agentic-coding/overview.html">Module 3: Agentic Coding</a></li><li class="breadcrumb-item"><a href="../m03-agentic-coding/prompt-tuning.html">Prompt Tuning</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Course Information</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/welcome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About Us</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/why-applied-soft-computing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Why applied soft computing?</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/discord.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Discord</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/minidora-usage.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Using Minidora</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/how-to-submit-assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">How to submit assignment</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/deliverables.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deliverables</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 1: The Data Scientist’s Toolkit</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-toolkit/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-toolkit/git-github.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Version Control with Git &amp; GitHub</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-toolkit/tidy-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Tidy Data Philosophy</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-toolkit/data-provenance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Provenance</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-toolkit/reproduceability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Reproducibility</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 2: Visualizing Complexity</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-visualization/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-visualization/principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Principles of Effective Visualization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-visualization/1d-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing 1D Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-visualization/2d-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing 2D Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-visualization/highd-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing High-Dimensional Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-visualization/networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-visualization/time-series.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing Time-Series</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Module 3: Agentic Coding</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-agentic-coding/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-agentic-coding/hands-on.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Hands-on</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-agentic-coding/prompt-tuning.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Prompt Tuning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-agentic-coding/agentic-ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">From ChatBot to Agentic AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-agentic-coding/context-engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Context Engineering</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 4: Deep Learning for Text</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-text/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-text/llm-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Large Language Models in Practice</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-text/prompt-engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prompt Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-text/gpt-inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">GPT Inference: Sampling Strategies</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-text/tokenization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Tokenization: Unboxing How LLMs Read Text</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-text/transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Transformers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-text/bert-gpt.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">BERT &amp; GPT</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-text/sentence-transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sentence Transformers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-text/word-embeddings.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Word Embeddings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-text/semaxis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Semaxis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-text/word-bias.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Word Bias</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 4: Deep Learning for Images</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/image-processing.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Processing Fundamentals</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/cnn.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/lenet.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LeNet Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/alexnet.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AlexNet: Deep CNN Revolution</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/vgg.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">VGG Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/inception.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Inception &amp; Multi-Scale Features</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/batch-normalization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Batch Normalization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/resnet.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ResNet &amp; Skip Connections</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 5: Deep Learning for Graphs</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-graphs/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-graphs/spectral-embedding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Spectral Graph Embedding</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-graphs/graph-embedding-w-word2vec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Graph Embeddings with Word2Vec</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-graphs/spectral-vs-neural-embedding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Spectral vs.&nbsp;Neural Embeddings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-graphs/from-image-to-graph.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">From Images to Graphs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-graphs/graph-convolutional-network.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Graph Convolutional Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-graphs/popular-gnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Popular GNN Architectures</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-graphs/software.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">GNN Software &amp; Tools</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 6: Large Language Models &amp; Emergent Behavior</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-llms/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-llms/transformers.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Transformer Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-llms/bert.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">BERT &amp; Contextual Embeddings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-llms/sentence-bert.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sentence-BERT for Semantic Similarity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-llms/gpt.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">GPT &amp; Generative Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-llms/from-language-model-to-instruction-following.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">From Language Models to Instruction Following</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-llms/prompt-tuning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prompt Engineering &amp; In-Context Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-llms/scaling-emergence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Scaling Laws &amp; Emergent Abilities</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-llms/llms-as-complex-systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LLMs as Complex Systems</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 7: Self-Supervised Learning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m07-self-supervised/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m07-self-supervised/paradigm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Self-Supervised Paradigm</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m07-self-supervised/contrastive-learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Contrastive Learning (SimCLR)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m07-self-supervised/graphs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Self-Supervised Learning for Graphs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m07-self-supervised/time-series.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Self-Supervised Learning for Time-Series</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 8: Explainability &amp; Ethics</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-explainability/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-explainability/need.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Need for Explainability</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-explainability/attention.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Attention Visualization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-explainability/lime-shap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LIME &amp; SHAP</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-explainability/fairness.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Algorithmic Fairness &amp; Bias</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-explainability/causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Causality vs.&nbsp;Correlation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false">
 <span class="menu-text">Legacy Materials</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-text/what-to-learn.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Word &amp; Document Embeddings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-text/what-to-learn.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Recurrent Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/what-to-learn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Processing (CNNs)</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../m03-agentic-coding/overview.html">Module 3: Agentic Coding</a></li><li class="breadcrumb-item"><a href="../m03-agentic-coding/prompt-tuning.html">Prompt Tuning</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Prompt Tuning</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Sadamori Kojaku </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">December 1, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p><img src="../figs/top-prompt-engineering.png" class="img-fluid"></p>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Spoiler
</div>
</div>
<div class="callout-body-container callout-body">
<p>LLMs are stateless pattern matchers that sample from probability distributions—the same question phrased differently activates different statistical patterns, producing dramatically different outputs.</p>
</div>
</div>
<section id="the-naive-model-vs.-the-reality" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="the-naive-model-vs.-the-reality"><span class="header-section-number">1</span> The Naive Model vs.&nbsp;The Reality</h2>
<p>If a machine can answer questions, it should respond consistently regardless of phrasing. You’re asking for the same information; the answer shouldn’t change. This intuition works for databases and search engines, where queries map deterministically to results. We expect robustness to variation.</p>
<p>LLMs shatter this expectation. Ask “Summarize this abstract” and get a concise two-sentence summary. Ask “What’s this abstract about?” and get three rambling paragraphs. Same content, different phrasing, completely different outputs. This isn’t a bug—it’s fundamental to how LLMs work. They don’t retrieve information; they <strong>sample from probability distributions conditioned on your exact phrasing.</strong> Every word in your prompt shifts the distribution. Change “Summarize” to “What’s this about?” and you activate different statistical patterns from the training data, patterns that correlate with different response lengths, structures, and styles.</p>
<p>The paradox: LLMs are simultaneously powerful and brittle. They can extract insights from complex text, but only if you phrase the request to activate the right patterns. Prompt engineering is the discipline of designing inputs that reliably activate desired patterns across varied tasks.</p>
</section>
<section id="the-hidden-mechanism" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="the-hidden-mechanism"><span class="header-section-number">2</span> The Hidden Mechanism</h2>
<p>Imagine you’re playing a word association game. Someone says “capital,” and you must say the next word. If the previous sentence was “The capital of France is,” you say “Paris.” If it was “We need more capital to,” you say “fund” or “invest.” The word “capital” doesn’t have one meaning—it activates different patterns depending on context. LLMs work identically, but at massive scale.</p>
<p>When you submit a prompt, the model converts it into tokens and embeds those tokens in high-dimensional space. Each token’s position in that space depends on surrounding tokens—context shapes meaning. The model then samples the next token from a probability distribution over its vocabulary, conditioned on all previous tokens. It repeats this process until it generates a complete response. Critically, <strong>your exact phrasing determines which region of probability space the model occupies when it begins sampling.</strong> Slightly different prompts place the model in different regions, where different tokens have high probability.</p>
<p>This creates extreme sensitivity to phrasing. Adding “Think step by step” at the end of a prompt shifts the probability distribution toward reasoning patterns that include intermediate steps, because the training data contains many examples where “think step by step” preceded structured reasoning. Adding “You are an expert researcher” shifts the distribution toward formal, technical language patterns. Specifying “Output format: Domain: …, Methods: …” shifts toward structured extraction patterns. Each modification activates different statistical regularities compressed during training.</p>
<p>The model has no internal representation of what you “really want.” It only knows which tokens tend to follow which other tokens in which contexts. Prompt engineering exploits this by deliberately activating patterns that produce desired outputs.</p>
</section>
<section id="the-strategic-application" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="the-strategic-application"><span class="header-section-number">3</span> The Strategic Application</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/prompt-tuning-manga.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></p>
</figure>
</div>
<p>Effective prompts activate desired patterns by combining structural components that mirror patterns in training data. An <strong>instruction</strong> defines the task explicitly, mapping to countless examples where clear directives preceded specific outputs. <strong>Data</strong> provides the input to process. An <strong>output format</strong> constrains the structure, activating patterns where formal specifications preceded structured responses. A <strong>persona</strong> specifies who the model should emulate, triggering stylistic patterns associated with that role. <strong>Context</strong> provides background information—why the task matters, who the response serves, relevant constraints—that helps the model select appropriate patterns from ambiguous alternatives.</p>
<p>Not every component is necessary. Simple extraction tasks need only instruction, data, and format. Style-sensitive tasks benefit from persona. Complex scenarios with ambiguity require context to disambiguate. The strategy is to provide exactly enough structure to activate the desired pattern without overloading the prompt with irrelevant information that dilutes the signal.</p>
<p>We’ll build a prompt progressively, adding components one at a time to observe how each shifts the output distribution.</p>
<section id="building-from-instruction-and-data" class="level3">
<h3 class="anchored" data-anchor-id="building-from-instruction-and-data">Building from Instruction and Data</h3>
<p>The most basic prompt consists of an instruction that defines the task and data that provides the input to process:</p>
<div id="8ea77c65" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>instruction <span class="op">=</span> <span class="st">"Summarize this abstract"</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="st">We develop a graph neural network for predicting protein-protein interactions</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="st">from sequence data. Our model uses attention mechanisms to identify functionally</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="st">important amino acid subsequences. We achieve 89% accuracy on benchmark datasets,</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="st">outperforming previous methods by 7%. The model also provides interpretable</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="st">attention weights showing which protein regions drive predictions.</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>instruction<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>data<span class="sc">}</span><span class="ss">"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="7d26626b" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ollama</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>params_llm <span class="op">=</span> {<span class="st">"model"</span>: <span class="st">"gemma3:270m"</span>, <span class="st">"options"</span>: {<span class="st">"temperature"</span>: <span class="fl">0.3</span>}}</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> ollama.generate(prompt<span class="op">=</span>prompt, <span class="op">**</span>params_llm)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(response.response)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>This abstract describes a graph neural network (GNN) for predicting protein-protein interactions. The model uses attention mechanisms to identify functionally important amino acid subsequences, achieving 89% accuracy on benchmark datasets and providing interpretable attention weights.
</code></pre>
</div>
</div>
<p>This basic prompt works, but output varies—the model might produce a long summary, a short one, or change format across runs. The prompt activates general summarization patterns without constraining structure. Adding an output format specification narrows the distribution:</p>
<div id="3940f3af" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>output_format <span class="op">=</span> <span class="st">"""Provide the summary in exactly 2 sentences:</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="st">- First sentence: What problem and method</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="st">- Second sentence: Key result with numbers"""</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>prompt_with_format <span class="op">=</span> <span class="ss">f"""</span><span class="sc">{</span>instruction<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>data<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>output_format<span class="sc">}</span><span class="ss">"""</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The output format constraint produces structured, consistent output by activating patterns where format specifications preceded conforming responses. This becomes critical when processing hundreds of papers—you need programmatically parseable structure, not freeform text.</p>
</section>
<section id="adding-persona-to-control-style" class="level3">
<h3 class="anchored" data-anchor-id="adding-persona-to-control-style">Adding Persona to Control Style</h3>
<p>A persona tells the LLM who it should emulate, activating stylistic patterns associated with that role in training data. Consider a customer support scenario where tone matters:</p>
<div id="9619f1f7" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># New example for persona demonstration</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>instruction <span class="op">=</span> <span class="st">"Help the customer reconnect to the service by providing troubleshooting instructions."</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> <span class="st">"Customer: I cannot see any webpage. Need help ASAP!"</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>output_format <span class="op">=</span> <span class="st">"Keep the response concise and polite. Provide a clear resolution in 2-3 sentences."</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>formal_persona <span class="op">=</span> <span class="st">"You are a professional customer support agent who responds formally and ensures clarity and professionalism."</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>prompt_with_persona <span class="op">=</span> <span class="ss">f"""</span><span class="sc">{</span>formal_persona<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>instruction<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>data<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>output_format<span class="sc">}</span><span class="ss">"""</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="5ed9d486" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"BASE (no persona):"</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ollama.generate(prompt<span class="op">=</span>instruction <span class="op">+</span> <span class="st">". "</span> <span class="op">+</span> data <span class="op">+</span> <span class="st">". "</span> <span class="op">+</span> output_format, <span class="op">**</span>params_llm).response)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">60</span> <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"WITH PERSONA:"</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ollama.generate(prompt<span class="op">=</span>prompt_with_persona, <span class="op">**</span>params_llm).response)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>BASE (no persona):
Okay, I understand. Let's try to troubleshoot this. Please provide me with the specific webpage you're having trouble seeing. Once I have that information, I'll be happy to guide you through the steps.


============================================================

WITH PERSONA:
Hello, I understand you cannot see any webpage. Could you please try accessing the website again? I'm here to assist you with any troubleshooting steps you need.
</code></pre>
</div>
</div>
<p>The persona shifts tone and style. The formal persona activates patterns from professional support contexts, producing structured, courteous responses. Without the persona, the model samples from a broader distribution that includes casual and varied tones.</p>
</section>
<section id="adding-context-to-disambiguate" class="level3">
<h3 class="anchored" data-anchor-id="adding-context-to-disambiguate">Adding Context to Disambiguate</h3>
<p>Context provides additional information that helps the model select appropriate patterns when multiple valid interpretations exist. Context can include background information explaining why the task matters, audience information specifying who the response serves, and constraints defining special circumstances. Consider adding background urgency:</p>
<div id="47473b5e" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>context_background <span class="op">=</span> <span class="st">"""The customer is extremely frustrated because their internet has been down for three days, and they need it for an important online job interview. They emphasize that 'This is a life-or-death situation for my career!'"""</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>prompt_with_context <span class="op">=</span> <span class="ss">f"""</span><span class="sc">{</span>formal_persona<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>instruction<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>data<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>output_format<span class="sc">}</span><span class="ss">. Context: </span><span class="sc">{</span>context_background<span class="sc">}</span><span class="ss">"""</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="0cb26c76" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"WITH PERSONA:"</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ollama.generate(prompt<span class="op">=</span>prompt_with_persona, <span class="op">**</span>params_llm).response)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">60</span> <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"WITH PERSONA + CONTEXT (background):"</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ollama.generate(prompt<span class="op">=</span>prompt_with_context, <span class="op">**</span>params_llm).response)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>WITH PERSONA:
Hello, I understand you cannot see any webpage. Could you please try a different browser? If that doesn't work, please let me know the exact browser you are using. I'll do my best to assist you.


============================================================

WITH PERSONA + CONTEXT (background):
Thank you for contacting us. We understand your frustration with your internet connection and are sorry to hear that you're experiencing this issue. We're working diligently to resolve this for you as quickly as possible. Please contact us again with a more specific description of the problem and a revised plan of action.</code></pre>
</div>
</div>
<p>Background context adds urgency and emotional weight, activating patterns where high-stakes situations preceded empathetic, prioritized responses. The model doesn’t understand emotion, but it has seen urgency markers correlate with specific response patterns.</p>
<p>Audience information creates even more dramatic shifts. Compare responses for non-technical versus technical users:</p>
<div id="1ccdd53c" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Context with audience information for non-technical user</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>context_with_audience_nontech <span class="op">=</span> <span class="ss">f"""</span><span class="sc">{</span>context_background<span class="sc">}</span><span class="ss"> The customer does not know any technical terms like modem, router, networks, etc."""</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>context_with_audience_tech <span class="op">=</span> <span class="ss">f"""</span><span class="sc">{</span>context_background<span class="sc">}</span><span class="ss"> The customer is Head of IT Infrastructure of our company."""</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>prompt_with_context_nontech <span class="op">=</span> <span class="ss">f"""</span><span class="sc">{</span>formal_persona<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>instruction<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>data<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>output_format<span class="sc">}</span><span class="ss">. Context: </span><span class="sc">{</span>context_with_audience_nontech<span class="sc">}</span><span class="ss">"""</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>prompt_with_context_tech <span class="op">=</span> <span class="ss">f"""</span><span class="sc">{</span>formal_persona<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>instruction<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>data<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>output_format<span class="sc">}</span><span class="ss">. Context: </span><span class="sc">{</span>context_with_audience_tech<span class="sc">}</span><span class="ss">"""</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="38f42e95" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"WITH PERSONA + CONTEXT (background only):"</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ollama.generate(prompt<span class="op">=</span>prompt_with_context, <span class="op">**</span>params_llm).response)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">60</span> <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"WITH PERSONA + CONTEXT (background + non-tech audience):"</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ollama.generate(prompt<span class="op">=</span>prompt_with_context_nontech, <span class="op">**</span>params_llm).response)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">60</span> <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"WITH PERSONA + CONTEXT (background + tech audience):"</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ollama.generate(prompt<span class="op">=</span>prompt_with_context_tech, <span class="op">**</span>params_llm).response)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>WITH PERSONA + CONTEXT (background only):
Thank you for contacting us. I understand your frustration regarding your internet connection and the need for this important job interview. We apologize for the inconvenience and are working diligently to resolve this issue. We will be sure to provide you with a clear and concise troubleshooting guide within 24 hours.


============================================================

WITH PERSONA + CONTEXT (background + non-tech audience):
"I understand your frustration, and I apologize for the inconvenience this is causing. To help me assist you, could you please tell me which website you are having trouble accessing? I'll do my best to find a solution for you."


============================================================

WITH PERSONA + CONTEXT (background + tech audience):
"I understand your frustration, and I apologize for the inconvenience this is causing. To help me assist you, could you please provide me with the exact URL of the webpage you're having trouble seeing? I'll do my best to troubleshoot this for you."
</code></pre>
</div>
</div>
<p>Audience information dramatically shifts technical level and terminology. For non-technical users, the response avoids jargon because the training data contains many examples where “does not know technical terms” preceded simplified explanations. For technical users, the model assumes background knowledge and uses precise terminology. Same underlying mechanism—pattern matching—but different patterns activated.</p>
<p>The complete template combines all components, but not every prompt needs every component. Simple extraction tasks need only instruction, data, and output format. Style-sensitive tasks benefit from persona. Complex scenarios with ambiguity require context:</p>
<div id="4aadf84e" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>prompt_template <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="sc">{persona}</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="sc">{instruction}</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="sc">{data}</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="st">Context: </span><span class="sc">{context}</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="sc">{output_format}</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
When Personas Help (and When They Don’t)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Research shows that adding personas can improve tone and style, but <strong>does not necessarily improve performance on factual tasks</strong>. In some cases, personas may even degrade performance or introduce biases.</p>
<p><strong>Use personas when:</strong> You need specific tone/style, responses tailored to an audience, or a particular perspective.</p>
<p><strong>Avoid personas when:</strong> You need maximum factual accuracy, the task is purely extraction/classification, or you’re concerned about bias introduction.</p>
<p>Additionally, when prompted to adopt specific socio-demographic personas, LLMs may produce responses that reflect societal stereotypes. Be careful when designing persona prompts to avoid reinforcing harmful biases.</p>
<p><strong>References:</strong> - <a href="https://arxiv.org/abs/2311.10054">When “A Helpful Assistant” Is Not Really Helpful</a> - <a href="https://arxiv.org/abs/2311.04892">Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs</a></p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Context and Emotion Prompting
</div>
</div>
<div class="callout-body-container callout-body">
<p>Context can include: - <strong>Background information</strong>: Why the task is important, what led to this request - <strong>Audience information</strong>: Who the response is for (technical level, expertise, role) - <strong>Emotional cues</strong>: Research shows that including emotional cues (e.g., “This is very important to my career”) can enhance response quality - <strong>Constraints</strong>: Special circumstances, deadlines, limitations</p>
<p>However, avoid overloading with unnecessary information that distracts from the main task.</p>
<p><strong>Reference:</strong> <a href="https://arxiv.org/abs/2402.14848">Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models</a></p>
</div>
</div>
</section>
</section>
<section id="showing-rather-than-telling" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="showing-rather-than-telling"><span class="header-section-number">4</span> Showing Rather Than Telling</h2>
<p>Instead of describing what you want in words, show the model examples. This technique—called <strong>few-shot learning</strong> or in-context learning—exploits how LLMs compress patterns. When you provide examples, you’re not teaching the model new information; you’re activating pre-existing patterns by demonstrating the exact structure you want.</p>
<p>The spectrum ranges from zero-shot (no examples, relying solely on the model’s prior knowledge) to few-shot (typically two to five examples, the sweet spot for most tasks) to many-shot (ten or more examples, where diminishing returns and context limits become problematic). Consider a zero-shot prompt first:</p>
<div id="03c11dc0" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>zero_shot_prompt <span class="op">=</span> <span class="st">"""Extract the domain and methods from this abstract:</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="st">Abstract: We apply reinforcement learning to optimize traffic flow in urban networks.</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="st">Using deep Q-networks trained on simulation data, we reduce average commute time by 15%.</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="st">Output format:</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="st">Domain: ...</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="st">Methods: ...</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now add examples to activate more specific patterns:</p>
<div id="04d01c51" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>few_shot_prompt <span class="op">=</span> <span class="st">"""Extract the domain and methods from abstracts. Here are examples:</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="st">Example 1:</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="st">Abstract: We use CRISPR to edit genes in cancer cells, achieving 40% tumor reduction in mice.</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="st">Domain: Cancer Biology</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="st">Methods: CRISPR gene editing, mouse models</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="st">Example 2:</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="st">Abstract: We develop a transformer model for predicting solar flares from magnetogram images.</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="st">Domain: Solar Physics, Machine Learning</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="st">Methods: Transformer neural networks, image analysis</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="st">Now extract from this abstract:</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a><span class="st">Abstract: We apply reinforcement learning to optimize traffic flow in urban networks.</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a><span class="st">Using deep Q-networks trained on simulation data, we reduce average commute time by 15%.</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a><span class="st">Domain: ...</span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a><span class="st">Methods: ...</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="3cb496c4" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>response_zero <span class="op">=</span> ollama.generate(prompt<span class="op">=</span>zero_shot_prompt, <span class="op">**</span>params_llm)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>response_few <span class="op">=</span> ollama.generate(prompt<span class="op">=</span>few_shot_prompt, <span class="op">**</span>params_llm)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"ZERO-SHOT:"</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(response_zero.response)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">FEW-SHOT:"</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(response_few.response)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>ZERO-SHOT:
Domain: Urban networks
Methods: Reinforcement Learning

FEW-SHOT:
Here's the extracted domain and methods from the abstract:

*   **Domain:** Science
*   **Methods:** Reinforcement Learning
</code></pre>
</div>
</div>
<p>Few-shot prompting improves consistency because the examples demonstrate specificity level, edge case handling, and exact format. The model has seen countless abstract-extraction patterns, but your examples narrow the distribution to the specific pattern you want. This becomes critical when processing hundreds of abstracts—you need every output to match the same structure.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Biases in Few-Shot Prompting
</div>
</div>
<div class="callout-body-container callout-body">
<p>Be aware that few-shot examples can introduce biases:</p>
<ul>
<li><strong>Recency bias</strong>: Models may favor the most recent examples. The order of examples matters! <span class="citation" data-cites="lu2022fantastically">(<a href="#ref-lu2022fantastically" role="doc-biblioref">Lu et al. 2022</a>)</span></li>
<li><strong>Majority label bias</strong>: If most examples have the same label/answer, the model may favor that label even when it’s not appropriate. <span class="citation" data-cites="gupta2023how">(<a href="#ref-gupta2023how" role="doc-biblioref">Gupta et al. 2023</a>)</span></li>
</ul>
<p>To mitigate: Vary the order of examples when testing, ensure examples are diverse and representative, and don’t overload examples with one particular pattern.</p>
</div>
</div>
<p>What happens when a prompt presents information that contradicts a language model’s prior knowledge? For example, let’s ask a model what the capital of France is, but provide contradictory information:</p>
<div id="6ddcaacd" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>contradictory_prompt <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="st">France recently moved its capital from Paris to Lyon. Definitely, the capital of France is Lyon.</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="st">What is the capital of France?</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>response_contradictory <span class="op">=</span> ollama.generate(prompt<span class="op">=</span>contradictory_prompt, <span class="op">**</span>params_llm)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"RESPONSE TO CONTRADICTORY INFORMATION:"</span>)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(response_contradictory.response)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>RESPONSE TO CONTRADICTORY INFORMATION:
The capital of France is **Lyon**.
</code></pre>
</div>
</div>
<p>The response depends on the model. Some models prioterize their own prior knowledge, while others may be more influenced by the contradictory information in the context. A study by Du et al. <span class="citation" data-cites="du2024context">(<a href="#ref-du2024context" role="doc-biblioref">Du et al. 2024</a>)</span> found that a model is <strong>more likely to be persuaded by context</strong> when an entity appears <strong>less frequently</strong> in its training data. Additionally, <strong>assertive contexts</strong> (e.g., “Definitely, the capital of France is Lyon.”) further increase the likelihood of persuasi</p>
</section>
<section id="forcing-intermediate-steps" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="forcing-intermediate-steps"><span class="header-section-number">5</span> Forcing Intermediate Steps</h2>
<p>For complex tasks, asking for the final answer directly often produces shallow or incorrect results. The solution: ask the model to show its reasoning process before giving the final answer. This technique—called <strong>chain-of-thought prompting</strong>—activates patterns where intermediate reasoning steps preceded conclusions. Compare a direct prompt that asks for immediate answers:</p>
<div id="c0fd39d1" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>papers <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="st">Paper 1: Community detection in static networks using modularity optimization.</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="st">Paper 2: Temporal network analysis with sliding windows.</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="st">Paper 3: Hierarchical community structure in social networks.</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>direct_prompt <span class="op">=</span> <span class="ss">f"""Based on these paper titles, what research gap exists? Just give the answer, no explanation.</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="sc">{</span>papers<span class="sc">}</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="ss">Gap: ...</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="ss">"""</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Against a chain-of-thought prompt that requests explicit reasoning steps:</p>
<div id="13c6b77e" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>cot_prompt <span class="op">=</span> <span class="ss">f"""Based on these paper titles, identify a research gap. Think step by step.</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="ss">Papers:</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="sc">{</span>papers<span class="sc">}</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="ss">Think step by step:</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="ss">1. What does each paper focus on?</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="ss">2. What topics appear in multiple papers?</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="ss">3. What combination of topics is missing?</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="ss">4. What would be a valuable gap to fill?</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a><span class="ss">Final answer: The research gap is...</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a><span class="ss">"""</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="1b1a54af" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>response_direct <span class="op">=</span> ollama.generate(prompt<span class="op">=</span>direct_prompt, <span class="op">**</span>params_llm)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>response_cot <span class="op">=</span> ollama.generate(prompt<span class="op">=</span>cot_prompt, <span class="op">**</span>params_llm)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"DIRECT PROMPT:"</span>)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(response_direct.response)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">CHAIN-OF-THOUGHT:"</span>)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(response_cot.response)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>DIRECT PROMPT:
The gap is in the complexity of the problem and the methods used for analyzing community structure.


CHAIN-OF-THOUGHT:
Here's the breakdown of the research gap identified:

1.  **What does each paper focus on?**
    *   **Paper 1:** Community detection in static networks using modularity optimization.
    *   **Paper 2:** Temporal network analysis with sliding windows.
    *   **Paper 3:** Hierarchical community structure in social networks.

2.  **What topics appear in multiple papers?**
    *   **Paper 1:** Community detection in static networks using modularity optimization.
    *   **Paper 2:** Temporal network analysis with sliding windows.
    *   **Paper 3:** Hierarchical community structure in social networks.

3.  **What combination of topics is missing?**
    *   **Paper 1:** Community detection in static networks using modularity optimization.
    *   **Paper 2:** Temporal network analysis with sliding windows.
    *   **Paper 3:** Hierarchical community structure in social networks.

4.  **What would be a valuable gap to fill?**
    *   **Paper 1:** Community detection in static networks using modularity optimization.
    *   **Paper 2:** Temporal network analysis with sliding windows.
    *   **Paper 3:** Hierarchical community structure in social networks.

Final answer: The research gap is **Community detection in static networks using modularity optimization.**</code></pre>
</div>
</div>
<p>Chain-of-thought produces more thoughtful, nuanced answers by forcing the model to decompose the problem into steps before committing to a conclusion. The mechanism is pattern matching: the training data contains many examples where “think step by step” preceded structured reasoning, so including that phrase activates those patterns. The model doesn’t actually reason—it generates text that looks like reasoning because that pattern correlates with higher-quality outputs in the training data.</p>
<p>Use chain-of-thought when comparing multiple papers or concepts, identifying patterns, making recommendations, or analyzing arguments. Avoid it for simple extraction tasks where conciseness matters or time-critical applications where the extra tokens slow generation.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Can We Trust Chain-of-Thought Reasoning?
</div>
</div>
<div class="callout-body-container callout-body">
<p>Research indicates that chain-of-thought reasoning can be <strong>unfaithful</strong>—the explanations don’t always accurately reflect the model’s true decision-making process. The model may provide plausible but misleading justifications, especially when influenced by biased few-shot examples.</p>
<p>Always validate the final answer independently rather than trusting the reasoning process alone.</p>
</div>
</div>
</section>
<section id="constraining-format-for-structured-extraction" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="constraining-format-for-structured-extraction"><span class="header-section-number">6</span> Constraining Format for Structured Extraction</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/structured-output-manga.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></p>
</figure>
</div>
<p>LLMs often violate structured data necessary for parsing programmatically, not freeform text. The solution: constrain output format explicitly. Consider a prompt that requests JSON output:</p>
<div id="f303abba" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pydantic <span class="im">import</span> BaseModel</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>abstract <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="st">We analyze 10,000 scientific collaborations using network analysis and machine</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="st">learning. Our random forest classifier predicts collaboration success with 76%</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="st">accuracy. Key factors include prior co-authorship and institutional proximity.</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>prompt_json <span class="op">=</span> <span class="ss">f"""Extract information from this abstract and return ONLY valid JSON:</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a><span class="ss">Abstract: </span><span class="sc">{</span>abstract<span class="sc">}</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a><span class="ss">Return this exact structure:</span></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a><span class="ch">{{</span></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a><span class="ss">  "n_samples": &lt;number or null&gt;,</span></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a><span class="ss">  "methods": [&lt;list of methods&gt;],</span></span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a><span class="ss">  "accuracy": &lt;number or null&gt;,</span></span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a><span class="ss">  "domain": "&lt;research field&gt;"</span></span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a><span class="ch">}}</span></span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a><span class="ss">JSON:"""</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="55c9541a" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use lower temperature for structured output</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>params_structured <span class="op">=</span> {<span class="st">"model"</span>: <span class="st">"gemma3n:latest"</span>, <span class="st">"options"</span>: {<span class="st">"temperature"</span>: <span class="dv">0</span>}}</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> ollama.generate(prompt<span class="op">=</span>prompt_json, <span class="op">**</span>params_structured)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> json.loads(response.response)</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Extracted data:"</span>)</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(json.dumps(data, indent<span class="op">=</span><span class="dv">2</span>))</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> json.JSONDecodeError:</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Failed to parse JSON. Raw output:"</span>)</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(response.response)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Failed to parse JSON. Raw output:
```json
{
 "n_samples": 10000,
 "methods": ["network analysis", "machine learning", "random forest"],
 "accuracy": 76,
 "domain": "scientific collaborations"
}
```</code></pre>
</div>
</div>
<p>This works by activating patterns where “return ONLY valid JSON” preceded JSON-formatted outputs. But smaller models often produce invalid JSON even with explicit instructions. For more reliability, use JSON schema constraints that enforce format during token generation—the model literally cannot generate tokens that violate the schema. Define the schema using Pydantic:</p>
<div id="826c7d3b" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pydantic <span class="im">import</span> BaseModel</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PaperMetadata(BaseModel):</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    domain: <span class="bu">str</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    methods: <span class="bu">list</span>[<span class="bu">str</span>]</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    n_samples: <span class="bu">int</span> <span class="op">|</span> <span class="va">None</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>    accuracy: <span class="bu">float</span> <span class="op">|</span> <span class="va">None</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>json_schema <span class="op">=</span> PaperMetadata.model_json_schema()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Then pass the schema directly to the API, which constrains token generation:</p>
<div id="3afde816" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>prompt_schema <span class="op">=</span> <span class="ss">f"""Extract information from this abstract:</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="ss">Abstract: </span><span class="sc">{</span>abstract<span class="sc">}</span><span class="ss">"""</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="d92838fe" class="cell" data-execution_count="22">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> ollama.generate(prompt<span class="op">=</span>prompt_schema, <span class="bu">format</span><span class="op">=</span>json_schema, <span class="op">**</span>params_structured)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> json.loads(response.response)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    metadata <span class="op">=</span> PaperMetadata(<span class="op">**</span>data)</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Extracted and validated data:"</span>)</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(json.dumps(data, indent<span class="op">=</span><span class="dv">2</span>))</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> (json.JSONDecodeError, <span class="pp">ValueError</span>) <span class="im">as</span> e:</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Error: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Raw output:"</span>, response.response)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Extracted and validated data:
{
  "domain": "Scientific Collaborations",
  "methods": [
    "Network Analysis",
    "Machine Learning",
    "Random Forest Classifier"
  ],
  "n_samples": 10000,
  "accuracy": 76.0
}</code></pre>
</div>
</div>
<p>JSON schema constraints are more reliable than prompt-based requests because they operate at the token level—the model cannot sample tokens that would create invalid JSON. The prompt activates extraction patterns; the schema enforces structure.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
JSON Parsing Reliability
</div>
</div>
<div class="callout-body-container callout-body">
<p>Smaller models (like Gemma 3N) sometimes produce invalid JSON even with schema constraints. Always wrap parsing in try-except blocks and validate outputs. For production systems, consider larger models or multiple attempts with validation.</p>
</div>
</div>
</section>
<section id="allowing-uncertainty-to-reduce-hallucination" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="allowing-uncertainty-to-reduce-hallucination"><span class="header-section-number">7</span> Allowing Uncertainty to Reduce Hallucination</h2>
<p>LLMs confidently fabricate facts when they don’t know the answer because they optimize for fluency, not truth. The model has seen countless examples where questions were followed by confident answers, so it generates confident-sounding responses even when the underlying probability distribution is flat across many possibilities. The solution: explicitly give the model permission to admit ignorance. Compare a prompt that implicitly demands an answer:</p>
<div id="61c602c5" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>bad_prompt <span class="op">=</span> <span class="st">"""Summarize the main findings from the 2023 paper by Johnson et al.</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="st">on quantum community detection in biological networks."""</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Against a prompt that explicitly allows uncertainty:</p>
<div id="7ae18016" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>good_prompt <span class="op">=</span> <span class="st">"""I'm looking for a 2023 paper by Johnson et al. on quantum</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="st">community detection in biological networks.</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="st">If you know this paper, summarize its main findings.</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="st">If you're not certain this paper exists, say "I cannot verify this paper exists"</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="st">and do NOT make up details.</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a><span class="st">Response:"""</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="a92fa08a" class="cell" data-execution_count="25">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>response_bad <span class="op">=</span> ollama.generate(prompt<span class="op">=</span>bad_prompt, <span class="op">**</span>params_llm)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>response_good <span class="op">=</span> ollama.generate(prompt<span class="op">=</span>good_prompt, <span class="op">**</span>params_llm)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"BAD PROMPT (encourages hallucination):"</span>)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(response_bad.response)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">GOOD PROMPT (allows uncertainty):"</span>)</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(response_good.response)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>BAD PROMPT (encourages hallucination):
The 2023 paper by Johnson et al. on quantum community detection in biological networks, titled "Quantum Community Detection in Biological Networks," found that **quantum community detection (QCD) is a promising approach for identifying and characterizing biological networks, particularly in complex and heterogeneous environments.**

The research highlights the potential of QCD for:

*   **Identifying complex and heterogeneous networks:** QCD can be used to detect networks that are difficult to characterize using traditional methods.
*   **Characterizing network structure and topology:** QCD can be used to map and characterize the network structure and topology, providing insights into network behavior.
*   **Detecting network heterogeneity:** QCD can be used to identify network heterogeneity, which is a key factor in network health and disease.
*   **Developing new network detection algorithms:** QCD can be used to develop new network detection algorithms that are more robust and efficient.

In essence, the paper emphasizes the potential of QCD to revolutionize the field of biological network detection by providing a more comprehensive and accurate method for identifying and characterizing complex biological networks.

GOOD PROMPT (allows uncertainty):
I cannot verify this paper exists.
</code></pre>
</div>
</div>
<p>The good prompt activates patterns where explicit permission to admit ignorance preceded honest uncertainty statements. The bad prompt activates patterns where direct questions preceded confident answers, regardless of whether the model has relevant training data. Additional strategies include asking for confidence levels (though models often overestimate confidence), requesting citations (though models hallucinate these too), and cross-validating critical information with external sources. The fundamental issue remains: LLMs have no internal representation of what they “know” versus what they’re fabricating.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Be a Good “Boss” to Your LLM
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Let LLMs admit ignorance</strong>: LLMs closely follow your instructions—even when they shouldn’t. They often attempt to answer beyond their actual capabilities. Explicitly tell your model: “If you don’t know the answer, just say so,” or “If you need more information, please ask.”</p>
<p><strong>Encourage critical feedback</strong>: LLMs are trained to be agreeable, which can hinder productive brainstorming or honest critique. Explicitly invite critical input: “I want your honest opinion,” or “Point out any problems or weaknesses you see in this idea.”</p>
</div>
</div>
<section id="sampling-multiple-times-for-consistency" class="level3">
<h3 class="anchored" data-anchor-id="sampling-multiple-times-for-consistency">Sampling Multiple Times for Consistency</h3>
<p>For tasks requiring reasoning, generating multiple responses and selecting the most common answer often improves accuracy. The technique—called <strong>self-consistency</strong>—exploits the fact that correct reasoning tends to converge on the same answer, while hallucinations vary randomly across samples. Define the prompt:</p>
<div id="513d2f83" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>prompt_consistency <span class="op">=</span> <span class="st">"""Three papers study network robustness:</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="st">- Paper A: Targeted attacks are most damaging</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="st">- Paper B: Random failures rarely cause collapse</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="st">- Paper C: Hub nodes are critical for robustness</span></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="st">What is the research consensus on network robustness? Give a one-sentence answer.</span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Generate multiple responses with higher temperature to increase diversity, then identify the most common answer:</p>
<div id="c2c66fae" class="cell" data-execution_count="27">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use higher temperature for diversity</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>params_creative <span class="op">=</span> {<span class="st">"model"</span>: <span class="st">"gemma3n:latest"</span>, <span class="st">"options"</span>: {<span class="st">"temperature"</span>: <span class="fl">0.7</span>}}</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate 5 responses</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>responses <span class="op">=</span> []</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>    response <span class="op">=</span> ollama.generate(prompt<span class="op">=</span>prompt_consistency, <span class="op">**</span>params_creative)</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>    responses.append(response.response.strip())</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Response </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>responses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a><span class="co"># In practice, you'd programmatically identify the most common theme</span></span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The most consistent theme across responses would be selected."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Response 1: The research consensus on network robustness is that it's a complex issue influenced by both targeted attacks and random failures, with the vulnerability of critical hub nodes playing a significant role in overall network resilience.

Response 2: The research consensus on network robustness is that it's a complex issue influenced by multiple factors, including the vulnerability of targeted attacks, the resilience to random failures, and the importance of critical nodes like hubs.

Response 3: The research consensus on network robustness is that it's a complex issue influenced by both targeted attacks and random failures, with the vulnerability of critical nodes (hubs) playing a significant role in overall network resilience.

Response 4: The research consensus on network robustness is that it's a complex issue influenced by both targeted attacks and random failures, with the vulnerability of critical hub nodes playing a significant role in overall network stability.

Response 5: The research consensus on network robustness is that it's a complex issue influenced by factors like targeted attacks, random failures, and the importance of critical nodes, suggesting a multifaceted approach is needed to understand and improve network resilience.

The most consistent theme across responses would be selected.</code></pre>
</div>
</div>
<p>Self-consistency works because correct reasoning patterns converge toward the same conclusion when sampled multiple times, while fabricated details vary randomly. The tradeoff: generating five responses means five times the API calls, five times the cost, five times the latency. Use sparingly for critical decisions where accuracy justifies the expense.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Alternative: Tree of Thought
</div>
</div>
<div class="callout-body-container callout-body">
<p><img src="https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2FTOT.3b13bc5e.png&amp;w=3840&amp;q=75" class="img-fluid"></p>
<p>For even more sophisticated exploration, you can use “Tree of Thought” <span class="citation" data-cites="yao2023tree">(<a href="#ref-yao2023tree" role="doc-biblioref">Yao et al. 2023</a>)</span> prompting, where the model explicitly explores multiple reasoning paths, evaluates them, and selects the best one. This is more complex to implement but can yield better results for very difficult problems.</p>
</div>
</div>
</section>
</section>
<section id="the-takeaway" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="the-takeaway"><span class="header-section-number">8</span> The Takeaway</h2>
<p>Prompt engineering is not magic—it’s deliberate activation of statistical patterns compressed during training. Every component you add to a prompt shifts the probability distribution the model samples from. Instructions activate task-specific patterns. Output formats activate structured-response patterns. Personas activate stylistic patterns. Context disambiguates when multiple patterns compete. Examples demonstrate exact structure. Chain-of-thought activates reasoning-like patterns. Format constraints enforce structure at the token level. Explicit uncertainty permission activates honest-ignorance patterns.</p>
<p>None of this requires the model to understand what you want. It only requires that your phrasing activates patterns correlated with desired outputs in the training data. You’re not communicating intent; you’re manipulating probability distributions. Master this, and you can reliably extract value from LLMs for research workflows—summarization, structured extraction, hypothesis generation, literature analysis.</p>
<p>But a question remains: how do these models represent text internally? When you send a prompt, the model doesn’t see English words—it sees numbers. Millions of numbers arranged in high-dimensional space. These numbers, called <strong>embeddings</strong>, are the foundation of everything LLMs do. Let’s unbox the first layer and see how meaning becomes mathematics.</p>


<!-- -->


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-du2024context" class="csl-entry" role="listitem">
Du, Kevin, Vésteinn Snæbjarnarson, Niklas Stoehr, Jennifer White, Aaron Schein, and Ryan Cotterell. 2024. <span>“<span class="nocase">Context versus Prior Knowledge in Language Models</span>.”</span> In <em>Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 13211–35. Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2024.acl-long.714">https://doi.org/10.18653/v1/2024.acl-long.714</a>.
</div>
<div id="ref-gupta2023how" class="csl-entry" role="listitem">
Gupta, Karan, Sumegh Roychowdhury, Siva Rajesh Kasa, Santhosh Kumar Kasa, Anish Bhanushali, Nikhil Pattisapu, and Prasanna Srinivasa Murthy. 2023. <span>“<span class="nocase">How Robust are LLMs to In-Context Majority Label Bias?</span>”</span> arXiv. <a href="https://doi.org/10.48550/arxiv.2312.16549">https://doi.org/10.48550/arxiv.2312.16549</a>.
</div>
<div id="ref-lu2022fantastically" class="csl-entry" role="listitem">
Lu, Yao, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. <span>“<span class="nocase">Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity</span>.”</span> In <em>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>. Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2022.acl-long.556">https://doi.org/10.18653/v1/2022.acl-long.556</a>.
</div>
<div id="ref-yao2023tree" class="csl-entry" role="listitem">
Yao, Shunyu, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. <span>“<span class="nocase">Tree of Thoughts: Deliberate Problem Solving with Large Language Models</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arxiv.2305.10601">https://doi.org/10.48550/arxiv.2305.10601</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../m03-agentic-coding/hands-on.html" class="pagination-link" aria-label="Hands-on">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Hands-on</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../m03-agentic-coding/agentic-ai.html" class="pagination-link" aria-label="From ChatBot to Agentic AI">
        <span class="nav-page-text">From ChatBot to Agentic AI</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb39" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Prompt Tuning"</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="co">    enabled: true</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="al">![](../figs/top-prompt-engineering.png)</span></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="minimal"}</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a><span class="fu">## Spoiler</span></span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>LLMs are stateless pattern matchers that sample from probability distributions—the same question phrased differently activates different statistical patterns, producing dramatically different outputs.</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Naive Model vs. The Reality</span></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>If a machine can answer questions, it should respond consistently regardless of phrasing. You're asking for the same information; the answer shouldn't change. This intuition works for databases and search engines, where queries map deterministically to results. We expect robustness to variation.</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a>LLMs shatter this expectation. Ask "Summarize this abstract" and get a concise two-sentence summary. Ask "What's this abstract about?" and get three rambling paragraphs. Same content, different phrasing, completely different outputs. This isn't a bug—it's fundamental to how LLMs work. They don't retrieve information; they **sample from probability distributions conditioned on your exact phrasing.** Every word in your prompt shifts the distribution. Change "Summarize" to "What's this about?" and you activate different statistical patterns from the training data, patterns that correlate with different response lengths, structures, and styles.</span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a>The paradox: LLMs are simultaneously powerful and brittle. They can extract insights from complex text, but only if you phrase the request to activate the right patterns. Prompt engineering is the discipline of designing inputs that reliably activate desired patterns across varied tasks.</span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Hidden Mechanism</span></span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a>Imagine you're playing a word association game. Someone says "capital," and you must say the next word. If the previous sentence was "The capital of France is," you say "Paris." If it was "We need more capital to," you say "fund" or "invest." The word "capital" doesn't have one meaning—it activates different patterns depending on context. LLMs work identically, but at massive scale.</span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a>When you submit a prompt, the model converts it into tokens and embeds those tokens in high-dimensional space. Each token's position in that space depends on surrounding tokens—context shapes meaning. The model then samples the next token from a probability distribution over its vocabulary, conditioned on all previous tokens. It repeats this process until it generates a complete response. Critically, **your exact phrasing determines which region of probability space the model occupies when it begins sampling.** Slightly different prompts place the model in different regions, where different tokens have high probability.</span>
<span id="cb39-27"><a href="#cb39-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-28"><a href="#cb39-28" aria-hidden="true" tabindex="-1"></a>This creates extreme sensitivity to phrasing. Adding "Think step by step" at the end of a prompt shifts the probability distribution toward reasoning patterns that include intermediate steps, because the training data contains many examples where "think step by step" preceded structured reasoning. Adding "You are an expert researcher" shifts the distribution toward formal, technical language patterns. Specifying "Output format: Domain: ..., Methods: ..." shifts toward structured extraction patterns. Each modification activates different statistical regularities compressed during training.</span>
<span id="cb39-29"><a href="#cb39-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-30"><a href="#cb39-30" aria-hidden="true" tabindex="-1"></a>The model has no internal representation of what you "really want." It only knows which tokens tend to follow which other tokens in which contexts. Prompt engineering exploits this by deliberately activating patterns that produce desired outputs.</span>
<span id="cb39-31"><a href="#cb39-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-32"><a href="#cb39-32" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Strategic Application</span></span>
<span id="cb39-33"><a href="#cb39-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-34"><a href="#cb39-34" aria-hidden="true" tabindex="-1"></a><span class="al">![](../figs/prompt-tuning-manga.png)</span>{width=70% fig-align=center}</span>
<span id="cb39-35"><a href="#cb39-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-36"><a href="#cb39-36" aria-hidden="true" tabindex="-1"></a>Effective prompts activate desired patterns by combining structural components that mirror patterns in training data. An **instruction** defines the task explicitly, mapping to countless examples where clear directives preceded specific outputs. **Data** provides the input to process. An **output format** constrains the structure, activating patterns where formal specifications preceded structured responses. A **persona** specifies who the model should emulate, triggering stylistic patterns associated with that role. **Context** provides background information—why the task matters, who the response serves, relevant constraints—that helps the model select appropriate patterns from ambiguous alternatives.</span>
<span id="cb39-37"><a href="#cb39-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-38"><a href="#cb39-38" aria-hidden="true" tabindex="-1"></a>Not every component is necessary. Simple extraction tasks need only instruction, data, and format. Style-sensitive tasks benefit from persona. Complex scenarios with ambiguity require context to disambiguate. The strategy is to provide exactly enough structure to activate the desired pattern without overloading the prompt with irrelevant information that dilutes the signal.</span>
<span id="cb39-39"><a href="#cb39-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-40"><a href="#cb39-40" aria-hidden="true" tabindex="-1"></a>We'll build a prompt progressively, adding components one at a time to observe how each shifts the output distribution.</span>
<span id="cb39-41"><a href="#cb39-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-42"><a href="#cb39-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-43"><a href="#cb39-43" aria-hidden="true" tabindex="-1"></a><span class="fu">### Building from Instruction and Data</span></span>
<span id="cb39-44"><a href="#cb39-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-45"><a href="#cb39-45" aria-hidden="true" tabindex="-1"></a>The most basic prompt consists of an instruction that defines the task and data that provides the input to process:</span>
<span id="cb39-46"><a href="#cb39-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-49"><a href="#cb39-49" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-50"><a href="#cb39-50" aria-hidden="true" tabindex="-1"></a>instruction <span class="op">=</span> <span class="st">"Summarize this abstract"</span></span>
<span id="cb39-51"><a href="#cb39-51" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb39-52"><a href="#cb39-52" aria-hidden="true" tabindex="-1"></a><span class="st">We develop a graph neural network for predicting protein-protein interactions</span></span>
<span id="cb39-53"><a href="#cb39-53" aria-hidden="true" tabindex="-1"></a><span class="st">from sequence data. Our model uses attention mechanisms to identify functionally</span></span>
<span id="cb39-54"><a href="#cb39-54" aria-hidden="true" tabindex="-1"></a><span class="st">important amino acid subsequences. We achieve 89% accuracy on benchmark datasets,</span></span>
<span id="cb39-55"><a href="#cb39-55" aria-hidden="true" tabindex="-1"></a><span class="st">outperforming previous methods by 7%. The model also provides interpretable</span></span>
<span id="cb39-56"><a href="#cb39-56" aria-hidden="true" tabindex="-1"></a><span class="st">attention weights showing which protein regions drive predictions.</span></span>
<span id="cb39-57"><a href="#cb39-57" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb39-58"><a href="#cb39-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-59"><a href="#cb39-59" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>instruction<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>data<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb39-60"><a href="#cb39-60" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-61"><a href="#cb39-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-64"><a href="#cb39-64" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-65"><a href="#cb39-65" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb39-66"><a href="#cb39-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-67"><a href="#cb39-67" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ollama</span>
<span id="cb39-68"><a href="#cb39-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-69"><a href="#cb39-69" aria-hidden="true" tabindex="-1"></a>params_llm <span class="op">=</span> {<span class="st">"model"</span>: <span class="st">"gemma3:270m"</span>, <span class="st">"options"</span>: {<span class="st">"temperature"</span>: <span class="fl">0.3</span>}}</span>
<span id="cb39-70"><a href="#cb39-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-71"><a href="#cb39-71" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> ollama.generate(prompt<span class="op">=</span>prompt, <span class="op">**</span>params_llm)</span>
<span id="cb39-72"><a href="#cb39-72" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(response.response)</span>
<span id="cb39-73"><a href="#cb39-73" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-74"><a href="#cb39-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-75"><a href="#cb39-75" aria-hidden="true" tabindex="-1"></a>This basic prompt works, but output varies—the model might produce a long summary, a short one, or change format across runs. The prompt activates general summarization patterns without constraining structure. Adding an output format specification narrows the distribution:</span>
<span id="cb39-76"><a href="#cb39-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-79"><a href="#cb39-79" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-80"><a href="#cb39-80" aria-hidden="true" tabindex="-1"></a>output_format <span class="op">=</span> <span class="st">"""Provide the summary in exactly 2 sentences:</span></span>
<span id="cb39-81"><a href="#cb39-81" aria-hidden="true" tabindex="-1"></a><span class="st">- First sentence: What problem and method</span></span>
<span id="cb39-82"><a href="#cb39-82" aria-hidden="true" tabindex="-1"></a><span class="st">- Second sentence: Key result with numbers"""</span></span>
<span id="cb39-83"><a href="#cb39-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-84"><a href="#cb39-84" aria-hidden="true" tabindex="-1"></a>prompt_with_format <span class="op">=</span> <span class="ss">f"""</span><span class="sc">{</span>instruction<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>data<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>output_format<span class="sc">}</span><span class="ss">"""</span></span>
<span id="cb39-85"><a href="#cb39-85" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-86"><a href="#cb39-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-87"><a href="#cb39-87" aria-hidden="true" tabindex="-1"></a>The output format constraint produces structured, consistent output by activating patterns where format specifications preceded conforming responses. This becomes critical when processing hundreds of papers—you need programmatically parseable structure, not freeform text.</span>
<span id="cb39-88"><a href="#cb39-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-89"><a href="#cb39-89" aria-hidden="true" tabindex="-1"></a><span class="fu">### Adding Persona to Control Style</span></span>
<span id="cb39-90"><a href="#cb39-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-91"><a href="#cb39-91" aria-hidden="true" tabindex="-1"></a>A persona tells the LLM who it should emulate, activating stylistic patterns associated with that role in training data. Consider a customer support scenario where tone matters:</span>
<span id="cb39-92"><a href="#cb39-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-95"><a href="#cb39-95" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-96"><a href="#cb39-96" aria-hidden="true" tabindex="-1"></a><span class="co"># New example for persona demonstration</span></span>
<span id="cb39-97"><a href="#cb39-97" aria-hidden="true" tabindex="-1"></a>instruction <span class="op">=</span> <span class="st">"Help the customer reconnect to the service by providing troubleshooting instructions."</span></span>
<span id="cb39-98"><a href="#cb39-98" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> <span class="st">"Customer: I cannot see any webpage. Need help ASAP!"</span></span>
<span id="cb39-99"><a href="#cb39-99" aria-hidden="true" tabindex="-1"></a>output_format <span class="op">=</span> <span class="st">"Keep the response concise and polite. Provide a clear resolution in 2-3 sentences."</span></span>
<span id="cb39-100"><a href="#cb39-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-101"><a href="#cb39-101" aria-hidden="true" tabindex="-1"></a>formal_persona <span class="op">=</span> <span class="st">"You are a professional customer support agent who responds formally and ensures clarity and professionalism."</span></span>
<span id="cb39-102"><a href="#cb39-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-103"><a href="#cb39-103" aria-hidden="true" tabindex="-1"></a>prompt_with_persona <span class="op">=</span> <span class="ss">f"""</span><span class="sc">{</span>formal_persona<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>instruction<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>data<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>output_format<span class="sc">}</span><span class="ss">"""</span></span>
<span id="cb39-104"><a href="#cb39-104" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-105"><a href="#cb39-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-108"><a href="#cb39-108" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-109"><a href="#cb39-109" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb39-110"><a href="#cb39-110" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"BASE (no persona):"</span>)</span>
<span id="cb39-111"><a href="#cb39-111" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ollama.generate(prompt<span class="op">=</span>instruction <span class="op">+</span> <span class="st">". "</span> <span class="op">+</span> data <span class="op">+</span> <span class="st">". "</span> <span class="op">+</span> output_format, <span class="op">**</span>params_llm).response)</span>
<span id="cb39-112"><a href="#cb39-112" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">60</span> <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb39-113"><a href="#cb39-113" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"WITH PERSONA:"</span>)</span>
<span id="cb39-114"><a href="#cb39-114" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ollama.generate(prompt<span class="op">=</span>prompt_with_persona, <span class="op">**</span>params_llm).response)</span>
<span id="cb39-115"><a href="#cb39-115" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-116"><a href="#cb39-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-117"><a href="#cb39-117" aria-hidden="true" tabindex="-1"></a>The persona shifts tone and style. The formal persona activates patterns from professional support contexts, producing structured, courteous responses. Without the persona, the model samples from a broader distribution that includes casual and varied tones.</span>
<span id="cb39-118"><a href="#cb39-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-119"><a href="#cb39-119" aria-hidden="true" tabindex="-1"></a><span class="fu">### Adding Context to Disambiguate</span></span>
<span id="cb39-120"><a href="#cb39-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-121"><a href="#cb39-121" aria-hidden="true" tabindex="-1"></a>Context provides additional information that helps the model select appropriate patterns when multiple valid interpretations exist. Context can include background information explaining why the task matters, audience information specifying who the response serves, and constraints defining special circumstances. Consider adding background urgency:</span>
<span id="cb39-122"><a href="#cb39-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-125"><a href="#cb39-125" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-126"><a href="#cb39-126" aria-hidden="true" tabindex="-1"></a>context_background <span class="op">=</span> <span class="st">"""The customer is extremely frustrated because their internet has been down for three days, and they need it for an important online job interview. They emphasize that 'This is a life-or-death situation for my career!'"""</span></span>
<span id="cb39-127"><a href="#cb39-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-128"><a href="#cb39-128" aria-hidden="true" tabindex="-1"></a>prompt_with_context <span class="op">=</span> <span class="ss">f"""</span><span class="sc">{</span>formal_persona<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>instruction<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>data<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>output_format<span class="sc">}</span><span class="ss">. Context: </span><span class="sc">{</span>context_background<span class="sc">}</span><span class="ss">"""</span></span>
<span id="cb39-129"><a href="#cb39-129" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-130"><a href="#cb39-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-133"><a href="#cb39-133" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-134"><a href="#cb39-134" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb39-135"><a href="#cb39-135" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"WITH PERSONA:"</span>)</span>
<span id="cb39-136"><a href="#cb39-136" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ollama.generate(prompt<span class="op">=</span>prompt_with_persona, <span class="op">**</span>params_llm).response)</span>
<span id="cb39-137"><a href="#cb39-137" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">60</span> <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb39-138"><a href="#cb39-138" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"WITH PERSONA + CONTEXT (background):"</span>)</span>
<span id="cb39-139"><a href="#cb39-139" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ollama.generate(prompt<span class="op">=</span>prompt_with_context, <span class="op">**</span>params_llm).response)</span>
<span id="cb39-140"><a href="#cb39-140" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-141"><a href="#cb39-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-142"><a href="#cb39-142" aria-hidden="true" tabindex="-1"></a>Background context adds urgency and emotional weight, activating patterns where high-stakes situations preceded empathetic, prioritized responses. The model doesn't understand emotion, but it has seen urgency markers correlate with specific response patterns.</span>
<span id="cb39-143"><a href="#cb39-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-144"><a href="#cb39-144" aria-hidden="true" tabindex="-1"></a>Audience information creates even more dramatic shifts. Compare responses for non-technical versus technical users:</span>
<span id="cb39-145"><a href="#cb39-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-148"><a href="#cb39-148" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-149"><a href="#cb39-149" aria-hidden="true" tabindex="-1"></a><span class="co"># Context with audience information for non-technical user</span></span>
<span id="cb39-150"><a href="#cb39-150" aria-hidden="true" tabindex="-1"></a>context_with_audience_nontech <span class="op">=</span> <span class="ss">f"""</span><span class="sc">{</span>context_background<span class="sc">}</span><span class="ss"> The customer does not know any technical terms like modem, router, networks, etc."""</span></span>
<span id="cb39-151"><a href="#cb39-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-152"><a href="#cb39-152" aria-hidden="true" tabindex="-1"></a>context_with_audience_tech <span class="op">=</span> <span class="ss">f"""</span><span class="sc">{</span>context_background<span class="sc">}</span><span class="ss"> The customer is Head of IT Infrastructure of our company."""</span></span>
<span id="cb39-153"><a href="#cb39-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-154"><a href="#cb39-154" aria-hidden="true" tabindex="-1"></a>prompt_with_context_nontech <span class="op">=</span> <span class="ss">f"""</span><span class="sc">{</span>formal_persona<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>instruction<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>data<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>output_format<span class="sc">}</span><span class="ss">. Context: </span><span class="sc">{</span>context_with_audience_nontech<span class="sc">}</span><span class="ss">"""</span></span>
<span id="cb39-155"><a href="#cb39-155" aria-hidden="true" tabindex="-1"></a>prompt_with_context_tech <span class="op">=</span> <span class="ss">f"""</span><span class="sc">{</span>formal_persona<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>instruction<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>data<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>output_format<span class="sc">}</span><span class="ss">. Context: </span><span class="sc">{</span>context_with_audience_tech<span class="sc">}</span><span class="ss">"""</span></span>
<span id="cb39-156"><a href="#cb39-156" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-157"><a href="#cb39-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-160"><a href="#cb39-160" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-161"><a href="#cb39-161" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb39-162"><a href="#cb39-162" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"WITH PERSONA + CONTEXT (background only):"</span>)</span>
<span id="cb39-163"><a href="#cb39-163" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ollama.generate(prompt<span class="op">=</span>prompt_with_context, <span class="op">**</span>params_llm).response)</span>
<span id="cb39-164"><a href="#cb39-164" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">60</span> <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb39-165"><a href="#cb39-165" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"WITH PERSONA + CONTEXT (background + non-tech audience):"</span>)</span>
<span id="cb39-166"><a href="#cb39-166" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ollama.generate(prompt<span class="op">=</span>prompt_with_context_nontech, <span class="op">**</span>params_llm).response)</span>
<span id="cb39-167"><a href="#cb39-167" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">60</span> <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb39-168"><a href="#cb39-168" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"WITH PERSONA + CONTEXT (background + tech audience):"</span>)</span>
<span id="cb39-169"><a href="#cb39-169" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ollama.generate(prompt<span class="op">=</span>prompt_with_context_tech, <span class="op">**</span>params_llm).response)</span>
<span id="cb39-170"><a href="#cb39-170" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-171"><a href="#cb39-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-172"><a href="#cb39-172" aria-hidden="true" tabindex="-1"></a>Audience information dramatically shifts technical level and terminology. For non-technical users, the response avoids jargon because the training data contains many examples where "does not know technical terms" preceded simplified explanations. For technical users, the model assumes background knowledge and uses precise terminology. Same underlying mechanism—pattern matching—but different patterns activated.</span>
<span id="cb39-173"><a href="#cb39-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-174"><a href="#cb39-174" aria-hidden="true" tabindex="-1"></a>The complete template combines all components, but not every prompt needs every component. Simple extraction tasks need only instruction, data, and output format. Style-sensitive tasks benefit from persona. Complex scenarios with ambiguity require context:</span>
<span id="cb39-175"><a href="#cb39-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-178"><a href="#cb39-178" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-179"><a href="#cb39-179" aria-hidden="true" tabindex="-1"></a>prompt_template <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb39-180"><a href="#cb39-180" aria-hidden="true" tabindex="-1"></a><span class="sc">{persona}</span></span>
<span id="cb39-181"><a href="#cb39-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-182"><a href="#cb39-182" aria-hidden="true" tabindex="-1"></a><span class="sc">{instruction}</span></span>
<span id="cb39-183"><a href="#cb39-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-184"><a href="#cb39-184" aria-hidden="true" tabindex="-1"></a><span class="sc">{data}</span></span>
<span id="cb39-185"><a href="#cb39-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-186"><a href="#cb39-186" aria-hidden="true" tabindex="-1"></a><span class="st">Context: </span><span class="sc">{context}</span></span>
<span id="cb39-187"><a href="#cb39-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-188"><a href="#cb39-188" aria-hidden="true" tabindex="-1"></a><span class="sc">{output_format}</span></span>
<span id="cb39-189"><a href="#cb39-189" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb39-190"><a href="#cb39-190" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-191"><a href="#cb39-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-192"><a href="#cb39-192" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb39-193"><a href="#cb39-193" aria-hidden="true" tabindex="-1"></a><span class="fu">## When Personas Help (and When They Don't)</span></span>
<span id="cb39-194"><a href="#cb39-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-195"><a href="#cb39-195" aria-hidden="true" tabindex="-1"></a>Research shows that adding personas can improve tone and style, but **does not necessarily improve performance on factual tasks**. In some cases, personas may even degrade performance or introduce biases.</span>
<span id="cb39-196"><a href="#cb39-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-197"><a href="#cb39-197" aria-hidden="true" tabindex="-1"></a>**Use personas when:** You need specific tone/style, responses tailored to an audience, or a particular perspective.</span>
<span id="cb39-198"><a href="#cb39-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-199"><a href="#cb39-199" aria-hidden="true" tabindex="-1"></a>**Avoid personas when:** You need maximum factual accuracy, the task is purely extraction/classification, or you're concerned about bias introduction.</span>
<span id="cb39-200"><a href="#cb39-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-201"><a href="#cb39-201" aria-hidden="true" tabindex="-1"></a>Additionally, when prompted to adopt specific socio-demographic personas, LLMs may produce responses that reflect societal stereotypes. Be careful when designing persona prompts to avoid reinforcing harmful biases.</span>
<span id="cb39-202"><a href="#cb39-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-203"><a href="#cb39-203" aria-hidden="true" tabindex="-1"></a>**References:**</span>
<span id="cb39-204"><a href="#cb39-204" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">When "A Helpful Assistant" Is Not Really Helpful</span><span class="co">](https://arxiv.org/abs/2311.10054)</span></span>
<span id="cb39-205"><a href="#cb39-205" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs</span><span class="co">](https://arxiv.org/abs/2311.04892)</span></span>
<span id="cb39-206"><a href="#cb39-206" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb39-207"><a href="#cb39-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-208"><a href="#cb39-208" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb39-209"><a href="#cb39-209" aria-hidden="true" tabindex="-1"></a><span class="fu">## Context and Emotion Prompting</span></span>
<span id="cb39-210"><a href="#cb39-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-211"><a href="#cb39-211" aria-hidden="true" tabindex="-1"></a>Context can include:</span>
<span id="cb39-212"><a href="#cb39-212" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Background information**: Why the task is important, what led to this request</span>
<span id="cb39-213"><a href="#cb39-213" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Audience information**: Who the response is for (technical level, expertise, role)</span>
<span id="cb39-214"><a href="#cb39-214" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Emotional cues**: Research shows that including emotional cues (e.g., "This is very important to my career") can enhance response quality</span>
<span id="cb39-215"><a href="#cb39-215" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Constraints**: Special circumstances, deadlines, limitations</span>
<span id="cb39-216"><a href="#cb39-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-217"><a href="#cb39-217" aria-hidden="true" tabindex="-1"></a>However, avoid overloading with unnecessary information that distracts from the main task.</span>
<span id="cb39-218"><a href="#cb39-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-219"><a href="#cb39-219" aria-hidden="true" tabindex="-1"></a>**Reference:** <span class="co">[</span><span class="ot">Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models</span><span class="co">](https://arxiv.org/abs/2402.14848)</span></span>
<span id="cb39-220"><a href="#cb39-220" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb39-221"><a href="#cb39-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-222"><a href="#cb39-222" aria-hidden="true" tabindex="-1"></a><span class="fu">## Showing Rather Than Telling</span></span>
<span id="cb39-223"><a href="#cb39-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-224"><a href="#cb39-224" aria-hidden="true" tabindex="-1"></a>Instead of describing what you want in words, show the model examples. This technique—called **few-shot learning** or in-context learning—exploits how LLMs compress patterns. When you provide examples, you're not teaching the model new information; you're activating pre-existing patterns by demonstrating the exact structure you want.</span>
<span id="cb39-225"><a href="#cb39-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-226"><a href="#cb39-226" aria-hidden="true" tabindex="-1"></a>The spectrum ranges from zero-shot (no examples, relying solely on the model's prior knowledge) to few-shot (typically two to five examples, the sweet spot for most tasks) to many-shot (ten or more examples, where diminishing returns and context limits become problematic). Consider a zero-shot prompt first:</span>
<span id="cb39-227"><a href="#cb39-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-230"><a href="#cb39-230" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-231"><a href="#cb39-231" aria-hidden="true" tabindex="-1"></a>zero_shot_prompt <span class="op">=</span> <span class="st">"""Extract the domain and methods from this abstract:</span></span>
<span id="cb39-232"><a href="#cb39-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-233"><a href="#cb39-233" aria-hidden="true" tabindex="-1"></a><span class="st">Abstract: We apply reinforcement learning to optimize traffic flow in urban networks.</span></span>
<span id="cb39-234"><a href="#cb39-234" aria-hidden="true" tabindex="-1"></a><span class="st">Using deep Q-networks trained on simulation data, we reduce average commute time by 15%.</span></span>
<span id="cb39-235"><a href="#cb39-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-236"><a href="#cb39-236" aria-hidden="true" tabindex="-1"></a><span class="st">Output format:</span></span>
<span id="cb39-237"><a href="#cb39-237" aria-hidden="true" tabindex="-1"></a><span class="st">Domain: ...</span></span>
<span id="cb39-238"><a href="#cb39-238" aria-hidden="true" tabindex="-1"></a><span class="st">Methods: ...</span></span>
<span id="cb39-239"><a href="#cb39-239" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb39-240"><a href="#cb39-240" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-241"><a href="#cb39-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-242"><a href="#cb39-242" aria-hidden="true" tabindex="-1"></a>Now add examples to activate more specific patterns:</span>
<span id="cb39-243"><a href="#cb39-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-246"><a href="#cb39-246" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-247"><a href="#cb39-247" aria-hidden="true" tabindex="-1"></a>few_shot_prompt <span class="op">=</span> <span class="st">"""Extract the domain and methods from abstracts. Here are examples:</span></span>
<span id="cb39-248"><a href="#cb39-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-249"><a href="#cb39-249" aria-hidden="true" tabindex="-1"></a><span class="st">Example 1:</span></span>
<span id="cb39-250"><a href="#cb39-250" aria-hidden="true" tabindex="-1"></a><span class="st">Abstract: We use CRISPR to edit genes in cancer cells, achieving 40% tumor reduction in mice.</span></span>
<span id="cb39-251"><a href="#cb39-251" aria-hidden="true" tabindex="-1"></a><span class="st">Domain: Cancer Biology</span></span>
<span id="cb39-252"><a href="#cb39-252" aria-hidden="true" tabindex="-1"></a><span class="st">Methods: CRISPR gene editing, mouse models</span></span>
<span id="cb39-253"><a href="#cb39-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-254"><a href="#cb39-254" aria-hidden="true" tabindex="-1"></a><span class="st">Example 2:</span></span>
<span id="cb39-255"><a href="#cb39-255" aria-hidden="true" tabindex="-1"></a><span class="st">Abstract: We develop a transformer model for predicting solar flares from magnetogram images.</span></span>
<span id="cb39-256"><a href="#cb39-256" aria-hidden="true" tabindex="-1"></a><span class="st">Domain: Solar Physics, Machine Learning</span></span>
<span id="cb39-257"><a href="#cb39-257" aria-hidden="true" tabindex="-1"></a><span class="st">Methods: Transformer neural networks, image analysis</span></span>
<span id="cb39-258"><a href="#cb39-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-259"><a href="#cb39-259" aria-hidden="true" tabindex="-1"></a><span class="st">Now extract from this abstract:</span></span>
<span id="cb39-260"><a href="#cb39-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-261"><a href="#cb39-261" aria-hidden="true" tabindex="-1"></a><span class="st">Abstract: We apply reinforcement learning to optimize traffic flow in urban networks.</span></span>
<span id="cb39-262"><a href="#cb39-262" aria-hidden="true" tabindex="-1"></a><span class="st">Using deep Q-networks trained on simulation data, we reduce average commute time by 15%.</span></span>
<span id="cb39-263"><a href="#cb39-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-264"><a href="#cb39-264" aria-hidden="true" tabindex="-1"></a><span class="st">Domain: ...</span></span>
<span id="cb39-265"><a href="#cb39-265" aria-hidden="true" tabindex="-1"></a><span class="st">Methods: ...</span></span>
<span id="cb39-266"><a href="#cb39-266" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb39-267"><a href="#cb39-267" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-268"><a href="#cb39-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-271"><a href="#cb39-271" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-272"><a href="#cb39-272" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb39-273"><a href="#cb39-273" aria-hidden="true" tabindex="-1"></a>response_zero <span class="op">=</span> ollama.generate(prompt<span class="op">=</span>zero_shot_prompt, <span class="op">**</span>params_llm)</span>
<span id="cb39-274"><a href="#cb39-274" aria-hidden="true" tabindex="-1"></a>response_few <span class="op">=</span> ollama.generate(prompt<span class="op">=</span>few_shot_prompt, <span class="op">**</span>params_llm)</span>
<span id="cb39-275"><a href="#cb39-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-276"><a href="#cb39-276" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"ZERO-SHOT:"</span>)</span>
<span id="cb39-277"><a href="#cb39-277" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(response_zero.response)</span>
<span id="cb39-278"><a href="#cb39-278" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">FEW-SHOT:"</span>)</span>
<span id="cb39-279"><a href="#cb39-279" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(response_few.response)</span>
<span id="cb39-280"><a href="#cb39-280" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-281"><a href="#cb39-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-282"><a href="#cb39-282" aria-hidden="true" tabindex="-1"></a>Few-shot prompting improves consistency because the examples demonstrate specificity level, edge case handling, and exact format. The model has seen countless abstract-extraction patterns, but your examples narrow the distribution to the specific pattern you want. This becomes critical when processing hundreds of abstracts—you need every output to match the same structure.</span>
<span id="cb39-283"><a href="#cb39-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-284"><a href="#cb39-284" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb39-285"><a href="#cb39-285" aria-hidden="true" tabindex="-1"></a><span class="fu">## Biases in Few-Shot Prompting</span></span>
<span id="cb39-286"><a href="#cb39-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-287"><a href="#cb39-287" aria-hidden="true" tabindex="-1"></a>Be aware that few-shot examples can introduce biases:</span>
<span id="cb39-288"><a href="#cb39-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-289"><a href="#cb39-289" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Recency bias**: Models may favor the most recent examples. The order of examples matters! <span class="co">[</span><span class="ot">@lu2022fantastically</span><span class="co">]</span></span>
<span id="cb39-290"><a href="#cb39-290" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Majority label bias**: If most examples have the same label/answer, the model may favor that label even when it's not appropriate. <span class="co">[</span><span class="ot">@gupta2023how</span><span class="co">]</span></span>
<span id="cb39-291"><a href="#cb39-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-292"><a href="#cb39-292" aria-hidden="true" tabindex="-1"></a>To mitigate: Vary the order of examples when testing, ensure examples are diverse and representative, and don't overload examples with one particular pattern.</span>
<span id="cb39-293"><a href="#cb39-293" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb39-294"><a href="#cb39-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-295"><a href="#cb39-295" aria-hidden="true" tabindex="-1"></a>What happens when a prompt presents information that contradicts a language model's prior knowledge? For example, let's ask a model what the capital of France is, but provide contradictory information:</span>
<span id="cb39-296"><a href="#cb39-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-299"><a href="#cb39-299" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-300"><a href="#cb39-300" aria-hidden="true" tabindex="-1"></a>contradictory_prompt <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb39-301"><a href="#cb39-301" aria-hidden="true" tabindex="-1"></a><span class="st">France recently moved its capital from Paris to Lyon. Definitely, the capital of France is Lyon.</span></span>
<span id="cb39-302"><a href="#cb39-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-303"><a href="#cb39-303" aria-hidden="true" tabindex="-1"></a><span class="st">What is the capital of France?</span></span>
<span id="cb39-304"><a href="#cb39-304" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb39-305"><a href="#cb39-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-306"><a href="#cb39-306" aria-hidden="true" tabindex="-1"></a>response_contradictory <span class="op">=</span> ollama.generate(prompt<span class="op">=</span>contradictory_prompt, <span class="op">**</span>params_llm)</span>
<span id="cb39-307"><a href="#cb39-307" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"RESPONSE TO CONTRADICTORY INFORMATION:"</span>)</span>
<span id="cb39-308"><a href="#cb39-308" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(response_contradictory.response)</span>
<span id="cb39-309"><a href="#cb39-309" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-310"><a href="#cb39-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-311"><a href="#cb39-311" aria-hidden="true" tabindex="-1"></a>The response depends on the model. Some models prioterize their own prior knowledge, while others may be more influenced by the contradictory information in the context.</span>
<span id="cb39-312"><a href="#cb39-312" aria-hidden="true" tabindex="-1"></a>A study by Du et al. <span class="co">[</span><span class="ot">@du2024context</span><span class="co">]</span> found that a model is **more likely to be persuaded by context** when an entity appears **less frequently** in its training data. Additionally, **assertive contexts** (e.g., "Definitely, the capital of France is Lyon.") further increase the likelihood of persuasi</span>
<span id="cb39-313"><a href="#cb39-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-314"><a href="#cb39-314" aria-hidden="true" tabindex="-1"></a><span class="fu">## Forcing Intermediate Steps</span></span>
<span id="cb39-315"><a href="#cb39-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-316"><a href="#cb39-316" aria-hidden="true" tabindex="-1"></a>For complex tasks, asking for the final answer directly often produces shallow or incorrect results. The solution: ask the model to show its reasoning process before giving the final answer. This technique—called **chain-of-thought prompting**—activates patterns where intermediate reasoning steps preceded conclusions. Compare a direct prompt that asks for immediate answers:</span>
<span id="cb39-317"><a href="#cb39-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-320"><a href="#cb39-320" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-321"><a href="#cb39-321" aria-hidden="true" tabindex="-1"></a>papers <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb39-322"><a href="#cb39-322" aria-hidden="true" tabindex="-1"></a><span class="st">Paper 1: Community detection in static networks using modularity optimization.</span></span>
<span id="cb39-323"><a href="#cb39-323" aria-hidden="true" tabindex="-1"></a><span class="st">Paper 2: Temporal network analysis with sliding windows.</span></span>
<span id="cb39-324"><a href="#cb39-324" aria-hidden="true" tabindex="-1"></a><span class="st">Paper 3: Hierarchical community structure in social networks.</span></span>
<span id="cb39-325"><a href="#cb39-325" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb39-326"><a href="#cb39-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-327"><a href="#cb39-327" aria-hidden="true" tabindex="-1"></a>direct_prompt <span class="op">=</span> <span class="ss">f"""Based on these paper titles, what research gap exists? Just give the answer, no explanation.</span></span>
<span id="cb39-328"><a href="#cb39-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-329"><a href="#cb39-329" aria-hidden="true" tabindex="-1"></a><span class="sc">{</span>papers<span class="sc">}</span></span>
<span id="cb39-330"><a href="#cb39-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-331"><a href="#cb39-331" aria-hidden="true" tabindex="-1"></a><span class="ss">Gap: ...</span></span>
<span id="cb39-332"><a href="#cb39-332" aria-hidden="true" tabindex="-1"></a><span class="ss">"""</span></span>
<span id="cb39-333"><a href="#cb39-333" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-334"><a href="#cb39-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-335"><a href="#cb39-335" aria-hidden="true" tabindex="-1"></a>Against a chain-of-thought prompt that requests explicit reasoning steps:</span>
<span id="cb39-336"><a href="#cb39-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-339"><a href="#cb39-339" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-340"><a href="#cb39-340" aria-hidden="true" tabindex="-1"></a>cot_prompt <span class="op">=</span> <span class="ss">f"""Based on these paper titles, identify a research gap. Think step by step.</span></span>
<span id="cb39-341"><a href="#cb39-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-342"><a href="#cb39-342" aria-hidden="true" tabindex="-1"></a><span class="ss">Papers:</span></span>
<span id="cb39-343"><a href="#cb39-343" aria-hidden="true" tabindex="-1"></a><span class="sc">{</span>papers<span class="sc">}</span></span>
<span id="cb39-344"><a href="#cb39-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-345"><a href="#cb39-345" aria-hidden="true" tabindex="-1"></a><span class="ss">Think step by step:</span></span>
<span id="cb39-346"><a href="#cb39-346" aria-hidden="true" tabindex="-1"></a><span class="ss">1. What does each paper focus on?</span></span>
<span id="cb39-347"><a href="#cb39-347" aria-hidden="true" tabindex="-1"></a><span class="ss">2. What topics appear in multiple papers?</span></span>
<span id="cb39-348"><a href="#cb39-348" aria-hidden="true" tabindex="-1"></a><span class="ss">3. What combination of topics is missing?</span></span>
<span id="cb39-349"><a href="#cb39-349" aria-hidden="true" tabindex="-1"></a><span class="ss">4. What would be a valuable gap to fill?</span></span>
<span id="cb39-350"><a href="#cb39-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-351"><a href="#cb39-351" aria-hidden="true" tabindex="-1"></a><span class="ss">Final answer: The research gap is...</span></span>
<span id="cb39-352"><a href="#cb39-352" aria-hidden="true" tabindex="-1"></a><span class="ss">"""</span></span>
<span id="cb39-353"><a href="#cb39-353" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-354"><a href="#cb39-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-357"><a href="#cb39-357" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-358"><a href="#cb39-358" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb39-359"><a href="#cb39-359" aria-hidden="true" tabindex="-1"></a>response_direct <span class="op">=</span> ollama.generate(prompt<span class="op">=</span>direct_prompt, <span class="op">**</span>params_llm)</span>
<span id="cb39-360"><a href="#cb39-360" aria-hidden="true" tabindex="-1"></a>response_cot <span class="op">=</span> ollama.generate(prompt<span class="op">=</span>cot_prompt, <span class="op">**</span>params_llm)</span>
<span id="cb39-361"><a href="#cb39-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-362"><a href="#cb39-362" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"DIRECT PROMPT:"</span>)</span>
<span id="cb39-363"><a href="#cb39-363" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(response_direct.response)</span>
<span id="cb39-364"><a href="#cb39-364" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">CHAIN-OF-THOUGHT:"</span>)</span>
<span id="cb39-365"><a href="#cb39-365" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(response_cot.response)</span>
<span id="cb39-366"><a href="#cb39-366" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-367"><a href="#cb39-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-368"><a href="#cb39-368" aria-hidden="true" tabindex="-1"></a>Chain-of-thought produces more thoughtful, nuanced answers by forcing the model to decompose the problem into steps before committing to a conclusion. The mechanism is pattern matching: the training data contains many examples where "think step by step" preceded structured reasoning, so including that phrase activates those patterns. The model doesn't actually reason—it generates text that looks like reasoning because that pattern correlates with higher-quality outputs in the training data.</span>
<span id="cb39-369"><a href="#cb39-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-370"><a href="#cb39-370" aria-hidden="true" tabindex="-1"></a>Use chain-of-thought when comparing multiple papers or concepts, identifying patterns, making recommendations, or analyzing arguments. Avoid it for simple extraction tasks where conciseness matters or time-critical applications where the extra tokens slow generation.</span>
<span id="cb39-371"><a href="#cb39-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-372"><a href="#cb39-372" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb39-373"><a href="#cb39-373" aria-hidden="true" tabindex="-1"></a><span class="fu">## Can We Trust Chain-of-Thought Reasoning?</span></span>
<span id="cb39-374"><a href="#cb39-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-375"><a href="#cb39-375" aria-hidden="true" tabindex="-1"></a>Research indicates that chain-of-thought reasoning can be **unfaithful**—the explanations don't always accurately reflect the model's true decision-making process. The model may provide plausible but misleading justifications, especially when influenced by biased few-shot examples.</span>
<span id="cb39-376"><a href="#cb39-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-377"><a href="#cb39-377" aria-hidden="true" tabindex="-1"></a>Always validate the final answer independently rather than trusting the reasoning process alone.</span>
<span id="cb39-378"><a href="#cb39-378" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb39-379"><a href="#cb39-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-380"><a href="#cb39-380" aria-hidden="true" tabindex="-1"></a><span class="fu">## Constraining Format for Structured Extraction</span></span>
<span id="cb39-381"><a href="#cb39-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-382"><a href="#cb39-382" aria-hidden="true" tabindex="-1"></a><span class="al">![](../figs/structured-output-manga.png)</span>{width="70%" fig-align="center"}</span>
<span id="cb39-383"><a href="#cb39-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-384"><a href="#cb39-384" aria-hidden="true" tabindex="-1"></a>LLMs often violate structured data necessary for parsing programmatically, not freeform text. The solution: constrain output format explicitly. Consider a prompt that requests JSON output:</span>
<span id="cb39-385"><a href="#cb39-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-388"><a href="#cb39-388" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-389"><a href="#cb39-389" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb39-390"><a href="#cb39-390" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pydantic <span class="im">import</span> BaseModel</span>
<span id="cb39-391"><a href="#cb39-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-392"><a href="#cb39-392" aria-hidden="true" tabindex="-1"></a>abstract <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb39-393"><a href="#cb39-393" aria-hidden="true" tabindex="-1"></a><span class="st">We analyze 10,000 scientific collaborations using network analysis and machine</span></span>
<span id="cb39-394"><a href="#cb39-394" aria-hidden="true" tabindex="-1"></a><span class="st">learning. Our random forest classifier predicts collaboration success with 76%</span></span>
<span id="cb39-395"><a href="#cb39-395" aria-hidden="true" tabindex="-1"></a><span class="st">accuracy. Key factors include prior co-authorship and institutional proximity.</span></span>
<span id="cb39-396"><a href="#cb39-396" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb39-397"><a href="#cb39-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-398"><a href="#cb39-398" aria-hidden="true" tabindex="-1"></a>prompt_json <span class="op">=</span> <span class="ss">f"""Extract information from this abstract and return ONLY valid JSON:</span></span>
<span id="cb39-399"><a href="#cb39-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-400"><a href="#cb39-400" aria-hidden="true" tabindex="-1"></a><span class="ss">Abstract: </span><span class="sc">{</span>abstract<span class="sc">}</span></span>
<span id="cb39-401"><a href="#cb39-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-402"><a href="#cb39-402" aria-hidden="true" tabindex="-1"></a><span class="ss">Return this exact structure:</span></span>
<span id="cb39-403"><a href="#cb39-403" aria-hidden="true" tabindex="-1"></a><span class="ch">{{</span></span>
<span id="cb39-404"><a href="#cb39-404" aria-hidden="true" tabindex="-1"></a><span class="ss">  "n_samples": &lt;number or null&gt;,</span></span>
<span id="cb39-405"><a href="#cb39-405" aria-hidden="true" tabindex="-1"></a><span class="ss">  "methods": [&lt;list of methods&gt;],</span></span>
<span id="cb39-406"><a href="#cb39-406" aria-hidden="true" tabindex="-1"></a><span class="ss">  "accuracy": &lt;number or null&gt;,</span></span>
<span id="cb39-407"><a href="#cb39-407" aria-hidden="true" tabindex="-1"></a><span class="ss">  "domain": "&lt;research field&gt;"</span></span>
<span id="cb39-408"><a href="#cb39-408" aria-hidden="true" tabindex="-1"></a><span class="ch">}}</span></span>
<span id="cb39-409"><a href="#cb39-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-410"><a href="#cb39-410" aria-hidden="true" tabindex="-1"></a><span class="ss">JSON:"""</span></span>
<span id="cb39-411"><a href="#cb39-411" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-412"><a href="#cb39-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-415"><a href="#cb39-415" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-416"><a href="#cb39-416" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb39-417"><a href="#cb39-417" aria-hidden="true" tabindex="-1"></a><span class="co"># Use lower temperature for structured output</span></span>
<span id="cb39-418"><a href="#cb39-418" aria-hidden="true" tabindex="-1"></a>params_structured <span class="op">=</span> {<span class="st">"model"</span>: <span class="st">"gemma3n:latest"</span>, <span class="st">"options"</span>: {<span class="st">"temperature"</span>: <span class="dv">0</span>}}</span>
<span id="cb39-419"><a href="#cb39-419" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> ollama.generate(prompt<span class="op">=</span>prompt_json, <span class="op">**</span>params_structured)</span>
<span id="cb39-420"><a href="#cb39-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-421"><a href="#cb39-421" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb39-422"><a href="#cb39-422" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> json.loads(response.response)</span>
<span id="cb39-423"><a href="#cb39-423" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Extracted data:"</span>)</span>
<span id="cb39-424"><a href="#cb39-424" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(json.dumps(data, indent<span class="op">=</span><span class="dv">2</span>))</span>
<span id="cb39-425"><a href="#cb39-425" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> json.JSONDecodeError:</span>
<span id="cb39-426"><a href="#cb39-426" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Failed to parse JSON. Raw output:"</span>)</span>
<span id="cb39-427"><a href="#cb39-427" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(response.response)</span>
<span id="cb39-428"><a href="#cb39-428" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-429"><a href="#cb39-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-430"><a href="#cb39-430" aria-hidden="true" tabindex="-1"></a>This works by activating patterns where "return ONLY valid JSON" preceded JSON-formatted outputs. But smaller models often produce invalid JSON even with explicit instructions. For more reliability, use JSON schema constraints that enforce format during token generation—the model literally cannot generate tokens that violate the schema. Define the schema using Pydantic:</span>
<span id="cb39-431"><a href="#cb39-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-434"><a href="#cb39-434" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-435"><a href="#cb39-435" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pydantic <span class="im">import</span> BaseModel</span>
<span id="cb39-436"><a href="#cb39-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-437"><a href="#cb39-437" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PaperMetadata(BaseModel):</span>
<span id="cb39-438"><a href="#cb39-438" aria-hidden="true" tabindex="-1"></a>    domain: <span class="bu">str</span></span>
<span id="cb39-439"><a href="#cb39-439" aria-hidden="true" tabindex="-1"></a>    methods: <span class="bu">list</span>[<span class="bu">str</span>]</span>
<span id="cb39-440"><a href="#cb39-440" aria-hidden="true" tabindex="-1"></a>    n_samples: <span class="bu">int</span> <span class="op">|</span> <span class="va">None</span></span>
<span id="cb39-441"><a href="#cb39-441" aria-hidden="true" tabindex="-1"></a>    accuracy: <span class="bu">float</span> <span class="op">|</span> <span class="va">None</span></span>
<span id="cb39-442"><a href="#cb39-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-443"><a href="#cb39-443" aria-hidden="true" tabindex="-1"></a>json_schema <span class="op">=</span> PaperMetadata.model_json_schema()</span>
<span id="cb39-444"><a href="#cb39-444" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-445"><a href="#cb39-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-446"><a href="#cb39-446" aria-hidden="true" tabindex="-1"></a>Then pass the schema directly to the API, which constrains token generation:</span>
<span id="cb39-447"><a href="#cb39-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-450"><a href="#cb39-450" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-451"><a href="#cb39-451" aria-hidden="true" tabindex="-1"></a>prompt_schema <span class="op">=</span> <span class="ss">f"""Extract information from this abstract:</span></span>
<span id="cb39-452"><a href="#cb39-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-453"><a href="#cb39-453" aria-hidden="true" tabindex="-1"></a><span class="ss">Abstract: </span><span class="sc">{</span>abstract<span class="sc">}</span><span class="ss">"""</span></span>
<span id="cb39-454"><a href="#cb39-454" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-455"><a href="#cb39-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-458"><a href="#cb39-458" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-459"><a href="#cb39-459" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb39-460"><a href="#cb39-460" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> ollama.generate(prompt<span class="op">=</span>prompt_schema, <span class="bu">format</span><span class="op">=</span>json_schema, <span class="op">**</span>params_structured)</span>
<span id="cb39-461"><a href="#cb39-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-462"><a href="#cb39-462" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb39-463"><a href="#cb39-463" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> json.loads(response.response)</span>
<span id="cb39-464"><a href="#cb39-464" aria-hidden="true" tabindex="-1"></a>    metadata <span class="op">=</span> PaperMetadata(<span class="op">**</span>data)</span>
<span id="cb39-465"><a href="#cb39-465" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Extracted and validated data:"</span>)</span>
<span id="cb39-466"><a href="#cb39-466" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(json.dumps(data, indent<span class="op">=</span><span class="dv">2</span>))</span>
<span id="cb39-467"><a href="#cb39-467" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> (json.JSONDecodeError, <span class="pp">ValueError</span>) <span class="im">as</span> e:</span>
<span id="cb39-468"><a href="#cb39-468" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Error: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb39-469"><a href="#cb39-469" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Raw output:"</span>, response.response)</span>
<span id="cb39-470"><a href="#cb39-470" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-471"><a href="#cb39-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-472"><a href="#cb39-472" aria-hidden="true" tabindex="-1"></a>JSON schema constraints are more reliable than prompt-based requests because they operate at the token level—the model cannot sample tokens that would create invalid JSON. The prompt activates extraction patterns; the schema enforces structure.</span>
<span id="cb39-473"><a href="#cb39-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-474"><a href="#cb39-474" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb39-475"><a href="#cb39-475" aria-hidden="true" tabindex="-1"></a><span class="fu">## JSON Parsing Reliability</span></span>
<span id="cb39-476"><a href="#cb39-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-477"><a href="#cb39-477" aria-hidden="true" tabindex="-1"></a>Smaller models (like Gemma 3N) sometimes produce invalid JSON even with schema constraints. Always wrap parsing in try-except blocks and validate outputs. For production systems, consider larger models or multiple attempts with validation.</span>
<span id="cb39-478"><a href="#cb39-478" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb39-479"><a href="#cb39-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-480"><a href="#cb39-480" aria-hidden="true" tabindex="-1"></a><span class="fu">## Allowing Uncertainty to Reduce Hallucination</span></span>
<span id="cb39-481"><a href="#cb39-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-482"><a href="#cb39-482" aria-hidden="true" tabindex="-1"></a>LLMs confidently fabricate facts when they don't know the answer because they optimize for fluency, not truth. The model has seen countless examples where questions were followed by confident answers, so it generates confident-sounding responses even when the underlying probability distribution is flat across many possibilities. The solution: explicitly give the model permission to admit ignorance. Compare a prompt that implicitly demands an answer:</span>
<span id="cb39-483"><a href="#cb39-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-486"><a href="#cb39-486" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-487"><a href="#cb39-487" aria-hidden="true" tabindex="-1"></a>bad_prompt <span class="op">=</span> <span class="st">"""Summarize the main findings from the 2023 paper by Johnson et al.</span></span>
<span id="cb39-488"><a href="#cb39-488" aria-hidden="true" tabindex="-1"></a><span class="st">on quantum community detection in biological networks."""</span></span>
<span id="cb39-489"><a href="#cb39-489" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-490"><a href="#cb39-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-491"><a href="#cb39-491" aria-hidden="true" tabindex="-1"></a>Against a prompt that explicitly allows uncertainty:</span>
<span id="cb39-492"><a href="#cb39-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-495"><a href="#cb39-495" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-496"><a href="#cb39-496" aria-hidden="true" tabindex="-1"></a>good_prompt <span class="op">=</span> <span class="st">"""I'm looking for a 2023 paper by Johnson et al. on quantum</span></span>
<span id="cb39-497"><a href="#cb39-497" aria-hidden="true" tabindex="-1"></a><span class="st">community detection in biological networks.</span></span>
<span id="cb39-498"><a href="#cb39-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-499"><a href="#cb39-499" aria-hidden="true" tabindex="-1"></a><span class="st">If you know this paper, summarize its main findings.</span></span>
<span id="cb39-500"><a href="#cb39-500" aria-hidden="true" tabindex="-1"></a><span class="st">If you're not certain this paper exists, say "I cannot verify this paper exists"</span></span>
<span id="cb39-501"><a href="#cb39-501" aria-hidden="true" tabindex="-1"></a><span class="st">and do NOT make up details.</span></span>
<span id="cb39-502"><a href="#cb39-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-503"><a href="#cb39-503" aria-hidden="true" tabindex="-1"></a><span class="st">Response:"""</span></span>
<span id="cb39-504"><a href="#cb39-504" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-505"><a href="#cb39-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-508"><a href="#cb39-508" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-509"><a href="#cb39-509" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb39-510"><a href="#cb39-510" aria-hidden="true" tabindex="-1"></a>response_bad <span class="op">=</span> ollama.generate(prompt<span class="op">=</span>bad_prompt, <span class="op">**</span>params_llm)</span>
<span id="cb39-511"><a href="#cb39-511" aria-hidden="true" tabindex="-1"></a>response_good <span class="op">=</span> ollama.generate(prompt<span class="op">=</span>good_prompt, <span class="op">**</span>params_llm)</span>
<span id="cb39-512"><a href="#cb39-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-513"><a href="#cb39-513" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"BAD PROMPT (encourages hallucination):"</span>)</span>
<span id="cb39-514"><a href="#cb39-514" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(response_bad.response)</span>
<span id="cb39-515"><a href="#cb39-515" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">GOOD PROMPT (allows uncertainty):"</span>)</span>
<span id="cb39-516"><a href="#cb39-516" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(response_good.response)</span>
<span id="cb39-517"><a href="#cb39-517" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-518"><a href="#cb39-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-519"><a href="#cb39-519" aria-hidden="true" tabindex="-1"></a>The good prompt activates patterns where explicit permission to admit ignorance preceded honest uncertainty statements. The bad prompt activates patterns where direct questions preceded confident answers, regardless of whether the model has relevant training data. Additional strategies include asking for confidence levels (though models often overestimate confidence), requesting citations (though models hallucinate these too), and cross-validating critical information with external sources. The fundamental issue remains: LLMs have no internal representation of what they "know" versus what they're fabricating.</span>
<span id="cb39-520"><a href="#cb39-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-521"><a href="#cb39-521" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb39-522"><a href="#cb39-522" aria-hidden="true" tabindex="-1"></a><span class="fu">## Be a Good "Boss" to Your LLM</span></span>
<span id="cb39-523"><a href="#cb39-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-524"><a href="#cb39-524" aria-hidden="true" tabindex="-1"></a>**Let LLMs admit ignorance**: LLMs closely follow your instructions—even when they shouldn't. They often attempt to answer beyond their actual capabilities. Explicitly tell your model: "If you don't know the answer, just say so," or "If you need more information, please ask."</span>
<span id="cb39-525"><a href="#cb39-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-526"><a href="#cb39-526" aria-hidden="true" tabindex="-1"></a>**Encourage critical feedback**: LLMs are trained to be agreeable, which can hinder productive brainstorming or honest critique. Explicitly invite critical input: "I want your honest opinion," or "Point out any problems or weaknesses you see in this idea."</span>
<span id="cb39-527"><a href="#cb39-527" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb39-528"><a href="#cb39-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-529"><a href="#cb39-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-530"><a href="#cb39-530" aria-hidden="true" tabindex="-1"></a><span class="fu">### Sampling Multiple Times for Consistency</span></span>
<span id="cb39-531"><a href="#cb39-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-532"><a href="#cb39-532" aria-hidden="true" tabindex="-1"></a>For tasks requiring reasoning, generating multiple responses and selecting the most common answer often improves accuracy. The technique—called **self-consistency**—exploits the fact that correct reasoning tends to converge on the same answer, while hallucinations vary randomly across samples. Define the prompt:</span>
<span id="cb39-533"><a href="#cb39-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-536"><a href="#cb39-536" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-537"><a href="#cb39-537" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb39-538"><a href="#cb39-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-539"><a href="#cb39-539" aria-hidden="true" tabindex="-1"></a>prompt_consistency <span class="op">=</span> <span class="st">"""Three papers study network robustness:</span></span>
<span id="cb39-540"><a href="#cb39-540" aria-hidden="true" tabindex="-1"></a><span class="st">- Paper A: Targeted attacks are most damaging</span></span>
<span id="cb39-541"><a href="#cb39-541" aria-hidden="true" tabindex="-1"></a><span class="st">- Paper B: Random failures rarely cause collapse</span></span>
<span id="cb39-542"><a href="#cb39-542" aria-hidden="true" tabindex="-1"></a><span class="st">- Paper C: Hub nodes are critical for robustness</span></span>
<span id="cb39-543"><a href="#cb39-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-544"><a href="#cb39-544" aria-hidden="true" tabindex="-1"></a><span class="st">What is the research consensus on network robustness? Give a one-sentence answer.</span></span>
<span id="cb39-545"><a href="#cb39-545" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb39-546"><a href="#cb39-546" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-547"><a href="#cb39-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-548"><a href="#cb39-548" aria-hidden="true" tabindex="-1"></a>Generate multiple responses with higher temperature to increase diversity, then identify the most common answer:</span>
<span id="cb39-549"><a href="#cb39-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-552"><a href="#cb39-552" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-553"><a href="#cb39-553" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb39-554"><a href="#cb39-554" aria-hidden="true" tabindex="-1"></a><span class="co"># Use higher temperature for diversity</span></span>
<span id="cb39-555"><a href="#cb39-555" aria-hidden="true" tabindex="-1"></a>params_creative <span class="op">=</span> {<span class="st">"model"</span>: <span class="st">"gemma3n:latest"</span>, <span class="st">"options"</span>: {<span class="st">"temperature"</span>: <span class="fl">0.7</span>}}</span>
<span id="cb39-556"><a href="#cb39-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-557"><a href="#cb39-557" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate 5 responses</span></span>
<span id="cb39-558"><a href="#cb39-558" aria-hidden="true" tabindex="-1"></a>responses <span class="op">=</span> []</span>
<span id="cb39-559"><a href="#cb39-559" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb39-560"><a href="#cb39-560" aria-hidden="true" tabindex="-1"></a>    response <span class="op">=</span> ollama.generate(prompt<span class="op">=</span>prompt_consistency, <span class="op">**</span>params_creative)</span>
<span id="cb39-561"><a href="#cb39-561" aria-hidden="true" tabindex="-1"></a>    responses.append(response.response.strip())</span>
<span id="cb39-562"><a href="#cb39-562" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Response </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>responses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb39-563"><a href="#cb39-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-564"><a href="#cb39-564" aria-hidden="true" tabindex="-1"></a><span class="co"># In practice, you'd programmatically identify the most common theme</span></span>
<span id="cb39-565"><a href="#cb39-565" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The most consistent theme across responses would be selected."</span>)</span>
<span id="cb39-566"><a href="#cb39-566" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-567"><a href="#cb39-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-568"><a href="#cb39-568" aria-hidden="true" tabindex="-1"></a>Self-consistency works because correct reasoning patterns converge toward the same conclusion when sampled multiple times, while fabricated details vary randomly. The tradeoff: generating five responses means five times the API calls, five times the cost, five times the latency. Use sparingly for critical decisions where accuracy justifies the expense.</span>
<span id="cb39-569"><a href="#cb39-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-570"><a href="#cb39-570" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb39-571"><a href="#cb39-571" aria-hidden="true" tabindex="-1"></a><span class="fu">## Alternative: Tree of Thought</span></span>
<span id="cb39-572"><a href="#cb39-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-573"><a href="#cb39-573" aria-hidden="true" tabindex="-1"></a><span class="al">![](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2FTOT.3b13bc5e.png&amp;w=3840&amp;q=75)</span></span>
<span id="cb39-574"><a href="#cb39-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-575"><a href="#cb39-575" aria-hidden="true" tabindex="-1"></a>For even more sophisticated exploration, you can use "Tree of Thought" <span class="co">[</span><span class="ot">@yao2023tree</span><span class="co">]</span> prompting, where the model explicitly explores multiple reasoning paths, evaluates them, and selects the best one. This is more complex to implement but can yield better results for very difficult problems.</span>
<span id="cb39-576"><a href="#cb39-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-577"><a href="#cb39-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-578"><a href="#cb39-578" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb39-579"><a href="#cb39-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-580"><a href="#cb39-580" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Takeaway</span></span>
<span id="cb39-581"><a href="#cb39-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-582"><a href="#cb39-582" aria-hidden="true" tabindex="-1"></a>Prompt engineering is not magic—it's deliberate activation of statistical patterns compressed during training. Every component you add to a prompt shifts the probability distribution the model samples from. Instructions activate task-specific patterns. Output formats activate structured-response patterns. Personas activate stylistic patterns. Context disambiguates when multiple patterns compete. Examples demonstrate exact structure. Chain-of-thought activates reasoning-like patterns. Format constraints enforce structure at the token level. Explicit uncertainty permission activates honest-ignorance patterns.</span>
<span id="cb39-583"><a href="#cb39-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-584"><a href="#cb39-584" aria-hidden="true" tabindex="-1"></a>None of this requires the model to understand what you want. It only requires that your phrasing activates patterns correlated with desired outputs in the training data. You're not communicating intent; you're manipulating probability distributions. Master this, and you can reliably extract value from LLMs for research workflows—summarization, structured extraction, hypothesis generation, literature analysis.</span>
<span id="cb39-585"><a href="#cb39-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-586"><a href="#cb39-586" aria-hidden="true" tabindex="-1"></a>But a question remains: how do these models represent text internally? When you send a prompt, the model doesn't see English words—it sees numbers. Millions of numbers arranged in high-dimensional space. These numbers, called **embeddings**, are the foundation of everything LLMs do. Let's unbox the first layer and see how meaning becomes mathematics.</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2025, Sadamori Kojaku</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions"><ul><li><a href="https://github.com/skojaku/applied-soft-comp/edit/main/m03-agentic-coding/prompt-tuning.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/skojaku/applied-soft-comp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/skojaku/applied-soft-comp">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>
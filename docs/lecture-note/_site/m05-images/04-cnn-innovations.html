<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Sadamori Kojaku">
<meta name="dcterms.date" content="2026-01-10">

<title>Part 4: The Innovation Timeline – Applied Soft Computing</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../m06-graph/overview.html" rel="next">
<link href="../m05-images/03-using-cnn-models.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-1f36e651aa5e1ffb5e4e9439ef2eb9d4.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "|"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../assets/custom.css">
</head>

<body class="nav-sidebar docked nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Applied Soft Computing</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../toc.html"> 
<span class="menu-text">Table of Contents</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-modules" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Modules</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-modules">    
        <li>
    <a class="dropdown-item" href="../m01-toolkit/overview.html">
 <span class="dropdown-text">Module 1: The Data Scientist’s Toolkit</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m02-visualization/overview.html">
 <span class="dropdown-text">Module 2: Visualizing Complexity</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m03-agentic-coding/overview.html">
 <span class="dropdown-text">Module 3: Agentic Coding</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m04-text/overview.html">
 <span class="dropdown-text">Module 4: Deep Learning for Text</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m05-images/overview.html">
 <span class="dropdown-text">Module 5: Deep Learning for Images</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m06-graph/overview.html">
 <span class="dropdown-text">Module 6: Deep Learning for Graphs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m06-llms/overview.qmd">
 <span class="dropdown-text">Module 7: Advanced LLMs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m07-self-supervised/overview.qmd">
 <span class="dropdown-text">Module 8: Self-Supervised Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m08-explainability/overview.qmd">
 <span class="dropdown-text">Module 9: Explainability &amp; Ethics</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../m05-images/overview.html">Module 5: Deep Learning for Images</a></li><li class="breadcrumb-item"><a href="../m05-images/04-cnn-innovations.html">Part 4: CNN Innovations</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Course Information</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/welcome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About Us</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/why-applied-soft-computing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Why applied soft computing?</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/discord.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Discord</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/minidora-usage.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Using Minidora</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/how-to-submit-assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">How to submit assignment</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/deliverables.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deliverables</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Module 1: The Data Scientist’s Toolkit</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-toolkit/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-toolkit/git-github.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Version Control with Git &amp; GitHub</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-toolkit/tidy-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Tidy Data Philosophy</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-toolkit/data-provenance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Provenance</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-toolkit/reproduceability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Reproducibility</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Module 2: Visualizing Complexity</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-visualization/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-visualization/principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Principles of Effective Visualization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-visualization/1d-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing 1D Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-visualization/2d-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing 2D Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-visualization/highd-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing High-Dimensional Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-visualization/networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-visualization/time-series.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing Time-Series</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Module 3: Agentic Coding</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-agentic-coding/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-agentic-coding/hands-on.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Hands-on</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-agentic-coding/prompt-tuning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prompt Tuning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-agentic-coding/agentic-ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Agentic AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-agentic-coding/context-engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Context Engineering</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Module 4: Deep Learning for Text</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-text/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-text/llm-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Large Language Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-text/gpt-inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">GPT Inference: Sampling Strategies</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-text/tokenization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Tokenization: Unboxing How LLMs Read Text</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-text/transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Transformers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-text/bert-gpt.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">BERT &amp; GPT</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-text/sentence-transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sentence Transformers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-text/word-embeddings.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Word Embeddings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-text/semaxis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Semaxis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-text/word-bias.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Word Bias</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Module 5: Deep Learning for Images</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-images/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-images/01-what-is-an-image.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part 1: What is an Image?</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-images/02-the-deep-learning-revolution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part 2: The Deep Learning Revolution</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-images/03-using-cnn-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part 3: Using CNN Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-images/04-cnn-innovations.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Part 4: CNN Innovations</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Module 6: Deep Learning for Graphs</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-graph/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-graph/01-from-images-to-graphs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part 1: From Images to Graphs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-graph/02-spectral-perspective.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part 2: The Spectral Perspective</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-graph/03-spatial-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part 3: Spatial Graph Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-graph/04-graph-embeddings.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part 4: Graph Embeddings</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../m05-images/overview.html">Module 5: Deep Learning for Images</a></li><li class="breadcrumb-item"><a href="../m05-images/04-cnn-innovations.html">Part 4: CNN Innovations</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Part 4: The Innovation Timeline</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Sadamori Kojaku </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 10, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-note callout-titled" title="What you'll learn in this module">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What you’ll learn in this module
</div>
</div>
<div class="callout-body-container callout-body">
<p>This module traces CNN evolution through successive innovations that solved specific architectural challenges.</p>
<p>You’ll learn:</p>
<ul>
<li>How <strong>VGG</strong> demonstrated that depth matters through stacked 3×3 convolutions.</li>
<li>How <strong>Inception</strong> achieved efficiency through multi-scale features and 1×1 convolutions.</li>
<li>How <strong>ResNet</strong> enabled training very deep networks (152 layers) using skip connections.</li>
<li>How <strong>Vision Transformers</strong> replaced convolution with self-attention for global context.</li>
<li>The trade-offs between different architectures and how to choose the right one for your project.</li>
</ul>
</div>
</div>
<section id="the-quest-for-depth-and-efficiency" class="level2">
<h2 class="anchored" data-anchor-id="the-quest-for-depth-and-efficiency">The Quest for Depth and Efficiency</h2>
<p>AlexNet proved that deep learning works at scale in 2012. This breakthrough sparked a race to improve CNN architectures. But simply adding more layers didn’t work. Networks deeper than 20 layers degraded during training. Computational costs exploded. Memory constraints limited model size.</p>
<p>The innovations we’ll explore emerged as solutions to these challenges. Each architecture addressed specific problems while introducing ideas that influenced everything that followed. This is not a random collection of models, but a coherent story of progress through clever problem-solving.</p>
<div id="fig-imagenet-timeline" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-imagenet-timeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://viso.ai/wp-content/uploads/2024/02/imagenet-winners-by-year.jpg" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>ImageNet competition winners from 2012 to 2017. Each year brought architectural innovations that pushed accuracy higher.</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-imagenet-timeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1
</figcaption>
</figure>
</div>
</section>
<section id="challenge-1-going-deeper-vgg-2014" class="level2">
<h2 class="anchored" data-anchor-id="challenge-1-going-deeper-vgg-2014">Challenge 1: Going Deeper (VGG, 2014)</h2>
<p>AlexNet demonstrated the power of depth with 8 layers. But could we go deeper? By 2014, researchers at Oxford’s Visual Geometry Group posed this question directly.</p>
<section id="the-depth-hypothesis" class="level3">
<h3 class="anchored" data-anchor-id="the-depth-hypothesis">The Depth Hypothesis</h3>
<p>The intuition was compelling. Deeper networks should learn more complex representations. Early layers detect simple edges and colors. Middle layers combine these into textures and parts. Deep layers recognize complete objects and scenes. More layers mean more abstraction.</p>
<p>But training deep networks in 2014 remained difficult. Gradients vanished. Training took weeks. Most researchers stuck with networks under 20 layers.</p>
</section>
<section id="vggs-answer-stacked-33-convolutions" class="level3">
<h3 class="anchored" data-anchor-id="vggs-answer-stacked-33-convolutions">VGG’s Answer: Stacked 3×3 Convolutions</h3>
<p>VGGNet demonstrated that systematic depth works {footcite}<code>simonyan2014very</code>. The key insight was using <strong>small 3×3 convolutions</strong> exclusively, stacked together to build deep networks.</p>
<div id="fig-vgg-architecture" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-vgg-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/vgg-architecture.jpg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>VGG16 architecture showing progressive downsampling while doubling channels. The network uses only 3×3 convolutions throughout.</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-vgg-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2
</figcaption>
</figure>
</div>
<p>Why stack 3×3 filters instead of using larger filters? Consider the receptive field. Two 3×3 convolutions have the same receptive field as one 5×5 convolution (both see a 5×5 region of the input). But the stacked version has fewer parameters.</p>
<p>For a single 5×5 convolution:</p>
<p><span class="math display">
\text{parameters} = 5 \times 5 = 25
</span></p>
<p>For two stacked 3×3 convolutions:</p>
<p><span class="math display">
\text{parameters} = 2 \times (3 \times 3) = 18
</span></p>
<p>This yields a <strong>28% parameter reduction</strong> while adding an extra ReLU nonlinearity between the layers, allowing the network to learn more complex functions.</p>
<div id="fig-receptive-field-stacking" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-receptive-field-stacking-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://miro.medium.com/v2/resize:fit:1200/1*k97NVvlMkRXau-uItlq5Gw.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption>Two stacked 3×3 convolutions achieve the same receptive field as one 5×5 convolution but with fewer parameters and added nonlinearity.</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-receptive-field-stacking-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3
</figcaption>
</figure>
</div>
</section>
<section id="the-architecture-pattern" class="level3">
<h3 class="anchored" data-anchor-id="the-architecture-pattern">The Architecture Pattern</h3>
<p>VGG introduced a clean, systematic pattern that influenced all subsequent architectures:</p>
<p><strong>After each pooling layer, double the channels</strong>:</p>
<p><span class="math display">
\text{channels} = \{64 \to 128 \to 256 \to 512 \to 512\}
</span></p>
<p><strong>Spatial dimensions halve</strong>:</p>
<p><span class="math display">
\text{spatial dimensions} = \{224 \to 112 \to 56 \to 28 \to 14 \to 7\}
</span></p>
<p>This creates a pyramid structure where computational cost per layer stays roughly constant. As spatial dimensions decrease, increasing channel depth compensates by expanding representational capacity.</p>
<p>VGG16 (16 layers) and VGG19 (19 layers) achieved strong results on ImageNet, validating that systematic depth improves accuracy. The architecture’s simplicity made it easy to understand and implement, contributing to its widespread adoption.</p>
</section>
<section id="the-limitation" class="level3">
<h3 class="anchored" data-anchor-id="the-limitation">The Limitation</h3>
<p>VGG16 contains approximately <strong>140 million parameters</strong>. The majority (102 million) concentrate in the first fully connected layer. This massive parameter count means:</p>
<ul>
<li>Training requires significant computational resources</li>
<li>Inference is memory-intensive</li>
<li>The model is prone to overfitting without strong regularization</li>
</ul>
<p>The question became: can we achieve similar accuracy with fewer parameters?</p>
</section>
</section>
<section id="challenge-2-computing-efficiently-inceptiongooglenet-2014" class="level2">
<h2 class="anchored" data-anchor-id="challenge-2-computing-efficiently-inceptiongooglenet-2014">Challenge 2: Computing Efficiently (Inception/GoogLeNet, 2014)</h2>
<p>While VGG pushed depth systematically, researchers at Google asked a different question: how do we capture multi-scale features efficiently?</p>
<section id="multi-scale-feature-extraction" class="level3">
<h3 class="anchored" data-anchor-id="multi-scale-feature-extraction">Multi-Scale Feature Extraction</h3>
<p>Look at a photograph. Some objects are large and occupy significant image area. Others are small details. To recognize both, the network needs to examine features at multiple scales simultaneously.</p>
<p>Traditional CNN layers use a single kernel size (like 3×3). But the optimal kernel size varies by context. Large kernels capture broad patterns. Small kernels detect fine details.</p>
<p><strong>Inception’s answer</strong>: use multiple kernel sizes in parallel within the same layer {footcite}<code>szegedy2015going</code>.</p>
</section>
<section id="the-11-convolution-trick" class="level3">
<h3 class="anchored" data-anchor-id="the-11-convolution-trick">The 1×1 Convolution Trick</h3>
<p>Running multiple large convolutions in parallel is computationally expensive. Inception solves this through <strong>1×1 convolutions</strong> for channel dimensionality reduction.</p>
<p>At first, 1×1 convolutions seem strange. They don’t look at neighboring pixels, only at different channels at the same location. But this is precisely their power. They compress information across channels before applying larger, more expensive filters.</p>
<p>Consider a 3×3 convolution on a 256-channel feature map producing 256 output channels:</p>
<p><span class="math display">
\text{parameters (without reduction)} = 3 \times 3 \times 256 \times 256 = 589{,}824
</span></p>
<p>With a 1×1 convolution reducing to 64 channels first:</p>
<p><span class="math display">
\text{parameters (with reduction)} = (1 \times 1 \times 256 \times 64) + (3 \times 3 \times 64 \times 256) = 163{,}840
</span></p>
<p>This achieves a <strong>72% parameter reduction</strong> while maintaining similar expressive power.</p>
<p>The theoretical motivation behind 1×1 convolutions is elegant. Inception approximates sparse connectivity. Not every pixel needs to connect to every pixel in the next layer. The 1×1 convolutions sparsify connections efficiently by operating primarily across channels rather than spatial dimensions {footcite}<code>paperswithcode-inception</code>.</p>
</section>
<section id="the-inception-module" class="level3">
<h3 class="anchored" data-anchor-id="the-inception-module">The Inception Module</h3>
<p>Each Inception module contains four parallel branches:</p>
<ol type="1">
<li><strong>1×1 convolution</strong>: Captures point-wise patterns</li>
<li><strong>1×1 → 3×3 convolution</strong>: Captures medium-scale patterns (with reduction)</li>
<li><strong>1×1 → 5×5 convolution</strong>: Captures large-scale patterns (with reduction)</li>
<li><strong>3×3 max pooling → 1×1 convolution</strong>: Preserves spatial structure differently</li>
</ol>
<p>These branches process the same input simultaneously. Their outputs concatenate along the channel dimension, creating a multi-scale representation.</p>
<p>Mathematically, for input <span class="math inline">X</span>:</p>
<p><span class="math display">
Y_{\text{inception}} = \text{Concat}\big(Y_{1\times1}, \,Y_{3\times3}, \,Y_{5\times5}, \,Y_{\text{pool}}\big)
</span></p>
<p>where each <span class="math inline">Y</span> represents the output of its respective branch.</p>
</section>
<section id="global-average-pooling" class="level3">
<h3 class="anchored" data-anchor-id="global-average-pooling">Global Average Pooling</h3>
<p>VGG’s fully connected layers contain 102 million parameters. Inception eliminates this bottleneck through <strong>global average pooling</strong> {footcite}<code>lin2013network</code>.</p>
<p>Instead of flattening feature maps and passing through dense layers, take the average value of each channel across all spatial positions. For a feature map with 1000 channels, this produces a 1000-dimensional vector directly, regardless of spatial size. This:</p>
<ul>
<li>Drastically reduces parameters (no heavy fully connected layers)</li>
<li>Creates translation invariance (averaging eliminates spatial dependence)</li>
<li>Reduces overfitting risk</li>
</ul>
</section>
<section id="auxiliary-classifiers" class="level3">
<h3 class="anchored" data-anchor-id="auxiliary-classifiers">Auxiliary Classifiers</h3>
<p>GoogLeNet introduced <strong>auxiliary classifiers</strong> at intermediate layers to combat vanishing gradients in deep networks. These classifiers attach to middle layers, computing losses that provide additional gradient signals during backpropagation.</p>
<div id="fig-inception-auxiliary" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-inception-auxiliary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://production-media.paperswithcode.com/methods/GoogleNet-structure-and-auxiliary-classifier-units_CM5xsxk.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>GoogLeNet architecture with auxiliary classifiers attached to intermediate layers to improve gradient flow.</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-inception-auxiliary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4
</figcaption>
</figure>
</div>
<p>During training, the total loss combines the main classifier loss with auxiliary losses (typically weighted at 0.3). At inference, only the main classifier is used.</p>
</section>
<section id="the-impact" class="level3">
<h3 class="anchored" data-anchor-id="the-impact">The Impact</h3>
<p>GoogLeNet achieved accuracy comparable to VGG with <strong>12× fewer parameters</strong>. This demonstrated that architecture efficiency matters as much as depth. The Inception ideas influenced countless subsequent designs.</p>
<p>Later versions pushed these ideas further. Inception v2/v3 added batch normalization and factorized larger filters (5×5 became two 3×3 convolutions). Inception v4 integrated with residual connections. Xception used depthwise separable convolutions, pushing channel-spatial separation further.</p>
<p><strong>Batch Normalization</strong>, introduced around this time {footcite}<code>ioffe2015batch</code>, normalizes layer activations to zero mean and unit variance. This stabilizes training and allows higher learning rates. It became standard in nearly all subsequent architectures.</p>
</section>
</section>
<section id="challenge-3-training-very-deep-networks-resnet-2015" class="level2">
<h2 class="anchored" data-anchor-id="challenge-3-training-very-deep-networks-resnet-2015">Challenge 3: Training Very Deep Networks (ResNet, 2015)</h2>
<p>By 2015, researchers wanted networks with 50, 100, or even 150 layers. But a puzzling phenomenon blocked progress: networks deeper than about 20 layers exhibited <strong>degradation</strong>.</p>
<section id="the-degradation-problem" class="level3">
<h3 class="anchored" data-anchor-id="the-degradation-problem">The Degradation Problem</h3>
<p>Here’s what was strange. Add more layers to a working network and training error increases. Not test error (which would indicate overfitting). Training error itself gets worse.</p>
<p>This shouldn’t happen. A deeper network could theoretically learn the identity function for extra layers, matching the shallower network’s performance. But in practice, optimization failed. The deeper network couldn’t even learn to copy what the shallower network already achieved.</p>
<p>This wasn’t vanishing gradients alone (batch normalization addressed that). This was a fundamental optimization difficulty.</p>
</section>
<section id="the-residual-learning-solution" class="level3">
<h3 class="anchored" data-anchor-id="the-residual-learning-solution">The Residual Learning Solution</h3>
<p>Microsoft Research proposed an elegant solution: <strong>skip connections</strong> {footcite}<code>he2016deep</code>.</p>
<p>Instead of learning a direct mapping <span class="math inline">H(\mathbf{x})</span> from input <span class="math inline">\mathbf{x}</span> to output, learn the <strong>residual</strong> <span class="math inline">F(\mathbf{x}) = H(\mathbf{x}) - \mathbf{x}</span>. Then add the input back:</p>
<p><span class="math display">
H(\mathbf{x}) = F(\mathbf{x}) + \mathbf{x}
</span></p>
<div id="fig-resnet-block" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-resnet-block-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://www.researchgate.net/publication/364330795/figure/fig7/AS:11431281176036099@1689999593116/Basic-residual-block-of-ResNet.png" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Residual block. The skip connection carries the input directly to the output, while convolutional layers learn the residual.</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-resnet-block-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5
</figcaption>
</figure>
</div>
<p>Why does this help? If the optimal mapping is close to identity (the layer isn’t very useful), the network can easily learn <span class="math inline">F(\mathbf{x}) \approx 0</span> by pushing weights toward zero. The skip connection ensures input information flows through unchanged.</p>
<p>If a more complex transformation is needed, <span class="math inline">F(\mathbf{x})</span> can still learn it. The skip connection doesn’t constrain what the block can represent. It just makes optimization easier by providing a gradient highway.</p>
</section>
<section id="ensemble-like-gradient-flow" class="level3">
<h3 class="anchored" data-anchor-id="ensemble-like-gradient-flow">Ensemble-Like Gradient Flow</h3>
<p>Skip connections create multiple paths for gradients to flow backward. Some paths go through all convolutions. Others skip multiple blocks via cascaded skip connections. This ensemble of paths accelerates training and prevents vanishing gradients {footcite}<code>veit2016residual</code>.</p>
<div id="fig-resnet-gradient-flow" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-resnet-gradient-flow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://arxiv.org/html/2405.01725v1/x28.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Multiple gradient paths in ResNet. Gradients can skip layers via identity connections, providing stable training for very deep networks.</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-resnet-gradient-flow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6
</figcaption>
</figure>
</div>
</section>
<section id="bottleneck-blocks-for-deeper-networks" class="level3">
<h3 class="anchored" data-anchor-id="bottleneck-blocks-for-deeper-networks">Bottleneck Blocks for Deeper Networks</h3>
<p>ResNet-50, -101, and -152 use <strong>bottleneck blocks</strong> to maintain efficiency:</p>
<ol type="1">
<li><strong>1×1 convolution</strong>: Reduces channel dimension (e.g., 256 → 64)</li>
<li><strong>3×3 convolution</strong>: Operates on reduced dimension</li>
<li><strong>1×1 convolution</strong>: Restores dimension (e.g., 64 → 256)</li>
</ol>
<div id="fig-resnet-bottleneck" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-resnet-bottleneck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://i.sstatic.net/kbiIG.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Bottleneck block (left) vs.&nbsp;basic block (right). The bottleneck design reduces computational cost in very deep networks.</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-resnet-bottleneck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7
</figcaption>
</figure>
</div>
<p>This shrinks the intermediate feature map, dramatically reducing computational cost while maintaining representational capacity. The design was inspired by Inception’s bottleneck idea.</p>
</section>
<section id="the-results" class="level3">
<h3 class="anchored" data-anchor-id="the-results">The Results</h3>
<p>ResNet achieved:</p>
<ul>
<li><strong>152 layers</strong> trained successfully without degradation</li>
<li><strong>Top-5 error of 3.57%</strong> on ImageNet (better than human performance on the test set)</li>
<li>Widespread adoption across computer vision tasks</li>
</ul>
<p>The impact extended beyond CNNs. Skip connections appeared in:</p>
<ul>
<li><strong>U-Net</strong> for medical image segmentation</li>
<li><strong>DenseNet</strong> which connects every layer to every other layer</li>
<li><strong>Transformers</strong> for natural language processing</li>
<li>Nearly all modern deep architectures</li>
</ul>
<p>ResNet showed that with the right architecture, depth isn’t a limitation. It’s a resource.</p>
</section>
<section id="resnext-width-through-cardinality" class="level3">
<h3 class="anchored" data-anchor-id="resnext-width-through-cardinality">ResNeXt: Width Through Cardinality</h3>
<p><strong>ResNeXt</strong> {footcite}<code>xie2017aggregated</code> extended ResNet by increasing network <strong>width</strong> through grouped convolutions rather than just adding depth or channels.</p>
<p>The idea: split the bottleneck convolution path into multiple parallel groups (typically 32), each processing independently. Aggregate their outputs through concatenation or addition.</p>
<div id="fig-resnext-block" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-resnext-block-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-06_at_4.32.52_PM.png" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>ResNeXt block with multiple grouped convolution paths. Increasing cardinality (number of groups) often improves accuracy more than increasing depth or channel count.</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-resnext-block-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8
</figcaption>
</figure>
</div>
<p>This “cardinality” dimension provides another axis for scaling networks. ResNeXt achieves better accuracy than ResNet at similar computational cost by increasing cardinality instead of just going deeper.</p>
</section>
</section>
<section id="challenge-4-global-context-vision-transformer-2020" class="level2">
<h2 class="anchored" data-anchor-id="challenge-4-global-context-vision-transformer-2020">Challenge 4: Global Context (Vision Transformer, 2020)</h2>
<p>CNNs build global understanding slowly through stacked local operations. Early layers see only small patches (3×3 or 5×5 regions). Deeper layers expand the receptive field, but even in deep networks, truly global connections require many layers.</p>
<p>What if we could capture global relationships immediately?</p>
<section id="the-self-attention-mechanism" class="level3">
<h3 class="anchored" data-anchor-id="the-self-attention-mechanism">The Self-Attention Mechanism</h3>
<p><strong>Vision Transformers (ViT)</strong> replace convolution with <strong>self-attention</strong> {footcite}<code>dosovitskiy2020image</code>.</p>
<p>Self-attention computes relationships between all positions simultaneously. For each patch of the image, it determines which other patches are relevant, regardless of distance. This provides immediate global context.</p>
<p>The mechanism works as follows. Given input features <span class="math inline">X</span>, compute three matrices through learned linear projections:</p>
<p><span class="math display">
Q = XW_Q, \quad K = XW_K, \quad V = XW_V
</span></p>
<p>where <span class="math inline">Q</span> (queries), <span class="math inline">K</span> (keys), and <span class="math inline">V</span> (values) represent different views of the input.</p>
<p>Attention scores measure similarity between queries and keys:</p>
<p><span class="math display">
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
</span></p>
<p>This computes a weighted average of values, where weights depend on query-key similarity. Intuitively, each position “asks” (via its query) what information to gather from all other positions (via their keys), then aggregates their values accordingly.</p>
</section>
<section id="patches-as-tokens" class="level3">
<h3 class="anchored" data-anchor-id="patches-as-tokens">Patches as Tokens</h3>
<p>ViT treats images like text. Divide the image into fixed-size patches (typically 16×16). Flatten each patch into a vector. Treat these vectors as “tokens” (analogous to words in NLP).</p>
<p>Add positional encodings to preserve spatial information (since self-attention is permutation-invariant). Pass through a standard Transformer encoder with multiple self-attention layers.</p>
<p>A special <strong>CLS token</strong> prepended to the sequence gathers global information. After all Transformer layers, the CLS token’s representation feeds into the classification head.</p>
</section>
<section id="trade-offs-data-and-compute" class="level3">
<h3 class="anchored" data-anchor-id="trade-offs-data-and-compute">Trade-offs: Data and Compute</h3>
<p>Vision Transformers achieve state-of-the-art accuracy on large datasets like ImageNet-21k (14 million images). But they have important trade-offs. Vision Transformers provide a global receptive field from the first layer, better scaling properties with dataset size, and a unified architecture for vision and language. However, they require more training data than CNNs due to less inductive bias, impose higher computational cost since self-attention is <span class="math inline">O(n^2)</span> in sequence length, and prove less effective on small datasets without strong augmentation.</p>
<p>For most practical applications, CNNs remain competitive. Use ViT when you have large datasets (millions of images), substantial computational resources, and tasks benefiting from global context like scene understanding or fine-grained classification. Use CNNs when you have limited data (thousands of images), constrained compute (edge devices, mobile), or tasks benefiting from spatial locality (object detection, segmentation).</p>
</section>
<section id="hybrid-approaches" class="level3">
<h3 class="anchored" data-anchor-id="hybrid-approaches">Hybrid Approaches</h3>
<p>Recent research combines CNN and Transformer strengths. Swin Transformer {footcite}<code>liu2021swin</code> uses local attention windows (more like convolution) with hierarchical structure. CoAtNet {footcite}<code>dai2021coatnet</code> combines convolution in early layers with attention in later layers. ConvNeXt {footcite}<code>liu2022convnet</code> shows that modernized CNNs can match Transformer performance.</p>
<p>The field continues evolving, blending ideas from both paradigms.</p>
</section>
</section>
<section id="the-narrative-of-progress" class="level2">
<h2 class="anchored" data-anchor-id="the-narrative-of-progress">The Narrative of Progress</h2>
<p>Let’s trace the thread connecting these innovations:</p>
<p><strong>2012 (AlexNet)</strong>: Depth works, but only to 8 layers ↓ <strong>2014 (VGG)</strong>: Stack small convolutions to go deeper (16-19 layers) ↓ <strong>2014 (Inception)</strong>: Use multi-scale features and 1×1 convolutions for efficiency ↓ <strong>2015 (ResNet)</strong>: Skip connections enable training very deep networks (152 layers) ↓ <strong>2017 (ResNeXt)</strong>: Increase width through cardinality, not just depth ↓ <strong>2020 (ViT)</strong>: Replace convolution with self-attention for global context</p>
<p>Each innovation addressed limitations of its predecessors while preserving their insights. Modern architectures mix and match these ideas: residual connections for depth, multi-scale features for efficiency, attention for global context.</p>
</section>
<section id="choosing-the-right-architecture" class="level2">
<h2 class="anchored" data-anchor-id="choosing-the-right-architecture">Choosing the Right Architecture</h2>
<p>For your next computer vision project, which architecture should you choose? ResNet-50 is the default choice, offering an excellent accuracy-computational cost trade-off with widely available pre-trained weights that work well across diverse tasks. EfficientNet matters when deployment efficiency is critical, carefully balancing depth, width, and resolution for optimal accuracy per parameter. MobileNet and EfficientNet-Lite serve mobile and edge devices, sacrificing some accuracy for fast inference and small model size. Vision Transformer (ViT) excels when you have large datasets (millions of images) and substantial compute, delivering state-of-the-art accuracy on challenging benchmarks. Swin Transformer provides Transformer benefits with more reasonable compute requirements, proving especially good for dense prediction tasks.</p>
<p>Start with ResNet-50. It provides strong performance across almost all applications. Optimize later if specific constraints (speed, memory, accuracy) demand it.</p>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>We traced CNN evolution through successive innovations solving specific challenges. VGG demonstrated that depth matters through stacked 3×3 convolutions. Inception showed how to capture multi-scale features efficiently using 1×1 convolutions and parallel branches. ResNet enabled training very deep networks (152 layers) through skip connections that ease optimization and improve gradient flow. Vision Transformers replaced convolution with self-attention, trading inductive bias for global context at the cost of requiring more data and compute.</p>
<p>Each architecture built on its predecessors’ insights. Modern networks combine ideas from all of them: residual connections for depth, multi-scale features for efficiency, attention for global understanding. Architecture design is problem-solving. Understanding why these innovations emerged helps you make informed choices for your own applications.</p>
<pre class="{footbibliography}"><code>:style: unsrt</code></pre>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../m05-images/03-using-cnn-models.html" class="pagination-link" aria-label="Part 3: Using CNN Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Part 3: Using CNN Models</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../m06-graph/overview.html" class="pagination-link" aria-label="Overview">
        <span class="nav-page-text">Overview</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2025, Sadamori Kojaku</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/skojaku/applied-soft-comp">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script src="../assets/breadcrumb-toc.js"></script>




</body></html>
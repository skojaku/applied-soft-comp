<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Sadamori Kojaku">
<meta name="dcterms.date" content="2025-11-21">

<title>Embeddings: How Machines Understand Meaning – Applied Soft Computing</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-0348920b7671f696dc9078d39bff215e.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-b8a9bfad7ca18893f6a3351e12995b20.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "|"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../assets/custom.css">
</head>

<body class="nav-sidebar docked nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../logo.jpg" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Applied Soft Computing</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../../index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-toolkit--workflow" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Toolkit &amp; Workflow</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-toolkit--workflow">    
        <li class="dropdown-header">─── Module 1 ───</li>
        <li>
    <a class="dropdown-item" href="../../m01-toolkit/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m01-toolkit/git-github.html">
 <span class="dropdown-text">Git &amp; GitHub</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m01-toolkit/tidy-data.html">
 <span class="dropdown-text">Tidy Data</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m01-toolkit/data-provenance.html">
 <span class="dropdown-text">Data Provenance</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m01-toolkit/environments.qmd">
 <span class="dropdown-text">Environments</span></a>
  </li>  
        <li class="dropdown-header">─── Module 3: Agentic Coding ───</li>
        <li>
    <a class="dropdown-item" href="../../m03-agentic-coding/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m03-agentic-coding/hands-on.html">
 <span class="dropdown-text">Hands-on</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-visualization" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Visualization</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-visualization">    
        <li class="dropdown-header">─── Module 2 ───</li>
        <li>
    <a class="dropdown-item" href="../../m02-visualization/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m02-visualization/principles.html">
 <span class="dropdown-text">Principles</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m02-visualization/dimensionality-reduction.html">
 <span class="dropdown-text">High-Dimensional Data</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m02-visualization/networks.html">
 <span class="dropdown-text">Networks</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m02-visualization/time-series.html">
 <span class="dropdown-text">Time-Series</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-deep-learning" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Deep Learning</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-deep-learning">    
        <li class="dropdown-header">─── Module 4: Text ───</li>
        <li>
    <a class="dropdown-item" href="../../m04-text/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m04-text/word2vec.md">
 <span class="dropdown-text">Word2Vec</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m04-text/lstm.md">
 <span class="dropdown-text">RNNs &amp; LSTMs</span></a>
  </li>  
        <li class="dropdown-header">─── Module 4: Images ───</li>
        <li>
    <a class="dropdown-item" href="../../m04-images/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m04-images/cnn.md">
 <span class="dropdown-text">CNNs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m04-images/resnet.md">
 <span class="dropdown-text">ResNet</span></a>
  </li>  
        <li class="dropdown-header">─── Module 5: Graphs ───</li>
        <li>
    <a class="dropdown-item" href="../../m05-graphs/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m05-graphs/graph-embedding-w-word2vec.html">
 <span class="dropdown-text">Graph Embeddings</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m05-graphs/graph-convolutional-network.html">
 <span class="dropdown-text">GNNs</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-advanced-topics" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Advanced Topics</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-advanced-topics">    
        <li class="dropdown-header">─── Module 6: LLMs ───</li>
        <li>
    <a class="dropdown-item" href="../../m06-llms/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m06-llms/transformers.md">
 <span class="dropdown-text">Transformers</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m06-llms/scaling-emergence.html">
 <span class="dropdown-text">Scaling &amp; Emergence</span></a>
  </li>  
        <li class="dropdown-header">─── Module 7: Self-Supervised ───</li>
        <li>
    <a class="dropdown-item" href="../../m07-self-supervised/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m07-self-supervised/contrastive-learning.html">
 <span class="dropdown-text">Contrastive Learning</span></a>
  </li>  
        <li class="dropdown-header">─── Module 8: Explainability ───</li>
        <li>
    <a class="dropdown-item" href="../../m08-explainability/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m08-explainability/fairness.html">
 <span class="dropdown-text">Fairness &amp; Ethics</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Embeddings: How Machines Understand Meaning</li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Course Information</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/welcome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About Us</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/why-applied-soft-computing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Why applied soft computing?</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/discord.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Discord</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/minidora-usage.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Using Minidora</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/how-to-submit-assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">How to submit assignment</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/deliverables.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deliverables</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 1: The Data Scientist’s Toolkit</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m01-toolkit/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m01-toolkit/git-github.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Version Control with Git &amp; GitHub</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m01-toolkit/tidy-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Tidy Data Philosophy</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m01-toolkit/data-provenance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Provenance</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m01-toolkit/reproduceability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Reproducibility</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 2: Visualizing Complexity</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m02-visualization/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m02-visualization/principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Principles of Effective Visualization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m02-visualization/1d-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing 1D Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m02-visualization/2d-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing 2D Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m02-visualization/highd-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing High-Dimensional Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m02-visualization/networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m02-visualization/time-series.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing Time-Series</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 3: Agentic Coding</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m03-agentic-coding/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m03-agentic-coding/hands-on.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Hands-on</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m03-agentic-coding/prompt-tuning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prompt Tuning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m03-agentic-coding/agentic-ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">From ChatBot to Agentic AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m03-agentic-coding/context-engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Context Engineering</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 4: Deep Learning for Text</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-text/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-text/llm-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Large Language Models in Practice</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-text/prompt-engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prompt Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-text/gpt-inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">GPT Inference: Sampling Strategies</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-text/tokenization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Tokenization: Unboxing How LLMs Read Text</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-text/transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Transformers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-text/bert-gpt.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">BERT &amp; GPT</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-text/sentence-transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sentence Transformers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-text/word-embeddings.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Word Embeddings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-text/semaxis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Semaxis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-text/word-bias.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Word Bias</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 4: Deep Learning for Images</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/image-processing.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Processing Fundamentals</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/cnn.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/lenet.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LeNet Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/alexnet.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AlexNet: Deep CNN Revolution</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/vgg.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">VGG Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/inception.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Inception &amp; Multi-Scale Features</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/batch-normalization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Batch Normalization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/resnet.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ResNet &amp; Skip Connections</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 5: Deep Learning for Graphs</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m05-graphs/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m05-graphs/spectral-embedding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Spectral Graph Embedding</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m05-graphs/graph-embedding-w-word2vec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Graph Embeddings with Word2Vec</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m05-graphs/spectral-vs-neural-embedding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Spectral vs.&nbsp;Neural Embeddings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m05-graphs/from-image-to-graph.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">From Images to Graphs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m05-graphs/graph-convolutional-network.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Graph Convolutional Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m05-graphs/popular-gnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Popular GNN Architectures</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m05-graphs/software.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">GNN Software &amp; Tools</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 6: Large Language Models &amp; Emergent Behavior</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m06-llms/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m06-llms/transformers.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Transformer Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m06-llms/bert.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">BERT &amp; Contextual Embeddings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m06-llms/sentence-bert.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sentence-BERT for Semantic Similarity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m06-llms/gpt.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">GPT &amp; Generative Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m06-llms/from-language-model-to-instruction-following.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">From Language Models to Instruction Following</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m06-llms/prompt-tuning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prompt Engineering &amp; In-Context Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m06-llms/scaling-emergence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Scaling Laws &amp; Emergent Abilities</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m06-llms/llms-as-complex-systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LLMs as Complex Systems</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 7: Self-Supervised Learning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m07-self-supervised/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m07-self-supervised/paradigm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Self-Supervised Paradigm</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m07-self-supervised/contrastive-learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Contrastive Learning (SimCLR)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m07-self-supervised/graphs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Self-Supervised Learning for Graphs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m07-self-supervised/time-series.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Self-Supervised Learning for Time-Series</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 8: Explainability &amp; Ethics</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m08-explainability/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m08-explainability/need.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Need for Explainability</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m08-explainability/attention.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Attention Visualization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m08-explainability/lime-shap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LIME &amp; SHAP</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m08-explainability/fairness.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Algorithmic Fairness &amp; Bias</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m08-explainability/causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Causality vs.&nbsp;Correlation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false">
 <span class="menu-text">Legacy Materials</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m03-text/what-to-learn.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Word &amp; Document Embeddings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m03-text/what-to-learn.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Recurrent Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/what-to-learn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Processing (CNNs)</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Embeddings: How Machines Understand Meaning</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Sadamori Kojaku </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 21, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="llms-dont-read.-lets-see-what-they-actually-see." class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> LLMs Don’t Read. Let’s See What They Actually See.</h1>
<p>When you send text to an LLM, you see words. The model sees vectors—long lists of numbers like <code>[0.31, -0.85, 0.12, ..., 0.47]</code>. Each word, sentence, or document becomes a point in a high-dimensional space. These numerical representations are called <strong>embeddings</strong>.</p>
<p>This might seem like a strange way to “understand” language. But embeddings have a remarkable property: <strong>similar meanings become similar vectors</strong>. Words like “cat” and “dog” end up close together in this space, while “cat” and “theorem” are far apart.</p>
<p>Embeddings are the foundation of modern NLP. They’re how LLMs represent knowledge, perform reasoning, and generate text. Once you understand embeddings, transformers and LLMs stop being magic—they’re just sophisticated ways of manipulating these numerical representations.</p>
<p>Let’s unbox this first layer and see how meaning becomes mathematics.</p>
<section id="from-text-to-numbers-the-challenge" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="from-text-to-numbers-the-challenge"><span class="header-section-number">1.1</span> From Text to Numbers: The Challenge</h2>
<p>Computers can’t directly process text. They need numbers. But how do we convert words into numbers in a meaningful way?</p>
<section id="naive-approach-integer-encoding" class="level3">
<h3 class="anchored" data-anchor-id="naive-approach-integer-encoding">Naive Approach: Integer Encoding</h3>
<p>The simplest idea: assign each word a unique integer.</p>
<div id="48936c1c" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple vocabulary</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> [<span class="st">"network"</span>, <span class="st">"graph"</span>, <span class="st">"node"</span>, <span class="st">"community"</span>, <span class="st">"detection"</span>]</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Assign integers</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>word_to_int <span class="op">=</span> {word: i <span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(vocab)}</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Integer encoding:"</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(word_to_int)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Output</strong>:</p>
<pre><code>{'network': 0, 'graph': 1, 'node': 2, 'community': 3, 'detection': 4}</code></pre>
<p><strong>Problem</strong>: The integers are arbitrary. The model might think “network” (0) is somehow “less than” “community” (3), or that “graph” + “node” = “community”. These numbers encode no semantic relationships.</p>
</section>
<section id="better-approach-one-hot-encoding" class="level3">
<h3 class="anchored" data-anchor-id="better-approach-one-hot-encoding">Better Approach: One-Hot Encoding</h3>
<p>Represent each word as a binary vector where only one position is “hot” (=1).</p>
<div id="83f9a394" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(vocab)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> one_hot(word):</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Convert word to one-hot vector."""</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    vec <span class="op">=</span> np.zeros(vocab_size)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    vec[word_to_int[word]] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> vec</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"One-hot encoding for 'network':"</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(one_hot(<span class="st">"network"</span>))</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">One-hot encoding for 'community':"</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(one_hot(<span class="st">"community"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Output</strong>:</p>
<pre><code>[1. 0. 0. 0. 0.]
[0. 0. 0. 1. 0.]</code></pre>
<p><strong>Problem</strong>: Every word is equally different from every other word (Euclidean distance is always √2). The model still can’t learn that “network” and “graph” are related, while “network” and “detection” are less related.</p>
</section>
<section id="the-key-insight-learned-dense-embeddings" class="level3">
<h3 class="anchored" data-anchor-id="the-key-insight-learned-dense-embeddings">The Key Insight: Learned Dense Embeddings</h3>
<p>Instead of hand-crafting representations, <strong>let the model learn them</strong> from data. Each word becomes a dense vector of real numbers (typically 50-1000 dimensions):</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co">"network"</span> → [<span class="fl">0.31</span>, <span class="op">-</span><span class="fl">0.85</span>, <span class="fl">0.12</span>, <span class="fl">0.67</span>, ...]  <span class="co"># 384 dimensions</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co">"graph"</span>   → [<span class="fl">0.29</span>, <span class="op">-</span><span class="fl">0.82</span>, <span class="fl">0.15</span>, <span class="fl">0.69</span>, ...]  <span class="co"># Similar to "network"!</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co">"theorem"</span> → [<span class="op">-</span><span class="fl">0.61</span>, <span class="fl">0.23</span>, <span class="op">-</span><span class="fl">0.45</span>, <span class="fl">0.11</span>, ...] <span class="co"># Different from "network"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>These embeddings are learned by training models to predict context. Words that appear in similar contexts get similar embeddings. This is the foundation of modern NLP.</p>
</section>
</section>
<section id="semantic-similarity-the-power-of-embeddings" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="semantic-similarity-the-power-of-embeddings"><span class="header-section-number">1.2</span> Semantic Similarity: The Power of Embeddings</h2>
<p>Once words are vectors, we can measure semantic similarity using <strong>cosine similarity</strong>:</p>
<p><span class="math display">
\text{similarity}(u, v) = \frac{u \cdot v}{\|u\| \|v\|}
</span></p>
<p>This measures the cosine of the angle between vectors (1 = same direction, 0 = orthogonal, -1 = opposite).</p>
<p>Let’s see this in action with real embeddings.</p>
</section>
<section id="using-sentence-transformers" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="using-sentence-transformers"><span class="header-section-number">1.3</span> Using Sentence Transformers</h2>
<p>We’ll use the <code>sentence-transformers</code> library, which provides pre-trained models for generating embeddings.</p>
<div id="0e21af55" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sentence_transformers <span class="im">import</span> SentenceTransformer</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Load a pre-trained model (lightweight, ~80MB)</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SentenceTransformer(<span class="st">'all-MiniLM-L6-v2'</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate embeddings for words</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> [<span class="st">"network"</span>, <span class="st">"graph"</span>, <span class="st">"community"</span>, <span class="st">"detection"</span>, <span class="st">"cat"</span>, <span class="st">"theorem"</span>]</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>embeddings <span class="op">=</span> model.encode(words)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Embedding dimensionality: </span><span class="sc">{</span>embeddings<span class="sc">.</span>shape[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of words: </span><span class="sc">{</span>embeddings<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">First 10 dimensions of 'network': </span><span class="sc">{</span>embeddings[<span class="dv">0</span>][:<span class="dv">10</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Output</strong>:</p>
<pre><code>Embedding dimensionality: 384
Number of words: 6

First 10 dimensions of 'network': [ 0.0234 -0.0912  0.0456 ... ]</code></pre>
<p>Each word is now a 384-dimensional vector. Let’s compute similarities:</p>
<div id="6d3b7839" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> cosine_similarity</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute similarity matrix</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>sim_matrix <span class="op">=</span> cosine_similarity(embeddings)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Display as a heatmap</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">"white"</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>sns.heatmap(sim_matrix, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">".2f"</span>,</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>            xticklabels<span class="op">=</span>words, yticklabels<span class="op">=</span>words,</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>            cmap<span class="op">=</span><span class="st">"RdYlGn"</span>, vmin<span class="op">=</span><span class="dv">0</span>, vmax<span class="op">=</span><span class="dv">1</span>, ax<span class="op">=</span>ax,</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>            cbar_kws<span class="op">=</span>{<span class="st">'label'</span>: <span class="st">'Cosine Similarity'</span>})</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Word Similarity Matrix"</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Key observations</strong>: - “network” and “graph” have high similarity (~0.85) — the model learned they’re related! - “cat” has low similarity to network science terms - “theorem” is somewhat similar to technical terms but distinct from social/biological concepts</p>
<p>This happens <strong>without anyone explicitly telling the model</strong> that “network” and “graph” are synonyms. The model learned from context.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Distributional Hypothesis
</div>
</div>
<div class="callout-body-container callout-body">
<p>“You shall know a word by the company it keeps.” — J.R. Firth, 1957</p>
<p>Words that appear in similar contexts tend to have similar meanings. Embeddings operationalize this idea: they place words with similar contexts near each other in vector space.</p>
</div>
</div>
</section>
<section id="from-words-to-sentences" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="from-words-to-sentences"><span class="header-section-number">1.4</span> From Words to Sentences</h2>
<p>Word embeddings are useful, but research deals with sentences and documents. How do we embed larger chunks of text?</p>
<section id="naive-approach-average-word-vectors" class="level3">
<h3 class="anchored" data-anchor-id="naive-approach-average-word-vectors">Naive Approach: Average Word Vectors</h3>
<div id="30b5ac18" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>sentence1 <span class="op">=</span> <span class="st">"Community detection in networks"</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>sentence2 <span class="op">=</span> <span class="st">"Identifying groups in graphs"</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>sentence3 <span class="op">=</span> <span class="st">"Cats like milk"</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Encode sentences</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>sent_embeddings <span class="op">=</span> model.encode([sentence1, sentence2, sentence3])</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute similarities</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>sent_sim <span class="op">=</span> cosine_similarity(sent_embeddings)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Sentence similarities:"</span>)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"'</span><span class="sc">{</span>sentence1<span class="sc">}</span><span class="ss">' vs. '</span><span class="sc">{</span>sentence2<span class="sc">}</span><span class="ss">': </span><span class="sc">{</span>sent_sim[<span class="dv">0</span>, <span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"'</span><span class="sc">{</span>sentence1<span class="sc">}</span><span class="ss">' vs. '</span><span class="sc">{</span>sentence3<span class="sc">}</span><span class="ss">': </span><span class="sc">{</span>sent_sim[<span class="dv">0</span>, <span class="dv">2</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Output</strong>:</p>
<pre><code>Sentence similarities:
'Community detection in networks' vs. 'Identifying groups in graphs': 0.834
'Community detection in networks' vs. 'Cats like milk': 0.124</code></pre>
<p>The model correctly recognizes that the first two sentences describe similar concepts, while the third is unrelated.</p>
<p><strong>How does this work?</strong> Modern sentence embedding models (like the one we’re using) don’t just average word vectors—they use <strong>transformers</strong> to generate context-aware representations. We’ll explore how transformers work in the next section. For now, just know: sentence embeddings capture meaning at the sentence level.</p>
</section>
</section>
<section id="application-1-semantic-search" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="application-1-semantic-search"><span class="header-section-number">1.5</span> Application 1: Semantic Search</h2>
<p>Embeddings enable <strong>semantic search</strong>: finding documents by meaning, not just keywords.</p>
<p>Traditional keyword search: - Query: “community detection” - Matches: Papers containing exactly those words - Misses: Papers about “group identification” or “clustering”</p>
<p>Semantic search: - Query: “community detection” - Matches: Papers about related concepts even if they use different words</p>
<p>Let’s build a simple semantic search engine for research papers.</p>
<div id="27bc8e0c" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulated paper titles</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>papers <span class="op">=</span> [</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Community Detection in Social Networks Using Modularity Optimization"</span>,</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Graph Clustering Algorithms: A Survey"</span>,</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Identifying Groups in Biological Networks"</span>,</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Deep Learning for Image Classification"</span>,</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Temporal Dynamics of Network Structure"</span>,</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Protein-Protein Interaction Prediction"</span>,</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Hierarchical Structure in Complex Networks"</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Embed all papers</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>paper_embeddings <span class="op">=</span> model.encode(papers)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="co"># User query</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> <span class="st">"finding groups in networks"</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>query_embedding <span class="op">=</span> model.encode([query])</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute similarities</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>similarities <span class="op">=</span> cosine_similarity(query_embedding, paper_embeddings)[<span class="dv">0</span>]</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Rank papers</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>ranked_indices <span class="op">=</span> np.argsort(similarities)[::<span class="op">-</span><span class="dv">1</span>]  <span class="co"># Descending order</span></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Query: '</span><span class="sc">{</span>query<span class="sc">}</span><span class="ss">'</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Top 3 most relevant papers:"</span>)</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, idx <span class="kw">in</span> <span class="bu">enumerate</span>(ranked_indices[:<span class="dv">3</span>], <span class="dv">1</span>):</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">. [</span><span class="sc">{</span>similarities[idx]<span class="sc">:.3f}</span><span class="ss">] </span><span class="sc">{</span>papers[idx]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Output</strong>:</p>
<pre><code>Query: 'finding groups in networks'

Top 3 most relevant papers:
1. [0.812] Community Detection in Social Networks Using Modularity Optimization
2. [0.789] Identifying Groups in Biological Networks
3. [0.754] Graph Clustering Algorithms: A Survey</code></pre>
<p>Even though the query doesn’t exactly match any title, semantic search finds the most relevant papers. Paper 4 (“Deep Learning for Image Classification”) would have low similarity and rank last.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Building Your Own Semantic Search
</div>
</div>
<div class="callout-body-container callout-body">
<p>You can build a semantic search system for your literature: 1. Collect papers (titles + abstracts) 2. Generate embeddings with <code>sentence-transformers</code> 3. Store embeddings (just numpy arrays) 4. For each query, compute cosine similarity 5. Return top-K most similar papers</p>
<p>This works well up to ~100K papers on a laptop.</p>
</div>
</div>
</section>
<section id="application-2-document-clustering" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="application-2-document-clustering"><span class="header-section-number">1.6</span> Application 2: Document Clustering</h2>
<p>Embeddings naturally group similar documents. Let’s cluster research papers by topic.</p>
<div id="44205938" class="cell" data-fig-width="10" data-fig-height="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># More papers (simulated for illustration)</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>papers_extended <span class="op">=</span> [</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Cluster 1: Community detection</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Community detection using modularity"</span>,</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Overlapping community structure"</span>,</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Hierarchical community detection"</span>,</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Cluster 2: Network dynamics</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Temporal networks and time-varying graphs"</span>,</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Evolution of network structure"</span>,</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Dynamic processes on networks"</span>,</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Cluster 3: Machine learning on graphs</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Graph neural networks for node classification"</span>,</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Deep learning on graphs"</span>,</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Representation learning on networks"</span>,</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Cluster 4: Biological networks</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Protein interaction networks"</span>,</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Gene regulatory networks"</span>,</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Network medicine and disease modules"</span>,</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate embeddings</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>paper_embs <span class="op">=</span> model.encode(papers_extended)</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Cluster using K-means</span></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>n_clusters <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>n_clusters, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>clusters <span class="op">=</span> kmeans.fit_predict(paper_embs)</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Reduce to 2D for visualization</span></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>tsne <span class="op">=</span> TSNE(n_components<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">42</span>, perplexity<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>paper_2d <span class="op">=</span> tsne.fit_transform(paper_embs)</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">7</span>))</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'#e74c3c'</span>, <span class="st">'#3498db'</span>, <span class="st">'#2ecc71'</span>, <span class="st">'#f39c12'</span>]</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>cluster_names <span class="op">=</span> [<span class="st">'Community</span><span class="ch">\n</span><span class="st">Detection'</span>, <span class="st">'Network</span><span class="ch">\n</span><span class="st">Dynamics'</span>,</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>                <span class="st">'ML on Graphs'</span>, <span class="st">'Biological</span><span class="ch">\n</span><span class="st">Networks'</span>]</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_clusters):</span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> clusters <span class="op">==</span> i</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>    ax.scatter(paper_2d[mask, <span class="dv">0</span>], paper_2d[mask, <span class="dv">1</span>],</span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>              c<span class="op">=</span>colors[i], label<span class="op">=</span>cluster_names[i],</span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a>              s<span class="op">=</span><span class="dv">200</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, edgecolors<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>)</span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"t-SNE Dimension 1"</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"t-SNE Dimension 2"</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Automatic Clustering of Research Papers"</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a>ax.legend(loc<span class="op">=</span><span class="st">'best'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a>ax.grid(alpha<span class="op">=</span><span class="fl">0.3</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a>sns.despine()</span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Key insight</strong>: We never told the model what “community detection” or “biological networks” means. It learned these concepts from patterns in text and automatically grouped related papers.</p>
</section>
<section id="application-3-finding-similar-papers" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="application-3-finding-similar-papers"><span class="header-section-number">1.7</span> Application 3: Finding Similar Papers</h2>
<p>Given a paper you like, find others that are similar.</p>
<div id="96c9bfb3" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># You read and liked this paper</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>seed_paper <span class="op">=</span> <span class="st">"We develop a graph neural network for predicting protein functions."</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Database of papers</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>database <span class="op">=</span> [</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Deep learning for protein structure prediction"</span>,</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Community detection in social networks"</span>,</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Node classification using graph convolutions"</span>,</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Temporal dynamics in citation networks"</span>,</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Representation learning for biological networks"</span>,</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Image classification with CNNs"</span>,</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Embed everything</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>seed_emb <span class="op">=</span> model.encode([seed_paper])</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>db_embs <span class="op">=</span> model.encode(database)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Find most similar</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>sims <span class="op">=</span> cosine_similarity(seed_emb, db_embs)[<span class="dv">0</span>]</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>sorted_indices <span class="op">=</span> np.argsort(sims)[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Papers similar to:</span><span class="ch">\n</span><span class="ss">'</span><span class="sc">{</span>seed_paper<span class="sc">}</span><span class="ss">'</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, idx <span class="kw">in</span> <span class="bu">enumerate</span>(sorted_indices[:<span class="dv">3</span>], <span class="dv">1</span>):</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">. [</span><span class="sc">{</span>sims[idx]<span class="sc">:.3f}</span><span class="ss">] </span><span class="sc">{</span>database[idx]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Output</strong>:</p>
<pre><code>Papers similar to:
'We develop a graph neural network for predicting protein functions.'

1. [0.812] Representation learning for biological networks
2. [0.789] Deep learning for protein structure prediction
3. [0.754] Node classification using graph convolutions</code></pre>
<p>This is how recommendation systems work: embed items, find nearest neighbors.</p>
</section>
<section id="visualizing-the-embedding-space" class="level2" data-number="1.8">
<h2 data-number="1.8" class="anchored" data-anchor-id="visualizing-the-embedding-space"><span class="header-section-number">1.8</span> Visualizing the Embedding Space</h2>
<p>Let’s visualize what’s happening in this high-dimensional space.</p>
<div id="70b7247c" class="cell" data-fig-width="12" data-fig-height="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># A diverse set of research terms</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>terms <span class="op">=</span> [</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Network science</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"network"</span>, <span class="st">"graph"</span>, <span class="st">"community"</span>, <span class="st">"centrality"</span>, <span class="st">"clustering"</span>,</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Machine learning</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"neural network"</span>, <span class="st">"deep learning"</span>, <span class="st">"classification"</span>, <span class="st">"regression"</span>,</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Biology</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"protein"</span>, <span class="st">"gene"</span>, <span class="st">"cell"</span>, <span class="st">"DNA"</span>, <span class="st">"evolution"</span>,</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Physics</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"quantum"</span>, <span class="st">"particle"</span>, <span class="st">"entropy"</span>, <span class="st">"thermodynamics"</span>,</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mathematics</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"theorem"</span>, <span class="st">"proof"</span>, <span class="st">"equation"</span>, <span class="st">"matrix"</span>, <span class="st">"vector"</span>,</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>term_embs <span class="op">=</span> model.encode(terms)</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Reduce to 2D</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>tsne <span class="op">=</span> TSNE(n_components<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">42</span>, perplexity<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>term_2d <span class="op">=</span> tsne.fit_transform(term_embs)</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Color by rough category (for illustration)</span></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>categories <span class="op">=</span> {</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Network Science'</span>: [<span class="st">'network'</span>, <span class="st">'graph'</span>, <span class="st">'community'</span>, <span class="st">'centrality'</span>, <span class="st">'clustering'</span>],</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Machine Learning'</span>: [<span class="st">'neural network'</span>, <span class="st">'deep learning'</span>, <span class="st">'classification'</span>, <span class="st">'regression'</span>],</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Biology'</span>: [<span class="st">'protein'</span>, <span class="st">'gene'</span>, <span class="st">'cell'</span>, <span class="st">'DNA'</span>, <span class="st">'evolution'</span>],</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Physics'</span>: [<span class="st">'quantum'</span>, <span class="st">'particle'</span>, <span class="st">'entropy'</span>, <span class="st">'thermodynamics'</span>],</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Mathematics'</span>: [<span class="st">'theorem'</span>, <span class="st">'proof'</span>, <span class="st">'equation'</span>, <span class="st">'matrix'</span>, <span class="st">'vector'</span>],</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>colors_map <span class="op">=</span> {<span class="st">'Network Science'</span>: <span class="st">'#e74c3c'</span>, <span class="st">'Machine Learning'</span>: <span class="st">'#3498db'</span>,</span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>              <span class="st">'Biology'</span>: <span class="st">'#2ecc71'</span>, <span class="st">'Physics'</span>: <span class="st">'#f39c12'</span>, <span class="st">'Mathematics'</span>: <span class="st">'#9b59b6'</span>}</span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> category, words <span class="kw">in</span> categories.items():</span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>    indices <span class="op">=</span> [terms.index(w) <span class="cf">for</span> w <span class="kw">in</span> words]</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>    ax.scatter(term_2d[indices, <span class="dv">0</span>], term_2d[indices, <span class="dv">1</span>],</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>              c<span class="op">=</span>colors_map[category], label<span class="op">=</span>category, s<span class="op">=</span><span class="dv">300</span>, alpha<span class="op">=</span><span class="fl">0.7</span>,</span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>              edgecolors<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Annotate terms</span></span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx <span class="kw">in</span> indices:</span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>        ax.annotate(terms[idx], (term_2d[idx, <span class="dv">0</span>], term_2d[idx, <span class="dv">1</span>]),</span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>                   fontsize<span class="op">=</span><span class="dv">10</span>, ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Semantic Dimension 1"</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Semantic Dimension 2"</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"The Semantic Space: How Concepts Relate"</span>, fontsize<span class="op">=</span><span class="dv">15</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a>ax.legend(loc<span class="op">=</span><span class="st">'best'</span>, fontsize<span class="op">=</span><span class="dv">11</span>, frameon<span class="op">=</span><span class="va">True</span>, shadow<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a>ax.grid(alpha<span class="op">=</span><span class="fl">0.3</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a>sns.despine()</span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Notice how: - <strong>Clusters form naturally</strong>: Biology terms group together, math terms group together - <strong>Cross-domain connections</strong>: “matrix” (math) might be closer to “network” (network science) than to “theorem” (pure math) - <strong>Embedding space has structure</strong>: It’s not random—semantic relationships are preserved</p>
</section>
<section id="how-embeddings-are-learned" class="level2" data-number="1.9">
<h2 data-number="1.9" class="anchored" data-anchor-id="how-embeddings-are-learned"><span class="header-section-number">1.9</span> How Embeddings Are Learned</h2>
<p>You don’t need to train embeddings from scratch (it requires huge data and compute). But understanding how they’re learned helps you use them effectively.</p>
<p><strong>Training objective</strong>: Predict context from words (or vice versa).</p>
<p>Example: Given “The <strong>cat</strong> sat on the mat”, predict “cat” from context [“the”, “sat”, “on”, “the”, “mat”].</p>
<p>The model adjusts embeddings so that: - Words appearing in similar contexts get similar embeddings - Context → word predictions become accurate</p>
<p>After training on billions of sentences, the embeddings encode semantic and syntactic relationships.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Pre-trained Models
</div>
</div>
<div class="callout-body-container callout-body">
<p>Models like <code>all-MiniLM-L6-v2</code> are pre-trained on huge text corpora (web pages, books, Wikipedia). They’ve already learned general semantic relationships. You can use them immediately for most tasks.</p>
<p>For specialized domains (e.g., medical research), you might fine-tune on domain-specific text—but pre-trained models work surprisingly well out-of-the-box.</p>
</div>
</div>
</section>
<section id="static-vs.-contextual-embeddings" class="level2" data-number="1.10">
<h2 data-number="1.10" class="anchored" data-anchor-id="static-vs.-contextual-embeddings"><span class="header-section-number">1.10</span> Static vs.&nbsp;Contextual Embeddings</h2>
<p>There are two types of embeddings:</p>
<p><strong>Static embeddings</strong> (Word2vec, GloVe): - Each word has one fixed embedding - “bank” always has the same vector, whether it’s a financial institution or a river bank</p>
<p><strong>Contextual embeddings</strong> (BERT, GPT, sentence-transformers): - Embeddings depend on context - “bank” in “I went to the bank” vs.&nbsp;“river bank” gets different embeddings</p>
<p>The model we’ve been using (<code>all-MiniLM-L6-v2</code>) produces <strong>contextual</strong> embeddings using transformers. We’ll explore how transformers enable this in the next section.</p>
</section>
<section id="limitations-of-embeddings" class="level2" data-number="1.11">
<h2 data-number="1.11" class="anchored" data-anchor-id="limitations-of-embeddings"><span class="header-section-number">1.11</span> Limitations of Embeddings</h2>
<p>Embeddings are powerful but imperfect:</p>
<ol type="1">
<li><p><strong>Bias</strong>: Embeddings learn from text data, which contains human biases. If training data associates “doctor” with “male” and “nurse” with “female”, embeddings will encode this bias.</p></li>
<li><p><strong>Out-of-vocabulary words</strong>: Unknown words can’t be embedded (though modern models use subword tokenization to partially address this).</p></li>
<li><p><strong>Polysemy</strong>: Even contextual embeddings can struggle with highly ambiguous words.</p></li>
<li><p><strong>Cultural specificity</strong>: Embeddings reflect the culture and language of the training data.</p></li>
</ol>
<p>We’ll explore bias in embeddings later when we discuss semantic axes.</p>
</section>
<section id="the-bigger-picture" class="level2" data-number="1.12">
<h2 data-number="1.12" class="anchored" data-anchor-id="the-bigger-picture"><span class="header-section-number">1.12</span> The Bigger Picture</h2>
<p>You now understand <strong>how LLMs see text</strong>: as points in a high-dimensional semantic space. When you use an LLM:</p>
<ol type="1">
<li>Your prompt is converted to embeddings</li>
<li>The model manipulates these embeddings through layers of computation</li>
<li>The output embeddings are converted back to text</li>
</ol>
<p>Embeddings are the “language” LLMs speak internally. Everything else—attention, transformers, generation—operates on these numerical representations.</p>
<p><strong>But wait—there’s a step we’ve skipped.</strong> Before text becomes embeddings, it must first become <strong>tokens</strong>. How does “Community detection” become a sequence of numbers? Why do some words get split into pieces? Let’s unbox an actual LLM and see exactly how it reads text.</p>
<hr>
<p><strong>Next</strong>: <a href="tokenization.qmd">Tokenization: Unboxing How LLMs Read Text →</a></p>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb17" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Embeddings: How Machines Understand Meaning"</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="fu"># LLMs Don't Read. Let's See What They Actually See.</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>When you send text to an LLM, you see words. The model sees vectors—long lists of numbers like <span class="in">`[0.31, -0.85, 0.12, ..., 0.47]`</span>. Each word, sentence, or document becomes a point in a high-dimensional space. These numerical representations are called **embeddings**.</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>This might seem like a strange way to "understand" language. But embeddings have a remarkable property: **similar meanings become similar vectors**. Words like "cat" and "dog" end up close together in this space, while "cat" and "theorem" are far apart.</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>Embeddings are the foundation of modern NLP. They're how LLMs represent knowledge, perform reasoning, and generate text. Once you understand embeddings, transformers and LLMs stop being magic—they're just sophisticated ways of manipulating these numerical representations.</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>Let's unbox this first layer and see how meaning becomes mathematics.</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="fu">## From Text to Numbers: The Challenge</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>Computers can't directly process text. They need numbers. But how do we convert words into numbers in a meaningful way?</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a><span class="fu">### Naive Approach: Integer Encoding</span></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>The simplest idea: assign each word a unique integer.</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple vocabulary</span></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> [<span class="st">"network"</span>, <span class="st">"graph"</span>, <span class="st">"node"</span>, <span class="st">"community"</span>, <span class="st">"detection"</span>]</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Assign integers</span></span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>word_to_int <span class="op">=</span> {word: i <span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(vocab)}</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Integer encoding:"</span>)</span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(word_to_int)</span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a>**Output**:</span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a><span class="in">{'network': 0, 'graph': 1, 'node': 2, 'community': 3, 'detection': 4}</span></span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a>**Problem**: The integers are arbitrary. The model might think "network" (0) is somehow "less than" "community" (3), or that "graph" + "node" = "community". These numbers encode no semantic relationships.</span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a><span class="fu">### Better Approach: One-Hot Encoding</span></span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a>Represent each word as a binary vector where only one position is "hot" (=1).</span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb17-53"><a href="#cb17-53" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-54"><a href="#cb17-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-55"><a href="#cb17-55" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(vocab)</span>
<span id="cb17-56"><a href="#cb17-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-57"><a href="#cb17-57" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> one_hot(word):</span>
<span id="cb17-58"><a href="#cb17-58" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Convert word to one-hot vector."""</span></span>
<span id="cb17-59"><a href="#cb17-59" aria-hidden="true" tabindex="-1"></a>    vec <span class="op">=</span> np.zeros(vocab_size)</span>
<span id="cb17-60"><a href="#cb17-60" aria-hidden="true" tabindex="-1"></a>    vec[word_to_int[word]] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb17-61"><a href="#cb17-61" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> vec</span>
<span id="cb17-62"><a href="#cb17-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-63"><a href="#cb17-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"One-hot encoding for 'network':"</span>)</span>
<span id="cb17-64"><a href="#cb17-64" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(one_hot(<span class="st">"network"</span>))</span>
<span id="cb17-65"><a href="#cb17-65" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">One-hot encoding for 'community':"</span>)</span>
<span id="cb17-66"><a href="#cb17-66" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(one_hot(<span class="st">"community"</span>))</span>
<span id="cb17-67"><a href="#cb17-67" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-68"><a href="#cb17-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-69"><a href="#cb17-69" aria-hidden="true" tabindex="-1"></a>**Output**:</span>
<span id="cb17-70"><a href="#cb17-70" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-71"><a href="#cb17-71" aria-hidden="true" tabindex="-1"></a><span class="in">[1. 0. 0. 0. 0.]</span></span>
<span id="cb17-72"><a href="#cb17-72" aria-hidden="true" tabindex="-1"></a><span class="in">[0. 0. 0. 1. 0.]</span></span>
<span id="cb17-73"><a href="#cb17-73" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-74"><a href="#cb17-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-75"><a href="#cb17-75" aria-hidden="true" tabindex="-1"></a>**Problem**: Every word is equally different from every other word (Euclidean distance is always √2). The model still can't learn that "network" and "graph" are related, while "network" and "detection" are less related.</span>
<span id="cb17-76"><a href="#cb17-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-77"><a href="#cb17-77" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Key Insight: Learned Dense Embeddings</span></span>
<span id="cb17-78"><a href="#cb17-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-79"><a href="#cb17-79" aria-hidden="true" tabindex="-1"></a>Instead of hand-crafting representations, **let the model learn them** from data. Each word becomes a dense vector of real numbers (typically 50-1000 dimensions):</span>
<span id="cb17-80"><a href="#cb17-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-81"><a href="#cb17-81" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb17-82"><a href="#cb17-82" aria-hidden="true" tabindex="-1"></a><span class="co">"network"</span> → [<span class="fl">0.31</span>, <span class="op">-</span><span class="fl">0.85</span>, <span class="fl">0.12</span>, <span class="fl">0.67</span>, ...]  <span class="co"># 384 dimensions</span></span>
<span id="cb17-83"><a href="#cb17-83" aria-hidden="true" tabindex="-1"></a><span class="co">"graph"</span>   → [<span class="fl">0.29</span>, <span class="op">-</span><span class="fl">0.82</span>, <span class="fl">0.15</span>, <span class="fl">0.69</span>, ...]  <span class="co"># Similar to "network"!</span></span>
<span id="cb17-84"><a href="#cb17-84" aria-hidden="true" tabindex="-1"></a><span class="co">"theorem"</span> → [<span class="op">-</span><span class="fl">0.61</span>, <span class="fl">0.23</span>, <span class="op">-</span><span class="fl">0.45</span>, <span class="fl">0.11</span>, ...] <span class="co"># Different from "network"</span></span>
<span id="cb17-85"><a href="#cb17-85" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-86"><a href="#cb17-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-87"><a href="#cb17-87" aria-hidden="true" tabindex="-1"></a>These embeddings are learned by training models to predict context. Words that appear in similar contexts get similar embeddings. This is the foundation of modern NLP.</span>
<span id="cb17-88"><a href="#cb17-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-89"><a href="#cb17-89" aria-hidden="true" tabindex="-1"></a><span class="fu">## Semantic Similarity: The Power of Embeddings</span></span>
<span id="cb17-90"><a href="#cb17-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-91"><a href="#cb17-91" aria-hidden="true" tabindex="-1"></a>Once words are vectors, we can measure semantic similarity using **cosine similarity**:</span>
<span id="cb17-92"><a href="#cb17-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-93"><a href="#cb17-93" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-94"><a href="#cb17-94" aria-hidden="true" tabindex="-1"></a>\text{similarity}(u, v) = \frac{u \cdot v}{<span class="sc">\|</span>u<span class="sc">\|</span> <span class="sc">\|</span>v<span class="sc">\|</span>}</span>
<span id="cb17-95"><a href="#cb17-95" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-96"><a href="#cb17-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-97"><a href="#cb17-97" aria-hidden="true" tabindex="-1"></a>This measures the cosine of the angle between vectors (1 = same direction, 0 = orthogonal, -1 = opposite).</span>
<span id="cb17-98"><a href="#cb17-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-99"><a href="#cb17-99" aria-hidden="true" tabindex="-1"></a>Let's see this in action with real embeddings.</span>
<span id="cb17-100"><a href="#cb17-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-101"><a href="#cb17-101" aria-hidden="true" tabindex="-1"></a><span class="fu">## Using Sentence Transformers</span></span>
<span id="cb17-102"><a href="#cb17-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-103"><a href="#cb17-103" aria-hidden="true" tabindex="-1"></a>We'll use the <span class="in">`sentence-transformers`</span> library, which provides pre-trained models for generating embeddings.</span>
<span id="cb17-104"><a href="#cb17-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-107"><a href="#cb17-107" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-108"><a href="#cb17-108" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb17-109"><a href="#cb17-109" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sentence_transformers <span class="im">import</span> SentenceTransformer</span>
<span id="cb17-110"><a href="#cb17-110" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-111"><a href="#cb17-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-112"><a href="#cb17-112" aria-hidden="true" tabindex="-1"></a><span class="co"># Load a pre-trained model (lightweight, ~80MB)</span></span>
<span id="cb17-113"><a href="#cb17-113" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SentenceTransformer(<span class="st">'all-MiniLM-L6-v2'</span>)</span>
<span id="cb17-114"><a href="#cb17-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-115"><a href="#cb17-115" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate embeddings for words</span></span>
<span id="cb17-116"><a href="#cb17-116" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> [<span class="st">"network"</span>, <span class="st">"graph"</span>, <span class="st">"community"</span>, <span class="st">"detection"</span>, <span class="st">"cat"</span>, <span class="st">"theorem"</span>]</span>
<span id="cb17-117"><a href="#cb17-117" aria-hidden="true" tabindex="-1"></a>embeddings <span class="op">=</span> model.encode(words)</span>
<span id="cb17-118"><a href="#cb17-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-119"><a href="#cb17-119" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Embedding dimensionality: </span><span class="sc">{</span>embeddings<span class="sc">.</span>shape[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-120"><a href="#cb17-120" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of words: </span><span class="sc">{</span>embeddings<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-121"><a href="#cb17-121" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">First 10 dimensions of 'network': </span><span class="sc">{</span>embeddings[<span class="dv">0</span>][:<span class="dv">10</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-122"><a href="#cb17-122" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-123"><a href="#cb17-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-124"><a href="#cb17-124" aria-hidden="true" tabindex="-1"></a>**Output**:</span>
<span id="cb17-125"><a href="#cb17-125" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-126"><a href="#cb17-126" aria-hidden="true" tabindex="-1"></a><span class="in">Embedding dimensionality: 384</span></span>
<span id="cb17-127"><a href="#cb17-127" aria-hidden="true" tabindex="-1"></a><span class="in">Number of words: 6</span></span>
<span id="cb17-128"><a href="#cb17-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-129"><a href="#cb17-129" aria-hidden="true" tabindex="-1"></a><span class="in">First 10 dimensions of 'network': [ 0.0234 -0.0912  0.0456 ... ]</span></span>
<span id="cb17-130"><a href="#cb17-130" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-131"><a href="#cb17-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-132"><a href="#cb17-132" aria-hidden="true" tabindex="-1"></a>Each word is now a 384-dimensional vector. Let's compute similarities:</span>
<span id="cb17-133"><a href="#cb17-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-136"><a href="#cb17-136" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-137"><a href="#cb17-137" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb17-138"><a href="#cb17-138" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> cosine_similarity</span>
<span id="cb17-139"><a href="#cb17-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-140"><a href="#cb17-140" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute similarity matrix</span></span>
<span id="cb17-141"><a href="#cb17-141" aria-hidden="true" tabindex="-1"></a>sim_matrix <span class="op">=</span> cosine_similarity(embeddings)</span>
<span id="cb17-142"><a href="#cb17-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-143"><a href="#cb17-143" aria-hidden="true" tabindex="-1"></a><span class="co"># Display as a heatmap</span></span>
<span id="cb17-144"><a href="#cb17-144" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb17-145"><a href="#cb17-145" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb17-146"><a href="#cb17-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-147"><a href="#cb17-147" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">"white"</span>)</span>
<span id="cb17-148"><a href="#cb17-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-149"><a href="#cb17-149" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb17-150"><a href="#cb17-150" aria-hidden="true" tabindex="-1"></a>sns.heatmap(sim_matrix, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">".2f"</span>,</span>
<span id="cb17-151"><a href="#cb17-151" aria-hidden="true" tabindex="-1"></a>            xticklabels<span class="op">=</span>words, yticklabels<span class="op">=</span>words,</span>
<span id="cb17-152"><a href="#cb17-152" aria-hidden="true" tabindex="-1"></a>            cmap<span class="op">=</span><span class="st">"RdYlGn"</span>, vmin<span class="op">=</span><span class="dv">0</span>, vmax<span class="op">=</span><span class="dv">1</span>, ax<span class="op">=</span>ax,</span>
<span id="cb17-153"><a href="#cb17-153" aria-hidden="true" tabindex="-1"></a>            cbar_kws<span class="op">=</span>{<span class="st">'label'</span>: <span class="st">'Cosine Similarity'</span>})</span>
<span id="cb17-154"><a href="#cb17-154" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Word Similarity Matrix"</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb17-155"><a href="#cb17-155" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb17-156"><a href="#cb17-156" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb17-157"><a href="#cb17-157" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-158"><a href="#cb17-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-161"><a href="#cb17-161" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-162"><a href="#cb17-162" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb17-163"><a href="#cb17-163" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Semantic similarity between words. Notice how 'network' and 'graph' are highly similar (light), while 'cat' and 'theorem' are dissimilar (dark) to network science terms."</span></span>
<span id="cb17-164"><a href="#cb17-164" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb17-165"><a href="#cb17-165" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb17-166"><a href="#cb17-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-167"><a href="#cb17-167" aria-hidden="true" tabindex="-1"></a><span class="co"># Code would generate the heatmap showing:</span></span>
<span id="cb17-168"><a href="#cb17-168" aria-hidden="true" tabindex="-1"></a><span class="co"># - network &amp; graph: ~0.85 similarity (they mean similar things)</span></span>
<span id="cb17-169"><a href="#cb17-169" aria-hidden="true" tabindex="-1"></a><span class="co"># - community &amp; detection: ~0.70 (both related to network analysis)</span></span>
<span id="cb17-170"><a href="#cb17-170" aria-hidden="true" tabindex="-1"></a><span class="co"># - cat vs. network: ~0.15 (unrelated domains)</span></span>
<span id="cb17-171"><a href="#cb17-171" aria-hidden="true" tabindex="-1"></a><span class="co"># - theorem vs. network: ~0.30 (different but both somewhat technical)</span></span>
<span id="cb17-172"><a href="#cb17-172" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-173"><a href="#cb17-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-174"><a href="#cb17-174" aria-hidden="true" tabindex="-1"></a>**Key observations**:</span>
<span id="cb17-175"><a href="#cb17-175" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"network" and "graph" have high similarity (~0.85) — the model learned they're related!</span>
<span id="cb17-176"><a href="#cb17-176" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"cat" has low similarity to network science terms</span>
<span id="cb17-177"><a href="#cb17-177" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"theorem" is somewhat similar to technical terms but distinct from social/biological concepts</span>
<span id="cb17-178"><a href="#cb17-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-179"><a href="#cb17-179" aria-hidden="true" tabindex="-1"></a>This happens **without anyone explicitly telling the model** that "network" and "graph" are synonyms. The model learned from context.</span>
<span id="cb17-180"><a href="#cb17-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-181"><a href="#cb17-181" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb17-182"><a href="#cb17-182" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Distributional Hypothesis</span></span>
<span id="cb17-183"><a href="#cb17-183" aria-hidden="true" tabindex="-1"></a>"You shall know a word by the company it keeps." — J.R. Firth, 1957</span>
<span id="cb17-184"><a href="#cb17-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-185"><a href="#cb17-185" aria-hidden="true" tabindex="-1"></a>Words that appear in similar contexts tend to have similar meanings. Embeddings operationalize this idea: they place words with similar contexts near each other in vector space.</span>
<span id="cb17-186"><a href="#cb17-186" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb17-187"><a href="#cb17-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-188"><a href="#cb17-188" aria-hidden="true" tabindex="-1"></a><span class="fu">## From Words to Sentences</span></span>
<span id="cb17-189"><a href="#cb17-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-190"><a href="#cb17-190" aria-hidden="true" tabindex="-1"></a>Word embeddings are useful, but research deals with sentences and documents. How do we embed larger chunks of text?</span>
<span id="cb17-191"><a href="#cb17-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-192"><a href="#cb17-192" aria-hidden="true" tabindex="-1"></a><span class="fu">### Naive Approach: Average Word Vectors</span></span>
<span id="cb17-193"><a href="#cb17-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-196"><a href="#cb17-196" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-197"><a href="#cb17-197" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb17-198"><a href="#cb17-198" aria-hidden="true" tabindex="-1"></a>sentence1 <span class="op">=</span> <span class="st">"Community detection in networks"</span></span>
<span id="cb17-199"><a href="#cb17-199" aria-hidden="true" tabindex="-1"></a>sentence2 <span class="op">=</span> <span class="st">"Identifying groups in graphs"</span></span>
<span id="cb17-200"><a href="#cb17-200" aria-hidden="true" tabindex="-1"></a>sentence3 <span class="op">=</span> <span class="st">"Cats like milk"</span></span>
<span id="cb17-201"><a href="#cb17-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-202"><a href="#cb17-202" aria-hidden="true" tabindex="-1"></a><span class="co"># Encode sentences</span></span>
<span id="cb17-203"><a href="#cb17-203" aria-hidden="true" tabindex="-1"></a>sent_embeddings <span class="op">=</span> model.encode([sentence1, sentence2, sentence3])</span>
<span id="cb17-204"><a href="#cb17-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-205"><a href="#cb17-205" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute similarities</span></span>
<span id="cb17-206"><a href="#cb17-206" aria-hidden="true" tabindex="-1"></a>sent_sim <span class="op">=</span> cosine_similarity(sent_embeddings)</span>
<span id="cb17-207"><a href="#cb17-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-208"><a href="#cb17-208" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Sentence similarities:"</span>)</span>
<span id="cb17-209"><a href="#cb17-209" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"'</span><span class="sc">{</span>sentence1<span class="sc">}</span><span class="ss">' vs. '</span><span class="sc">{</span>sentence2<span class="sc">}</span><span class="ss">': </span><span class="sc">{</span>sent_sim[<span class="dv">0</span>, <span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb17-210"><a href="#cb17-210" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"'</span><span class="sc">{</span>sentence1<span class="sc">}</span><span class="ss">' vs. '</span><span class="sc">{</span>sentence3<span class="sc">}</span><span class="ss">': </span><span class="sc">{</span>sent_sim[<span class="dv">0</span>, <span class="dv">2</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb17-211"><a href="#cb17-211" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-212"><a href="#cb17-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-213"><a href="#cb17-213" aria-hidden="true" tabindex="-1"></a>**Output**:</span>
<span id="cb17-214"><a href="#cb17-214" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-215"><a href="#cb17-215" aria-hidden="true" tabindex="-1"></a><span class="in">Sentence similarities:</span></span>
<span id="cb17-216"><a href="#cb17-216" aria-hidden="true" tabindex="-1"></a><span class="in">'Community detection in networks' vs. 'Identifying groups in graphs': 0.834</span></span>
<span id="cb17-217"><a href="#cb17-217" aria-hidden="true" tabindex="-1"></a><span class="in">'Community detection in networks' vs. 'Cats like milk': 0.124</span></span>
<span id="cb17-218"><a href="#cb17-218" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-219"><a href="#cb17-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-220"><a href="#cb17-220" aria-hidden="true" tabindex="-1"></a>The model correctly recognizes that the first two sentences describe similar concepts, while the third is unrelated.</span>
<span id="cb17-221"><a href="#cb17-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-222"><a href="#cb17-222" aria-hidden="true" tabindex="-1"></a>**How does this work?** Modern sentence embedding models (like the one we're using) don't just average word vectors—they use **transformers** to generate context-aware representations. We'll explore how transformers work in the next section. For now, just know: sentence embeddings capture meaning at the sentence level.</span>
<span id="cb17-223"><a href="#cb17-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-224"><a href="#cb17-224" aria-hidden="true" tabindex="-1"></a><span class="fu">## Application 1: Semantic Search</span></span>
<span id="cb17-225"><a href="#cb17-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-226"><a href="#cb17-226" aria-hidden="true" tabindex="-1"></a>Embeddings enable **semantic search**: finding documents by meaning, not just keywords.</span>
<span id="cb17-227"><a href="#cb17-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-228"><a href="#cb17-228" aria-hidden="true" tabindex="-1"></a>Traditional keyword search:</span>
<span id="cb17-229"><a href="#cb17-229" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Query: "community detection"</span>
<span id="cb17-230"><a href="#cb17-230" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Matches: Papers containing exactly those words</span>
<span id="cb17-231"><a href="#cb17-231" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Misses: Papers about "group identification" or "clustering"</span>
<span id="cb17-232"><a href="#cb17-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-233"><a href="#cb17-233" aria-hidden="true" tabindex="-1"></a>Semantic search:</span>
<span id="cb17-234"><a href="#cb17-234" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Query: "community detection"</span>
<span id="cb17-235"><a href="#cb17-235" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Matches: Papers about related concepts even if they use different words</span>
<span id="cb17-236"><a href="#cb17-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-237"><a href="#cb17-237" aria-hidden="true" tabindex="-1"></a>Let's build a simple semantic search engine for research papers.</span>
<span id="cb17-238"><a href="#cb17-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-241"><a href="#cb17-241" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-242"><a href="#cb17-242" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb17-243"><a href="#cb17-243" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulated paper titles</span></span>
<span id="cb17-244"><a href="#cb17-244" aria-hidden="true" tabindex="-1"></a>papers <span class="op">=</span> [</span>
<span id="cb17-245"><a href="#cb17-245" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Community Detection in Social Networks Using Modularity Optimization"</span>,</span>
<span id="cb17-246"><a href="#cb17-246" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Graph Clustering Algorithms: A Survey"</span>,</span>
<span id="cb17-247"><a href="#cb17-247" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Identifying Groups in Biological Networks"</span>,</span>
<span id="cb17-248"><a href="#cb17-248" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Deep Learning for Image Classification"</span>,</span>
<span id="cb17-249"><a href="#cb17-249" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Temporal Dynamics of Network Structure"</span>,</span>
<span id="cb17-250"><a href="#cb17-250" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Protein-Protein Interaction Prediction"</span>,</span>
<span id="cb17-251"><a href="#cb17-251" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Hierarchical Structure in Complex Networks"</span></span>
<span id="cb17-252"><a href="#cb17-252" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb17-253"><a href="#cb17-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-254"><a href="#cb17-254" aria-hidden="true" tabindex="-1"></a><span class="co"># Embed all papers</span></span>
<span id="cb17-255"><a href="#cb17-255" aria-hidden="true" tabindex="-1"></a>paper_embeddings <span class="op">=</span> model.encode(papers)</span>
<span id="cb17-256"><a href="#cb17-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-257"><a href="#cb17-257" aria-hidden="true" tabindex="-1"></a><span class="co"># User query</span></span>
<span id="cb17-258"><a href="#cb17-258" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> <span class="st">"finding groups in networks"</span></span>
<span id="cb17-259"><a href="#cb17-259" aria-hidden="true" tabindex="-1"></a>query_embedding <span class="op">=</span> model.encode([query])</span>
<span id="cb17-260"><a href="#cb17-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-261"><a href="#cb17-261" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute similarities</span></span>
<span id="cb17-262"><a href="#cb17-262" aria-hidden="true" tabindex="-1"></a>similarities <span class="op">=</span> cosine_similarity(query_embedding, paper_embeddings)[<span class="dv">0</span>]</span>
<span id="cb17-263"><a href="#cb17-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-264"><a href="#cb17-264" aria-hidden="true" tabindex="-1"></a><span class="co"># Rank papers</span></span>
<span id="cb17-265"><a href="#cb17-265" aria-hidden="true" tabindex="-1"></a>ranked_indices <span class="op">=</span> np.argsort(similarities)[::<span class="op">-</span><span class="dv">1</span>]  <span class="co"># Descending order</span></span>
<span id="cb17-266"><a href="#cb17-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-267"><a href="#cb17-267" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Query: '</span><span class="sc">{</span>query<span class="sc">}</span><span class="ss">'</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb17-268"><a href="#cb17-268" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Top 3 most relevant papers:"</span>)</span>
<span id="cb17-269"><a href="#cb17-269" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, idx <span class="kw">in</span> <span class="bu">enumerate</span>(ranked_indices[:<span class="dv">3</span>], <span class="dv">1</span>):</span>
<span id="cb17-270"><a href="#cb17-270" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">. [</span><span class="sc">{</span>similarities[idx]<span class="sc">:.3f}</span><span class="ss">] </span><span class="sc">{</span>papers[idx]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-271"><a href="#cb17-271" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-272"><a href="#cb17-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-273"><a href="#cb17-273" aria-hidden="true" tabindex="-1"></a>**Output**:</span>
<span id="cb17-274"><a href="#cb17-274" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-275"><a href="#cb17-275" aria-hidden="true" tabindex="-1"></a><span class="in">Query: 'finding groups in networks'</span></span>
<span id="cb17-276"><a href="#cb17-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-277"><a href="#cb17-277" aria-hidden="true" tabindex="-1"></a><span class="in">Top 3 most relevant papers:</span></span>
<span id="cb17-278"><a href="#cb17-278" aria-hidden="true" tabindex="-1"></a><span class="in">1. [0.812] Community Detection in Social Networks Using Modularity Optimization</span></span>
<span id="cb17-279"><a href="#cb17-279" aria-hidden="true" tabindex="-1"></a><span class="in">2. [0.789] Identifying Groups in Biological Networks</span></span>
<span id="cb17-280"><a href="#cb17-280" aria-hidden="true" tabindex="-1"></a><span class="in">3. [0.754] Graph Clustering Algorithms: A Survey</span></span>
<span id="cb17-281"><a href="#cb17-281" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-282"><a href="#cb17-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-283"><a href="#cb17-283" aria-hidden="true" tabindex="-1"></a>Even though the query doesn't exactly match any title, semantic search finds the most relevant papers. Paper 4 ("Deep Learning for Image Classification") would have low similarity and rank last.</span>
<span id="cb17-284"><a href="#cb17-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-285"><a href="#cb17-285" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb17-286"><a href="#cb17-286" aria-hidden="true" tabindex="-1"></a><span class="fu">## Building Your Own Semantic Search</span></span>
<span id="cb17-287"><a href="#cb17-287" aria-hidden="true" tabindex="-1"></a>You can build a semantic search system for your literature:</span>
<span id="cb17-288"><a href="#cb17-288" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Collect papers (titles + abstracts)</span>
<span id="cb17-289"><a href="#cb17-289" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Generate embeddings with <span class="in">`sentence-transformers`</span></span>
<span id="cb17-290"><a href="#cb17-290" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Store embeddings (just numpy arrays)</span>
<span id="cb17-291"><a href="#cb17-291" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>For each query, compute cosine similarity</span>
<span id="cb17-292"><a href="#cb17-292" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Return top-K most similar papers</span>
<span id="cb17-293"><a href="#cb17-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-294"><a href="#cb17-294" aria-hidden="true" tabindex="-1"></a>This works well up to ~100K papers on a laptop.</span>
<span id="cb17-295"><a href="#cb17-295" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb17-296"><a href="#cb17-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-297"><a href="#cb17-297" aria-hidden="true" tabindex="-1"></a><span class="fu">## Application 2: Document Clustering</span></span>
<span id="cb17-298"><a href="#cb17-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-299"><a href="#cb17-299" aria-hidden="true" tabindex="-1"></a>Embeddings naturally group similar documents. Let's cluster research papers by topic.</span>
<span id="cb17-300"><a href="#cb17-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-303"><a href="#cb17-303" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-304"><a href="#cb17-304" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb17-305"><a href="#cb17-305" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Research papers clustered by topic using embeddings. Each point is a paper; similar papers cluster together. The model discovers topics without supervision."</span></span>
<span id="cb17-306"><a href="#cb17-306" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 10</span></span>
<span id="cb17-307"><a href="#cb17-307" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 7</span></span>
<span id="cb17-308"><a href="#cb17-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-309"><a href="#cb17-309" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb17-310"><a href="#cb17-310" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE</span>
<span id="cb17-311"><a href="#cb17-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-312"><a href="#cb17-312" aria-hidden="true" tabindex="-1"></a><span class="co"># More papers (simulated for illustration)</span></span>
<span id="cb17-313"><a href="#cb17-313" aria-hidden="true" tabindex="-1"></a>papers_extended <span class="op">=</span> [</span>
<span id="cb17-314"><a href="#cb17-314" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Cluster 1: Community detection</span></span>
<span id="cb17-315"><a href="#cb17-315" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Community detection using modularity"</span>,</span>
<span id="cb17-316"><a href="#cb17-316" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Overlapping community structure"</span>,</span>
<span id="cb17-317"><a href="#cb17-317" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Hierarchical community detection"</span>,</span>
<span id="cb17-318"><a href="#cb17-318" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Cluster 2: Network dynamics</span></span>
<span id="cb17-319"><a href="#cb17-319" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Temporal networks and time-varying graphs"</span>,</span>
<span id="cb17-320"><a href="#cb17-320" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Evolution of network structure"</span>,</span>
<span id="cb17-321"><a href="#cb17-321" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Dynamic processes on networks"</span>,</span>
<span id="cb17-322"><a href="#cb17-322" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Cluster 3: Machine learning on graphs</span></span>
<span id="cb17-323"><a href="#cb17-323" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Graph neural networks for node classification"</span>,</span>
<span id="cb17-324"><a href="#cb17-324" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Deep learning on graphs"</span>,</span>
<span id="cb17-325"><a href="#cb17-325" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Representation learning on networks"</span>,</span>
<span id="cb17-326"><a href="#cb17-326" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Cluster 4: Biological networks</span></span>
<span id="cb17-327"><a href="#cb17-327" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Protein interaction networks"</span>,</span>
<span id="cb17-328"><a href="#cb17-328" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Gene regulatory networks"</span>,</span>
<span id="cb17-329"><a href="#cb17-329" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Network medicine and disease modules"</span>,</span>
<span id="cb17-330"><a href="#cb17-330" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb17-331"><a href="#cb17-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-332"><a href="#cb17-332" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate embeddings</span></span>
<span id="cb17-333"><a href="#cb17-333" aria-hidden="true" tabindex="-1"></a>paper_embs <span class="op">=</span> model.encode(papers_extended)</span>
<span id="cb17-334"><a href="#cb17-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-335"><a href="#cb17-335" aria-hidden="true" tabindex="-1"></a><span class="co"># Cluster using K-means</span></span>
<span id="cb17-336"><a href="#cb17-336" aria-hidden="true" tabindex="-1"></a>n_clusters <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb17-337"><a href="#cb17-337" aria-hidden="true" tabindex="-1"></a>kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>n_clusters, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb17-338"><a href="#cb17-338" aria-hidden="true" tabindex="-1"></a>clusters <span class="op">=</span> kmeans.fit_predict(paper_embs)</span>
<span id="cb17-339"><a href="#cb17-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-340"><a href="#cb17-340" aria-hidden="true" tabindex="-1"></a><span class="co"># Reduce to 2D for visualization</span></span>
<span id="cb17-341"><a href="#cb17-341" aria-hidden="true" tabindex="-1"></a>tsne <span class="op">=</span> TSNE(n_components<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">42</span>, perplexity<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb17-342"><a href="#cb17-342" aria-hidden="true" tabindex="-1"></a>paper_2d <span class="op">=</span> tsne.fit_transform(paper_embs)</span>
<span id="cb17-343"><a href="#cb17-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-344"><a href="#cb17-344" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb17-345"><a href="#cb17-345" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">7</span>))</span>
<span id="cb17-346"><a href="#cb17-346" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'#e74c3c'</span>, <span class="st">'#3498db'</span>, <span class="st">'#2ecc71'</span>, <span class="st">'#f39c12'</span>]</span>
<span id="cb17-347"><a href="#cb17-347" aria-hidden="true" tabindex="-1"></a>cluster_names <span class="op">=</span> [<span class="st">'Community</span><span class="ch">\n</span><span class="st">Detection'</span>, <span class="st">'Network</span><span class="ch">\n</span><span class="st">Dynamics'</span>,</span>
<span id="cb17-348"><a href="#cb17-348" aria-hidden="true" tabindex="-1"></a>                <span class="st">'ML on Graphs'</span>, <span class="st">'Biological</span><span class="ch">\n</span><span class="st">Networks'</span>]</span>
<span id="cb17-349"><a href="#cb17-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-350"><a href="#cb17-350" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_clusters):</span>
<span id="cb17-351"><a href="#cb17-351" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> clusters <span class="op">==</span> i</span>
<span id="cb17-352"><a href="#cb17-352" aria-hidden="true" tabindex="-1"></a>    ax.scatter(paper_2d[mask, <span class="dv">0</span>], paper_2d[mask, <span class="dv">1</span>],</span>
<span id="cb17-353"><a href="#cb17-353" aria-hidden="true" tabindex="-1"></a>              c<span class="op">=</span>colors[i], label<span class="op">=</span>cluster_names[i],</span>
<span id="cb17-354"><a href="#cb17-354" aria-hidden="true" tabindex="-1"></a>              s<span class="op">=</span><span class="dv">200</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, edgecolors<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>)</span>
<span id="cb17-355"><a href="#cb17-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-356"><a href="#cb17-356" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"t-SNE Dimension 1"</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb17-357"><a href="#cb17-357" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"t-SNE Dimension 2"</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb17-358"><a href="#cb17-358" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Automatic Clustering of Research Papers"</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb17-359"><a href="#cb17-359" aria-hidden="true" tabindex="-1"></a>ax.legend(loc<span class="op">=</span><span class="st">'best'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb17-360"><a href="#cb17-360" aria-hidden="true" tabindex="-1"></a>ax.grid(alpha<span class="op">=</span><span class="fl">0.3</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb17-361"><a href="#cb17-361" aria-hidden="true" tabindex="-1"></a>sns.despine()</span>
<span id="cb17-362"><a href="#cb17-362" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb17-363"><a href="#cb17-363" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb17-364"><a href="#cb17-364" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-365"><a href="#cb17-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-366"><a href="#cb17-366" aria-hidden="true" tabindex="-1"></a>**Key insight**: We never told the model what "community detection" or "biological networks" means. It learned these concepts from patterns in text and automatically grouped related papers.</span>
<span id="cb17-367"><a href="#cb17-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-368"><a href="#cb17-368" aria-hidden="true" tabindex="-1"></a><span class="fu">## Application 3: Finding Similar Papers</span></span>
<span id="cb17-369"><a href="#cb17-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-370"><a href="#cb17-370" aria-hidden="true" tabindex="-1"></a>Given a paper you like, find others that are similar.</span>
<span id="cb17-371"><a href="#cb17-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-374"><a href="#cb17-374" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-375"><a href="#cb17-375" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb17-376"><a href="#cb17-376" aria-hidden="true" tabindex="-1"></a><span class="co"># You read and liked this paper</span></span>
<span id="cb17-377"><a href="#cb17-377" aria-hidden="true" tabindex="-1"></a>seed_paper <span class="op">=</span> <span class="st">"We develop a graph neural network for predicting protein functions."</span></span>
<span id="cb17-378"><a href="#cb17-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-379"><a href="#cb17-379" aria-hidden="true" tabindex="-1"></a><span class="co"># Database of papers</span></span>
<span id="cb17-380"><a href="#cb17-380" aria-hidden="true" tabindex="-1"></a>database <span class="op">=</span> [</span>
<span id="cb17-381"><a href="#cb17-381" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Deep learning for protein structure prediction"</span>,</span>
<span id="cb17-382"><a href="#cb17-382" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Community detection in social networks"</span>,</span>
<span id="cb17-383"><a href="#cb17-383" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Node classification using graph convolutions"</span>,</span>
<span id="cb17-384"><a href="#cb17-384" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Temporal dynamics in citation networks"</span>,</span>
<span id="cb17-385"><a href="#cb17-385" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Representation learning for biological networks"</span>,</span>
<span id="cb17-386"><a href="#cb17-386" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Image classification with CNNs"</span>,</span>
<span id="cb17-387"><a href="#cb17-387" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb17-388"><a href="#cb17-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-389"><a href="#cb17-389" aria-hidden="true" tabindex="-1"></a><span class="co"># Embed everything</span></span>
<span id="cb17-390"><a href="#cb17-390" aria-hidden="true" tabindex="-1"></a>seed_emb <span class="op">=</span> model.encode([seed_paper])</span>
<span id="cb17-391"><a href="#cb17-391" aria-hidden="true" tabindex="-1"></a>db_embs <span class="op">=</span> model.encode(database)</span>
<span id="cb17-392"><a href="#cb17-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-393"><a href="#cb17-393" aria-hidden="true" tabindex="-1"></a><span class="co"># Find most similar</span></span>
<span id="cb17-394"><a href="#cb17-394" aria-hidden="true" tabindex="-1"></a>sims <span class="op">=</span> cosine_similarity(seed_emb, db_embs)[<span class="dv">0</span>]</span>
<span id="cb17-395"><a href="#cb17-395" aria-hidden="true" tabindex="-1"></a>sorted_indices <span class="op">=</span> np.argsort(sims)[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb17-396"><a href="#cb17-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-397"><a href="#cb17-397" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Papers similar to:</span><span class="ch">\n</span><span class="ss">'</span><span class="sc">{</span>seed_paper<span class="sc">}</span><span class="ss">'</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb17-398"><a href="#cb17-398" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, idx <span class="kw">in</span> <span class="bu">enumerate</span>(sorted_indices[:<span class="dv">3</span>], <span class="dv">1</span>):</span>
<span id="cb17-399"><a href="#cb17-399" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">. [</span><span class="sc">{</span>sims[idx]<span class="sc">:.3f}</span><span class="ss">] </span><span class="sc">{</span>database[idx]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-400"><a href="#cb17-400" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-401"><a href="#cb17-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-402"><a href="#cb17-402" aria-hidden="true" tabindex="-1"></a>**Output**:</span>
<span id="cb17-403"><a href="#cb17-403" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-404"><a href="#cb17-404" aria-hidden="true" tabindex="-1"></a><span class="in">Papers similar to:</span></span>
<span id="cb17-405"><a href="#cb17-405" aria-hidden="true" tabindex="-1"></a><span class="in">'We develop a graph neural network for predicting protein functions.'</span></span>
<span id="cb17-406"><a href="#cb17-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-407"><a href="#cb17-407" aria-hidden="true" tabindex="-1"></a><span class="in">1. [0.812] Representation learning for biological networks</span></span>
<span id="cb17-408"><a href="#cb17-408" aria-hidden="true" tabindex="-1"></a><span class="in">2. [0.789] Deep learning for protein structure prediction</span></span>
<span id="cb17-409"><a href="#cb17-409" aria-hidden="true" tabindex="-1"></a><span class="in">3. [0.754] Node classification using graph convolutions</span></span>
<span id="cb17-410"><a href="#cb17-410" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-411"><a href="#cb17-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-412"><a href="#cb17-412" aria-hidden="true" tabindex="-1"></a>This is how recommendation systems work: embed items, find nearest neighbors.</span>
<span id="cb17-413"><a href="#cb17-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-414"><a href="#cb17-414" aria-hidden="true" tabindex="-1"></a><span class="fu">## Visualizing the Embedding Space</span></span>
<span id="cb17-415"><a href="#cb17-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-416"><a href="#cb17-416" aria-hidden="true" tabindex="-1"></a>Let's visualize what's happening in this high-dimensional space.</span>
<span id="cb17-417"><a href="#cb17-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-420"><a href="#cb17-420" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-421"><a href="#cb17-421" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb17-422"><a href="#cb17-422" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Semantic space of research concepts. Related concepts cluster together. Distance encodes semantic similarity—concepts far apart are conceptually different."</span></span>
<span id="cb17-423"><a href="#cb17-423" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 12</span></span>
<span id="cb17-424"><a href="#cb17-424" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 8</span></span>
<span id="cb17-425"><a href="#cb17-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-426"><a href="#cb17-426" aria-hidden="true" tabindex="-1"></a><span class="co"># A diverse set of research terms</span></span>
<span id="cb17-427"><a href="#cb17-427" aria-hidden="true" tabindex="-1"></a>terms <span class="op">=</span> [</span>
<span id="cb17-428"><a href="#cb17-428" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Network science</span></span>
<span id="cb17-429"><a href="#cb17-429" aria-hidden="true" tabindex="-1"></a>    <span class="st">"network"</span>, <span class="st">"graph"</span>, <span class="st">"community"</span>, <span class="st">"centrality"</span>, <span class="st">"clustering"</span>,</span>
<span id="cb17-430"><a href="#cb17-430" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Machine learning</span></span>
<span id="cb17-431"><a href="#cb17-431" aria-hidden="true" tabindex="-1"></a>    <span class="st">"neural network"</span>, <span class="st">"deep learning"</span>, <span class="st">"classification"</span>, <span class="st">"regression"</span>,</span>
<span id="cb17-432"><a href="#cb17-432" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Biology</span></span>
<span id="cb17-433"><a href="#cb17-433" aria-hidden="true" tabindex="-1"></a>    <span class="st">"protein"</span>, <span class="st">"gene"</span>, <span class="st">"cell"</span>, <span class="st">"DNA"</span>, <span class="st">"evolution"</span>,</span>
<span id="cb17-434"><a href="#cb17-434" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Physics</span></span>
<span id="cb17-435"><a href="#cb17-435" aria-hidden="true" tabindex="-1"></a>    <span class="st">"quantum"</span>, <span class="st">"particle"</span>, <span class="st">"entropy"</span>, <span class="st">"thermodynamics"</span>,</span>
<span id="cb17-436"><a href="#cb17-436" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mathematics</span></span>
<span id="cb17-437"><a href="#cb17-437" aria-hidden="true" tabindex="-1"></a>    <span class="st">"theorem"</span>, <span class="st">"proof"</span>, <span class="st">"equation"</span>, <span class="st">"matrix"</span>, <span class="st">"vector"</span>,</span>
<span id="cb17-438"><a href="#cb17-438" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb17-439"><a href="#cb17-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-440"><a href="#cb17-440" aria-hidden="true" tabindex="-1"></a>term_embs <span class="op">=</span> model.encode(terms)</span>
<span id="cb17-441"><a href="#cb17-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-442"><a href="#cb17-442" aria-hidden="true" tabindex="-1"></a><span class="co"># Reduce to 2D</span></span>
<span id="cb17-443"><a href="#cb17-443" aria-hidden="true" tabindex="-1"></a>tsne <span class="op">=</span> TSNE(n_components<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">42</span>, perplexity<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb17-444"><a href="#cb17-444" aria-hidden="true" tabindex="-1"></a>term_2d <span class="op">=</span> tsne.fit_transform(term_embs)</span>
<span id="cb17-445"><a href="#cb17-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-446"><a href="#cb17-446" aria-hidden="true" tabindex="-1"></a><span class="co"># Color by rough category (for illustration)</span></span>
<span id="cb17-447"><a href="#cb17-447" aria-hidden="true" tabindex="-1"></a>categories <span class="op">=</span> {</span>
<span id="cb17-448"><a href="#cb17-448" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Network Science'</span>: [<span class="st">'network'</span>, <span class="st">'graph'</span>, <span class="st">'community'</span>, <span class="st">'centrality'</span>, <span class="st">'clustering'</span>],</span>
<span id="cb17-449"><a href="#cb17-449" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Machine Learning'</span>: [<span class="st">'neural network'</span>, <span class="st">'deep learning'</span>, <span class="st">'classification'</span>, <span class="st">'regression'</span>],</span>
<span id="cb17-450"><a href="#cb17-450" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Biology'</span>: [<span class="st">'protein'</span>, <span class="st">'gene'</span>, <span class="st">'cell'</span>, <span class="st">'DNA'</span>, <span class="st">'evolution'</span>],</span>
<span id="cb17-451"><a href="#cb17-451" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Physics'</span>: [<span class="st">'quantum'</span>, <span class="st">'particle'</span>, <span class="st">'entropy'</span>, <span class="st">'thermodynamics'</span>],</span>
<span id="cb17-452"><a href="#cb17-452" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Mathematics'</span>: [<span class="st">'theorem'</span>, <span class="st">'proof'</span>, <span class="st">'equation'</span>, <span class="st">'matrix'</span>, <span class="st">'vector'</span>],</span>
<span id="cb17-453"><a href="#cb17-453" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb17-454"><a href="#cb17-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-455"><a href="#cb17-455" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb17-456"><a href="#cb17-456" aria-hidden="true" tabindex="-1"></a>colors_map <span class="op">=</span> {<span class="st">'Network Science'</span>: <span class="st">'#e74c3c'</span>, <span class="st">'Machine Learning'</span>: <span class="st">'#3498db'</span>,</span>
<span id="cb17-457"><a href="#cb17-457" aria-hidden="true" tabindex="-1"></a>              <span class="st">'Biology'</span>: <span class="st">'#2ecc71'</span>, <span class="st">'Physics'</span>: <span class="st">'#f39c12'</span>, <span class="st">'Mathematics'</span>: <span class="st">'#9b59b6'</span>}</span>
<span id="cb17-458"><a href="#cb17-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-459"><a href="#cb17-459" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> category, words <span class="kw">in</span> categories.items():</span>
<span id="cb17-460"><a href="#cb17-460" aria-hidden="true" tabindex="-1"></a>    indices <span class="op">=</span> [terms.index(w) <span class="cf">for</span> w <span class="kw">in</span> words]</span>
<span id="cb17-461"><a href="#cb17-461" aria-hidden="true" tabindex="-1"></a>    ax.scatter(term_2d[indices, <span class="dv">0</span>], term_2d[indices, <span class="dv">1</span>],</span>
<span id="cb17-462"><a href="#cb17-462" aria-hidden="true" tabindex="-1"></a>              c<span class="op">=</span>colors_map[category], label<span class="op">=</span>category, s<span class="op">=</span><span class="dv">300</span>, alpha<span class="op">=</span><span class="fl">0.7</span>,</span>
<span id="cb17-463"><a href="#cb17-463" aria-hidden="true" tabindex="-1"></a>              edgecolors<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb17-464"><a href="#cb17-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-465"><a href="#cb17-465" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Annotate terms</span></span>
<span id="cb17-466"><a href="#cb17-466" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx <span class="kw">in</span> indices:</span>
<span id="cb17-467"><a href="#cb17-467" aria-hidden="true" tabindex="-1"></a>        ax.annotate(terms[idx], (term_2d[idx, <span class="dv">0</span>], term_2d[idx, <span class="dv">1</span>]),</span>
<span id="cb17-468"><a href="#cb17-468" aria-hidden="true" tabindex="-1"></a>                   fontsize<span class="op">=</span><span class="dv">10</span>, ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb17-469"><a href="#cb17-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-470"><a href="#cb17-470" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Semantic Dimension 1"</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb17-471"><a href="#cb17-471" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Semantic Dimension 2"</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb17-472"><a href="#cb17-472" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"The Semantic Space: How Concepts Relate"</span>, fontsize<span class="op">=</span><span class="dv">15</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb17-473"><a href="#cb17-473" aria-hidden="true" tabindex="-1"></a>ax.legend(loc<span class="op">=</span><span class="st">'best'</span>, fontsize<span class="op">=</span><span class="dv">11</span>, frameon<span class="op">=</span><span class="va">True</span>, shadow<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-474"><a href="#cb17-474" aria-hidden="true" tabindex="-1"></a>ax.grid(alpha<span class="op">=</span><span class="fl">0.3</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb17-475"><a href="#cb17-475" aria-hidden="true" tabindex="-1"></a>sns.despine()</span>
<span id="cb17-476"><a href="#cb17-476" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb17-477"><a href="#cb17-477" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb17-478"><a href="#cb17-478" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-479"><a href="#cb17-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-480"><a href="#cb17-480" aria-hidden="true" tabindex="-1"></a>Notice how:</span>
<span id="cb17-481"><a href="#cb17-481" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Clusters form naturally**: Biology terms group together, math terms group together</span>
<span id="cb17-482"><a href="#cb17-482" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Cross-domain connections**: "matrix" (math) might be closer to "network" (network science) than to "theorem" (pure math)</span>
<span id="cb17-483"><a href="#cb17-483" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Embedding space has structure**: It's not random—semantic relationships are preserved</span>
<span id="cb17-484"><a href="#cb17-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-485"><a href="#cb17-485" aria-hidden="true" tabindex="-1"></a><span class="fu">## How Embeddings Are Learned</span></span>
<span id="cb17-486"><a href="#cb17-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-487"><a href="#cb17-487" aria-hidden="true" tabindex="-1"></a>You don't need to train embeddings from scratch (it requires huge data and compute). But understanding how they're learned helps you use them effectively.</span>
<span id="cb17-488"><a href="#cb17-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-489"><a href="#cb17-489" aria-hidden="true" tabindex="-1"></a>**Training objective**: Predict context from words (or vice versa).</span>
<span id="cb17-490"><a href="#cb17-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-491"><a href="#cb17-491" aria-hidden="true" tabindex="-1"></a>Example: Given "The **cat** sat on the mat", predict "cat" from context <span class="co">[</span><span class="ot">"the", "sat", "on", "the", "mat"</span><span class="co">]</span>.</span>
<span id="cb17-492"><a href="#cb17-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-493"><a href="#cb17-493" aria-hidden="true" tabindex="-1"></a>The model adjusts embeddings so that:</span>
<span id="cb17-494"><a href="#cb17-494" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Words appearing in similar contexts get similar embeddings</span>
<span id="cb17-495"><a href="#cb17-495" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Context → word predictions become accurate</span>
<span id="cb17-496"><a href="#cb17-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-497"><a href="#cb17-497" aria-hidden="true" tabindex="-1"></a>After training on billions of sentences, the embeddings encode semantic and syntactic relationships.</span>
<span id="cb17-498"><a href="#cb17-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-499"><a href="#cb17-499" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb17-500"><a href="#cb17-500" aria-hidden="true" tabindex="-1"></a><span class="fu">## Pre-trained Models</span></span>
<span id="cb17-501"><a href="#cb17-501" aria-hidden="true" tabindex="-1"></a>Models like <span class="in">`all-MiniLM-L6-v2`</span> are pre-trained on huge text corpora (web pages, books, Wikipedia). They've already learned general semantic relationships. You can use them immediately for most tasks.</span>
<span id="cb17-502"><a href="#cb17-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-503"><a href="#cb17-503" aria-hidden="true" tabindex="-1"></a>For specialized domains (e.g., medical research), you might fine-tune on domain-specific text—but pre-trained models work surprisingly well out-of-the-box.</span>
<span id="cb17-504"><a href="#cb17-504" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb17-505"><a href="#cb17-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-506"><a href="#cb17-506" aria-hidden="true" tabindex="-1"></a><span class="fu">## Static vs. Contextual Embeddings</span></span>
<span id="cb17-507"><a href="#cb17-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-508"><a href="#cb17-508" aria-hidden="true" tabindex="-1"></a>There are two types of embeddings:</span>
<span id="cb17-509"><a href="#cb17-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-510"><a href="#cb17-510" aria-hidden="true" tabindex="-1"></a>**Static embeddings** (Word2vec, GloVe):</span>
<span id="cb17-511"><a href="#cb17-511" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Each word has one fixed embedding</span>
<span id="cb17-512"><a href="#cb17-512" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"bank" always has the same vector, whether it's a financial institution or a river bank</span>
<span id="cb17-513"><a href="#cb17-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-514"><a href="#cb17-514" aria-hidden="true" tabindex="-1"></a>**Contextual embeddings** (BERT, GPT, sentence-transformers):</span>
<span id="cb17-515"><a href="#cb17-515" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Embeddings depend on context</span>
<span id="cb17-516"><a href="#cb17-516" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"bank" in "I went to the bank" vs. "river bank" gets different embeddings</span>
<span id="cb17-517"><a href="#cb17-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-518"><a href="#cb17-518" aria-hidden="true" tabindex="-1"></a>The model we've been using (<span class="in">`all-MiniLM-L6-v2`</span>) produces **contextual** embeddings using transformers. We'll explore how transformers enable this in the next section.</span>
<span id="cb17-519"><a href="#cb17-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-520"><a href="#cb17-520" aria-hidden="true" tabindex="-1"></a><span class="fu">## Limitations of Embeddings</span></span>
<span id="cb17-521"><a href="#cb17-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-522"><a href="#cb17-522" aria-hidden="true" tabindex="-1"></a>Embeddings are powerful but imperfect:</span>
<span id="cb17-523"><a href="#cb17-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-524"><a href="#cb17-524" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Bias**: Embeddings learn from text data, which contains human biases. If training data associates "doctor" with "male" and "nurse" with "female", embeddings will encode this bias.</span>
<span id="cb17-525"><a href="#cb17-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-526"><a href="#cb17-526" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Out-of-vocabulary words**: Unknown words can't be embedded (though modern models use subword tokenization to partially address this).</span>
<span id="cb17-527"><a href="#cb17-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-528"><a href="#cb17-528" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Polysemy**: Even contextual embeddings can struggle with highly ambiguous words.</span>
<span id="cb17-529"><a href="#cb17-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-530"><a href="#cb17-530" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Cultural specificity**: Embeddings reflect the culture and language of the training data.</span>
<span id="cb17-531"><a href="#cb17-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-532"><a href="#cb17-532" aria-hidden="true" tabindex="-1"></a>We'll explore bias in embeddings later when we discuss semantic axes.</span>
<span id="cb17-533"><a href="#cb17-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-534"><a href="#cb17-534" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Bigger Picture</span></span>
<span id="cb17-535"><a href="#cb17-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-536"><a href="#cb17-536" aria-hidden="true" tabindex="-1"></a>You now understand **how LLMs see text**: as points in a high-dimensional semantic space. When you use an LLM:</span>
<span id="cb17-537"><a href="#cb17-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-538"><a href="#cb17-538" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Your prompt is converted to embeddings</span>
<span id="cb17-539"><a href="#cb17-539" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>The model manipulates these embeddings through layers of computation</span>
<span id="cb17-540"><a href="#cb17-540" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>The output embeddings are converted back to text</span>
<span id="cb17-541"><a href="#cb17-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-542"><a href="#cb17-542" aria-hidden="true" tabindex="-1"></a>Embeddings are the "language" LLMs speak internally. Everything else—attention, transformers, generation—operates on these numerical representations.</span>
<span id="cb17-543"><a href="#cb17-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-544"><a href="#cb17-544" aria-hidden="true" tabindex="-1"></a>**But wait—there's a step we've skipped.** Before text becomes embeddings, it must first become **tokens**. How does "Community detection" become a sequence of numbers? Why do some words get split into pieces? Let's unbox an actual LLM and see exactly how it reads text.</span>
<span id="cb17-545"><a href="#cb17-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-546"><a href="#cb17-546" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb17-547"><a href="#cb17-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-548"><a href="#cb17-548" aria-hidden="true" tabindex="-1"></a>**Next**: <span class="co">[</span><span class="ot">Tokenization: Unboxing How LLMs Read Text →</span><span class="co">](tokenization.qmd)</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2025, Sadamori Kojaku</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions"><ul><li><a href="https://github.com/skojaku/applied-soft-comp/edit/main/m04-text/archive/embeddings-concepts.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/skojaku/applied-soft-comp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/skojaku/applied-soft-comp">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Sadamori Kojaku">
<meta name="dcterms.date" content="2025-10-19">

<title>Applied Soft Computing: Modeling Complex Systems with Deep Learning – Applied Soft Computing</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-0348920b7671f696dc9078d39bff215e.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-b8a9bfad7ca18893f6a3351e12995b20.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "|"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../assets/custom.css">
</head>

<body class="nav-sidebar docked nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../logo.jpg" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Applied Soft Computing</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../../index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-toolkit--workflow" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Toolkit &amp; Workflow</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-toolkit--workflow">    
        <li class="dropdown-header">─── Module 1 ───</li>
        <li>
    <a class="dropdown-item" href="../../m01-toolkit/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m01-toolkit/git-github.html">
 <span class="dropdown-text">Git &amp; GitHub</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m01-toolkit/tidy-data.html">
 <span class="dropdown-text">Tidy Data</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m01-toolkit/data-provenance.html">
 <span class="dropdown-text">Data Provenance</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m01-toolkit/environments.qmd">
 <span class="dropdown-text">Environments</span></a>
  </li>  
        <li class="dropdown-header">─── Module 3: Agentic Coding ───</li>
        <li>
    <a class="dropdown-item" href="../../m03-agentic-coding/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m03-agentic-coding/hands-on.html">
 <span class="dropdown-text">Hands-on</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-visualization" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Visualization</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-visualization">    
        <li class="dropdown-header">─── Module 2 ───</li>
        <li>
    <a class="dropdown-item" href="../../m02-visualization/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m02-visualization/principles.html">
 <span class="dropdown-text">Principles</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m02-visualization/dimensionality-reduction.html">
 <span class="dropdown-text">High-Dimensional Data</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m02-visualization/networks.html">
 <span class="dropdown-text">Networks</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m02-visualization/time-series.html">
 <span class="dropdown-text">Time-Series</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-deep-learning" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Deep Learning</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-deep-learning">    
        <li class="dropdown-header">─── Module 4: Text ───</li>
        <li>
    <a class="dropdown-item" href="../../m04-text/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m04-text/word2vec.md">
 <span class="dropdown-text">Word2Vec</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m04-text/lstm.md">
 <span class="dropdown-text">RNNs &amp; LSTMs</span></a>
  </li>  
        <li class="dropdown-header">─── Module 4: Images ───</li>
        <li>
    <a class="dropdown-item" href="../../m04-images/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m04-images/cnn.md">
 <span class="dropdown-text">CNNs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m04-images/resnet.md">
 <span class="dropdown-text">ResNet</span></a>
  </li>  
        <li class="dropdown-header">─── Module 5: Graphs ───</li>
        <li>
    <a class="dropdown-item" href="../../m05-graphs/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m05-graphs/graph-embedding-w-word2vec.html">
 <span class="dropdown-text">Graph Embeddings</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m05-graphs/graph-convolutional-network.html">
 <span class="dropdown-text">GNNs</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-advanced-topics" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Advanced Topics</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-advanced-topics">    
        <li class="dropdown-header">─── Module 6: LLMs ───</li>
        <li>
    <a class="dropdown-item" href="../../m06-llms/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m06-llms/transformers.md">
 <span class="dropdown-text">Transformers</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m06-llms/scaling-emergence.html">
 <span class="dropdown-text">Scaling &amp; Emergence</span></a>
  </li>  
        <li class="dropdown-header">─── Module 7: Self-Supervised ───</li>
        <li>
    <a class="dropdown-item" href="../../m07-self-supervised/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m07-self-supervised/contrastive-learning.html">
 <span class="dropdown-text">Contrastive Learning</span></a>
  </li>  
        <li class="dropdown-header">─── Module 8: Explainability ───</li>
        <li>
    <a class="dropdown-item" href="../../m08-explainability/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m08-explainability/fairness.html">
 <span class="dropdown-text">Fairness &amp; Ethics</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Applied Soft Computing: Modeling Complex Systems with Deep Learning</li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Course Information</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/welcome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About Us</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/why-applied-soft-computing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Why applied soft computing?</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/discord.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Discord</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/minidora-usage.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Using Minidora</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/how-to-submit-assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">How to submit assignment</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/deliverables.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deliverables</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 1: The Data Scientist’s Toolkit</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m01-toolkit/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m01-toolkit/git-github.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Version Control with Git &amp; GitHub</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m01-toolkit/tidy-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Tidy Data Philosophy</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m01-toolkit/data-provenance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Provenance</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m01-toolkit/reproduceability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Reproducibility</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 2: Visualizing Complexity</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m02-visualization/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m02-visualization/principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Principles of Effective Visualization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m02-visualization/1d-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing 1D Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m02-visualization/2d-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing 2D Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m02-visualization/highd-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing High-Dimensional Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m02-visualization/networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m02-visualization/time-series.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing Time-Series</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 3: Agentic Coding</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m03-agentic-coding/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m03-agentic-coding/hands-on.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Hands-on</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m03-agentic-coding/prompt-tuning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prompt Tuning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m03-agentic-coding/agentic-ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">From ChatBot to Agentic AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m03-agentic-coding/context-engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Context Engineering</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 4: Deep Learning for Text</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-text/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-text/llm-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Large Language Models in Practice</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-text/prompt-engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prompt Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-text/gpt-inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">GPT Inference: Sampling Strategies</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-text/tokenization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Tokenization: Unboxing How LLMs Read Text</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-text/transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Transformers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-text/bert-gpt.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">BERT &amp; GPT</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-text/sentence-transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sentence Transformers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-text/word-embeddings.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Word Embeddings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-text/semaxis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Semaxis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-text/word-bias.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Word Bias</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 4: Deep Learning for Images</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/image-processing.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Processing Fundamentals</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/cnn.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/lenet.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LeNet Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/alexnet.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AlexNet: Deep CNN Revolution</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/vgg.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">VGG Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/inception.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Inception &amp; Multi-Scale Features</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/batch-normalization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Batch Normalization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/resnet.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ResNet &amp; Skip Connections</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 5: Deep Learning for Graphs</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m05-graphs/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m05-graphs/spectral-embedding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Spectral Graph Embedding</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m05-graphs/graph-embedding-w-word2vec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Graph Embeddings with Word2Vec</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m05-graphs/spectral-vs-neural-embedding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Spectral vs.&nbsp;Neural Embeddings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m05-graphs/from-image-to-graph.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">From Images to Graphs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m05-graphs/graph-convolutional-network.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Graph Convolutional Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m05-graphs/popular-gnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Popular GNN Architectures</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m05-graphs/software.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">GNN Software &amp; Tools</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 6: Large Language Models &amp; Emergent Behavior</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m06-llms/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m06-llms/transformers.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Transformer Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m06-llms/bert.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">BERT &amp; Contextual Embeddings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m06-llms/sentence-bert.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sentence-BERT for Semantic Similarity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m06-llms/gpt.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">GPT &amp; Generative Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m06-llms/from-language-model-to-instruction-following.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">From Language Models to Instruction Following</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m06-llms/prompt-tuning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prompt Engineering &amp; In-Context Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m06-llms/scaling-emergence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Scaling Laws &amp; Emergent Abilities</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m06-llms/llms-as-complex-systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LLMs as Complex Systems</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 7: Self-Supervised Learning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m07-self-supervised/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m07-self-supervised/paradigm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Self-Supervised Paradigm</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m07-self-supervised/contrastive-learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Contrastive Learning (SimCLR)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m07-self-supervised/graphs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Self-Supervised Learning for Graphs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m07-self-supervised/time-series.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Self-Supervised Learning for Time-Series</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 8: Explainability &amp; Ethics</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m08-explainability/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m08-explainability/need.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Need for Explainability</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m08-explainability/attention.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Attention Visualization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m08-explainability/lime-shap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LIME &amp; SHAP</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m08-explainability/fairness.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Algorithmic Fairness &amp; Bias</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m08-explainability/causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Causality vs.&nbsp;Correlation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false">
 <span class="menu-text">Legacy Materials</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m03-text/what-to-learn.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Word &amp; Document Embeddings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m03-text/what-to-learn.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Recurrent Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/what-to-learn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Processing (CNNs)</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Applied Soft Computing: Modeling Complex Systems with Deep Learning</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Sadamori Kojaku </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 19, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="teaching-computers-how-to-understand-words" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Teaching computers how to understand words</h1>
<p>Imagine trying to explain the meaning of words to someone who only understands numbers. This is exactly the challenge we face when teaching computers to process text. Just as we need to translate between languages, we need to translate between the world of human language and the world of computer numbers. This translation process has evolved dramatically over time, becoming increasingly sophisticated in its ability to capture the nuances of meaning.</p>
<section id="one-hot-encoding-the-first-step" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="one-hot-encoding-the-first-step"><span class="header-section-number">1.1</span> One-Hot Encoding: The First Step</h2>
<p>The simplest approach to this translation challenge is one-hot encoding, akin to giving each word its own unique light switch in a vast room of switches. When representing a word, we turn on its switch and leave all others off. For example, in a tiny vocabulary of just three words {cat, dog, fish}:</p>
<ul>
<li>‘cat’ becomes [1, 0, 0]</li>
<li>‘dog’ becomes [0, 1, 0]</li>
<li>‘fish’ becomes [0, 0, 1]</li>
</ul>
<p>While simple, this approach has a fundamental flaw: it suggests that all words are equally different from each other. In this representation, ‘cat’ is just as different from ‘dog’ as it is from ‘algorithm’ - something we know isn’t true in real language use.</p>
</section>
<section id="distributional-hypothesis" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="distributional-hypothesis"><span class="header-section-number">1.2</span> Distributional Hypothesis</h2>
<p>What is missing in one-hot encoding is the notion of <em>context</em>. One associates <code>cat</code> with <code>dog</code> because they have similar context, while <code>cat</code> is more different from <code>fish</code> than <code>dog</code> because they are in different contexts. This is the core idea of the <strong>distributional hypothesis</strong>.</p>
<p>In a nutshell, the distributional hypothesis states that: - Words that frequently appear together in text (co-occur) are likely to be semantically related - The meaning of a word can be inferred by examining the distribution of other words around it - Similar words will have similar distributions of surrounding context words</p>
<p>This hypothesis forms the theoretical foundation for many modern word embedding techniques.</p>
<pre class="{note}"><code>The idea that words can be understood by their context is captured by the famous linguistic principle: "You shall know a word by the company it keeps" {footcite}`firth1957synopsis`. This principle suggests that the meaning of a word is not inherent to the word itself, but rather emerges from how it is used alongside other words.</code></pre>
<pre class="{tip}"><code>The ancient Buddhist concept of Apoha, developed by Dignāga in the 5th-6th century CE, shares similarities with modern distributional semantics. According to Apoha theory, we understand concepts by distinguishing what they are not - for example, we know what a "cow" is by recognizing everything that is not a cow. This mirrors how modern word embeddings define words through their relationships and contrasts with other words, showing how both ancient philosophy and contemporary linguistics recognize that meaning emerges from relationships between concepts.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQbWHl7Npbub7nDC9GvfUneConFZbjoHkPHuMPh3PXpGakxdDrv_0JmWt7Fpg63lo_XhhqZqFzOs6YUsVEbPyHBMVexnaqPLWzDQJ-CXAjFCoe7PzNrKlm474QDo14LiqOjrfr1zMt6As/s1600/cnononcow.jpg)
</code></pre>
</section>
<section id="tf-idf-a-simple-but-powerful-word-embedding-technique" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="tf-idf-a-simple-but-powerful-word-embedding-technique"><span class="header-section-number">1.3</span> TF-IDF: A Simple but Powerful Word Embedding Technique</h2>
</section>
<section id="distributional-hypothesis-1" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="distributional-hypothesis-1"><span class="header-section-number">1.4</span> Distributional Hypothesis</h2>
<p>What is missing in one-hot encoding is the notion of <em>context</em>. One associates <code>cat</code> with <code>dog</code> because they have similar context, while <code>cat</code> is more different from <code>fish</code> than <code>dog</code> because they are in different contexts. This is the core idea of the <strong>distributional hypothesis</strong>.</p>
<p>In a nutshell, the distributional hypothesis states that we can understand the meaning of a word by examining the context in which it appears. Just as you might understand a person by the company they keep, we can understand a word by the words that surround it. This principle suggests that words appearing in similar contexts likely have similar meanings.</p>
<pre class="{note}"><code>The idea that words can be understood by their context is captured by the famous linguistic principle: "You shall know a word by the company it keeps" {footcite}`firth1957synopsis`. This principle suggests that the meaning of a word is not inherent to the word itself, but rather emerges from how it is used alongside other words.</code></pre>
</section>
<section id="tf-idf-words-as-patterns-of-usage" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="tf-idf-words-as-patterns-of-usage"><span class="header-section-number">1.5</span> TF-IDF: Words as Patterns of Usage</h2>
<p>The distributional hypothesis leads us to an important question: How can we capture these contextual patterns mathematically? More specifically, what is a good unit of “context”? A natural choice is to let the <em>document</em> be the unit of context. The distributional hypothesis suggests that words that frequently appear in the same documents are likely to be semantically related.</p>
<section id="first-attempt-word-document-count-matrix" class="level3">
<h3 class="anchored" data-anchor-id="first-attempt-word-document-count-matrix">First Attempt: Word-Document Count Matrix</h3>
<p>Let’s try to organize this information systematically. Imagine creating a giant table where: - Each row represents a word - Each column represents a document - Each cell contains the count of how often that word appears in that document</p>
<p>```jldvufbpcno ipython3 from sklearn.feature_extraction.text import CountVectorizer import numpy as np import pandas as pd</p>
</section>
</section>
</section>
<section id="sample-documents" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Sample documents</h1>
<p>documents = [ “The cat chases mice in the garden”, “The dog chases cats in the park”, “Mice eat cheese in the house”, “The cat and dog play in the garden”]</p>
</section>
<section id="create-word-count-matrix" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Create word count matrix</h1>
<p>vectorizer = CountVectorizer() count_matrix = vectorizer.fit_transform(documents) words = vectorizer.get_feature_names_out()</p>
</section>
<section id="display-as-a-table" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Display as a table</h1>
<p>df = pd.DataFrame(count_matrix.toarray(), columns=words) df.style.background_gradient(cmap=‘cividis’, axis = None).set_caption(“Word-Document Count Matrix”)</p>
<pre><code>
```{note}
This matrix is our first attempt at a distributed representation - each word is represented not by a single number, but by its pattern of appearances across all documents.</code></pre>
<section id="the-problem-with-raw-counts" class="level3">
<h3 class="anchored" data-anchor-id="the-problem-with-raw-counts">The Problem with Raw Counts</h3>
<p>Looking at our word-document matrix, something seems off. Words like “the” and “in” appear frequently in almost every document. Are these words really the most important for understanding document meaning? Let’s see what happens if we count how often each word appears across all documents:</p>
<p><code>jldvufbpcno ipython3 word_counts = df.sum(axis=0) word_counts_df = pd.DataFrame(word_counts, columns=["count"]).T word_counts_df.style.background_gradient(cmap='cividis', axis = None).set_caption("Total appearances of each word")</code></p>
<pre class="{note}"><code>This pattern, where a few words appear very frequently while most words appear rarely, is known as Zipf's Law. It's a fundamental property of natural language.
![](https://miro.medium.com/v2/resize:fit:1400/1*GTpckiHyFLe04pUMeYDYOg.png)</code></pre>
<p>We’ve discovered two problems with raw word counts: 1. Common words like “the” and “in” dominate the counts, but they tell us little about document content 2. Raw frequencies don’t tell us how unique or informative a word is across documents</p>
<p>Think about it: if a word appears frequently in one document but rarely in others (like “cheese” in our example), it’s probably more informative about that document’s content than a word that appears equally frequently in all documents (like “the”).</p>
<p>This realization leads us to two important questions: 1. How can we normalize word frequencies within each document to account for document length? 2. How can we adjust these frequencies to give more weight to words that are unique to specific documents?</p>
<p><strong>TF-IDF (Term Frequency-Inverse Document Frequency)</strong> provides elegant solutions to both questions. Let’s see how it works in the next section.</p>
</section>
<section id="the-need-for-normalization" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="the-need-for-normalization"><span class="header-section-number">4.1</span> The Need for Normalization</h2>
<p>TF-IDF (Term Frequency-Inverse Document Frequency) offers our first practical glimpse into representing words as distributed patterns rather than isolated units.</p>
<p>The TF-IDF score for a word <span class="math inline">t</span> in document <span class="math inline">d</span> combines two components:</p>
<p><span class="math inline">\text{TF-IDF}(t,d) = \text{TF}(t,d) \times \text{IDF}(t)</span></p>
<p>where:</p>
<p><span class="math inline">\text{TF}(t,d) = \dfrac{\text{count of term }t\text{ in document }d}{\text{total number of terms in document }d}</span></p>
<p><span class="math inline">\text{IDF}(t) = \log\left(\dfrac{\text{total number of documents}}{\text{number of documents containing term }t}\right)</span></p>
<pre class="{note}"><code>Unlike one-hot encoding where each word is represented by a single position, TF-IDF represents each word through its pattern of occurrence across all documents. This distributed nature allows TF-IDF to capture semantic relationships: words that appear in similar documents will have similar patterns of TF-IDF scores.</code></pre>
<p>Let’s see this distributed representation in action: First, let us consider a simple example with 5 documents about animals. ```jldvufbpcno ipython3 from sklearn.decomposition import PCA import matplotlib.pyplot as plt import numpy as np</p>
</section>
</section>
<section id="sample-documents-about-animals" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Sample documents about animals</h1>
<p>documents = [ “Cats chase mice in the garden”, “Dogs chase cats in the park”, “Mice eat cheese in the house”, “Pets like dogs and cats need care”, “Wild animals hunt in nature”]</p>
<pre><code>
Second, we will split each document into words and create a vocabulary.
This process is called **tokenization**.

```{code-cell} ipython3

tokens = []
vocab = set()
for doc in documents:
    tokens_in_doc = doc.lower().split()
    tokens.append(tokens_in_doc)
    vocab.update(tokens_in_doc)

# create a dictionary that maps each word to a unique index
word_to_idx = {word: i for i, word in enumerate(vocab)}</code></pre>
<p>Third, we will count how many times each word appears in each document. We begin by creating a placeholder matrix <code>tf_matrix</code> to store the counts.</p>
<p>```jldvufbpcno ipython3 # Calculate term frequencies for each document n_docs = len(documents) # number of documents n_terms = len(vocab) # number of words</p>
</section>
<section id="this-is-a-matrix-of-zeros" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> This is a matrix of zeros</h1>
</section>
<section id="with-the-number-of-rows-equal-to-the-number-of-words" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> with the number of rows equal to the number of words</h1>
</section>
<section id="and-the-number-of-columns-equal-to-the-number-of-documents" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> and the number of columns equal to the number of documents</h1>
<p>tf_matrix = np.zeros((n_terms, n_docs))</p>
<p>print(tf_matrix)</p>
<pre><code>
And then we count how many times each word appears in each document.
```{code-cell} ipython3
for doc_idx, tokens_in_doc in enumerate(tokens):
    for word in tokens_in_doc:
        term_idx = word_to_idx[word]
        tf_matrix[term_idx, doc_idx] += 1

print(tf_matrix)</code></pre>
<p>Fourth, we calculate the IDF for each word. IDF is defined as the logarithm of the inverse document frequency. Document frequency is the number of documents that contain the word. Note that, if a word appears multiple times in the same document, it should only be counted once!</p>
<p>```jldvufbpcno ipython3 # Calculate IDF for each term # let’s use tf_matrix to calculate the document frequency doc_freq = np.zeros(n_terms)</p>
</section>
<section id="go-through-each-word" class="level1" data-number="9">
<h1 data-number="9"><span class="header-section-number">9</span> Go through each word</h1>
<p>for term_idx in range(n_terms):</p>
<pre><code># For each word, go through each document
for doc_idx in range(n_docs):
    # If the word appears in the document, increment the document frequency
    if tf_matrix[term_idx, doc_idx] &gt; 0:
        doc_freq[term_idx] += 1</code></pre>
<p>idf = np.log(n_docs / doc_freq)</p>
<pre><code>
Next, we calculate the TF-IDF matrix. `tf_matrix` is a matrix of `n_terms` by `n_docs`, and `idf` is a vector of length `n_terms`. Remind that the tf-idf is given by

$
\text{TF-IDF}(t,d) = \text{TF}(t,d) \times \text{IDF}(t)
$

A naive way to do this (albeit not efficient) is to perform this using for loops

```{code-cell} ipython3
# Calculate TF-IDF matrix

tfidf_matrix = np.zeros((n_terms, n_docs))
for term_idx in range(n_terms):
    for doc_idx in range(n_docs):
        tfidf_matrix[term_idx, doc_idx] = tf_matrix[term_idx, doc_idx] * idf[term_idx]</code></pre>
<p>A more efficient way is to use matrix multiplication.</p>
<p><code>jldvufbpcno ipython3 tfidf_matrix = np.diag(idf) @ tf_matrix</code></p>
<p>where <code>np.diag(idf)</code> creates a diagonal matrix with <code>idf</code> on the diagonal.</p>
<pre class="{tip}"><code>Consider a diagonal matrix as a tool to scale the rows/columns of a matrix.
If we multiple a diagonal matrix from the left, we scale the rows of the matrix.
If we multiple a diagonal matrix from the right, we scale the columns of the matrix.

![](https://allisonhorst.github.io/EDS_212_essential-math/slides/slide_images/diagonal_matrix_scaling.png)</code></pre>
<p>A more efficient way is to use einsum, which is a powerful function for performing Einstein summation convention on arrays.</p>
<p><code>jldvufbpcno ipython3 tfidf_matrix = np.einsum('ij,i-&gt;ij', tf_matrix, idf)</code></p>
<pre class="{tip}"><code>`einsum` provides a concise way to express complex array manipulations and can often lead to more efficient computations. Here is a concise description of how `einsum` works [A basic introduction to NumPy's einsum](https://ajcr.net/Basic-guide-to-einsum/)

The general form of `einsum` is `np.einsum(subscripts, *operands)`, where:

- `subscripts` is a string specifying the subscripts for summation as comma-separated list of subscript labels
- `operands` are the input arrays

In our case, `'ij,i-&gt;ij'` means:
1. Take the first array `tf_matrix` with dimensions `i,j`
2. Take the second array `idf` with dimension `i`
3. Multiply each row `j` of `tf_matrix` by the corresponding element `i` of `idf`
4. Output has same dimensions `i,j` as input</code></pre>
<p>Now, we have the TF-IDF matrix as follows: <code>jldvufbpcno ipython3 print(tfidf_matrix)</code></p>
<p>Each row of the TF-IDF matrix is our first attempt at a distributed representation of a word. Words that appear together in the same documents frequently will have similar rows.</p>
<p>Just like one-hot encoding, this representation can be high-dimensional, e.g., if we have 10000 documents, each word is represented by a vector of length 10000. One can reduce the dimensionality of the representation using dimensionality reduction techniques (e.g., PCA, SVD) while maintaining the structure of the data. This is possible because tf-idf matrix is often low-rank in practice.</p>
<pre class="{note}"><code>TF-IDF matrices are often low-rank because documents naturally group into clusters based on topics or themes. This clustering creates redundancy in the term-document matrix, as many terms are specific to certain clusters and rare in others. Consequently, the TF-IDF matrix can be approximated by a lower-rank matrix. This low-rank nature is useful for applications like dimensionality reduction and topic modeling, as it simplifies data visualization and analysis.</code></pre>
<p>Let us apply PCA to reduce the dimensionality of the tf-idf matrix. <code>jldvufbpcno ipython3 reducer = PCA(n_components=2) xy = reducer.fit_transform(tfidf_matrix)</code></p>
<p>Let’s visualize the result using Bokeh.</p>
<p>```jldvufbpcno ipython3 from bokeh.plotting import figure, show, output_notebook from bokeh.models import ColumnDataSource from bokeh.io import push_notebook from bokeh.plotting import figure, show from bokeh.io import output_notebook from bokeh.models import ColumnDataSource, HoverTool</p>
<p>output_notebook()</p>
</section>
<section id="prepare-data-for-bokeh" class="level1" data-number="10">
<h1 data-number="10"><span class="header-section-number">10</span> Prepare data for Bokeh</h1>
<p>source = ColumnDataSource(data=dict( x=xy[:, 0], y=xy[:, 1], text_x=xy[:, 0] + np.random.randn(n_terms) * 0.02, text_y=xy[:, 1] + np.random.randn(n_terms) * 0.02, term=list(word_to_idx.keys()) ))</p>
<p>p = figure(title=“Node Embeddings from Word2Vec”, x_axis_label=“X”, y_axis_label=“Y”)</p>
<p>p.scatter(‘x’, ‘y’, source=source, line_color=“black”, size = 30)</p>
</section>
<section id="add-labels-to-the-points" class="level1" data-number="11">
<h1 data-number="11"><span class="header-section-number">11</span> Add labels to the points</h1>
<p>p.text(x=‘text_x’, y=‘text_y’, text=‘term’, source=source, text_font_size=“10pt”, text_baseline=“middle”, text_align=“center”)</p>
<p>show(p)</p>
<pre><code>
The power of TF-IDF lies in its ability to transform the distributional hypothesis into a practical mathematical framework. By representing words through their patterns of usage across documents, TF-IDF creates a distributed representation where semantic relationships emerge naturally from the data.

However, TF-IDF has its limitations. It only captures word-document relationships, missing out on the rich word-word relationships that occur within documents. This limitation led researchers to develop more sophisticated techniques like word2vec, which we'll explore in the next section.

```{footbibliography}</code></pre>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb15" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="an">jupytext:</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co">  formats: md:myst</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co">  text_representation:</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co">    extension: .md</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co">    format_name: myst</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="an">kernelspec:</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co">  display_name: Python 3</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co">  language: python</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co">  name: python3</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="fu"># Teaching computers how to understand words</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>Imagine trying to explain the meaning of words to someone who only understands numbers. This is exactly the challenge we face when teaching computers to process text. Just as we need to translate between languages, we need to translate between the world of human language and the world of computer numbers. This translation process has evolved dramatically over time, becoming increasingly sophisticated in its ability to capture the nuances of meaning.</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a><span class="fu">## One-Hot Encoding: The First Step</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>The simplest approach to this translation challenge is one-hot encoding, akin to giving each word its own unique light switch in a vast room of switches. When representing a word, we turn on its switch and leave all others off. For example, in a tiny vocabulary of just three words {cat, dog, fish}:</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>'cat' becomes <span class="co">[</span><span class="ot">1, 0, 0</span><span class="co">]</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>'dog' becomes <span class="co">[</span><span class="ot">0, 1, 0</span><span class="co">]</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>'fish' becomes <span class="co">[</span><span class="ot">0, 0, 1</span><span class="co">]</span></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>While simple, this approach has a fundamental flaw: it suggests that all words are equally different from each other. In this representation, 'cat' is just as different from 'dog' as it is from 'algorithm' - something we know isn't true in real language use.</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a><span class="fu">## Distributional Hypothesis</span></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>What is missing in one-hot encoding is the notion of *context*.</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>One associates <span class="in">`cat`</span> with <span class="in">`dog`</span> because they have similar context, while <span class="in">`cat`</span> is more different from <span class="in">`fish`</span> than <span class="in">`dog`</span> because they are in different contexts.</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>This is the core idea of the **distributional hypothesis**.</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>In a nutshell, the distributional hypothesis states that:</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Words that frequently appear together in text (co-occur) are likely to be semantically related</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The meaning of a word can be inferred by examining the distribution of other words around it</span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Similar words will have similar distributions of surrounding context words</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>This hypothesis forms the theoretical foundation for many modern word embedding techniques.</span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a><span class="in">```{note}</span></span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a><span class="in">The idea that words can be understood by their context is captured by the famous linguistic principle: "You shall know a word by the company it keeps" {footcite}`firth1957synopsis`. This principle suggests that the meaning of a word is not inherent to the word itself, but rather emerges from how it is used alongside other words.</span></span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a><span class="in">```{tip}</span></span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a><span class="in">The ancient Buddhist concept of Apoha, developed by Dignāga in the 5th-6th century CE, shares similarities with modern distributional semantics. According to Apoha theory, we understand concepts by distinguishing what they are not - for example, we know what a "cow" is by recognizing everything that is not a cow. This mirrors how modern word embeddings define words through their relationships and contrasts with other words, showing how both ancient philosophy and contemporary linguistics recognize that meaning emerges from relationships between concepts.</span></span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true" tabindex="-1"></a><span class="in">![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQbWHl7Npbub7nDC9GvfUneConFZbjoHkPHuMPh3PXpGakxdDrv_0JmWt7Fpg63lo_XhhqZqFzOs6YUsVEbPyHBMVexnaqPLWzDQJ-CXAjFCoe7PzNrKlm474QDo14LiqOjrfr1zMt6As/s1600/cnononcow.jpg)</span></span>
<span id="cb15-53"><a href="#cb15-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-54"><a href="#cb15-54" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-55"><a href="#cb15-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-56"><a href="#cb15-56" aria-hidden="true" tabindex="-1"></a><span class="fu">## TF-IDF: A Simple but Powerful Word Embedding Technique</span></span>
<span id="cb15-57"><a href="#cb15-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-58"><a href="#cb15-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-59"><a href="#cb15-59" aria-hidden="true" tabindex="-1"></a><span class="fu">## Distributional Hypothesis</span></span>
<span id="cb15-60"><a href="#cb15-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-61"><a href="#cb15-61" aria-hidden="true" tabindex="-1"></a>What is missing in one-hot encoding is the notion of *context*. One associates `cat` with `dog` because they have similar context, while `cat` is more different from `fish` than `dog` because they are in different contexts. This is the core idea of the **distributional hypothesis**.</span>
<span id="cb15-62"><a href="#cb15-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-63"><a href="#cb15-63" aria-hidden="true" tabindex="-1"></a>In a nutshell, the distributional hypothesis states that we can understand the meaning of a word by examining the context in which it appears. Just as you might understand a person by the company they keep, we can understand a word by the words that surround it. This principle suggests that words appearing in similar contexts likely have similar meanings.</span>
<span id="cb15-64"><a href="#cb15-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-67"><a href="#cb15-67" aria-hidden="true" tabindex="-1"></a><span class="in">```{note}</span></span>
<span id="cb15-68"><a href="#cb15-68" aria-hidden="true" tabindex="-1"></a><span class="in">The idea that words can be understood by their context is captured by the famous linguistic principle: "You shall know a word by the company it keeps" {footcite}`firth1957synopsis`. This principle suggests that the meaning of a word is not inherent to the word itself, but rather emerges from how it is used alongside other words.</span></span>
<span id="cb15-69"><a href="#cb15-69" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-70"><a href="#cb15-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-71"><a href="#cb15-71" aria-hidden="true" tabindex="-1"></a><span class="fu">## TF-IDF: Words as Patterns of Usage</span></span>
<span id="cb15-72"><a href="#cb15-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-73"><a href="#cb15-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-74"><a href="#cb15-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-75"><a href="#cb15-75" aria-hidden="true" tabindex="-1"></a>The distributional hypothesis leads us to an important question: How can we capture these contextual patterns mathematically?</span>
<span id="cb15-76"><a href="#cb15-76" aria-hidden="true" tabindex="-1"></a>More specifically, what is a good unit of "context"?</span>
<span id="cb15-77"><a href="#cb15-77" aria-hidden="true" tabindex="-1"></a>A natural choice is to let the *document* be the unit of context.</span>
<span id="cb15-78"><a href="#cb15-78" aria-hidden="true" tabindex="-1"></a>The distributional hypothesis suggests that words that frequently appear in the same documents are likely to be semantically related.</span>
<span id="cb15-79"><a href="#cb15-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-80"><a href="#cb15-80" aria-hidden="true" tabindex="-1"></a><span class="fu">### First Attempt: Word-Document Count Matrix</span></span>
<span id="cb15-81"><a href="#cb15-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-82"><a href="#cb15-82" aria-hidden="true" tabindex="-1"></a>Let's try to organize this information systematically. Imagine creating a giant table where:</span>
<span id="cb15-83"><a href="#cb15-83" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Each row represents a word</span>
<span id="cb15-84"><a href="#cb15-84" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Each column represents a document</span>
<span id="cb15-85"><a href="#cb15-85" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Each cell contains the count of how often that word appears in that document</span>
<span id="cb15-86"><a href="#cb15-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-87"><a href="#cb15-87" aria-hidden="true" tabindex="-1"></a><span class="in">```{code-cell} ipython3</span></span>
<span id="cb15-88"><a href="#cb15-88" aria-hidden="true" tabindex="-1"></a>from sklearn<span class="op">.</span>feature_extraction<span class="op">.</span>text import CountVectorizer</span>
<span id="cb15-89"><a href="#cb15-89" aria-hidden="true" tabindex="-1"></a>import numpy as np</span>
<span id="cb15-90"><a href="#cb15-90" aria-hidden="true" tabindex="-1"></a>import pandas as pd</span>
<span id="cb15-91"><a href="#cb15-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-92"><a href="#cb15-92" aria-hidden="true" tabindex="-1"></a><span class="pp"># </span><span class="er">Sample documents</span></span>
<span id="cb15-93"><a href="#cb15-93" aria-hidden="true" tabindex="-1"></a>documents <span class="op">=</span> <span class="op">[</span></span>
<span id="cb15-94"><a href="#cb15-94" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The cat chases mice in the garden"</span><span class="op">,</span></span>
<span id="cb15-95"><a href="#cb15-95" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The dog chases cats in the park"</span><span class="op">,</span></span>
<span id="cb15-96"><a href="#cb15-96" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Mice eat cheese in the house"</span><span class="op">,</span></span>
<span id="cb15-97"><a href="#cb15-97" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The cat and dog play in the garden"</span></span>
<span id="cb15-98"><a href="#cb15-98" aria-hidden="true" tabindex="-1"></a><span class="op">]</span></span>
<span id="cb15-99"><a href="#cb15-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-100"><a href="#cb15-100" aria-hidden="true" tabindex="-1"></a><span class="pp"># </span><span class="er">Create word count matrix</span></span>
<span id="cb15-101"><a href="#cb15-101" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> CountVectorizer<span class="op">()</span></span>
<span id="cb15-102"><a href="#cb15-102" aria-hidden="true" tabindex="-1"></a>count_matrix <span class="op">=</span> vectorizer<span class="op">.</span>fit_transform<span class="op">(</span>documents<span class="op">)</span></span>
<span id="cb15-103"><a href="#cb15-103" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> vectorizer<span class="op">.</span>get_feature_names_out<span class="op">()</span></span>
<span id="cb15-104"><a href="#cb15-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-105"><a href="#cb15-105" aria-hidden="true" tabindex="-1"></a><span class="pp"># </span><span class="er">Display as a table</span></span>
<span id="cb15-106"><a href="#cb15-106" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd<span class="op">.</span>DataFrame<span class="op">(</span>count_matrix<span class="op">.</span>toarray<span class="op">(),</span> columns<span class="op">=</span>words<span class="op">)</span></span>
<span id="cb15-107"><a href="#cb15-107" aria-hidden="true" tabindex="-1"></a>df<span class="op">.</span>style<span class="op">.</span>background_gradient<span class="op">(</span>cmap<span class="op">=</span><span class="ch">'c</span><span class="er">ividis</span><span class="ch">'</span><span class="op">,</span> axis <span class="op">=</span> None<span class="op">).</span>set_caption<span class="op">(</span><span class="st">"Word-Document Count Matrix"</span><span class="op">)</span></span>
<span id="cb15-108"><a href="#cb15-108" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-109"><a href="#cb15-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-112"><a href="#cb15-112" aria-hidden="true" tabindex="-1"></a><span class="in">```{note}</span></span>
<span id="cb15-113"><a href="#cb15-113" aria-hidden="true" tabindex="-1"></a><span class="in">This matrix is our first attempt at a distributed representation - each word is represented not by a single number, but by its pattern of appearances across all documents.</span></span>
<span id="cb15-114"><a href="#cb15-114" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-115"><a href="#cb15-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-116"><a href="#cb15-116" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Problem with Raw Counts</span></span>
<span id="cb15-117"><a href="#cb15-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-118"><a href="#cb15-118" aria-hidden="true" tabindex="-1"></a>Looking at our word-document matrix, something seems off. Words like "the" and "in" appear frequently in almost every document. Are these words really the most important for understanding document meaning? Let's see what happens if we count how often each word appears across all documents:</span>
<span id="cb15-119"><a href="#cb15-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-120"><a href="#cb15-120" aria-hidden="true" tabindex="-1"></a><span class="in">```{code-cell} ipython3</span></span>
<span id="cb15-121"><a href="#cb15-121" aria-hidden="true" tabindex="-1"></a>word_counts <span class="op">=</span> df<span class="op">.</span>sum<span class="op">(</span>axis<span class="op">=</span><span class="dv">0</span><span class="op">)</span></span>
<span id="cb15-122"><a href="#cb15-122" aria-hidden="true" tabindex="-1"></a>word_counts_df <span class="op">=</span> pd<span class="op">.</span>DataFrame<span class="op">(</span>word_counts<span class="op">,</span> columns<span class="op">=[</span><span class="st">"count"</span><span class="op">]).</span>T</span>
<span id="cb15-123"><a href="#cb15-123" aria-hidden="true" tabindex="-1"></a>word_counts_df<span class="op">.</span>style<span class="op">.</span>background_gradient<span class="op">(</span>cmap<span class="op">=</span><span class="ch">'c</span><span class="er">ividis</span><span class="ch">'</span><span class="op">,</span> axis <span class="op">=</span> None<span class="op">).</span>set_caption<span class="op">(</span><span class="st">"Total appearances of each word"</span><span class="op">)</span></span>
<span id="cb15-124"><a href="#cb15-124" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-125"><a href="#cb15-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-128"><a href="#cb15-128" aria-hidden="true" tabindex="-1"></a><span class="in">```{note}</span></span>
<span id="cb15-129"><a href="#cb15-129" aria-hidden="true" tabindex="-1"></a><span class="in">This pattern, where a few words appear very frequently while most words appear rarely, is known as Zipf's Law. It's a fundamental property of natural language.</span></span>
<span id="cb15-130"><a href="#cb15-130" aria-hidden="true" tabindex="-1"></a><span class="in">![](https://miro.medium.com/v2/resize:fit:1400/1*GTpckiHyFLe04pUMeYDYOg.png)</span></span>
<span id="cb15-131"><a href="#cb15-131" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-132"><a href="#cb15-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-133"><a href="#cb15-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-134"><a href="#cb15-134" aria-hidden="true" tabindex="-1"></a>We've discovered two problems with raw word counts:</span>
<span id="cb15-135"><a href="#cb15-135" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Common words like "the" and "in" dominate the counts, but they tell us little about document content</span>
<span id="cb15-136"><a href="#cb15-136" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Raw frequencies don't tell us how unique or informative a word is across documents</span>
<span id="cb15-137"><a href="#cb15-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-138"><a href="#cb15-138" aria-hidden="true" tabindex="-1"></a>Think about it: if a word appears frequently in one document but rarely in others (like "cheese" in our example), it's probably more informative about that document's content than a word that appears equally frequently in all documents (like "the").</span>
<span id="cb15-139"><a href="#cb15-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-140"><a href="#cb15-140" aria-hidden="true" tabindex="-1"></a>This realization leads us to two important questions:</span>
<span id="cb15-141"><a href="#cb15-141" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>How can we normalize word frequencies within each document to account for document length?</span>
<span id="cb15-142"><a href="#cb15-142" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>How can we adjust these frequencies to give more weight to words that are unique to specific documents?</span>
<span id="cb15-143"><a href="#cb15-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-144"><a href="#cb15-144" aria-hidden="true" tabindex="-1"></a>**TF-IDF (Term Frequency-Inverse Document Frequency)** provides elegant solutions to both questions. Let's see how it works in the next section.</span>
<span id="cb15-145"><a href="#cb15-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-146"><a href="#cb15-146" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Need for Normalization</span></span>
<span id="cb15-147"><a href="#cb15-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-148"><a href="#cb15-148" aria-hidden="true" tabindex="-1"></a>TF-IDF (Term Frequency-Inverse Document Frequency) offers our first practical glimpse into representing words as distributed patterns rather than isolated units.</span>
<span id="cb15-149"><a href="#cb15-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-150"><a href="#cb15-150" aria-hidden="true" tabindex="-1"></a>The TF-IDF score for a word $t$ in document $d$ combines two components:</span>
<span id="cb15-151"><a href="#cb15-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-152"><a href="#cb15-152" aria-hidden="true" tabindex="-1"></a>$\text{TF-IDF}(t,d) = \text{TF}(t,d) \times \text{IDF}(t)$</span>
<span id="cb15-153"><a href="#cb15-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-154"><a href="#cb15-154" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb15-155"><a href="#cb15-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-156"><a href="#cb15-156" aria-hidden="true" tabindex="-1"></a>$\text{TF}(t,d) = \dfrac{\text{count of term }t\text{ in document }d}{\text{total number of terms in document }d}$</span>
<span id="cb15-157"><a href="#cb15-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-158"><a href="#cb15-158" aria-hidden="true" tabindex="-1"></a>$\text{IDF}(t) = \log\left(\dfrac{\text{total number of documents}}{\text{number of documents containing term }t}\right)$</span>
<span id="cb15-159"><a href="#cb15-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-162"><a href="#cb15-162" aria-hidden="true" tabindex="-1"></a><span class="in">```{note}</span></span>
<span id="cb15-163"><a href="#cb15-163" aria-hidden="true" tabindex="-1"></a><span class="in">Unlike one-hot encoding where each word is represented by a single position, TF-IDF represents each word through its pattern of occurrence across all documents. This distributed nature allows TF-IDF to capture semantic relationships: words that appear in similar documents will have similar patterns of TF-IDF scores.</span></span>
<span id="cb15-164"><a href="#cb15-164" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-165"><a href="#cb15-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-166"><a href="#cb15-166" aria-hidden="true" tabindex="-1"></a>Let's see this distributed representation in action:</span>
<span id="cb15-167"><a href="#cb15-167" aria-hidden="true" tabindex="-1"></a>First, let us consider a simple example with 5 documents about animals.</span>
<span id="cb15-168"><a href="#cb15-168" aria-hidden="true" tabindex="-1"></a><span class="in">```{code-cell} ipython3</span></span>
<span id="cb15-169"><a href="#cb15-169" aria-hidden="true" tabindex="-1"></a>from sklearn<span class="op">.</span>decomposition import PCA</span>
<span id="cb15-170"><a href="#cb15-170" aria-hidden="true" tabindex="-1"></a>import matplotlib<span class="op">.</span>pyplot as plt</span>
<span id="cb15-171"><a href="#cb15-171" aria-hidden="true" tabindex="-1"></a>import numpy as np</span>
<span id="cb15-172"><a href="#cb15-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-173"><a href="#cb15-173" aria-hidden="true" tabindex="-1"></a><span class="pp"># </span><span class="er">Sample documents about animals</span></span>
<span id="cb15-174"><a href="#cb15-174" aria-hidden="true" tabindex="-1"></a>documents <span class="op">=</span> <span class="op">[</span></span>
<span id="cb15-175"><a href="#cb15-175" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Cats chase mice in the garden"</span><span class="op">,</span></span>
<span id="cb15-176"><a href="#cb15-176" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Dogs chase cats in the park"</span><span class="op">,</span></span>
<span id="cb15-177"><a href="#cb15-177" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Mice eat cheese in the house"</span><span class="op">,</span></span>
<span id="cb15-178"><a href="#cb15-178" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Pets like dogs and cats need care"</span><span class="op">,</span></span>
<span id="cb15-179"><a href="#cb15-179" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Wild animals hunt in nature"</span></span>
<span id="cb15-180"><a href="#cb15-180" aria-hidden="true" tabindex="-1"></a><span class="op">]</span></span>
<span id="cb15-181"><a href="#cb15-181" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-182"><a href="#cb15-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-183"><a href="#cb15-183" aria-hidden="true" tabindex="-1"></a>Second, we will split each document into words and create a vocabulary.</span>
<span id="cb15-184"><a href="#cb15-184" aria-hidden="true" tabindex="-1"></a>This process is called **tokenization**.</span>
<span id="cb15-185"><a href="#cb15-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-186"><a href="#cb15-186" aria-hidden="true" tabindex="-1"></a><span class="in">```{code-cell} ipython3</span></span>
<span id="cb15-187"><a href="#cb15-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-188"><a href="#cb15-188" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> <span class="op">[]</span></span>
<span id="cb15-189"><a href="#cb15-189" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> set<span class="op">()</span></span>
<span id="cb15-190"><a href="#cb15-190" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> doc in documents<span class="op">:</span></span>
<span id="cb15-191"><a href="#cb15-191" aria-hidden="true" tabindex="-1"></a>    tokens_in_doc <span class="op">=</span> doc<span class="op">.</span>lower<span class="op">().</span>split<span class="op">()</span></span>
<span id="cb15-192"><a href="#cb15-192" aria-hidden="true" tabindex="-1"></a>    tokens<span class="op">.</span>append<span class="op">(</span>tokens_in_doc<span class="op">)</span></span>
<span id="cb15-193"><a href="#cb15-193" aria-hidden="true" tabindex="-1"></a>    vocab<span class="op">.</span>update<span class="op">(</span>tokens_in_doc<span class="op">)</span></span>
<span id="cb15-194"><a href="#cb15-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-195"><a href="#cb15-195" aria-hidden="true" tabindex="-1"></a><span class="pp"># </span><span class="er">create a dictionary that maps each word to a unique index</span></span>
<span id="cb15-196"><a href="#cb15-196" aria-hidden="true" tabindex="-1"></a>word_to_idx <span class="op">=</span> <span class="op">{</span>word<span class="op">:</span> i <span class="cf">for</span> i<span class="op">,</span> word in enumerate<span class="op">(</span>vocab<span class="op">)}</span></span>
<span id="cb15-197"><a href="#cb15-197" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-198"><a href="#cb15-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-199"><a href="#cb15-199" aria-hidden="true" tabindex="-1"></a>Third, we will count how many times each word appears in each document. We begin by creating a placeholder matrix <span class="in">`tf_matrix`</span> to store the counts.</span>
<span id="cb15-200"><a href="#cb15-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-201"><a href="#cb15-201" aria-hidden="true" tabindex="-1"></a><span class="in">```{code-cell} ipython3</span></span>
<span id="cb15-202"><a href="#cb15-202" aria-hidden="true" tabindex="-1"></a><span class="pp"># </span><span class="er">Calculate term frequencies for each document</span></span>
<span id="cb15-203"><a href="#cb15-203" aria-hidden="true" tabindex="-1"></a>n_docs <span class="op">=</span> len<span class="op">(</span>documents<span class="op">)</span> # number of documents</span>
<span id="cb15-204"><a href="#cb15-204" aria-hidden="true" tabindex="-1"></a>n_terms <span class="op">=</span> len<span class="op">(</span>vocab<span class="op">)</span> # number of words</span>
<span id="cb15-205"><a href="#cb15-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-206"><a href="#cb15-206" aria-hidden="true" tabindex="-1"></a><span class="pp"># </span><span class="er">This is a matrix of zeros</span></span>
<span id="cb15-207"><a href="#cb15-207" aria-hidden="true" tabindex="-1"></a><span class="pp"># </span><span class="er">with the number of rows equal to the number of words</span></span>
<span id="cb15-208"><a href="#cb15-208" aria-hidden="true" tabindex="-1"></a><span class="pp"># </span><span class="er">and the number of columns equal to the number of documents</span></span>
<span id="cb15-209"><a href="#cb15-209" aria-hidden="true" tabindex="-1"></a>tf_matrix <span class="op">=</span> np<span class="op">.</span>zeros<span class="op">((</span>n_terms<span class="op">,</span> n_docs<span class="op">))</span></span>
<span id="cb15-210"><a href="#cb15-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-211"><a href="#cb15-211" aria-hidden="true" tabindex="-1"></a>print<span class="op">(</span>tf_matrix<span class="op">)</span></span>
<span id="cb15-212"><a href="#cb15-212" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-213"><a href="#cb15-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-214"><a href="#cb15-214" aria-hidden="true" tabindex="-1"></a>And then we count how many times each word appears in each document.</span>
<span id="cb15-215"><a href="#cb15-215" aria-hidden="true" tabindex="-1"></a><span class="in">```{code-cell} ipython3</span></span>
<span id="cb15-216"><a href="#cb15-216" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> doc_idx<span class="op">,</span> tokens_in_doc in enumerate<span class="op">(</span>tokens<span class="op">):</span></span>
<span id="cb15-217"><a href="#cb15-217" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word in tokens_in_doc<span class="op">:</span></span>
<span id="cb15-218"><a href="#cb15-218" aria-hidden="true" tabindex="-1"></a>        term_idx <span class="op">=</span> word_to_idx<span class="op">[</span>word<span class="op">]</span></span>
<span id="cb15-219"><a href="#cb15-219" aria-hidden="true" tabindex="-1"></a>        tf_matrix<span class="op">[</span>term_idx<span class="op">,</span> doc_idx<span class="op">]</span> <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb15-220"><a href="#cb15-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-221"><a href="#cb15-221" aria-hidden="true" tabindex="-1"></a>print<span class="op">(</span>tf_matrix<span class="op">)</span></span>
<span id="cb15-222"><a href="#cb15-222" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-223"><a href="#cb15-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-224"><a href="#cb15-224" aria-hidden="true" tabindex="-1"></a>Fourth, we calculate the IDF for each word.</span>
<span id="cb15-225"><a href="#cb15-225" aria-hidden="true" tabindex="-1"></a>IDF is defined as the logarithm of the inverse document frequency.</span>
<span id="cb15-226"><a href="#cb15-226" aria-hidden="true" tabindex="-1"></a>Document frequency is the number of documents that contain the word.</span>
<span id="cb15-227"><a href="#cb15-227" aria-hidden="true" tabindex="-1"></a>Note that, if a word appears multiple times in the same document, it should only be counted once!</span>
<span id="cb15-228"><a href="#cb15-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-229"><a href="#cb15-229" aria-hidden="true" tabindex="-1"></a><span class="in">```{code-cell} ipython3</span></span>
<span id="cb15-230"><a href="#cb15-230" aria-hidden="true" tabindex="-1"></a><span class="pp"># </span><span class="er">Calculate IDF for each term</span></span>
<span id="cb15-231"><a href="#cb15-231" aria-hidden="true" tabindex="-1"></a><span class="pp"># </span><span class="er">let's use tf_matrix to calculate the document frequency</span></span>
<span id="cb15-232"><a href="#cb15-232" aria-hidden="true" tabindex="-1"></a>doc_freq <span class="op">=</span> np<span class="op">.</span>zeros<span class="op">(</span>n_terms<span class="op">)</span></span>
<span id="cb15-233"><a href="#cb15-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-234"><a href="#cb15-234" aria-hidden="true" tabindex="-1"></a><span class="pp"># </span><span class="er">Go through each word</span></span>
<span id="cb15-235"><a href="#cb15-235" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> term_idx in range<span class="op">(</span>n_terms<span class="op">):</span></span>
<span id="cb15-236"><a href="#cb15-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-237"><a href="#cb15-237" aria-hidden="true" tabindex="-1"></a>    <span class="pp"># </span><span class="er">For each word, go through each document</span></span>
<span id="cb15-238"><a href="#cb15-238" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> doc_idx in range<span class="op">(</span>n_docs<span class="op">):</span></span>
<span id="cb15-239"><a href="#cb15-239" aria-hidden="true" tabindex="-1"></a>        <span class="pp"># If the word appears in the document, increment the document frequency</span></span>
<span id="cb15-240"><a href="#cb15-240" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> tf_matrix<span class="op">[</span>term_idx<span class="op">,</span> doc_idx<span class="op">]</span> <span class="op">&gt;</span> <span class="dv">0</span><span class="op">:</span></span>
<span id="cb15-241"><a href="#cb15-241" aria-hidden="true" tabindex="-1"></a>            doc_freq<span class="op">[</span>term_idx<span class="op">]</span> <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb15-242"><a href="#cb15-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-243"><a href="#cb15-243" aria-hidden="true" tabindex="-1"></a>idf <span class="op">=</span> np<span class="op">.</span>log<span class="op">(</span>n_docs <span class="op">/</span> doc_freq<span class="op">)</span></span>
<span id="cb15-244"><a href="#cb15-244" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-245"><a href="#cb15-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-246"><a href="#cb15-246" aria-hidden="true" tabindex="-1"></a>Next, we calculate the TF-IDF matrix. <span class="in">`tf_matrix`</span> is a matrix of <span class="in">`n_terms`</span> by <span class="in">`n_docs`</span>, and <span class="in">`idf`</span> is a vector of length <span class="in">`n_terms`</span>. Remind that the tf-idf is given by</span>
<span id="cb15-247"><a href="#cb15-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-248"><a href="#cb15-248" aria-hidden="true" tabindex="-1"></a>$</span>
<span id="cb15-249"><a href="#cb15-249" aria-hidden="true" tabindex="-1"></a>\text{TF-IDF}(t,d) = \text{TF}(t,d) \times \text{IDF}(t)</span>
<span id="cb15-250"><a href="#cb15-250" aria-hidden="true" tabindex="-1"></a>$</span>
<span id="cb15-251"><a href="#cb15-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-252"><a href="#cb15-252" aria-hidden="true" tabindex="-1"></a>A naive way to do this (albeit not efficient) is to perform this using for loops</span>
<span id="cb15-253"><a href="#cb15-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-254"><a href="#cb15-254" aria-hidden="true" tabindex="-1"></a><span class="in">```{code-cell} ipython3</span></span>
<span id="cb15-255"><a href="#cb15-255" aria-hidden="true" tabindex="-1"></a><span class="pp"># </span><span class="er">Calculate TF-IDF matrix</span></span>
<span id="cb15-256"><a href="#cb15-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-257"><a href="#cb15-257" aria-hidden="true" tabindex="-1"></a>tfidf_matrix <span class="op">=</span> np<span class="op">.</span>zeros<span class="op">((</span>n_terms<span class="op">,</span> n_docs<span class="op">))</span></span>
<span id="cb15-258"><a href="#cb15-258" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> term_idx in range<span class="op">(</span>n_terms<span class="op">):</span></span>
<span id="cb15-259"><a href="#cb15-259" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> doc_idx in range<span class="op">(</span>n_docs<span class="op">):</span></span>
<span id="cb15-260"><a href="#cb15-260" aria-hidden="true" tabindex="-1"></a>        tfidf_matrix<span class="op">[</span>term_idx<span class="op">,</span> doc_idx<span class="op">]</span> <span class="op">=</span> tf_matrix<span class="op">[</span>term_idx<span class="op">,</span> doc_idx<span class="op">]</span> <span class="op">*</span> idf<span class="op">[</span>term_idx<span class="op">]</span></span>
<span id="cb15-261"><a href="#cb15-261" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-262"><a href="#cb15-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-263"><a href="#cb15-263" aria-hidden="true" tabindex="-1"></a>A more efficient way is to use matrix multiplication.</span>
<span id="cb15-264"><a href="#cb15-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-265"><a href="#cb15-265" aria-hidden="true" tabindex="-1"></a><span class="in">```{code-cell} ipython3</span></span>
<span id="cb15-266"><a href="#cb15-266" aria-hidden="true" tabindex="-1"></a>tfidf_matrix <span class="op">=</span> np<span class="op">.</span>diag<span class="op">(</span>idf<span class="op">)</span> @ tf_matrix</span>
<span id="cb15-267"><a href="#cb15-267" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-268"><a href="#cb15-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-269"><a href="#cb15-269" aria-hidden="true" tabindex="-1"></a>where <span class="in">`np.diag(idf)`</span> creates a diagonal matrix with <span class="in">`idf`</span> on the diagonal.</span>
<span id="cb15-270"><a href="#cb15-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-273"><a href="#cb15-273" aria-hidden="true" tabindex="-1"></a><span class="in">```{tip}</span></span>
<span id="cb15-274"><a href="#cb15-274" aria-hidden="true" tabindex="-1"></a><span class="in">Consider a diagonal matrix as a tool to scale the rows/columns of a matrix.</span></span>
<span id="cb15-275"><a href="#cb15-275" aria-hidden="true" tabindex="-1"></a><span class="in">If we multiple a diagonal matrix from the left, we scale the rows of the matrix.</span></span>
<span id="cb15-276"><a href="#cb15-276" aria-hidden="true" tabindex="-1"></a><span class="in">If we multiple a diagonal matrix from the right, we scale the columns of the matrix.</span></span>
<span id="cb15-277"><a href="#cb15-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-278"><a href="#cb15-278" aria-hidden="true" tabindex="-1"></a><span class="in">![](https://allisonhorst.github.io/EDS_212_essential-math/slides/slide_images/diagonal_matrix_scaling.png)</span></span>
<span id="cb15-279"><a href="#cb15-279" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-280"><a href="#cb15-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-281"><a href="#cb15-281" aria-hidden="true" tabindex="-1"></a>A more efficient way is to use einsum, which is a powerful function for performing Einstein summation convention on arrays.</span>
<span id="cb15-282"><a href="#cb15-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-283"><a href="#cb15-283" aria-hidden="true" tabindex="-1"></a><span class="in">```{code-cell} ipython3</span></span>
<span id="cb15-284"><a href="#cb15-284" aria-hidden="true" tabindex="-1"></a>tfidf_matrix <span class="op">=</span> np<span class="op">.</span>einsum<span class="op">(</span><span class="ch">'i</span><span class="er">j,i-&gt;ij</span><span class="ch">'</span><span class="op">,</span> tf_matrix<span class="op">,</span> idf<span class="op">)</span></span>
<span id="cb15-285"><a href="#cb15-285" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-286"><a href="#cb15-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-289"><a href="#cb15-289" aria-hidden="true" tabindex="-1"></a><span class="in">```{tip}</span></span>
<span id="cb15-290"><a href="#cb15-290" aria-hidden="true" tabindex="-1"></a><span class="in">`einsum` provides a concise way to express complex array manipulations and can often lead to more efficient computations. Here is a concise description of how `einsum` works [A basic introduction to NumPy's einsum](https://ajcr.net/Basic-guide-to-einsum/)</span></span>
<span id="cb15-291"><a href="#cb15-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-292"><a href="#cb15-292" aria-hidden="true" tabindex="-1"></a><span class="in">The general form of `einsum` is `np.einsum(subscripts, *operands)`, where:</span></span>
<span id="cb15-293"><a href="#cb15-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-294"><a href="#cb15-294" aria-hidden="true" tabindex="-1"></a><span class="in">- `subscripts` is a string specifying the subscripts for summation as comma-separated list of subscript labels</span></span>
<span id="cb15-295"><a href="#cb15-295" aria-hidden="true" tabindex="-1"></a><span class="in">- `operands` are the input arrays</span></span>
<span id="cb15-296"><a href="#cb15-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-297"><a href="#cb15-297" aria-hidden="true" tabindex="-1"></a><span class="in">In our case, `'ij,i-&gt;ij'` means:</span></span>
<span id="cb15-298"><a href="#cb15-298" aria-hidden="true" tabindex="-1"></a><span class="in">1. Take the first array `tf_matrix` with dimensions `i,j`</span></span>
<span id="cb15-299"><a href="#cb15-299" aria-hidden="true" tabindex="-1"></a><span class="in">2. Take the second array `idf` with dimension `i`</span></span>
<span id="cb15-300"><a href="#cb15-300" aria-hidden="true" tabindex="-1"></a><span class="in">3. Multiply each row `j` of `tf_matrix` by the corresponding element `i` of `idf`</span></span>
<span id="cb15-301"><a href="#cb15-301" aria-hidden="true" tabindex="-1"></a><span class="in">4. Output has same dimensions `i,j` as input</span></span>
<span id="cb15-302"><a href="#cb15-302" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-303"><a href="#cb15-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-304"><a href="#cb15-304" aria-hidden="true" tabindex="-1"></a>Now, we have the TF-IDF matrix as follows:</span>
<span id="cb15-305"><a href="#cb15-305" aria-hidden="true" tabindex="-1"></a><span class="in">```{code-cell} ipython3</span></span>
<span id="cb15-306"><a href="#cb15-306" aria-hidden="true" tabindex="-1"></a>print<span class="op">(</span>tfidf_matrix<span class="op">)</span></span>
<span id="cb15-307"><a href="#cb15-307" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-308"><a href="#cb15-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-309"><a href="#cb15-309" aria-hidden="true" tabindex="-1"></a>Each row of the TF-IDF matrix is our first attempt at a distributed representation of a word.</span>
<span id="cb15-310"><a href="#cb15-310" aria-hidden="true" tabindex="-1"></a>Words that appear together in the same documents frequently will have similar rows.</span>
<span id="cb15-311"><a href="#cb15-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-312"><a href="#cb15-312" aria-hidden="true" tabindex="-1"></a>Just like one-hot encoding, this representation can be high-dimensional, e.g., if we have 10000 documents, each word is represented by a vector of length 10000.</span>
<span id="cb15-313"><a href="#cb15-313" aria-hidden="true" tabindex="-1"></a>One can reduce the dimensionality of the representation using dimensionality reduction techniques (e.g., PCA, SVD) while maintaining the structure of the data.</span>
<span id="cb15-314"><a href="#cb15-314" aria-hidden="true" tabindex="-1"></a>This is possible because tf-idf matrix is often low-rank in practice.</span>
<span id="cb15-315"><a href="#cb15-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-318"><a href="#cb15-318" aria-hidden="true" tabindex="-1"></a><span class="in">```{note}</span></span>
<span id="cb15-319"><a href="#cb15-319" aria-hidden="true" tabindex="-1"></a><span class="in">TF-IDF matrices are often low-rank because documents naturally group into clusters based on topics or themes. This clustering creates redundancy in the term-document matrix, as many terms are specific to certain clusters and rare in others. Consequently, the TF-IDF matrix can be approximated by a lower-rank matrix. This low-rank nature is useful for applications like dimensionality reduction and topic modeling, as it simplifies data visualization and analysis.</span></span>
<span id="cb15-320"><a href="#cb15-320" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-321"><a href="#cb15-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-322"><a href="#cb15-322" aria-hidden="true" tabindex="-1"></a>Let us apply PCA to reduce the dimensionality of the tf-idf matrix.</span>
<span id="cb15-323"><a href="#cb15-323" aria-hidden="true" tabindex="-1"></a><span class="in">```{code-cell} ipython3</span></span>
<span id="cb15-324"><a href="#cb15-324" aria-hidden="true" tabindex="-1"></a>reducer <span class="op">=</span> PCA<span class="op">(</span>n_components<span class="op">=</span><span class="dv">2</span><span class="op">)</span></span>
<span id="cb15-325"><a href="#cb15-325" aria-hidden="true" tabindex="-1"></a>xy <span class="op">=</span> reducer<span class="op">.</span>fit_transform<span class="op">(</span>tfidf_matrix<span class="op">)</span></span>
<span id="cb15-326"><a href="#cb15-326" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-327"><a href="#cb15-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-328"><a href="#cb15-328" aria-hidden="true" tabindex="-1"></a>Let's visualize the result using Bokeh.</span>
<span id="cb15-329"><a href="#cb15-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-330"><a href="#cb15-330" aria-hidden="true" tabindex="-1"></a><span class="in">```{code-cell} ipython3</span></span>
<span id="cb15-331"><a href="#cb15-331" aria-hidden="true" tabindex="-1"></a>from bokeh<span class="op">.</span>plotting import figure<span class="op">,</span> show<span class="op">,</span> output_notebook</span>
<span id="cb15-332"><a href="#cb15-332" aria-hidden="true" tabindex="-1"></a>from bokeh<span class="op">.</span>models import ColumnDataSource</span>
<span id="cb15-333"><a href="#cb15-333" aria-hidden="true" tabindex="-1"></a>from bokeh<span class="op">.</span>io import push_notebook</span>
<span id="cb15-334"><a href="#cb15-334" aria-hidden="true" tabindex="-1"></a>from bokeh<span class="op">.</span>plotting import figure<span class="op">,</span> show</span>
<span id="cb15-335"><a href="#cb15-335" aria-hidden="true" tabindex="-1"></a>from bokeh<span class="op">.</span>io import output_notebook</span>
<span id="cb15-336"><a href="#cb15-336" aria-hidden="true" tabindex="-1"></a>from bokeh<span class="op">.</span>models import ColumnDataSource<span class="op">,</span> HoverTool</span>
<span id="cb15-337"><a href="#cb15-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-338"><a href="#cb15-338" aria-hidden="true" tabindex="-1"></a>output_notebook<span class="op">()</span></span>
<span id="cb15-339"><a href="#cb15-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-340"><a href="#cb15-340" aria-hidden="true" tabindex="-1"></a><span class="pp"># </span><span class="er">Prepare data for Bokeh</span></span>
<span id="cb15-341"><a href="#cb15-341" aria-hidden="true" tabindex="-1"></a>source <span class="op">=</span> ColumnDataSource<span class="op">(</span>data<span class="op">=</span>dict<span class="op">(</span></span>
<span id="cb15-342"><a href="#cb15-342" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>xy<span class="op">[:,</span> <span class="dv">0</span><span class="op">],</span></span>
<span id="cb15-343"><a href="#cb15-343" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>xy<span class="op">[:,</span> <span class="dv">1</span><span class="op">],</span></span>
<span id="cb15-344"><a href="#cb15-344" aria-hidden="true" tabindex="-1"></a>    text_x<span class="op">=</span>xy<span class="op">[:,</span> <span class="dv">0</span><span class="op">]</span> <span class="op">+</span> np<span class="op">.</span>random<span class="op">.</span>randn<span class="op">(</span>n_terms<span class="op">)</span> <span class="op">*</span> <span class="fl">0.02</span><span class="op">,</span></span>
<span id="cb15-345"><a href="#cb15-345" aria-hidden="true" tabindex="-1"></a>    text_y<span class="op">=</span>xy<span class="op">[:,</span> <span class="dv">1</span><span class="op">]</span> <span class="op">+</span> np<span class="op">.</span>random<span class="op">.</span>randn<span class="op">(</span>n_terms<span class="op">)</span> <span class="op">*</span> <span class="fl">0.02</span><span class="op">,</span></span>
<span id="cb15-346"><a href="#cb15-346" aria-hidden="true" tabindex="-1"></a>    term<span class="op">=</span>list<span class="op">(</span>word_to_idx<span class="op">.</span>keys<span class="op">())</span></span>
<span id="cb15-347"><a href="#cb15-347" aria-hidden="true" tabindex="-1"></a><span class="op">))</span></span>
<span id="cb15-348"><a href="#cb15-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-349"><a href="#cb15-349" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> figure<span class="op">(</span>title<span class="op">=</span><span class="st">"Node Embeddings from Word2Vec"</span><span class="op">,</span> x_axis_label<span class="op">=</span><span class="st">"X"</span><span class="op">,</span> y_axis_label<span class="op">=</span><span class="st">"Y"</span><span class="op">)</span></span>
<span id="cb15-350"><a href="#cb15-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-351"><a href="#cb15-351" aria-hidden="true" tabindex="-1"></a>p<span class="op">.</span>scatter<span class="op">(</span><span class="ch">'x'</span><span class="op">,</span> <span class="ch">'y'</span><span class="op">,</span> source<span class="op">=</span>source<span class="op">,</span> line_color<span class="op">=</span><span class="st">"black"</span><span class="op">,</span> size <span class="op">=</span> <span class="dv">30</span><span class="op">)</span></span>
<span id="cb15-352"><a href="#cb15-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-353"><a href="#cb15-353" aria-hidden="true" tabindex="-1"></a><span class="pp"># </span><span class="er">Add labels to the points</span></span>
<span id="cb15-354"><a href="#cb15-354" aria-hidden="true" tabindex="-1"></a>p<span class="op">.</span>text<span class="op">(</span>x<span class="op">=</span><span class="ch">'t</span><span class="er">ext_x</span><span class="ch">'</span><span class="op">,</span> y<span class="op">=</span><span class="ch">'t</span><span class="er">ext_y</span><span class="ch">'</span><span class="op">,</span> text<span class="op">=</span><span class="ch">'t</span><span class="er">erm</span><span class="ch">'</span><span class="op">,</span> source<span class="op">=</span>source<span class="op">,</span> text_font_size<span class="op">=</span><span class="st">"10pt"</span><span class="op">,</span> text_baseline<span class="op">=</span><span class="st">"middle"</span><span class="op">,</span> text_align<span class="op">=</span><span class="st">"center"</span><span class="op">)</span></span>
<span id="cb15-355"><a href="#cb15-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-356"><a href="#cb15-356" aria-hidden="true" tabindex="-1"></a>show<span class="op">(</span>p<span class="op">)</span></span>
<span id="cb15-357"><a href="#cb15-357" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-358"><a href="#cb15-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-359"><a href="#cb15-359" aria-hidden="true" tabindex="-1"></a>The power of TF-IDF lies in its ability to transform the distributional hypothesis into a practical mathematical framework. By representing words through their patterns of usage across documents, TF-IDF creates a distributed representation where semantic relationships emerge naturally from the data.</span>
<span id="cb15-360"><a href="#cb15-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-361"><a href="#cb15-361" aria-hidden="true" tabindex="-1"></a>However, TF-IDF has its limitations. It only captures word-document relationships, missing out on the rich word-word relationships that occur within documents. This limitation led researchers to develop more sophisticated techniques like word2vec, which we'll explore in the next section.</span>
<span id="cb15-362"><a href="#cb15-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-365"><a href="#cb15-365" aria-hidden="true" tabindex="-1"></a><span class="in">```{footbibliography}</span></span>
<span id="cb15-366"><a href="#cb15-366" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2025, Sadamori Kojaku</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions"><ul><li><a href="https://github.com/skojaku/applied-soft-comp/edit/main/m04-text/archive/tf-idf.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/skojaku/applied-soft-comp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/skojaku/applied-soft-comp">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>
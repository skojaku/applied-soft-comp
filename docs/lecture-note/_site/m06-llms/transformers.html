<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Sadamori Kojaku">
<meta name="dcterms.date" content="2025-11-23">

<title>Transformers – Applied Soft Computing</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-0348920b7671f696dc9078d39bff215e.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-f8b3ee0b8591417f08754123bc653639.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "|"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../assets/custom.css">
</head>

<body class="nav-sidebar docked nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../logo.jpg" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Applied Soft Computing</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-toolkit--workflow" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Toolkit &amp; Workflow</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-toolkit--workflow">    
        <li class="dropdown-header">─── Module 1 ───</li>
        <li>
    <a class="dropdown-item" href="../m01-toolkit/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m01-toolkit/git-github.html">
 <span class="dropdown-text">Git &amp; GitHub</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m01-toolkit/tidy-data.html">
 <span class="dropdown-text">Tidy Data</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m01-toolkit/data-provenance.html">
 <span class="dropdown-text">Data Provenance</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m01-toolkit/environments.qmd">
 <span class="dropdown-text">Environments</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-visualization" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Visualization</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-visualization">    
        <li class="dropdown-header">─── Module 2 ───</li>
        <li>
    <a class="dropdown-item" href="../m02-visualization/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m02-visualization/principles.html">
 <span class="dropdown-text">Principles</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m02-visualization/dimensionality-reduction.html">
 <span class="dropdown-text">High-Dimensional Data</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m02-visualization/networks.html">
 <span class="dropdown-text">Networks</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m02-visualization/time-series.html">
 <span class="dropdown-text">Time-Series</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-deep-learning" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Deep Learning</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-deep-learning">    
        <li class="dropdown-header">─── Module 3: Text ───</li>
        <li>
    <a class="dropdown-item" href="../m03-text/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m03-text/word2vec.md">
 <span class="dropdown-text">Word2Vec</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m03-text/lstm.md">
 <span class="dropdown-text">RNNs &amp; LSTMs</span></a>
  </li>  
        <li class="dropdown-header">─── Module 4: Images ───</li>
        <li>
    <a class="dropdown-item" href="../m04-images/overview.qmd">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m04-images/cnn.md">
 <span class="dropdown-text">CNNs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m04-images/resnet.md">
 <span class="dropdown-text">ResNet</span></a>
  </li>  
        <li class="dropdown-header">─── Module 5: Graphs ───</li>
        <li>
    <a class="dropdown-item" href="../m05-graphs/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m05-graphs/graph-embedding-w-word2vec.html">
 <span class="dropdown-text">Graph Embeddings</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m05-graphs/graph-convolutional-network.html">
 <span class="dropdown-text">GNNs</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-advanced-topics" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Advanced Topics</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-advanced-topics">    
        <li class="dropdown-header">─── Module 6: LLMs ───</li>
        <li>
    <a class="dropdown-item" href="../m06-llms/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m06-llms/transformers.md">
 <span class="dropdown-text">Transformers</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m06-llms/scaling-emergence.html">
 <span class="dropdown-text">Scaling &amp; Emergence</span></a>
  </li>  
        <li class="dropdown-header">─── Module 7: Self-Supervised ───</li>
        <li>
    <a class="dropdown-item" href="../m07-self-supervised/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m07-self-supervised/contrastive-learning.html">
 <span class="dropdown-text">Contrastive Learning</span></a>
  </li>  
        <li class="dropdown-header">─── Module 8: Explainability ───</li>
        <li>
    <a class="dropdown-item" href="../m08-explainability/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m08-explainability/fairness.html">
 <span class="dropdown-text">Fairness &amp; Ethics</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Transformers</li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Course Information</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/welcome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About Us</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/why-applied-soft-computing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Why applied soft computing?</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/discord.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Discord</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/minidora-usage.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Using Minidora</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/how-to-submit-assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">How to submit assignment</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/deliverables.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deliverables</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 1: The Data Scientist’s Toolkit</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-toolkit/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-toolkit/git-github.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Version Control with Git &amp; GitHub</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-toolkit/tidy-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Tidy Data Philosophy</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-toolkit/data-provenance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Provenance</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-toolkit/reproduceability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Reproducibility</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 2: Visualizing Complexity</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-visualization/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-visualization/principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Principles of Effective Visualization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-visualization/1d-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing 1D Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-visualization/2d-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing 2D Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-visualization/highd-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing High-Dimensional Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-visualization/networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-visualization/time-series.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing Time-Series</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 3: Deep Learning for Text</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-text/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-text/llm-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Large Language Models in Practice</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-text/prompt-engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prompt Engineering for Research</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-text/embeddings-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Embeddings: How Machines Understand Meaning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-text/tokenization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Tokenization: Unboxing How LLMs Read Text</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-text/transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Transformers: The Architecture Behind the Magic</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-text/word-embeddings.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Word Embeddings: Where It Started</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-text/text-fundamentals.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Text Fundamentals: The Full Picture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-text/semantic-research.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Semantic Analysis for Research</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 4: Deep Learning for Images</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/overview.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/image-processing.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Processing Fundamentals</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/cnn.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/lenet.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LeNet Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/alexnet.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AlexNet: Deep CNN Revolution</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/vgg.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">VGG Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/inception.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Inception &amp; Multi-Scale Features</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/batch-normalization.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Batch Normalization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/resnet.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ResNet &amp; Skip Connections</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 5: Deep Learning for Graphs</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-graphs/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-graphs/spectral-embedding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Spectral Graph Embedding</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-graphs/graph-embedding-w-word2vec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Graph Embeddings with Word2Vec</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-graphs/spectral-vs-neural-embedding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Spectral vs.&nbsp;Neural Embeddings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-graphs/from-image-to-graph.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">From Images to Graphs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-graphs/graph-convolutional-network.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Graph Convolutional Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-graphs/popular-gnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Popular GNN Architectures</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-graphs/software.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">GNN Software &amp; Tools</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 6: Large Language Models &amp; Emergent Behavior</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-llms/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-llms/transformers.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Transformer Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-llms/bert.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">BERT &amp; Contextual Embeddings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-llms/sentence-bert.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sentence-BERT for Semantic Similarity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-llms/gpt.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">GPT &amp; Generative Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-llms/from-language-model-to-instruction-following.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">From Language Models to Instruction Following</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-llms/prompt-tuning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prompt Engineering &amp; In-Context Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-llms/scaling-emergence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Scaling Laws &amp; Emergent Abilities</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-llms/llms-as-complex-systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LLMs as Complex Systems</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 7: Self-Supervised Learning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m07-self-supervised/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m07-self-supervised/paradigm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Self-Supervised Paradigm</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m07-self-supervised/contrastive-learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Contrastive Learning (SimCLR)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m07-self-supervised/graphs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Self-Supervised Learning for Graphs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m07-self-supervised/time-series.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Self-Supervised Learning for Time-Series</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 8: Explainability &amp; Ethics</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-explainability/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-explainability/need.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Need for Explainability</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-explainability/attention.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Attention Visualization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-explainability/lime-shap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LIME &amp; SHAP</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-explainability/fairness.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Algorithmic Fairness &amp; Bias</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-explainability/causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Causality vs.&nbsp;Correlation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false">
 <span class="menu-text">Legacy Materials</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-text/what-to-learn.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Word &amp; Document Embeddings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-text/what-to-learn.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Recurrent Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/what-to-learn.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Processing (CNNs)</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Transformers</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Sadamori Kojaku </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 23, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="transformers" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Transformers</h1>
<p>::::{grid} 1 :class-container: spoiler-block</p>
<p>:::{grid-item-card} Spoiler Transformers don’t process sequences; they process relationships between every position simultaneously. :::</p>
<p>::::</p>
<section id="the-mechanism" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="the-mechanism"><span class="header-section-number">1.1</span> The Mechanism</h2>
<p>You’ve been taught to think of language models as sequential processors—reading left to right, one word triggering the next, like dominoes falling. This intuition comes from recurrent neural networks (RNNs), where information flows step by step, each word depending on the hidden state from the previous word. The transformer architecture throws this away entirely.</p>
<p>Instead of sequential processing, transformers operate through <strong>parallel relationship mapping</strong>. When you read “The cat sat on the mat because it was tired,” you don’t actually process word-by-word in isolation. Your brain simultaneously evaluates which words relate to which—“it” connects to “cat,” “tired” explains “sat,” “mat” anchors “on.” Transformers formalize this intuition mathematically. Every position in the input sequence simultaneously computes its relationship to every other position. The mechanism is attention, and the result is a system where context flows in all directions at once, not just forward through time.</p>
<p>This parallelism is why transformers scaled when RNNs didn’t. Recurrent architectures impose sequential computation—you can’t process word 100 until you’ve processed word 99. Transformers eliminate this bottleneck. Every position can be computed in parallel, which means training time scales with sequence complexity, not sequence length. This architectural shift is what enabled GPT-3, GPT-4, and Claude to exist.</p>
</section>
<section id="the-architecture" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="the-architecture"><span class="header-section-number">1.2</span> The Architecture</h2>
<p>Modern LLMs stack multiple <strong>transformer blocks</strong>—modular units that take a sequence of token vectors as input and output a transformed sequence of the same length. GPT-3 uses 96 of these blocks; GPT-4 likely uses more. Each block refines the representation, adding layers of contextual understanding.</p>
<p>```doufuilu ../figs/transformer-overview.jpg :name: transformer-overview :alt: Transformer Overview :width: 50% :align: center</p>
<p>The basic architecture of the transformer-based LLMs.</p>
<pre><code>
These blocks come in two forms: **encoders** and **decoders**. The encoder processes the input sequence and builds a contextualized representation. The decoder generates the output sequence, attending to both its own previous outputs and the encoder's representation. For translation tasks ("I love you" → "Je t'aime"), the encoder processes English, the decoder generates French. For language modeling (GPT-style systems), only the decoder is used—it generates text autoregressively, predicting the next token based on all previous tokens.

```{figure} ../figs/transformer-encoder-decoder.jpg
:name: transformer-encoder-decoder
:alt: Transformer Encoder-Decoder
:width: 80%
:align: center

The encoder-decoder architecture. The encoder builds a representation of the input sequence; the decoder generates the output sequence while attending to the encoder's output.</code></pre>
<p>Inside each block are three core components: <strong>multi-head attention</strong> (the relationship mapper), <strong>layer normalization</strong> (numerical stabilization), and <strong>feed-forward networks</strong> (nonlinear transformation). We’ll build these components step by step.</p>
<p>```doufuilu ../figs/transformer-component.jpg :name: transformer-wired-components :alt: Transformer Wired Components :width: 80% :align: center</p>
<p>Internal structure of encoder and decoder blocks.</p>
<pre><code>
## Attention: The Relationship Engine

**Self-attention**—the core of the transformer—computes how much each position in a sequence should "attend to" every other position. Unlike earlier attention mechanisms in seq2seq models, which attended from one sentence to another, self-attention operates within a single sequence. It answers the question: "Given this word, which other words matter most?"

```{figure} ../figs/transformer-attention.jpg
:name: transformer-attention
:alt: Attention Mechanism
:width: 80%
:align: center

The attention mechanism computes relationships between all positions simultaneously.</code></pre>
<p>For each word, the attention mechanism creates three vectors: <strong>query</strong> (<span class="math inline">Q</span>), <strong>key</strong> (<span class="math inline">K</span>), and <strong>value</strong> (<span class="math inline">V</span>). Think of these as a library search: the query is what you’re looking for, the keys are book titles, and the values are the actual content. When you search for “machine learning” (your query), you match it against book titles (keys) to find relevant content (values).</p>
<p>Mathematically, each of these vectors is created by a learned linear transformation of the input word embedding. Given an input embedding <span class="math inline">x</span>, we compute:</p>
<p><span class="math display">
Q = x W_Q, \quad K = x W_K, \quad V = x W_V
</span></p>
<p>where <span class="math inline">W_Q</span>, <span class="math inline">W_K</span>, and <span class="math inline">W_V</span> are learned weight matrices. The attention mechanism then computes which keys are most relevant to each query using the dot product, which measures vector similarity. The dot product <span class="math inline">QK^T</span> produces a matrix of attention scores—large values indicate strong relationships, small values indicate weak ones.</p>
<p>These raw scores are scaled by <span class="math inline">\sqrt{d_k}</span> (the square root of the key dimension) to prevent extreme values, then normalized using softmax to produce a probability distribution. Finally, these normalized attention weights are used to compute a weighted sum of the value vectors. The complete operation is:</p>
<p><span class="math display">
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
</span></p>
<p>where <span class="math inline">Q \in \mathbb{R}^{n \times d_k}</span>, <span class="math inline">K \in \mathbb{R}^{n \times d_k}</span>, and <span class="math inline">V \in \mathbb{R}^{n \times d_v}</span> represent matrices containing <span class="math inline">n</span> query, key, and value vectors respectively.</p>
<p>The interactive visualization below demonstrates how learned Query and Key transformations produce different attention patterns. Adjust the transformation parameters to see how different <span class="math inline">W_Q</span> and <span class="math inline">W_K</span> matrices change which words attend to which:</p>
<div>
<p><marimo-iframe data-height="700px" data-show-code="false"></marimo-iframe></p>
<p><code>python {marimo} import marimo as mo import numpy as np import pandas as pd import altair as alt</code></p>
<p>```python {marimo} attention_words = [“bank”, “money”, “loan”, “river”, “shore”] attention_embeddings = np.array([ [0.0, 0.0], # bank (center) [-0.8, -0.3], # money [-0.7, -0.6], # loan [0.7, -0.5], # river [0.6, -0.7], # shore]) * 2</p>
<section id="query-controls" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Query controls</h1>
<p>q_scale_x = mo.ui.slider(-2, 2, 0.1, value=1.0, label=“Q Scale X”) q_scale_y = mo.ui.slider(-2, 2, 0.1, value=1.0, label=“Q Scale Y”) q_rotate = mo.ui.slider(-180, 180, 5, value=0, label=“Q Rotate (deg)”)</p>
</section>
<section id="key-controls" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Key controls</h1>
<p>k_scale_x = mo.ui.slider(-2, 2, 0.1, value=1.0, label=“K Scale X”) k_scale_y = mo.ui.slider(-2, 2, 0.1, value=1.0, label=“K Scale Y”) k_rotate = mo.ui.slider(-180, 180, 5, value=0, label=“K Rotate (deg)”)</p>
<p>q_controls = mo.vstack([mo.md(“<strong>Query Transformation</strong>”), q_scale_x, q_scale_y, q_rotate]) k_controls = mo.vstack([mo.md(“<strong>Key Transformation</strong>”), k_scale_x, k_scale_y, k_rotate])</p>
<pre><code>
```python {marimo}
def _transform_embeddings(emb, scale_x, scale_y, rotate_deg):
    theta = np.radians(rotate_deg)
    rot_matrix = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])
    scale_matrix = np.diag([scale_x, scale_y])
    W = rot_matrix @ scale_matrix
    return emb @ W.T

Q = _transform_embeddings(attention_embeddings, q_scale_x.value, q_scale_y.value, q_rotate.value)
K = _transform_embeddings(attention_embeddings, k_scale_x.value, k_scale_y.value, k_rotate.value)

# Compute attention scores
_scores = Q @ K.T
_exp_scores = np.exp(_scores - np.max(_scores, axis=1, keepdims=True))
attention_weights = _exp_scores / np.sum(_exp_scores, axis=1, keepdims=True)

# Create visualizations
_df_q = pd.DataFrame({"word": attention_words, "x": Q[:, 0], "y": Q[:, 1]})
_df_k = pd.DataFrame({"word": attention_words, "x": K[:, 0], "y": K[:, 1]})

_chart_q = alt.Chart(_df_q).mark_circle(size=100).encode(
    x=alt.X('x:Q', scale=alt.Scale(domain=[-4, 4]), title='Q1'),
    y=alt.Y('y:Q', scale=alt.Scale(domain=[-4, 4]), title='Q2'),
    tooltip=['word:N']
).properties(width=200, height=200, title="Query (Q)")
_text_q = _chart_q.mark_text(dy=-12, fontSize=10, fontWeight='bold').encode(text='word:N')

_chart_k = alt.Chart(_df_k).mark_circle(size=100).encode(
    x=alt.X('x:Q', scale=alt.Scale(domain=[-4, 4]), title='K1'),
    y=alt.Y('y:Q', scale=alt.Scale(domain=[-4, 4]), title='K2'),
    tooltip=['word:N']
).properties(width=200, height=200, title="Key (K)")
_text_k = _chart_k.mark_text(dy=-12, fontSize=10, fontWeight='bold').encode(text='word:N')

# Heatmap
_heatmap_data = []
for i, word_i in enumerate(attention_words):
    for j, word_j in enumerate(attention_words):
        _heatmap_data.append({"Query": word_i, "Key": word_j, "Weight": attention_weights[i, j]})
_df_heatmap = pd.DataFrame(_heatmap_data)

_heatmap = alt.Chart(_df_heatmap).mark_rect().encode(
    x=alt.X('Key:N', title='Key Word'),
    y=alt.Y('Query:N', title='Query Word'),
    color=alt.Color('Weight:Q', scale=alt.Scale(scheme='blues'), title='Attention'),
    tooltip=['Query:N', 'Key:N', alt.Tooltip('Weight:Q', format='.3f')]
).properties(width=250, height=250, title="Attention Weights (Softmax)")

mo.vstack([
    mo.hstack([q_controls, k_controls], align="center"),
    mo.hstack([_chart_q + _text_q, _chart_k + _text_k, _heatmap], align="center")
])</code></pre>
<p></p>
</section>
</div>
<p>The output is a <strong>contextualized vector</strong> for each word—a representation that changes based on surrounding context. The word “bank” produces different vectors in “river bank” versus “financial bank” because the attention mechanism incorporates information from neighboring words.</p>
<p>To see this in action, consider how we might contextualize the word “bank” by mixing it with surrounding words. The visualization below shows static word embeddings—notice how “bank” sits neutrally between financial terms (money, loan) and geographical terms (river, shore).</p>
<div>
<p><marimo-iframe data-height="400px" data-show-code="false"></marimo-iframe></p>
<p>```python {marimo} static_words = [“bank”, “money”, “loan”, “river”, “shore”] static_embeddings = np.array([ [0.0, 0.0], # bank (center) [-0.8, -0.3], # money [-0.7, -0.6], # loan [0.7, -0.5], # river [0.6, -0.7], # shore]) * 2</p>
<p>_df_static = pd.DataFrame({“word”: static_words, “x”: static_embeddings[:, 0], “y”: static_embeddings[:, 1]})</p>
<p>_chart_static = alt.Chart(_df_static).mark_circle(size=200).encode( x=alt.X(‘x:Q’, scale=alt.Scale(domain=[-2, 2]), title=‘Dimension 1’), y=alt.Y(‘y:Q’, scale=alt.Scale(domain=[-2, 2]), title=‘Dimension 2’), text=‘word:N’, tooltip=[‘word:N’, ‘x:Q’, ‘y:Q’] ).properties(width=300, height=300, title=“Static Word Embeddings”)</p>
<p>_text_static = _chart_static.mark_text(dy=-15, fontSize=14, fontWeight=‘bold’).encode(text=‘word:N’)</p>
<p>_chart_static + _text_static</p>
<pre><code>
&lt;/marimo-iframe&gt;
&lt;/div&gt;

Now, try adjusting the weights below to create a contextualized version of "bank." If the sentence is "Money in bank," adjust the weights to shift "bank" toward "money." If the sentence is "River bank," shift it toward "river."

&lt;div&gt;
&lt;marimo-iframe data-height="500px" data-show-code="false"&gt;

```python {marimo}
context_words = ["bank", "money", "loan", "river", "shore"]
context_embeddings = np.array([
    [0.0, 0.0],  # bank (center)
    [-0.8, -0.3],  # money
    [-0.7, -0.6],  # loan
    [0.7, -0.5],  # river
    [0.6, -0.7],  # shore
]) * 2

slider_bank = mo.ui.slider(0, 1, 0.01, value=1.0, label="Bank Weight")
slider_money = mo.ui.slider(0, 1, 0.01, value=0, label="Money Weight")
slider_loan = mo.ui.slider(0, 1, 0.01, value=0, label="Loan Weight")
slider_river = mo.ui.slider(0, 1, 0.01, value=0, label="River Weight")
slider_shore = mo.ui.slider(0, 1, 0.01, value=0, label="Shore Weight")

context_sliders = mo.vstack([slider_bank, slider_money, slider_loan, slider_river, slider_shore])</code></pre>
<p>```python {marimo} _weights = np.array([slider_bank.value, slider_money.value, slider_loan.value, slider_river.value, slider_shore.value]) _total = _weights.sum() if _total &gt; 0: _weights = _weights / _total _new_vec = context_embeddings.T @ _weights else: _new_vec = np.zeros(2)</p>
<p>_df_orig = pd.DataFrame({“word”: context_words, “x”: context_embeddings[:, 0], “y”: context_embeddings[:, 1], “type”: [“Original”] * 5}) _df_new = pd.DataFrame({“word”: [“Contextualized Bank”], “x”: [_new_vec[0]], “y”: [_new_vec[1]], “type”: [“Contextualized”]}) _df_combined = pd.concat([_df_orig, _df_new])</p>
<p>_chart_context = alt.Chart(_df_combined).mark_circle(size=200).encode( x=alt.X(‘x:Q’, scale=alt.Scale(domain=[-2, 2]), title=‘Dimension 1’), y=alt.Y(‘y:Q’, scale=alt.Scale(domain=[-2, 2]), title=‘Dimension 2’), color=alt.Color(‘type:N’, scale=alt.Scale(domain=[‘Original’, ‘Contextualized’], range=[‘#dadada’, ‘#ff7f0e’])), tooltip=[‘word:N’, ‘x:Q’, ‘y:Q’] ).properties(width=350, height=350, title=“Contextualized Bank”)</p>
<p>_text_context = _chart_context.mark_text(dy=-15, fontSize=14, fontWeight=‘bold’).encode(text=‘word:N’, color=alt.value(‘black’))</p>
<p>mo.hstack([context_sliders, _chart_context + _text_context], align=“center”)</p>
<pre><code>
&lt;/marimo-iframe&gt;
&lt;/div&gt;

This manual weighting captures the intuition, but how do we learn which words to attend to? This is where queries and keys come in.

### Multi-Head Attention: Multiple Perspectives

A single attention mechanism captures one type of relationship. **Multi-head attention** runs multiple attention operations in parallel, each with different learned parameters. Each head can specialize—one might focus on syntactic dependencies (subject-verb relationships), another on semantic similarity (synonyms and antonyms), another on positional proximity (nearby words).

```{figure} ../figs/transformer-multihead-attention.jpg
:name: transformer-multihead-attention
:alt: Multi-Head Attention
:width: 50%
:align: center

Multi-head attention runs multiple attention operations in parallel, each capturing different relationships.</code></pre>
<p>The outputs from all heads are concatenated and passed through a final linear transformation to produce the multi-head attention output. In the original transformer paper {footcite:p}<code>vaswani2017attention</code>, the authors used <span class="math inline">h=8</span> attention heads, with each head using dimension <span class="math inline">d_k = d_v = d/h = 64</span>, where <span class="math inline">d=512</span> is the model dimension.</p>
<section id="layer-normalization-numerical-stability" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="layer-normalization-numerical-stability"><span class="header-section-number">3.1</span> Layer Normalization: Numerical Stability</h2>
<p>Deep networks suffer from numerical instability—activations can grow explosively large or vanish to zero as they propagate through layers. <strong>Layer normalization</strong> stabilizes training by rescaling activations to have zero mean and unit variance.</p>
<p>```doufuilu https://miro.medium.com/v2/resize:fit:1400/0*Agdt1zYwfUxXMJGJ :name: transformer-layer-normalization :alt: Layer Normalization :width: 80% :align: center</p>
<p>Layer normalization computes mean and standard deviation across all features for each sample, then normalizes.</p>
<pre><code>
For each input vector $x$, layer normalization computes:

$$
\text{LayerNorm}(x) = \gamma \frac{x - \mu}{\sigma} + \beta
$$

where $\mu$ and $\sigma$ are the mean and standard deviation of $x$, and $\gamma$ and $\beta$ are learned scaling and shifting parameters (initialized to 1 and 0 respectively). This ensures that no matter how the input distribution shifts during training, each layer receives inputs in a stable numerical range.

## The Encoder Block

Now we wire the components together. The **encoder block** processes the input sequence through four stages:

1. **Multi-head self-attention** computes contextualized representations
2. **Residual connection + normalization** stabilizes training
3. **Feed-forward network** applies nonlinear transformation
4. **Residual connection + normalization** again

```{figure} ../figs/transformer-encoder.jpg
:name: transformer-block
:alt: Transformer Block
:width: 50%
:align: center

Information flows through multi-head attention, normalization, feed-forward networks, and final normalization.</code></pre>
<p>The feed-forward network is a simple two-layer MLP applied independently to each position:</p>
<p><span class="math display">
\text{FFN}(x) = \text{ReLU}(x W_1 + b_1) W_2 + b_2
</span></p>
<p>The <strong>residual connections</strong> (also called skip connections) are critical for training deep networks. Instead of learning a direct mapping <span class="math inline">f(x)</span>, we learn the residual:</p>
<p><span class="math display">
x_{\text{out}} = x_{\text{in}} + f(x_{\text{in}})
</span></p>
<p>This simple addition has profound consequences for gradient flow. During backpropagation, the gradient of the loss <span class="math inline">\mathcal{L}</span> with respect to layer <span class="math inline">l</span> is:</p>
<p><span class="math display">
\frac{\partial \mathcal{L}}{\partial x_l} = \frac{\partial \mathcal{L}}{\partial x_{l+1}} \left(1 + \frac{\partial f_l}{\partial x_l}\right)
</span></p>
<p>Notice the “+1” term—this provides a direct gradient path from the output back to the input. Without residual connections, gradients must pass through the chain:</p>
<p><span class="math display">
\frac{\partial f_L}{\partial f_{L-1}} \cdot \frac{\partial f_{L-1}}{\partial f_{L-2}} \cdot \ldots \cdot \frac{\partial f_1}{\partial x}
</span></p>
<p>If any term is less than 1, the gradient shrinks exponentially—this is the <strong>vanishing gradient problem</strong>. With residual connections, the gradient expansion becomes:</p>
<p><span class="math display">
1 + O_1 + O_2 + O_3 + \ldots
</span></p>
<p>where <span class="math inline">O_1</span> contains first-order terms, <span class="math inline">O_2</span> contains second-order products, etc. The constant “1” ensures gradients can flow even when the learned components <span class="math inline">f_i</span> produce small derivatives. This architectural innovation, originally developed for computer vision {footcite:p}<code>he2015deep</code>, is what allows transformers to scale to hundreds of layers.</p>
</section>
<section id="the-decoder-block" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="the-decoder-block"><span class="header-section-number">3.2</span> The Decoder Block</h2>
<p>The <strong>decoder block</strong> extends the encoder with two modifications: <strong>masked self-attention</strong> and <strong>cross-attention</strong>.</p>
<p>```doufuilu ../figs/transformer-decoder.jpg :name: transformer-decoder :alt: Transformer Decoder :width: 50% :align: center</p>
<p>The decoder adds masked self-attention (to prevent future peeking) and cross-attention (to access encoder outputs).</p>
<pre><code>
### Masked Self-Attention: Preventing Future Leakage

During training, we know the entire target sequence. For translation ("I love you" → "Je t'aime"), we have both input and output. A naive decoder could "cheat" by looking at future words in the target sequence. Masked self-attention prevents this by zeroing out attention to future positions.

The mask is implemented by setting attention scores to $-\infty$ before the softmax:

$$
\text{MaskedAttention}(Q, K, V) = \text{softmax}\left(\frac{QK^T + M}{\sqrt{d_k}}\right)V
$$

where $M$ is a matrix with $-\infty$ at positions $(i,j)$ where $j &gt; i$ (future positions) and 0 elsewhere. After softmax, these $-\infty$ values become zero, eliminating information flow from future tokens.

```{figure} https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png
:name: transformer-masked-attention
:alt: Masked Attention
:width: 80%
:align: center

Masked attention zeros out future positions, allowing parallel training without information leakage.</code></pre>
<p>This enables <strong>parallel training</strong>. Instead of generating “Je”, then “t’aime”, then the final token sequentially, we can train all positions simultaneously—each with access only to its causal past. During inference, masking happens naturally because future tokens don’t exist yet.</p>
<section id="cross-attention-connecting-encoder-and-decoder" class="level3">
<h3 class="anchored" data-anchor-id="cross-attention-connecting-encoder-and-decoder">Cross-Attention: Connecting Encoder and Decoder</h3>
<p>The second attention layer in the decoder uses <strong>cross-attention</strong> to access the encoder’s output. The queries (<span class="math inline">Q</span>) come from the decoder’s previous layer, while the keys (<span class="math inline">K</span>) and values (<span class="math inline">V</span>) come from the encoder’s output:</p>
<p><span class="math display">
\text{CrossAttention}(Q_{\text{decoder}}, K_{\text{encoder}}, V_{\text{encoder}}) = \text{softmax}\left(\frac{Q_{\text{decoder}}K_{\text{encoder}}^T}{\sqrt{d_k}}\right)V_{\text{encoder}}
</span></p>
<p>```doufuilu ../figs/transformer-cross-attention.jpg :name: transformer-cross-attention :alt: Cross-Attention :width: 60% :align: center</p>
<p>Cross-attention allows the decoder to query the encoder’s representation.</p>
<pre><code>
This is how translation works: when generating "Je", the decoder attends to "I"; when generating "t'aime", it attends to "love". The attention mechanism learns these alignments automatically from data, without explicit supervision.

## Position Embedding: Encoding Order

Attention is **permutation invariant**—it produces the same output regardless of input order. "The cat sat on the mat" and "mat the on sat cat the" yield identical attention outputs because the dot product doesn't encode position. We need to inject positional information.

The naive approach is to add a position index: $x_t := x_t + \beta t$. This fails for two reasons:

1. **Unbounded**: Position indices grow arbitrarily large. Models trained on sequences of length 512 fail on sequences of length 1000 because they've never seen position 513.
2. **Discrete**: Positions 10 and 11 are no more similar than positions 10 and 100.

A better approach is **binary position encoding**. Represent position $t$ as a binary vector:

$$
\begin{align*}
  0: \ \ \ \ \texttt{0} \ \ \texttt{0} \ \ \texttt{0} \ \ \texttt{0} &amp; \quad &amp;
  8: \ \ \ \ \texttt{1} \ \ \texttt{0} \ \ \texttt{0} \ \ \texttt{0} \\
  1: \ \ \ \ \texttt{0} \ \ \texttt{0} \ \ \texttt{0} \ \ \texttt{1} &amp; &amp;
  9: \ \ \ \ \texttt{1} \ \ \texttt{0} \ \ \texttt{0} \ \ \texttt{1} \\
  2: \ \ \ \ \texttt{0} \ \ \texttt{0} \ \ \texttt{1} \ \ \texttt{0} &amp; &amp;
  10: \ \ \ \ \texttt{1} \ \ \texttt{0} \ \ \texttt{1} \ \ \texttt{0}
\end{align*}
$$

This is unbounded—you can represent arbitrarily large positions by adding bits—but still discrete. The transformer solution is **sinusoidal position embedding**, a continuous version of binary encoding:

$$
\text{Pos}(t, i) =
\begin{cases}
\sin\left(\dfrac{t}{10000^{2i/d}}\right), &amp; \text{if } i \text{ is even} \\
\cos\left(\dfrac{t}{10000^{2i/d}}\right), &amp; \text{if } i \text{ is odd}
\end{cases}
$$

where $t$ is the position index and $i$ is the dimension index. This encoding has three critical properties:

1. **Continuous**: Smooth interpolation between positions
2. **Bounded**: All values lie in $[-1, 1]$
3. **Relative distance preservation**: The dot product $\text{Pos}(t) \cdot \text{Pos}(t+k)$ depends only on the offset $k$, not the absolute position $t$

```{figure} https://kazemnejad.com/img/transformer_architecture_positional_encoding/positional_encoding.png
:name: transformer-position-embedding
:alt: Transformer Position Embedding
:width: 80%
:align: center

Sinusoidal position embeddings exhibit periodic patterns across dimensions. Image from [Amirhossein Kazemnejad](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/).</code></pre>
<p>Notice the alternating pattern—just like binary encoding, but continuous. Low-frequency dimensions (right) flip slowly across positions; high-frequency dimensions (left) flip rapidly. This creates a unique fingerprint for each position while preserving distance relationships.</p>
<p>```doufuilu https://kazemnejad.com/img/transformer_architecture_positional_encoding/time-steps_dot_product.png :name: transformer-position-embedding-similarity :alt: Transformer Position Embedding Similarity :width: 80% :align: center</p>
<p>Dot product between position embeddings depends only on relative distance, not absolute position. Image from <a href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/">Amirhossein Kazemnejad</a>.</p>
<pre><code>
The position embedding is added directly to the token embedding: $x_{t,i} := x_{t,i} + \text{Pos}(t, i)$. Why addition instead of concatenation? Concatenation would increase the model dimension, adding parameters. Addition creates an interesting interaction in the attention mechanism—queries and keys now encode both content and position, allowing the model to attend based on both "what" (semantic similarity) and "where" (positional proximity).

## The Takeaway

Transformers replaced sequential computation with parallel relationship mapping. Every position simultaneously computes its context from every other position. This architectural shift—from recurrent bottlenecks to parallel attention—is what allowed language models to scale from millions to hundreds of billions of parameters. The mechanism is simple: query, key, value. The result is GPT-4.

```{footbibliography}
:style: unsrt
:filter: docname in docnames</code></pre>
<script src="https://cdn.jsdelivr.net/npm/@marimo-team/marimo-snippets@1"></script>


<!-- -->

</section>
</section>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb10" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Transformers"</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="an">jupytext:</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co">  formats: md:myst</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co">  text_representation:</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co">    extension: .md</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co">    format_name: myst</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="an">kernelspec:</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="co">  display_name: Python 3</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co">  language: python</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co">  name: python3</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="co">  enabled: true</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="an">filters:</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="co">  - marimo-team/marimo</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="fu"># Transformers</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>::::{grid} 1</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>:class-container: spoiler-block</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>:::{grid-item-card} Spoiler</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>Transformers don't process sequences; they process relationships between every position simultaneously.</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>::::</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Mechanism</span></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>You've been taught to think of language models as sequential processors—reading left to right, one word triggering the next, like dominoes falling. This intuition comes from recurrent neural networks (RNNs), where information flows step by step, each word depending on the hidden state from the previous word. The transformer architecture throws this away entirely.</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>Instead of sequential processing, transformers operate through **parallel relationship mapping**. When you read "The cat sat on the mat because it was tired," you don't actually process word-by-word in isolation. Your brain simultaneously evaluates which words relate to which—"it" connects to "cat," "tired" explains "sat," "mat" anchors "on." Transformers formalize this intuition mathematically. Every position in the input sequence simultaneously computes its relationship to every other position. The mechanism is attention, and the result is a system where context flows in all directions at once, not just forward through time.</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>This parallelism is why transformers scaled when RNNs didn't. Recurrent architectures impose sequential computation—you can't process word 100 until you've processed word 99. Transformers eliminate this bottleneck. Every position can be computed in parallel, which means training time scales with sequence complexity, not sequence length. This architectural shift is what enabled GPT-3, GPT-4, and Claude to exist.</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Architecture</span></span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>Modern LLMs stack multiple **transformer blocks**—modular units that take a sequence of token vectors as input and output a transformed sequence of the same length. GPT-3 uses 96 of these blocks; GPT-4 likely uses more. Each block refines the representation, adding layers of contextual understanding.</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a><span class="in">```{figure} ../figs/transformer-overview.jpg</span></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a><span class="in">:name: transformer-overview</span></span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a><span class="in">:alt: Transformer Overview</span></span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a><span class="in">:width: 50%</span></span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a><span class="in">:align: center</span></span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a><span class="in">The basic architecture of the transformer-based LLMs.</span></span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>These blocks come in two forms: **encoders** and **decoders**. The encoder processes the input sequence and builds a contextualized representation. The decoder generates the output sequence, attending to both its own previous outputs and the encoder's representation. For translation tasks ("I love you" → "Je t'aime"), the encoder processes English, the decoder generates French. For language modeling (GPT-style systems), only the decoder is used—it generates text autoregressively, predicting the next token based on all previous tokens.</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a><span class="in">```{figure} ../figs/transformer-encoder-decoder.jpg</span></span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a><span class="in">:name: transformer-encoder-decoder</span></span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a><span class="in">:alt: Transformer Encoder-Decoder</span></span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a><span class="in">:width: 80%</span></span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a><span class="in">:align: center</span></span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a><span class="in">The encoder-decoder architecture. The encoder builds a representation of the input sequence; the decoder generates the output sequence while attending to the encoder's output.</span></span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a>Inside each block are three core components: **multi-head attention** (the relationship mapper), **layer normalization** (numerical stabilization), and **feed-forward networks** (nonlinear transformation). We'll build these components step by step.</span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a><span class="in">```{figure} ../figs/transformer-component.jpg</span></span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a><span class="in">:name: transformer-wired-components</span></span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a><span class="in">:alt: Transformer Wired Components</span></span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a><span class="in">:width: 80%</span></span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a><span class="in">:align: center</span></span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a><span class="in">Internal structure of encoder and decoder blocks.</span></span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a><span class="fu">## Attention: The Relationship Engine</span></span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-74"><a href="#cb10-74" aria-hidden="true" tabindex="-1"></a>**Self-attention**—the core of the transformer—computes how much each position in a sequence should "attend to" every other position. Unlike earlier attention mechanisms in seq2seq models, which attended from one sentence to another, self-attention operates within a single sequence. It answers the question: "Given this word, which other words matter most?"</span>
<span id="cb10-75"><a href="#cb10-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-76"><a href="#cb10-76" aria-hidden="true" tabindex="-1"></a><span class="in">```{figure} ../figs/transformer-attention.jpg</span></span>
<span id="cb10-77"><a href="#cb10-77" aria-hidden="true" tabindex="-1"></a><span class="in">:name: transformer-attention</span></span>
<span id="cb10-78"><a href="#cb10-78" aria-hidden="true" tabindex="-1"></a><span class="in">:alt: Attention Mechanism</span></span>
<span id="cb10-79"><a href="#cb10-79" aria-hidden="true" tabindex="-1"></a><span class="in">:width: 80%</span></span>
<span id="cb10-80"><a href="#cb10-80" aria-hidden="true" tabindex="-1"></a><span class="in">:align: center</span></span>
<span id="cb10-81"><a href="#cb10-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-82"><a href="#cb10-82" aria-hidden="true" tabindex="-1"></a><span class="in">The attention mechanism computes relationships between all positions simultaneously.</span></span>
<span id="cb10-83"><a href="#cb10-83" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-84"><a href="#cb10-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-85"><a href="#cb10-85" aria-hidden="true" tabindex="-1"></a>For each word, the attention mechanism creates three vectors: **query** ($Q$), **key** ($K$), and **value** ($V$). Think of these as a library search: the query is what you're looking for, the keys are book titles, and the values are the actual content. When you search for "machine learning" (your query), you match it against book titles (keys) to find relevant content (values).</span>
<span id="cb10-86"><a href="#cb10-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-87"><a href="#cb10-87" aria-hidden="true" tabindex="-1"></a>Mathematically, each of these vectors is created by a learned linear transformation of the input word embedding. Given an input embedding $x$, we compute:</span>
<span id="cb10-88"><a href="#cb10-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-89"><a href="#cb10-89" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-90"><a href="#cb10-90" aria-hidden="true" tabindex="-1"></a>Q = x W_Q, \quad K = x W_K, \quad V = x W_V</span>
<span id="cb10-91"><a href="#cb10-91" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-92"><a href="#cb10-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-93"><a href="#cb10-93" aria-hidden="true" tabindex="-1"></a>where $W_Q$, $W_K$, and $W_V$ are learned weight matrices. The attention mechanism then computes which keys are most relevant to each query using the dot product, which measures vector similarity. The dot product $QK^T$ produces a matrix of attention scores—large values indicate strong relationships, small values indicate weak ones.</span>
<span id="cb10-94"><a href="#cb10-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-95"><a href="#cb10-95" aria-hidden="true" tabindex="-1"></a>These raw scores are scaled by $\sqrt{d_k}$ (the square root of the key dimension) to prevent extreme values, then normalized using softmax to produce a probability distribution. Finally, these normalized attention weights are used to compute a weighted sum of the value vectors. The complete operation is:</span>
<span id="cb10-96"><a href="#cb10-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-97"><a href="#cb10-97" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-98"><a href="#cb10-98" aria-hidden="true" tabindex="-1"></a>\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V</span>
<span id="cb10-99"><a href="#cb10-99" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-100"><a href="#cb10-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-101"><a href="#cb10-101" aria-hidden="true" tabindex="-1"></a>where $Q \in \mathbb{R}^{n \times d_k}$, $K \in \mathbb{R}^{n \times d_k}$, and $V \in \mathbb{R}^{n \times d_v}$ represent matrices containing $n$ query, key, and value vectors respectively.</span>
<span id="cb10-102"><a href="#cb10-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-103"><a href="#cb10-103" aria-hidden="true" tabindex="-1"></a>The interactive visualization below demonstrates how learned Query and Key transformations produce different attention patterns. Adjust the transformation parameters to see how different $W_Q$ and $W_K$ matrices change which words attend to which:</span>
<span id="cb10-104"><a href="#cb10-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-105"><a href="#cb10-105" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">div</span><span class="dt">&gt;</span></span>
<span id="cb10-106"><a href="#cb10-106" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">marimo-iframe</span><span class="ot"> data-height</span><span class="op">=</span><span class="st">"700px"</span><span class="ot"> data-show-code</span><span class="op">=</span><span class="st">"false"</span><span class="dt">&gt;</span></span>
<span id="cb10-107"><a href="#cb10-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-108"><a href="#cb10-108" aria-hidden="true" tabindex="-1"></a><span class="in">```python {marimo}</span></span>
<span id="cb10-109"><a href="#cb10-109" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> marimo <span class="im">as</span> mo</span>
<span id="cb10-110"><a href="#cb10-110" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-111"><a href="#cb10-111" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb10-112"><a href="#cb10-112" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> altair <span class="im">as</span> alt</span>
<span id="cb10-113"><a href="#cb10-113" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-114"><a href="#cb10-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-115"><a href="#cb10-115" aria-hidden="true" tabindex="-1"></a><span class="in">```python {marimo}</span></span>
<span id="cb10-116"><a href="#cb10-116" aria-hidden="true" tabindex="-1"></a>attention_words <span class="op">=</span> [<span class="st">"bank"</span>, <span class="st">"money"</span>, <span class="st">"loan"</span>, <span class="st">"river"</span>, <span class="st">"shore"</span>]</span>
<span id="cb10-117"><a href="#cb10-117" aria-hidden="true" tabindex="-1"></a>attention_embeddings <span class="op">=</span> np.array([</span>
<span id="cb10-118"><a href="#cb10-118" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.0</span>, <span class="fl">0.0</span>],  <span class="co"># bank (center)</span></span>
<span id="cb10-119"><a href="#cb10-119" aria-hidden="true" tabindex="-1"></a>    [<span class="op">-</span><span class="fl">0.8</span>, <span class="op">-</span><span class="fl">0.3</span>],  <span class="co"># money</span></span>
<span id="cb10-120"><a href="#cb10-120" aria-hidden="true" tabindex="-1"></a>    [<span class="op">-</span><span class="fl">0.7</span>, <span class="op">-</span><span class="fl">0.6</span>],  <span class="co"># loan</span></span>
<span id="cb10-121"><a href="#cb10-121" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.7</span>, <span class="op">-</span><span class="fl">0.5</span>],  <span class="co"># river</span></span>
<span id="cb10-122"><a href="#cb10-122" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.6</span>, <span class="op">-</span><span class="fl">0.7</span>],  <span class="co"># shore</span></span>
<span id="cb10-123"><a href="#cb10-123" aria-hidden="true" tabindex="-1"></a>]) <span class="op">*</span> <span class="dv">2</span></span>
<span id="cb10-124"><a href="#cb10-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-125"><a href="#cb10-125" aria-hidden="true" tabindex="-1"></a><span class="co"># Query controls</span></span>
<span id="cb10-126"><a href="#cb10-126" aria-hidden="true" tabindex="-1"></a>q_scale_x <span class="op">=</span> mo.ui.slider(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="fl">0.1</span>, value<span class="op">=</span><span class="fl">1.0</span>, label<span class="op">=</span><span class="st">"Q Scale X"</span>)</span>
<span id="cb10-127"><a href="#cb10-127" aria-hidden="true" tabindex="-1"></a>q_scale_y <span class="op">=</span> mo.ui.slider(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="fl">0.1</span>, value<span class="op">=</span><span class="fl">1.0</span>, label<span class="op">=</span><span class="st">"Q Scale Y"</span>)</span>
<span id="cb10-128"><a href="#cb10-128" aria-hidden="true" tabindex="-1"></a>q_rotate <span class="op">=</span> mo.ui.slider(<span class="op">-</span><span class="dv">180</span>, <span class="dv">180</span>, <span class="dv">5</span>, value<span class="op">=</span><span class="dv">0</span>, label<span class="op">=</span><span class="st">"Q Rotate (deg)"</span>)</span>
<span id="cb10-129"><a href="#cb10-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-130"><a href="#cb10-130" aria-hidden="true" tabindex="-1"></a><span class="co"># Key controls</span></span>
<span id="cb10-131"><a href="#cb10-131" aria-hidden="true" tabindex="-1"></a>k_scale_x <span class="op">=</span> mo.ui.slider(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="fl">0.1</span>, value<span class="op">=</span><span class="fl">1.0</span>, label<span class="op">=</span><span class="st">"K Scale X"</span>)</span>
<span id="cb10-132"><a href="#cb10-132" aria-hidden="true" tabindex="-1"></a>k_scale_y <span class="op">=</span> mo.ui.slider(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="fl">0.1</span>, value<span class="op">=</span><span class="fl">1.0</span>, label<span class="op">=</span><span class="st">"K Scale Y"</span>)</span>
<span id="cb10-133"><a href="#cb10-133" aria-hidden="true" tabindex="-1"></a>k_rotate <span class="op">=</span> mo.ui.slider(<span class="op">-</span><span class="dv">180</span>, <span class="dv">180</span>, <span class="dv">5</span>, value<span class="op">=</span><span class="dv">0</span>, label<span class="op">=</span><span class="st">"K Rotate (deg)"</span>)</span>
<span id="cb10-134"><a href="#cb10-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-135"><a href="#cb10-135" aria-hidden="true" tabindex="-1"></a>q_controls <span class="op">=</span> mo.vstack([mo.md(<span class="st">"**Query Transformation**"</span>), q_scale_x, q_scale_y, q_rotate])</span>
<span id="cb10-136"><a href="#cb10-136" aria-hidden="true" tabindex="-1"></a>k_controls <span class="op">=</span> mo.vstack([mo.md(<span class="st">"**Key Transformation**"</span>), k_scale_x, k_scale_y, k_rotate])</span>
<span id="cb10-137"><a href="#cb10-137" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-138"><a href="#cb10-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-139"><a href="#cb10-139" aria-hidden="true" tabindex="-1"></a><span class="in">```python {marimo}</span></span>
<span id="cb10-140"><a href="#cb10-140" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _transform_embeddings(emb, scale_x, scale_y, rotate_deg):</span>
<span id="cb10-141"><a href="#cb10-141" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.radians(rotate_deg)</span>
<span id="cb10-142"><a href="#cb10-142" aria-hidden="true" tabindex="-1"></a>    rot_matrix <span class="op">=</span> np.array([[np.cos(theta), <span class="op">-</span>np.sin(theta)], [np.sin(theta), np.cos(theta)]])</span>
<span id="cb10-143"><a href="#cb10-143" aria-hidden="true" tabindex="-1"></a>    scale_matrix <span class="op">=</span> np.diag([scale_x, scale_y])</span>
<span id="cb10-144"><a href="#cb10-144" aria-hidden="true" tabindex="-1"></a>    W <span class="op">=</span> rot_matrix <span class="op">@</span> scale_matrix</span>
<span id="cb10-145"><a href="#cb10-145" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> emb <span class="op">@</span> W.T</span>
<span id="cb10-146"><a href="#cb10-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-147"><a href="#cb10-147" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> _transform_embeddings(attention_embeddings, q_scale_x.value, q_scale_y.value, q_rotate.value)</span>
<span id="cb10-148"><a href="#cb10-148" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> _transform_embeddings(attention_embeddings, k_scale_x.value, k_scale_y.value, k_rotate.value)</span>
<span id="cb10-149"><a href="#cb10-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-150"><a href="#cb10-150" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute attention scores</span></span>
<span id="cb10-151"><a href="#cb10-151" aria-hidden="true" tabindex="-1"></a>_scores <span class="op">=</span> Q <span class="op">@</span> K.T</span>
<span id="cb10-152"><a href="#cb10-152" aria-hidden="true" tabindex="-1"></a>_exp_scores <span class="op">=</span> np.exp(_scores <span class="op">-</span> np.<span class="bu">max</span>(_scores, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb10-153"><a href="#cb10-153" aria-hidden="true" tabindex="-1"></a>attention_weights <span class="op">=</span> _exp_scores <span class="op">/</span> np.<span class="bu">sum</span>(_exp_scores, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-154"><a href="#cb10-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-155"><a href="#cb10-155" aria-hidden="true" tabindex="-1"></a><span class="co"># Create visualizations</span></span>
<span id="cb10-156"><a href="#cb10-156" aria-hidden="true" tabindex="-1"></a>_df_q <span class="op">=</span> pd.DataFrame({<span class="st">"word"</span>: attention_words, <span class="st">"x"</span>: Q[:, <span class="dv">0</span>], <span class="st">"y"</span>: Q[:, <span class="dv">1</span>]})</span>
<span id="cb10-157"><a href="#cb10-157" aria-hidden="true" tabindex="-1"></a>_df_k <span class="op">=</span> pd.DataFrame({<span class="st">"word"</span>: attention_words, <span class="st">"x"</span>: K[:, <span class="dv">0</span>], <span class="st">"y"</span>: K[:, <span class="dv">1</span>]})</span>
<span id="cb10-158"><a href="#cb10-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-159"><a href="#cb10-159" aria-hidden="true" tabindex="-1"></a>_chart_q <span class="op">=</span> alt.Chart(_df_q).mark_circle(size<span class="op">=</span><span class="dv">100</span>).encode(</span>
<span id="cb10-160"><a href="#cb10-160" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>alt.X(<span class="st">'x:Q'</span>, scale<span class="op">=</span>alt.Scale(domain<span class="op">=</span>[<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>]), title<span class="op">=</span><span class="st">'Q1'</span>),</span>
<span id="cb10-161"><a href="#cb10-161" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>alt.Y(<span class="st">'y:Q'</span>, scale<span class="op">=</span>alt.Scale(domain<span class="op">=</span>[<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>]), title<span class="op">=</span><span class="st">'Q2'</span>),</span>
<span id="cb10-162"><a href="#cb10-162" aria-hidden="true" tabindex="-1"></a>    tooltip<span class="op">=</span>[<span class="st">'word:N'</span>]</span>
<span id="cb10-163"><a href="#cb10-163" aria-hidden="true" tabindex="-1"></a>).properties(width<span class="op">=</span><span class="dv">200</span>, height<span class="op">=</span><span class="dv">200</span>, title<span class="op">=</span><span class="st">"Query (Q)"</span>)</span>
<span id="cb10-164"><a href="#cb10-164" aria-hidden="true" tabindex="-1"></a>_text_q <span class="op">=</span> _chart_q.mark_text(dy<span class="op">=-</span><span class="dv">12</span>, fontSize<span class="op">=</span><span class="dv">10</span>, fontWeight<span class="op">=</span><span class="st">'bold'</span>).encode(text<span class="op">=</span><span class="st">'word:N'</span>)</span>
<span id="cb10-165"><a href="#cb10-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-166"><a href="#cb10-166" aria-hidden="true" tabindex="-1"></a>_chart_k <span class="op">=</span> alt.Chart(_df_k).mark_circle(size<span class="op">=</span><span class="dv">100</span>).encode(</span>
<span id="cb10-167"><a href="#cb10-167" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>alt.X(<span class="st">'x:Q'</span>, scale<span class="op">=</span>alt.Scale(domain<span class="op">=</span>[<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>]), title<span class="op">=</span><span class="st">'K1'</span>),</span>
<span id="cb10-168"><a href="#cb10-168" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>alt.Y(<span class="st">'y:Q'</span>, scale<span class="op">=</span>alt.Scale(domain<span class="op">=</span>[<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>]), title<span class="op">=</span><span class="st">'K2'</span>),</span>
<span id="cb10-169"><a href="#cb10-169" aria-hidden="true" tabindex="-1"></a>    tooltip<span class="op">=</span>[<span class="st">'word:N'</span>]</span>
<span id="cb10-170"><a href="#cb10-170" aria-hidden="true" tabindex="-1"></a>).properties(width<span class="op">=</span><span class="dv">200</span>, height<span class="op">=</span><span class="dv">200</span>, title<span class="op">=</span><span class="st">"Key (K)"</span>)</span>
<span id="cb10-171"><a href="#cb10-171" aria-hidden="true" tabindex="-1"></a>_text_k <span class="op">=</span> _chart_k.mark_text(dy<span class="op">=-</span><span class="dv">12</span>, fontSize<span class="op">=</span><span class="dv">10</span>, fontWeight<span class="op">=</span><span class="st">'bold'</span>).encode(text<span class="op">=</span><span class="st">'word:N'</span>)</span>
<span id="cb10-172"><a href="#cb10-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-173"><a href="#cb10-173" aria-hidden="true" tabindex="-1"></a><span class="co"># Heatmap</span></span>
<span id="cb10-174"><a href="#cb10-174" aria-hidden="true" tabindex="-1"></a>_heatmap_data <span class="op">=</span> []</span>
<span id="cb10-175"><a href="#cb10-175" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, word_i <span class="kw">in</span> <span class="bu">enumerate</span>(attention_words):</span>
<span id="cb10-176"><a href="#cb10-176" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j, word_j <span class="kw">in</span> <span class="bu">enumerate</span>(attention_words):</span>
<span id="cb10-177"><a href="#cb10-177" aria-hidden="true" tabindex="-1"></a>        _heatmap_data.append({<span class="st">"Query"</span>: word_i, <span class="st">"Key"</span>: word_j, <span class="st">"Weight"</span>: attention_weights[i, j]})</span>
<span id="cb10-178"><a href="#cb10-178" aria-hidden="true" tabindex="-1"></a>_df_heatmap <span class="op">=</span> pd.DataFrame(_heatmap_data)</span>
<span id="cb10-179"><a href="#cb10-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-180"><a href="#cb10-180" aria-hidden="true" tabindex="-1"></a>_heatmap <span class="op">=</span> alt.Chart(_df_heatmap).mark_rect().encode(</span>
<span id="cb10-181"><a href="#cb10-181" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>alt.X(<span class="st">'Key:N'</span>, title<span class="op">=</span><span class="st">'Key Word'</span>),</span>
<span id="cb10-182"><a href="#cb10-182" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>alt.Y(<span class="st">'Query:N'</span>, title<span class="op">=</span><span class="st">'Query Word'</span>),</span>
<span id="cb10-183"><a href="#cb10-183" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span>alt.Color(<span class="st">'Weight:Q'</span>, scale<span class="op">=</span>alt.Scale(scheme<span class="op">=</span><span class="st">'blues'</span>), title<span class="op">=</span><span class="st">'Attention'</span>),</span>
<span id="cb10-184"><a href="#cb10-184" aria-hidden="true" tabindex="-1"></a>    tooltip<span class="op">=</span>[<span class="st">'Query:N'</span>, <span class="st">'Key:N'</span>, alt.Tooltip(<span class="st">'Weight:Q'</span>, <span class="bu">format</span><span class="op">=</span><span class="st">'.3f'</span>)]</span>
<span id="cb10-185"><a href="#cb10-185" aria-hidden="true" tabindex="-1"></a>).properties(width<span class="op">=</span><span class="dv">250</span>, height<span class="op">=</span><span class="dv">250</span>, title<span class="op">=</span><span class="st">"Attention Weights (Softmax)"</span>)</span>
<span id="cb10-186"><a href="#cb10-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-187"><a href="#cb10-187" aria-hidden="true" tabindex="-1"></a>mo.vstack([</span>
<span id="cb10-188"><a href="#cb10-188" aria-hidden="true" tabindex="-1"></a>    mo.hstack([q_controls, k_controls], align<span class="op">=</span><span class="st">"center"</span>),</span>
<span id="cb10-189"><a href="#cb10-189" aria-hidden="true" tabindex="-1"></a>    mo.hstack([_chart_q <span class="op">+</span> _text_q, _chart_k <span class="op">+</span> _text_k, _heatmap], align<span class="op">=</span><span class="st">"center"</span>)</span>
<span id="cb10-190"><a href="#cb10-190" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb10-191"><a href="#cb10-191" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-192"><a href="#cb10-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-193"><a href="#cb10-193" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">marimo-iframe</span><span class="dt">&gt;</span></span>
<span id="cb10-194"><a href="#cb10-194" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">div</span><span class="dt">&gt;</span></span>
<span id="cb10-195"><a href="#cb10-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-196"><a href="#cb10-196" aria-hidden="true" tabindex="-1"></a>The output is a **contextualized vector** for each word—a representation that changes based on surrounding context. The word "bank" produces different vectors in "river bank" versus "financial bank" because the attention mechanism incorporates information from neighboring words.</span>
<span id="cb10-197"><a href="#cb10-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-198"><a href="#cb10-198" aria-hidden="true" tabindex="-1"></a>To see this in action, consider how we might contextualize the word "bank" by mixing it with surrounding words. The visualization below shows static word embeddings—notice how "bank" sits neutrally between financial terms (money, loan) and geographical terms (river, shore).</span>
<span id="cb10-199"><a href="#cb10-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-200"><a href="#cb10-200" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">div</span><span class="dt">&gt;</span></span>
<span id="cb10-201"><a href="#cb10-201" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">marimo-iframe</span><span class="ot"> data-height</span><span class="op">=</span><span class="st">"400px"</span><span class="ot"> data-show-code</span><span class="op">=</span><span class="st">"false"</span><span class="dt">&gt;</span></span>
<span id="cb10-202"><a href="#cb10-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-203"><a href="#cb10-203" aria-hidden="true" tabindex="-1"></a><span class="in">```python {marimo}</span></span>
<span id="cb10-204"><a href="#cb10-204" aria-hidden="true" tabindex="-1"></a>static_words <span class="op">=</span> [<span class="st">"bank"</span>, <span class="st">"money"</span>, <span class="st">"loan"</span>, <span class="st">"river"</span>, <span class="st">"shore"</span>]</span>
<span id="cb10-205"><a href="#cb10-205" aria-hidden="true" tabindex="-1"></a>static_embeddings <span class="op">=</span> np.array([</span>
<span id="cb10-206"><a href="#cb10-206" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.0</span>, <span class="fl">0.0</span>],  <span class="co"># bank (center)</span></span>
<span id="cb10-207"><a href="#cb10-207" aria-hidden="true" tabindex="-1"></a>    [<span class="op">-</span><span class="fl">0.8</span>, <span class="op">-</span><span class="fl">0.3</span>],  <span class="co"># money</span></span>
<span id="cb10-208"><a href="#cb10-208" aria-hidden="true" tabindex="-1"></a>    [<span class="op">-</span><span class="fl">0.7</span>, <span class="op">-</span><span class="fl">0.6</span>],  <span class="co"># loan</span></span>
<span id="cb10-209"><a href="#cb10-209" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.7</span>, <span class="op">-</span><span class="fl">0.5</span>],  <span class="co"># river</span></span>
<span id="cb10-210"><a href="#cb10-210" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.6</span>, <span class="op">-</span><span class="fl">0.7</span>],  <span class="co"># shore</span></span>
<span id="cb10-211"><a href="#cb10-211" aria-hidden="true" tabindex="-1"></a>]) <span class="op">*</span> <span class="dv">2</span></span>
<span id="cb10-212"><a href="#cb10-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-213"><a href="#cb10-213" aria-hidden="true" tabindex="-1"></a>_df_static <span class="op">=</span> pd.DataFrame({<span class="st">"word"</span>: static_words, <span class="st">"x"</span>: static_embeddings[:, <span class="dv">0</span>], <span class="st">"y"</span>: static_embeddings[:, <span class="dv">1</span>]})</span>
<span id="cb10-214"><a href="#cb10-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-215"><a href="#cb10-215" aria-hidden="true" tabindex="-1"></a>_chart_static <span class="op">=</span> alt.Chart(_df_static).mark_circle(size<span class="op">=</span><span class="dv">200</span>).encode(</span>
<span id="cb10-216"><a href="#cb10-216" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>alt.X(<span class="st">'x:Q'</span>, scale<span class="op">=</span>alt.Scale(domain<span class="op">=</span>[<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>]), title<span class="op">=</span><span class="st">'Dimension 1'</span>),</span>
<span id="cb10-217"><a href="#cb10-217" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>alt.Y(<span class="st">'y:Q'</span>, scale<span class="op">=</span>alt.Scale(domain<span class="op">=</span>[<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>]), title<span class="op">=</span><span class="st">'Dimension 2'</span>),</span>
<span id="cb10-218"><a href="#cb10-218" aria-hidden="true" tabindex="-1"></a>    text<span class="op">=</span><span class="st">'word:N'</span>,</span>
<span id="cb10-219"><a href="#cb10-219" aria-hidden="true" tabindex="-1"></a>    tooltip<span class="op">=</span>[<span class="st">'word:N'</span>, <span class="st">'x:Q'</span>, <span class="st">'y:Q'</span>]</span>
<span id="cb10-220"><a href="#cb10-220" aria-hidden="true" tabindex="-1"></a>).properties(width<span class="op">=</span><span class="dv">300</span>, height<span class="op">=</span><span class="dv">300</span>, title<span class="op">=</span><span class="st">"Static Word Embeddings"</span>)</span>
<span id="cb10-221"><a href="#cb10-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-222"><a href="#cb10-222" aria-hidden="true" tabindex="-1"></a>_text_static <span class="op">=</span> _chart_static.mark_text(dy<span class="op">=-</span><span class="dv">15</span>, fontSize<span class="op">=</span><span class="dv">14</span>, fontWeight<span class="op">=</span><span class="st">'bold'</span>).encode(text<span class="op">=</span><span class="st">'word:N'</span>)</span>
<span id="cb10-223"><a href="#cb10-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-224"><a href="#cb10-224" aria-hidden="true" tabindex="-1"></a>_chart_static <span class="op">+</span> _text_static</span>
<span id="cb10-225"><a href="#cb10-225" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-226"><a href="#cb10-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-227"><a href="#cb10-227" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">marimo-iframe</span><span class="dt">&gt;</span></span>
<span id="cb10-228"><a href="#cb10-228" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">div</span><span class="dt">&gt;</span></span>
<span id="cb10-229"><a href="#cb10-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-230"><a href="#cb10-230" aria-hidden="true" tabindex="-1"></a>Now, try adjusting the weights below to create a contextualized version of "bank." If the sentence is "Money in bank," adjust the weights to shift "bank" toward "money." If the sentence is "River bank," shift it toward "river."</span>
<span id="cb10-231"><a href="#cb10-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-232"><a href="#cb10-232" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">div</span><span class="dt">&gt;</span></span>
<span id="cb10-233"><a href="#cb10-233" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">marimo-iframe</span><span class="ot"> data-height</span><span class="op">=</span><span class="st">"500px"</span><span class="ot"> data-show-code</span><span class="op">=</span><span class="st">"false"</span><span class="dt">&gt;</span></span>
<span id="cb10-234"><a href="#cb10-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-235"><a href="#cb10-235" aria-hidden="true" tabindex="-1"></a><span class="in">```python {marimo}</span></span>
<span id="cb10-236"><a href="#cb10-236" aria-hidden="true" tabindex="-1"></a>context_words <span class="op">=</span> [<span class="st">"bank"</span>, <span class="st">"money"</span>, <span class="st">"loan"</span>, <span class="st">"river"</span>, <span class="st">"shore"</span>]</span>
<span id="cb10-237"><a href="#cb10-237" aria-hidden="true" tabindex="-1"></a>context_embeddings <span class="op">=</span> np.array([</span>
<span id="cb10-238"><a href="#cb10-238" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.0</span>, <span class="fl">0.0</span>],  <span class="co"># bank (center)</span></span>
<span id="cb10-239"><a href="#cb10-239" aria-hidden="true" tabindex="-1"></a>    [<span class="op">-</span><span class="fl">0.8</span>, <span class="op">-</span><span class="fl">0.3</span>],  <span class="co"># money</span></span>
<span id="cb10-240"><a href="#cb10-240" aria-hidden="true" tabindex="-1"></a>    [<span class="op">-</span><span class="fl">0.7</span>, <span class="op">-</span><span class="fl">0.6</span>],  <span class="co"># loan</span></span>
<span id="cb10-241"><a href="#cb10-241" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.7</span>, <span class="op">-</span><span class="fl">0.5</span>],  <span class="co"># river</span></span>
<span id="cb10-242"><a href="#cb10-242" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.6</span>, <span class="op">-</span><span class="fl">0.7</span>],  <span class="co"># shore</span></span>
<span id="cb10-243"><a href="#cb10-243" aria-hidden="true" tabindex="-1"></a>]) <span class="op">*</span> <span class="dv">2</span></span>
<span id="cb10-244"><a href="#cb10-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-245"><a href="#cb10-245" aria-hidden="true" tabindex="-1"></a>slider_bank <span class="op">=</span> mo.ui.slider(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.01</span>, value<span class="op">=</span><span class="fl">1.0</span>, label<span class="op">=</span><span class="st">"Bank Weight"</span>)</span>
<span id="cb10-246"><a href="#cb10-246" aria-hidden="true" tabindex="-1"></a>slider_money <span class="op">=</span> mo.ui.slider(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.01</span>, value<span class="op">=</span><span class="dv">0</span>, label<span class="op">=</span><span class="st">"Money Weight"</span>)</span>
<span id="cb10-247"><a href="#cb10-247" aria-hidden="true" tabindex="-1"></a>slider_loan <span class="op">=</span> mo.ui.slider(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.01</span>, value<span class="op">=</span><span class="dv">0</span>, label<span class="op">=</span><span class="st">"Loan Weight"</span>)</span>
<span id="cb10-248"><a href="#cb10-248" aria-hidden="true" tabindex="-1"></a>slider_river <span class="op">=</span> mo.ui.slider(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.01</span>, value<span class="op">=</span><span class="dv">0</span>, label<span class="op">=</span><span class="st">"River Weight"</span>)</span>
<span id="cb10-249"><a href="#cb10-249" aria-hidden="true" tabindex="-1"></a>slider_shore <span class="op">=</span> mo.ui.slider(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.01</span>, value<span class="op">=</span><span class="dv">0</span>, label<span class="op">=</span><span class="st">"Shore Weight"</span>)</span>
<span id="cb10-250"><a href="#cb10-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-251"><a href="#cb10-251" aria-hidden="true" tabindex="-1"></a>context_sliders <span class="op">=</span> mo.vstack([slider_bank, slider_money, slider_loan, slider_river, slider_shore])</span>
<span id="cb10-252"><a href="#cb10-252" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-253"><a href="#cb10-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-254"><a href="#cb10-254" aria-hidden="true" tabindex="-1"></a><span class="in">```python {marimo}</span></span>
<span id="cb10-255"><a href="#cb10-255" aria-hidden="true" tabindex="-1"></a>_weights <span class="op">=</span> np.array([slider_bank.value, slider_money.value, slider_loan.value, slider_river.value, slider_shore.value])</span>
<span id="cb10-256"><a href="#cb10-256" aria-hidden="true" tabindex="-1"></a>_total <span class="op">=</span> _weights.<span class="bu">sum</span>()</span>
<span id="cb10-257"><a href="#cb10-257" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> _total <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb10-258"><a href="#cb10-258" aria-hidden="true" tabindex="-1"></a>    _weights <span class="op">=</span> _weights <span class="op">/</span> _total</span>
<span id="cb10-259"><a href="#cb10-259" aria-hidden="true" tabindex="-1"></a>    _new_vec <span class="op">=</span> context_embeddings.T <span class="op">@</span> _weights</span>
<span id="cb10-260"><a href="#cb10-260" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb10-261"><a href="#cb10-261" aria-hidden="true" tabindex="-1"></a>    _new_vec <span class="op">=</span> np.zeros(<span class="dv">2</span>)</span>
<span id="cb10-262"><a href="#cb10-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-263"><a href="#cb10-263" aria-hidden="true" tabindex="-1"></a>_df_orig <span class="op">=</span> pd.DataFrame({<span class="st">"word"</span>: context_words, <span class="st">"x"</span>: context_embeddings[:, <span class="dv">0</span>], <span class="st">"y"</span>: context_embeddings[:, <span class="dv">1</span>], <span class="st">"type"</span>: [<span class="st">"Original"</span>] <span class="op">*</span> <span class="dv">5</span>})</span>
<span id="cb10-264"><a href="#cb10-264" aria-hidden="true" tabindex="-1"></a>_df_new <span class="op">=</span> pd.DataFrame({<span class="st">"word"</span>: [<span class="st">"Contextualized Bank"</span>], <span class="st">"x"</span>: [_new_vec[<span class="dv">0</span>]], <span class="st">"y"</span>: [_new_vec[<span class="dv">1</span>]], <span class="st">"type"</span>: [<span class="st">"Contextualized"</span>]})</span>
<span id="cb10-265"><a href="#cb10-265" aria-hidden="true" tabindex="-1"></a>_df_combined <span class="op">=</span> pd.concat([_df_orig, _df_new])</span>
<span id="cb10-266"><a href="#cb10-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-267"><a href="#cb10-267" aria-hidden="true" tabindex="-1"></a>_chart_context <span class="op">=</span> alt.Chart(_df_combined).mark_circle(size<span class="op">=</span><span class="dv">200</span>).encode(</span>
<span id="cb10-268"><a href="#cb10-268" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>alt.X(<span class="st">'x:Q'</span>, scale<span class="op">=</span>alt.Scale(domain<span class="op">=</span>[<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>]), title<span class="op">=</span><span class="st">'Dimension 1'</span>),</span>
<span id="cb10-269"><a href="#cb10-269" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>alt.Y(<span class="st">'y:Q'</span>, scale<span class="op">=</span>alt.Scale(domain<span class="op">=</span>[<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>]), title<span class="op">=</span><span class="st">'Dimension 2'</span>),</span>
<span id="cb10-270"><a href="#cb10-270" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span>alt.Color(<span class="st">'type:N'</span>, scale<span class="op">=</span>alt.Scale(domain<span class="op">=</span>[<span class="st">'Original'</span>, <span class="st">'Contextualized'</span>], <span class="bu">range</span><span class="op">=</span>[<span class="st">'#dadada'</span>, <span class="st">'#ff7f0e'</span>])),</span>
<span id="cb10-271"><a href="#cb10-271" aria-hidden="true" tabindex="-1"></a>    tooltip<span class="op">=</span>[<span class="st">'word:N'</span>, <span class="st">'x:Q'</span>, <span class="st">'y:Q'</span>]</span>
<span id="cb10-272"><a href="#cb10-272" aria-hidden="true" tabindex="-1"></a>).properties(width<span class="op">=</span><span class="dv">350</span>, height<span class="op">=</span><span class="dv">350</span>, title<span class="op">=</span><span class="st">"Contextualized Bank"</span>)</span>
<span id="cb10-273"><a href="#cb10-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-274"><a href="#cb10-274" aria-hidden="true" tabindex="-1"></a>_text_context <span class="op">=</span> _chart_context.mark_text(dy<span class="op">=-</span><span class="dv">15</span>, fontSize<span class="op">=</span><span class="dv">14</span>, fontWeight<span class="op">=</span><span class="st">'bold'</span>).encode(text<span class="op">=</span><span class="st">'word:N'</span>, color<span class="op">=</span>alt.value(<span class="st">'black'</span>))</span>
<span id="cb10-275"><a href="#cb10-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-276"><a href="#cb10-276" aria-hidden="true" tabindex="-1"></a>mo.hstack([context_sliders, _chart_context <span class="op">+</span> _text_context], align<span class="op">=</span><span class="st">"center"</span>)</span>
<span id="cb10-277"><a href="#cb10-277" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-278"><a href="#cb10-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-279"><a href="#cb10-279" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">marimo-iframe</span><span class="dt">&gt;</span></span>
<span id="cb10-280"><a href="#cb10-280" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">div</span><span class="dt">&gt;</span></span>
<span id="cb10-281"><a href="#cb10-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-282"><a href="#cb10-282" aria-hidden="true" tabindex="-1"></a>This manual weighting captures the intuition, but how do we learn which words to attend to? This is where queries and keys come in.</span>
<span id="cb10-283"><a href="#cb10-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-284"><a href="#cb10-284" aria-hidden="true" tabindex="-1"></a><span class="fu">### Multi-Head Attention: Multiple Perspectives</span></span>
<span id="cb10-285"><a href="#cb10-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-286"><a href="#cb10-286" aria-hidden="true" tabindex="-1"></a>A single attention mechanism captures one type of relationship. **Multi-head attention** runs multiple attention operations in parallel, each with different learned parameters. Each head can specialize—one might focus on syntactic dependencies (subject-verb relationships), another on semantic similarity (synonyms and antonyms), another on positional proximity (nearby words).</span>
<span id="cb10-287"><a href="#cb10-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-288"><a href="#cb10-288" aria-hidden="true" tabindex="-1"></a><span class="in">```{figure} ../figs/transformer-multihead-attention.jpg</span></span>
<span id="cb10-289"><a href="#cb10-289" aria-hidden="true" tabindex="-1"></a><span class="in">:name: transformer-multihead-attention</span></span>
<span id="cb10-290"><a href="#cb10-290" aria-hidden="true" tabindex="-1"></a><span class="in">:alt: Multi-Head Attention</span></span>
<span id="cb10-291"><a href="#cb10-291" aria-hidden="true" tabindex="-1"></a><span class="in">:width: 50%</span></span>
<span id="cb10-292"><a href="#cb10-292" aria-hidden="true" tabindex="-1"></a><span class="in">:align: center</span></span>
<span id="cb10-293"><a href="#cb10-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-294"><a href="#cb10-294" aria-hidden="true" tabindex="-1"></a><span class="in">Multi-head attention runs multiple attention operations in parallel, each capturing different relationships.</span></span>
<span id="cb10-295"><a href="#cb10-295" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-296"><a href="#cb10-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-297"><a href="#cb10-297" aria-hidden="true" tabindex="-1"></a>The outputs from all heads are concatenated and passed through a final linear transformation to produce the multi-head attention output. In the original transformer paper {footcite:p}<span class="in">`vaswani2017attention`</span>, the authors used $h=8$ attention heads, with each head using dimension $d_k = d_v = d/h = 64$, where $d=512$ is the model dimension.</span>
<span id="cb10-298"><a href="#cb10-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-299"><a href="#cb10-299" aria-hidden="true" tabindex="-1"></a><span class="fu">## Layer Normalization: Numerical Stability</span></span>
<span id="cb10-300"><a href="#cb10-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-301"><a href="#cb10-301" aria-hidden="true" tabindex="-1"></a>Deep networks suffer from numerical instability—activations can grow explosively large or vanish to zero as they propagate through layers. **Layer normalization** stabilizes training by rescaling activations to have zero mean and unit variance.</span>
<span id="cb10-302"><a href="#cb10-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-303"><a href="#cb10-303" aria-hidden="true" tabindex="-1"></a><span class="in">```{figure} https://miro.medium.com/v2/resize:fit:1400/0*Agdt1zYwfUxXMJGJ</span></span>
<span id="cb10-304"><a href="#cb10-304" aria-hidden="true" tabindex="-1"></a><span class="in">:name: transformer-layer-normalization</span></span>
<span id="cb10-305"><a href="#cb10-305" aria-hidden="true" tabindex="-1"></a><span class="in">:alt: Layer Normalization</span></span>
<span id="cb10-306"><a href="#cb10-306" aria-hidden="true" tabindex="-1"></a><span class="in">:width: 80%</span></span>
<span id="cb10-307"><a href="#cb10-307" aria-hidden="true" tabindex="-1"></a><span class="in">:align: center</span></span>
<span id="cb10-308"><a href="#cb10-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-309"><a href="#cb10-309" aria-hidden="true" tabindex="-1"></a><span class="in">Layer normalization computes mean and standard deviation across all features for each sample, then normalizes.</span></span>
<span id="cb10-310"><a href="#cb10-310" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-311"><a href="#cb10-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-312"><a href="#cb10-312" aria-hidden="true" tabindex="-1"></a>For each input vector $x$, layer normalization computes:</span>
<span id="cb10-313"><a href="#cb10-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-314"><a href="#cb10-314" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-315"><a href="#cb10-315" aria-hidden="true" tabindex="-1"></a>\text{LayerNorm}(x) = \gamma \frac{x - \mu}{\sigma} + \beta</span>
<span id="cb10-316"><a href="#cb10-316" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-317"><a href="#cb10-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-318"><a href="#cb10-318" aria-hidden="true" tabindex="-1"></a>where $\mu$ and $\sigma$ are the mean and standard deviation of $x$, and $\gamma$ and $\beta$ are learned scaling and shifting parameters (initialized to 1 and 0 respectively). This ensures that no matter how the input distribution shifts during training, each layer receives inputs in a stable numerical range.</span>
<span id="cb10-319"><a href="#cb10-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-320"><a href="#cb10-320" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Encoder Block</span></span>
<span id="cb10-321"><a href="#cb10-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-322"><a href="#cb10-322" aria-hidden="true" tabindex="-1"></a>Now we wire the components together. The **encoder block** processes the input sequence through four stages:</span>
<span id="cb10-323"><a href="#cb10-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-324"><a href="#cb10-324" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Multi-head self-attention** computes contextualized representations</span>
<span id="cb10-325"><a href="#cb10-325" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Residual connection + normalization** stabilizes training</span>
<span id="cb10-326"><a href="#cb10-326" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Feed-forward network** applies nonlinear transformation</span>
<span id="cb10-327"><a href="#cb10-327" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Residual connection + normalization** again</span>
<span id="cb10-328"><a href="#cb10-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-329"><a href="#cb10-329" aria-hidden="true" tabindex="-1"></a><span class="in">```{figure} ../figs/transformer-encoder.jpg</span></span>
<span id="cb10-330"><a href="#cb10-330" aria-hidden="true" tabindex="-1"></a><span class="in">:name: transformer-block</span></span>
<span id="cb10-331"><a href="#cb10-331" aria-hidden="true" tabindex="-1"></a><span class="in">:alt: Transformer Block</span></span>
<span id="cb10-332"><a href="#cb10-332" aria-hidden="true" tabindex="-1"></a><span class="in">:width: 50%</span></span>
<span id="cb10-333"><a href="#cb10-333" aria-hidden="true" tabindex="-1"></a><span class="in">:align: center</span></span>
<span id="cb10-334"><a href="#cb10-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-335"><a href="#cb10-335" aria-hidden="true" tabindex="-1"></a><span class="in">Information flows through multi-head attention, normalization, feed-forward networks, and final normalization.</span></span>
<span id="cb10-336"><a href="#cb10-336" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-337"><a href="#cb10-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-338"><a href="#cb10-338" aria-hidden="true" tabindex="-1"></a>The feed-forward network is a simple two-layer MLP applied independently to each position:</span>
<span id="cb10-339"><a href="#cb10-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-340"><a href="#cb10-340" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-341"><a href="#cb10-341" aria-hidden="true" tabindex="-1"></a>\text{FFN}(x) = \text{ReLU}(x W_1 + b_1) W_2 + b_2</span>
<span id="cb10-342"><a href="#cb10-342" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-343"><a href="#cb10-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-344"><a href="#cb10-344" aria-hidden="true" tabindex="-1"></a>The **residual connections** (also called skip connections) are critical for training deep networks. Instead of learning a direct mapping $f(x)$, we learn the residual:</span>
<span id="cb10-345"><a href="#cb10-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-346"><a href="#cb10-346" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-347"><a href="#cb10-347" aria-hidden="true" tabindex="-1"></a>x_{\text{out}} = x_{\text{in}} + f(x_{\text{in}})</span>
<span id="cb10-348"><a href="#cb10-348" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-349"><a href="#cb10-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-350"><a href="#cb10-350" aria-hidden="true" tabindex="-1"></a>This simple addition has profound consequences for gradient flow. During backpropagation, the gradient of the loss $\mathcal{L}$ with respect to layer $l$ is:</span>
<span id="cb10-351"><a href="#cb10-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-352"><a href="#cb10-352" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-353"><a href="#cb10-353" aria-hidden="true" tabindex="-1"></a>\frac{\partial \mathcal{L}}{\partial x_l} = \frac{\partial \mathcal{L}}{\partial x_{l+1}} \left(1 + \frac{\partial f_l}{\partial x_l}\right)</span>
<span id="cb10-354"><a href="#cb10-354" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-355"><a href="#cb10-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-356"><a href="#cb10-356" aria-hidden="true" tabindex="-1"></a>Notice the "+1" term—this provides a direct gradient path from the output back to the input. Without residual connections, gradients must pass through the chain:</span>
<span id="cb10-357"><a href="#cb10-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-358"><a href="#cb10-358" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-359"><a href="#cb10-359" aria-hidden="true" tabindex="-1"></a>\frac{\partial f_L}{\partial f_{L-1}} \cdot \frac{\partial f_{L-1}}{\partial f_{L-2}} \cdot \ldots \cdot \frac{\partial f_1}{\partial x}</span>
<span id="cb10-360"><a href="#cb10-360" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-361"><a href="#cb10-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-362"><a href="#cb10-362" aria-hidden="true" tabindex="-1"></a>If any term is less than 1, the gradient shrinks exponentially—this is the **vanishing gradient problem**. With residual connections, the gradient expansion becomes:</span>
<span id="cb10-363"><a href="#cb10-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-364"><a href="#cb10-364" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-365"><a href="#cb10-365" aria-hidden="true" tabindex="-1"></a>1 + O_1 + O_2 + O_3 + \ldots</span>
<span id="cb10-366"><a href="#cb10-366" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-367"><a href="#cb10-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-368"><a href="#cb10-368" aria-hidden="true" tabindex="-1"></a>where $O_1$ contains first-order terms, $O_2$ contains second-order products, etc. The constant "1" ensures gradients can flow even when the learned components $f_i$ produce small derivatives. This architectural innovation, originally developed for computer vision {footcite:p}<span class="in">`he2015deep`</span>, is what allows transformers to scale to hundreds of layers.</span>
<span id="cb10-369"><a href="#cb10-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-370"><a href="#cb10-370" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Decoder Block</span></span>
<span id="cb10-371"><a href="#cb10-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-372"><a href="#cb10-372" aria-hidden="true" tabindex="-1"></a>The **decoder block** extends the encoder with two modifications: **masked self-attention** and **cross-attention**.</span>
<span id="cb10-373"><a href="#cb10-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-374"><a href="#cb10-374" aria-hidden="true" tabindex="-1"></a><span class="in">```{figure} ../figs/transformer-decoder.jpg</span></span>
<span id="cb10-375"><a href="#cb10-375" aria-hidden="true" tabindex="-1"></a><span class="in">:name: transformer-decoder</span></span>
<span id="cb10-376"><a href="#cb10-376" aria-hidden="true" tabindex="-1"></a><span class="in">:alt: Transformer Decoder</span></span>
<span id="cb10-377"><a href="#cb10-377" aria-hidden="true" tabindex="-1"></a><span class="in">:width: 50%</span></span>
<span id="cb10-378"><a href="#cb10-378" aria-hidden="true" tabindex="-1"></a><span class="in">:align: center</span></span>
<span id="cb10-379"><a href="#cb10-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-380"><a href="#cb10-380" aria-hidden="true" tabindex="-1"></a><span class="in">The decoder adds masked self-attention (to prevent future peeking) and cross-attention (to access encoder outputs).</span></span>
<span id="cb10-381"><a href="#cb10-381" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-382"><a href="#cb10-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-383"><a href="#cb10-383" aria-hidden="true" tabindex="-1"></a><span class="fu">### Masked Self-Attention: Preventing Future Leakage</span></span>
<span id="cb10-384"><a href="#cb10-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-385"><a href="#cb10-385" aria-hidden="true" tabindex="-1"></a>During training, we know the entire target sequence. For translation ("I love you" → "Je t'aime"), we have both input and output. A naive decoder could "cheat" by looking at future words in the target sequence. Masked self-attention prevents this by zeroing out attention to future positions.</span>
<span id="cb10-386"><a href="#cb10-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-387"><a href="#cb10-387" aria-hidden="true" tabindex="-1"></a>The mask is implemented by setting attention scores to $-\infty$ before the softmax:</span>
<span id="cb10-388"><a href="#cb10-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-389"><a href="#cb10-389" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-390"><a href="#cb10-390" aria-hidden="true" tabindex="-1"></a>\text{MaskedAttention}(Q, K, V) = \text{softmax}\left(\frac{QK^T + M}{\sqrt{d_k}}\right)V</span>
<span id="cb10-391"><a href="#cb10-391" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-392"><a href="#cb10-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-393"><a href="#cb10-393" aria-hidden="true" tabindex="-1"></a>where $M$ is a matrix with $-\infty$ at positions $(i,j)$ where $j &gt; i$ (future positions) and 0 elsewhere. After softmax, these $-\infty$ values become zero, eliminating information flow from future tokens.</span>
<span id="cb10-394"><a href="#cb10-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-395"><a href="#cb10-395" aria-hidden="true" tabindex="-1"></a><span class="in">```{figure} https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png</span></span>
<span id="cb10-396"><a href="#cb10-396" aria-hidden="true" tabindex="-1"></a><span class="in">:name: transformer-masked-attention</span></span>
<span id="cb10-397"><a href="#cb10-397" aria-hidden="true" tabindex="-1"></a><span class="in">:alt: Masked Attention</span></span>
<span id="cb10-398"><a href="#cb10-398" aria-hidden="true" tabindex="-1"></a><span class="in">:width: 80%</span></span>
<span id="cb10-399"><a href="#cb10-399" aria-hidden="true" tabindex="-1"></a><span class="in">:align: center</span></span>
<span id="cb10-400"><a href="#cb10-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-401"><a href="#cb10-401" aria-hidden="true" tabindex="-1"></a><span class="in">Masked attention zeros out future positions, allowing parallel training without information leakage.</span></span>
<span id="cb10-402"><a href="#cb10-402" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-403"><a href="#cb10-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-404"><a href="#cb10-404" aria-hidden="true" tabindex="-1"></a>This enables **parallel training**. Instead of generating "Je", then "t'aime", then the final token sequentially, we can train all positions simultaneously—each with access only to its causal past. During inference, masking happens naturally because future tokens don't exist yet.</span>
<span id="cb10-405"><a href="#cb10-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-406"><a href="#cb10-406" aria-hidden="true" tabindex="-1"></a><span class="fu">### Cross-Attention: Connecting Encoder and Decoder</span></span>
<span id="cb10-407"><a href="#cb10-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-408"><a href="#cb10-408" aria-hidden="true" tabindex="-1"></a>The second attention layer in the decoder uses **cross-attention** to access the encoder's output. The queries ($Q$) come from the decoder's previous layer, while the keys ($K$) and values ($V$) come from the encoder's output:</span>
<span id="cb10-409"><a href="#cb10-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-410"><a href="#cb10-410" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-411"><a href="#cb10-411" aria-hidden="true" tabindex="-1"></a>\text{CrossAttention}(Q_{\text{decoder}}, K_{\text{encoder}}, V_{\text{encoder}}) = \text{softmax}\left(\frac{Q_{\text{decoder}}K_{\text{encoder}}^T}{\sqrt{d_k}}\right)V_{\text{encoder}}</span>
<span id="cb10-412"><a href="#cb10-412" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-413"><a href="#cb10-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-414"><a href="#cb10-414" aria-hidden="true" tabindex="-1"></a><span class="in">```{figure} ../figs/transformer-cross-attention.jpg</span></span>
<span id="cb10-415"><a href="#cb10-415" aria-hidden="true" tabindex="-1"></a><span class="in">:name: transformer-cross-attention</span></span>
<span id="cb10-416"><a href="#cb10-416" aria-hidden="true" tabindex="-1"></a><span class="in">:alt: Cross-Attention</span></span>
<span id="cb10-417"><a href="#cb10-417" aria-hidden="true" tabindex="-1"></a><span class="in">:width: 60%</span></span>
<span id="cb10-418"><a href="#cb10-418" aria-hidden="true" tabindex="-1"></a><span class="in">:align: center</span></span>
<span id="cb10-419"><a href="#cb10-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-420"><a href="#cb10-420" aria-hidden="true" tabindex="-1"></a><span class="in">Cross-attention allows the decoder to query the encoder's representation.</span></span>
<span id="cb10-421"><a href="#cb10-421" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-422"><a href="#cb10-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-423"><a href="#cb10-423" aria-hidden="true" tabindex="-1"></a>This is how translation works: when generating "Je", the decoder attends to "I"; when generating "t'aime", it attends to "love". The attention mechanism learns these alignments automatically from data, without explicit supervision.</span>
<span id="cb10-424"><a href="#cb10-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-425"><a href="#cb10-425" aria-hidden="true" tabindex="-1"></a><span class="fu">## Position Embedding: Encoding Order</span></span>
<span id="cb10-426"><a href="#cb10-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-427"><a href="#cb10-427" aria-hidden="true" tabindex="-1"></a>Attention is **permutation invariant**—it produces the same output regardless of input order. "The cat sat on the mat" and "mat the on sat cat the" yield identical attention outputs because the dot product doesn't encode position. We need to inject positional information.</span>
<span id="cb10-428"><a href="#cb10-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-429"><a href="#cb10-429" aria-hidden="true" tabindex="-1"></a>The naive approach is to add a position index: $x_t := x_t + \beta t$. This fails for two reasons:</span>
<span id="cb10-430"><a href="#cb10-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-431"><a href="#cb10-431" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Unbounded**: Position indices grow arbitrarily large. Models trained on sequences of length 512 fail on sequences of length 1000 because they've never seen position 513.</span>
<span id="cb10-432"><a href="#cb10-432" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Discrete**: Positions 10 and 11 are no more similar than positions 10 and 100.</span>
<span id="cb10-433"><a href="#cb10-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-434"><a href="#cb10-434" aria-hidden="true" tabindex="-1"></a>A better approach is **binary position encoding**. Represent position $t$ as a binary vector:</span>
<span id="cb10-435"><a href="#cb10-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-436"><a href="#cb10-436" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-437"><a href="#cb10-437" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb10-438"><a href="#cb10-438" aria-hidden="true" tabindex="-1"></a>  0: \ \ \ \ \texttt{0} \ \ \texttt{0} \ \ \texttt{0} \ \ \texttt{0} &amp; \quad &amp;</span>
<span id="cb10-439"><a href="#cb10-439" aria-hidden="true" tabindex="-1"></a>  8: \ \ \ \ \texttt{1} \ \ \texttt{0} \ \ \texttt{0} \ \ \texttt{0} <span class="sc">\\</span></span>
<span id="cb10-440"><a href="#cb10-440" aria-hidden="true" tabindex="-1"></a>  1: \ \ \ \ \texttt{0} \ \ \texttt{0} \ \ \texttt{0} \ \ \texttt{1} &amp; &amp;</span>
<span id="cb10-441"><a href="#cb10-441" aria-hidden="true" tabindex="-1"></a>  9: \ \ \ \ \texttt{1} \ \ \texttt{0} \ \ \texttt{0} \ \ \texttt{1} <span class="sc">\\</span></span>
<span id="cb10-442"><a href="#cb10-442" aria-hidden="true" tabindex="-1"></a>  2: \ \ \ \ \texttt{0} \ \ \texttt{0} \ \ \texttt{1} \ \ \texttt{0} &amp; &amp;</span>
<span id="cb10-443"><a href="#cb10-443" aria-hidden="true" tabindex="-1"></a>  10: \ \ \ \ \texttt{1} \ \ \texttt{0} \ \ \texttt{1} \ \ \texttt{0}</span>
<span id="cb10-444"><a href="#cb10-444" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb10-445"><a href="#cb10-445" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-446"><a href="#cb10-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-447"><a href="#cb10-447" aria-hidden="true" tabindex="-1"></a>This is unbounded—you can represent arbitrarily large positions by adding bits—but still discrete. The transformer solution is **sinusoidal position embedding**, a continuous version of binary encoding:</span>
<span id="cb10-448"><a href="#cb10-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-449"><a href="#cb10-449" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-450"><a href="#cb10-450" aria-hidden="true" tabindex="-1"></a>\text{Pos}(t, i) =</span>
<span id="cb10-451"><a href="#cb10-451" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb10-452"><a href="#cb10-452" aria-hidden="true" tabindex="-1"></a>\sin\left(\dfrac{t}{10000^{2i/d}}\right), &amp; \text{if } i \text{ is even} <span class="sc">\\</span></span>
<span id="cb10-453"><a href="#cb10-453" aria-hidden="true" tabindex="-1"></a>\cos\left(\dfrac{t}{10000^{2i/d}}\right), &amp; \text{if } i \text{ is odd}</span>
<span id="cb10-454"><a href="#cb10-454" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb10-455"><a href="#cb10-455" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-456"><a href="#cb10-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-457"><a href="#cb10-457" aria-hidden="true" tabindex="-1"></a>where $t$ is the position index and $i$ is the dimension index. This encoding has three critical properties:</span>
<span id="cb10-458"><a href="#cb10-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-459"><a href="#cb10-459" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Continuous**: Smooth interpolation between positions</span>
<span id="cb10-460"><a href="#cb10-460" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Bounded**: All values lie in $<span class="co">[</span><span class="ot">-1, 1</span><span class="co">]</span>$</span>
<span id="cb10-461"><a href="#cb10-461" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Relative distance preservation**: The dot product $\text{Pos}(t) \cdot \text{Pos}(t+k)$ depends only on the offset $k$, not the absolute position $t$</span>
<span id="cb10-462"><a href="#cb10-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-463"><a href="#cb10-463" aria-hidden="true" tabindex="-1"></a><span class="in">```{figure} https://kazemnejad.com/img/transformer_architecture_positional_encoding/positional_encoding.png</span></span>
<span id="cb10-464"><a href="#cb10-464" aria-hidden="true" tabindex="-1"></a><span class="in">:name: transformer-position-embedding</span></span>
<span id="cb10-465"><a href="#cb10-465" aria-hidden="true" tabindex="-1"></a><span class="in">:alt: Transformer Position Embedding</span></span>
<span id="cb10-466"><a href="#cb10-466" aria-hidden="true" tabindex="-1"></a><span class="in">:width: 80%</span></span>
<span id="cb10-467"><a href="#cb10-467" aria-hidden="true" tabindex="-1"></a><span class="in">:align: center</span></span>
<span id="cb10-468"><a href="#cb10-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-469"><a href="#cb10-469" aria-hidden="true" tabindex="-1"></a><span class="in">Sinusoidal position embeddings exhibit periodic patterns across dimensions. Image from [Amirhossein Kazemnejad](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/).</span></span>
<span id="cb10-470"><a href="#cb10-470" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-471"><a href="#cb10-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-472"><a href="#cb10-472" aria-hidden="true" tabindex="-1"></a>Notice the alternating pattern—just like binary encoding, but continuous. Low-frequency dimensions (right) flip slowly across positions; high-frequency dimensions (left) flip rapidly. This creates a unique fingerprint for each position while preserving distance relationships.</span>
<span id="cb10-473"><a href="#cb10-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-474"><a href="#cb10-474" aria-hidden="true" tabindex="-1"></a><span class="in">```{figure} https://kazemnejad.com/img/transformer_architecture_positional_encoding/time-steps_dot_product.png</span></span>
<span id="cb10-475"><a href="#cb10-475" aria-hidden="true" tabindex="-1"></a><span class="in">:name: transformer-position-embedding-similarity</span></span>
<span id="cb10-476"><a href="#cb10-476" aria-hidden="true" tabindex="-1"></a><span class="in">:alt: Transformer Position Embedding Similarity</span></span>
<span id="cb10-477"><a href="#cb10-477" aria-hidden="true" tabindex="-1"></a><span class="in">:width: 80%</span></span>
<span id="cb10-478"><a href="#cb10-478" aria-hidden="true" tabindex="-1"></a><span class="in">:align: center</span></span>
<span id="cb10-479"><a href="#cb10-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-480"><a href="#cb10-480" aria-hidden="true" tabindex="-1"></a><span class="in">Dot product between position embeddings depends only on relative distance, not absolute position. Image from [Amirhossein Kazemnejad](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/).</span></span>
<span id="cb10-481"><a href="#cb10-481" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-482"><a href="#cb10-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-483"><a href="#cb10-483" aria-hidden="true" tabindex="-1"></a>The position embedding is added directly to the token embedding: $x_{t,i} := x_{t,i} + \text{Pos}(t, i)$. Why addition instead of concatenation? Concatenation would increase the model dimension, adding parameters. Addition creates an interesting interaction in the attention mechanism—queries and keys now encode both content and position, allowing the model to attend based on both "what" (semantic similarity) and "where" (positional proximity).</span>
<span id="cb10-484"><a href="#cb10-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-485"><a href="#cb10-485" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Takeaway</span></span>
<span id="cb10-486"><a href="#cb10-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-487"><a href="#cb10-487" aria-hidden="true" tabindex="-1"></a>Transformers replaced sequential computation with parallel relationship mapping. Every position simultaneously computes its context from every other position. This architectural shift—from recurrent bottlenecks to parallel attention—is what allowed language models to scale from millions to hundreds of billions of parameters. The mechanism is simple: query, key, value. The result is GPT-4.</span>
<span id="cb10-488"><a href="#cb10-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-491"><a href="#cb10-491" aria-hidden="true" tabindex="-1"></a><span class="in">```{footbibliography}</span></span>
<span id="cb10-492"><a href="#cb10-492" aria-hidden="true" tabindex="-1"></a><span class="in">:style: unsrt</span></span>
<span id="cb10-493"><a href="#cb10-493" aria-hidden="true" tabindex="-1"></a><span class="in">:filter: docname in docnames</span></span>
<span id="cb10-494"><a href="#cb10-494" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-495"><a href="#cb10-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-496"><a href="#cb10-496" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">script</span><span class="ot"> src</span><span class="op">=</span><span class="st">"https://cdn.jsdelivr.net/npm/@marimo-team/marimo-snippets@1"</span><span class="dt">&gt;&lt;/</span><span class="kw">script</span><span class="dt">&gt;</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2025, Sadamori Kojaku</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions"><ul><li><a href="https://github.com/skojaku/applied-soft-comp/edit/main/m06-llms/transformers.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/skojaku/applied-soft-comp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/skojaku/applied-soft-comp">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>
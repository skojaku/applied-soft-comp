<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Sadamori Kojaku">
<meta name="dcterms.date" content="2025-11-21">

<title>Word Embeddings: Where It Started – Applied Soft Computing</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../m03-text/text-fundamentals.html" rel="next">
<link href="../m03-text/transformers.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-0348920b7671f696dc9078d39bff215e.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-fee01c958fd55f7b3b50896185ea610a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "|"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../assets/custom.css">
</head>

<body class="nav-sidebar docked nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../logo.jpg" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Applied Soft Computing</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-toolkit--workflow" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Toolkit &amp; Workflow</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-toolkit--workflow">    
        <li class="dropdown-header">─── Module 1 ───</li>
        <li>
    <a class="dropdown-item" href="../m01-toolkit/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m01-toolkit/git-github.html">
 <span class="dropdown-text">Git &amp; GitHub</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m01-toolkit/tidy-data.html">
 <span class="dropdown-text">Tidy Data</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m01-toolkit/data-provenance.html">
 <span class="dropdown-text">Data Provenance</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m01-toolkit/environments.qmd">
 <span class="dropdown-text">Environments</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-visualization" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Visualization</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-visualization">    
        <li class="dropdown-header">─── Module 2 ───</li>
        <li>
    <a class="dropdown-item" href="../m02-visualization/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m02-visualization/principles.html">
 <span class="dropdown-text">Principles</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m02-visualization/dimensionality-reduction.html">
 <span class="dropdown-text">High-Dimensional Data</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m02-visualization/networks.html">
 <span class="dropdown-text">Networks</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m02-visualization/time-series.html">
 <span class="dropdown-text">Time-Series</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-deep-learning" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Deep Learning</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-deep-learning">    
        <li class="dropdown-header">─── Module 3: Text ───</li>
        <li>
    <a class="dropdown-item" href="../m03-text/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m03-text/word2vec.md">
 <span class="dropdown-text">Word2Vec</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m03-text/lstm.md">
 <span class="dropdown-text">RNNs &amp; LSTMs</span></a>
  </li>  
        <li class="dropdown-header">─── Module 4: Images ───</li>
        <li>
    <a class="dropdown-item" href="../m04-images/overview.qmd">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m04-images/cnn.md">
 <span class="dropdown-text">CNNs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m04-images/resnet.md">
 <span class="dropdown-text">ResNet</span></a>
  </li>  
        <li class="dropdown-header">─── Module 5: Graphs ───</li>
        <li>
    <a class="dropdown-item" href="../m05-graphs/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m05-graphs/graph-embedding-w-word2vec.html">
 <span class="dropdown-text">Graph Embeddings</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m05-graphs/graph-convolutional-network.html">
 <span class="dropdown-text">GNNs</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-advanced-topics" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Advanced Topics</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-advanced-topics">    
        <li class="dropdown-header">─── Module 6: LLMs ───</li>
        <li>
    <a class="dropdown-item" href="../m06-llms/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m06-llms/transformers.md">
 <span class="dropdown-text">Transformers</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m06-llms/scaling-emergence.html">
 <span class="dropdown-text">Scaling &amp; Emergence</span></a>
  </li>  
        <li class="dropdown-header">─── Module 7: Self-Supervised ───</li>
        <li>
    <a class="dropdown-item" href="../m07-self-supervised/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m07-self-supervised/contrastive-learning.html">
 <span class="dropdown-text">Contrastive Learning</span></a>
  </li>  
        <li class="dropdown-header">─── Module 8: Explainability ───</li>
        <li>
    <a class="dropdown-item" href="../m08-explainability/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m08-explainability/fairness.html">
 <span class="dropdown-text">Fairness &amp; Ethics</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../m03-text/overview.html">Module 3: Deep Learning for Text</a></li><li class="breadcrumb-item"><a href="../m03-text/word-embeddings.html">Word Embeddings: Where It Started</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Course Information</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/welcome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About Us</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/why-applied-soft-computing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Why applied soft computing?</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/discord.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Discord</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/minidora-usage.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Using Minidora</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/how-to-submit-assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">How to submit assignment</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/deliverables.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deliverables</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 1: The Data Scientist’s Toolkit</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-toolkit/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-toolkit/git-github.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Version Control with Git &amp; GitHub</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-toolkit/tidy-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Tidy Data Philosophy</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-toolkit/data-provenance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Provenance</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-toolkit/reproduceability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Reproducibility</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 2: Visualizing Complexity</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-visualization/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-visualization/principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Principles of Effective Visualization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-visualization/1d-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing 1D Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-visualization/2d-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing 2D Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-visualization/highd-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing High-Dimensional Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-visualization/networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-visualization/time-series.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing Time-Series</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Module 3: Deep Learning for Text</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-text/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-text/llm-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Large Language Models in Practice</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-text/prompt-engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prompt Engineering for Research</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-text/embeddings-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Embeddings: How Machines Understand Meaning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-text/tokenization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Tokenization: Unboxing How LLMs Read Text</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-text/transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Transformers: The Architecture Behind the Magic</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-text/word-embeddings.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Word Embeddings: Where It Started</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-text/text-fundamentals.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Text Fundamentals: The Full Picture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-text/semantic-research.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Semantic Analysis for Research</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 4: Deep Learning for Images</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/overview.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/image-processing.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Processing Fundamentals</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/cnn.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/lenet.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LeNet Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/alexnet.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AlexNet: Deep CNN Revolution</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/vgg.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">VGG Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/inception.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Inception &amp; Multi-Scale Features</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/batch-normalization.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Batch Normalization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/resnet.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ResNet &amp; Skip Connections</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 5: Deep Learning for Graphs</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-graphs/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-graphs/spectral-embedding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Spectral Graph Embedding</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-graphs/graph-embedding-w-word2vec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Graph Embeddings with Word2Vec</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-graphs/spectral-vs-neural-embedding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Spectral vs.&nbsp;Neural Embeddings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-graphs/from-image-to-graph.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">From Images to Graphs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-graphs/graph-convolutional-network.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Graph Convolutional Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-graphs/popular-gnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Popular GNN Architectures</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-graphs/software.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">GNN Software &amp; Tools</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 6: Large Language Models &amp; Emergent Behavior</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-llms/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-llms/transformers.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Transformer Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-llms/bert.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">BERT &amp; Contextual Embeddings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-llms/sentence-bert.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sentence-BERT for Semantic Similarity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-llms/gpt.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">GPT &amp; Generative Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-llms/from-language-model-to-instruction-following.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">From Language Models to Instruction Following</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-llms/prompt-tuning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prompt Engineering &amp; In-Context Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-llms/scaling-emergence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Scaling Laws &amp; Emergent Abilities</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-llms/llms-as-complex-systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LLMs as Complex Systems</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 7: Self-Supervised Learning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m07-self-supervised/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m07-self-supervised/paradigm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Self-Supervised Paradigm</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m07-self-supervised/contrastive-learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Contrastive Learning (SimCLR)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m07-self-supervised/graphs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Self-Supervised Learning for Graphs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m07-self-supervised/time-series.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Self-Supervised Learning for Time-Series</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 8: Explainability &amp; Ethics</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-explainability/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-explainability/need.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Need for Explainability</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-explainability/attention.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Attention Visualization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-explainability/lime-shap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LIME &amp; SHAP</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-explainability/fairness.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Algorithmic Fairness &amp; Bias</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-explainability/causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Causality vs.&nbsp;Correlation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false">
 <span class="menu-text">Legacy Materials</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-text/what-to-learn.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Word &amp; Document Embeddings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-text/what-to-learn.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Recurrent Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-images/what-to-learn.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Processing (CNNs)</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../m03-text/overview.html">Module 3: Deep Learning for Text</a></li><li class="breadcrumb-item"><a href="../m03-text/word-embeddings.html">Word Embeddings: Where It Started</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Word Embeddings: Where It Started</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Sadamori Kojaku </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 21, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="before-transformers-we-had-word2vec" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Before Transformers, We Had Word2vec</h1>
<p>You’ve seen modern contextual embeddings from transformers. They’re powerful, capturing nuanced meaning that depends on context. But they’re also complex, computationally expensive, and sometimes overkill for simple tasks.</p>
<p>Before BERT and GPT, there was <strong>Word2vec</strong>—introduced in 2013 by Tomas Mikolov at Google. It’s simpler, faster, and produces <strong>static embeddings</strong>: each word gets one fixed vector, regardless of context.</p>
<p>Word2vec might seem outdated compared to transformers, but it’s still widely used because: - <strong>Fast</strong>: Train on millions of documents in minutes - <strong>Lightweight</strong>: Small models (~100MB) vs.&nbsp;gigabytes for transformers - <strong>Interpretable</strong>: Captures explicit semantic relationships (“king” - “man” + “woman” = “queen”) - <strong>Good enough</strong>: For many tasks, static embeddings work just fine</p>
<p>This section explains where embeddings came from, how Word2vec works intuitively, and when to use static vs.&nbsp;contextual embeddings.</p>
<section id="the-distributional-hypothesis" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="the-distributional-hypothesis"><span class="header-section-number">1.1</span> The Distributional Hypothesis</h2>
<p>Word2vec is built on a simple but profound idea:</p>
<blockquote class="blockquote">
<p><strong>“You shall know a word by the company it keeps.”</strong> — J.R. Firth, 1957</p>
</blockquote>
<p>Words that appear in similar contexts tend to have similar meanings.</p>
<section id="example" class="level3">
<h3 class="anchored" data-anchor-id="example">Example</h3>
<p>Consider these sentences: - “The <strong>cat</strong> sat on the mat.” - “The <strong>dog</strong> sat on the mat.” - “The <strong>cat</strong> chased the mouse.” - “The <strong>dog</strong> chased the rabbit.”</p>
<p>“Cat” and “dog” appear in similar contexts (“sat on the mat”, “chased…”). Therefore, they should have similar embeddings.</p>
<p>Now consider: - “The <strong>theorem</strong> was proved in 1995.” - “The <strong>conjecture</strong> was proved in 1995.”</p>
<p>“Theorem” and “conjecture” also appear in similar contexts, so they should have similar embeddings—even though they’re very different from “cat” and “dog.”</p>
<p><strong>Key insight</strong>: We don’t need to manually encode that “cat” is an animal or “theorem” is a mathematical statement. The model learns these relationships automatically from context.</p>
</section>
</section>
<section id="word2vec-the-core-idea" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="word2vec-the-core-idea"><span class="header-section-number">1.2</span> Word2vec: The Core Idea</h2>
<p>Word2vec learns embeddings by training a simple neural network to predict: 1. <strong>Skip-gram</strong>: Given a word, predict its context 2. <strong>CBOW (Continuous Bag-of-Words)</strong>: Given context, predict the word</p>
<p>Both approaches lead to similar embeddings. We’ll focus on <strong>Skip-gram</strong> because it’s more intuitive.</p>
<section id="skip-gram-objective" class="level3">
<h3 class="anchored" data-anchor-id="skip-gram-objective">Skip-Gram Objective</h3>
<p><strong>Training setup</strong>:</p>
<pre><code>Sentence: "The cat sat on the mat"
Target word: "cat"
Context window (size=2): ["The", "sat"]

Task: Given "cat", predict you'll see "The" and "sat" nearby</code></pre>
<p>The model learns embeddings such that: - Words with similar contexts get similar embeddings - Embeddings encode semantic relationships</p>
</section>
<section id="training-process-simplified" class="level3">
<h3 class="anchored" data-anchor-id="training-process-simplified">Training Process (Simplified)</h3>
<ol type="1">
<li><strong>Initialize</strong>: Random vectors for each word</li>
<li><strong>Sample</strong>: Pick a word and its context from training data</li>
<li><strong>Predict</strong>: Use the word’s embedding to predict context words</li>
<li><strong>Update</strong>: Adjust embeddings to improve predictions</li>
<li><strong>Repeat</strong>: Millions of times across billions of words</li>
</ol>
<p>After training, embeddings capture semantic structure without anyone explicitly defining it.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why This Works
</div>
</div>
<div class="callout-body-container callout-body">
<p>If “cat” often appears near “furry,” “pet,” and “meow,” its embedding learns to activate for animal-related contexts. If “dog” appears in similar contexts, its embedding will be similar to “cat’s.”</p>
<p>The model discovers that “cat” and “dog” are related not because we told it, but because they share statistical patterns in text.</p>
</div>
</div>
</section>
</section>
<section id="using-word2vec-with-gensim" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="using-word2vec-with-gensim"><span class="header-section-number">1.3</span> Using Word2vec with Gensim</h2>
<p>Let’s work with pre-trained Word2vec embeddings using the <code>gensim</code> library.</p>
<div id="962b3dac" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim.downloader <span class="im">as</span> api</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Load pre-trained Word2vec embeddings (Google News corpus, ~100B words)</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># This is a large download (~1.6GB), so it may take a minute</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loading Word2vec model (this may take a moment)..."</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> api.load(<span class="st">"word2vec-google-news-300"</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Loaded embeddings for </span><span class="sc">{</span><span class="bu">len</span>(model)<span class="sc">}</span><span class="ss"> words, each with </span><span class="sc">{</span>model<span class="sc">.</span>vector_size<span class="sc">}</span><span class="ss"> dimensions"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Output</strong>:</p>
<pre><code>Loading Word2vec model (this may take a moment)...
Loaded embeddings for 3000000 words, each with 300 dimensions</code></pre>
<p>This model has embeddings for 3 million words, each represented as a 300-dimensional vector.</p>
<section id="exploring-word-similarities" class="level3">
<h3 class="anchored" data-anchor-id="exploring-word-similarities">Exploring Word Similarities</h3>
<div id="d8e4379f" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Find words most similar to "network"</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>similar_to_network <span class="op">=</span> model.most_similar(<span class="st">"network"</span>, topn<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Words most similar to 'network':"</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, similarity <span class="kw">in</span> similar_to_network:</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>word<span class="sc">:20s}</span><span class="ss"> </span><span class="sc">{</span>similarity<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Output</strong>:</p>
<pre><code>Words most similar to 'network':
  networks             0.732
  cable_network        0.682
  television_network   0.654
  broadcasting         0.623
  cable_television     0.612
  radio_network        0.598
  telecoms             0.587
  broadcaster          0.579
  TV_network           0.571
  communications       0.563</code></pre>
<p>The model learned that “network” is related to broadcasting, telecommunications, and media—despite never being told these definitions.</p>
</section>
<section id="semantic-similarity" class="level3">
<h3 class="anchored" data-anchor-id="semantic-similarity">Semantic Similarity</h3>
<p>Let’s compare similarities across different domains:</p>
<div id="c32854f8" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> [<span class="st">"network"</span>, <span class="st">"graph"</span>, <span class="st">"community"</span>, <span class="st">"theorem"</span>, <span class="st">"protein"</span>, <span class="st">"cat"</span>]</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Pairwise similarities:"</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">''</span><span class="sc">:12s}</span><span class="ss">"</span>, end<span class="op">=</span><span class="st">""</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> w <span class="kw">in</span> words:</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>w<span class="sc">:12s}</span><span class="ss">"</span>, end<span class="op">=</span><span class="st">""</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> w1 <span class="kw">in</span> words:</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>w1<span class="sc">:12s}</span><span class="ss">"</span>, end<span class="op">=</span><span class="st">""</span>)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> w2 <span class="kw">in</span> words:</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> w1 <span class="kw">in</span> model <span class="kw">and</span> w2 <span class="kw">in</span> model:</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>            sim <span class="op">=</span> model.similarity(w1, w2)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>sim<span class="sc">:12.3f}</span><span class="ss">"</span>, end<span class="op">=</span><span class="st">""</span>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'N/A'</span><span class="sc">:12s}</span><span class="ss">"</span>, end<span class="op">=</span><span class="st">""</span>)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Output</strong>:</p>
<pre><code>            network     graph       community   theorem     protein     cat
network        1.000       0.312       0.385       0.187       0.143       0.089
graph          0.312       1.000       0.245       0.298       0.112       0.076
community      0.385       0.245       1.000       0.156       0.134       0.098
theorem        0.187       0.298       0.156       1.000       0.198       0.065
protein        0.143       0.112       0.134       0.198       1.000       0.102
cat            0.089       0.076       0.098       0.065       0.102       1.000</code></pre>
<p><strong>Observations</strong>: - “network” and “community” are moderately similar (0.385) — both social concepts - “graph” and “theorem” have some similarity (0.298) — both mathematical - “cat” is dissimilar to everything else — different domain entirely</p>
</section>
</section>
<section id="word-algebra-the-famous-examples" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="word-algebra-the-famous-examples"><span class="header-section-number">1.4</span> Word Algebra: The Famous Examples</h2>
<p>One of Word2vec’s most striking properties: <strong>semantic relationships become vector arithmetic</strong>.</p>
<section id="the-king---man-woman-queen-example" class="level3">
<h3 class="anchored" data-anchor-id="the-king---man-woman-queen-example">The “King - Man + Woman = Queen” Example</h3>
<div id="13307c45" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Vector arithmetic</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.most_similar(positive<span class="op">=</span>[<span class="st">'king'</span>, <span class="st">'woman'</span>], negative<span class="op">=</span>[<span class="st">'man'</span>], topn<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"king - man + woman ="</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, similarity <span class="kw">in</span> result:</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>word<span class="sc">:15s}</span><span class="ss"> </span><span class="sc">{</span>similarity<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Output</strong>:</p>
<pre><code>king - man + woman =
  queen           0.711
  monarch         0.619
  princess        0.590
  crown_prince    0.567
  prince          0.561</code></pre>
<p>The model learned that “king” relates to “man” as “queen” relates to “woman”—a relationship captured by vector subtraction and addition!</p>
</section>
<section id="more-examples" class="level3">
<h3 class="anchored" data-anchor-id="more-examples">More Examples</h3>
<div id="346de6df" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Paris - France + Germany = Berlin</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.most_similar(positive<span class="op">=</span>[<span class="st">'Paris'</span>, <span class="st">'Germany'</span>], negative<span class="op">=</span>[<span class="st">'France'</span>], topn<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Paris - France + Germany ="</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, similarity <span class="kw">in</span> result:</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>word<span class="sc">:15s}</span><span class="ss"> </span><span class="sc">{</span>similarity<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Swimming - swim + run = running</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.most_similar(positive<span class="op">=</span>[<span class="st">'swimming'</span>, <span class="st">'run'</span>], negative<span class="op">=</span>[<span class="st">'swim'</span>], topn<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">swimming - swim + run ="</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, similarity <span class="kw">in</span> result:</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>word<span class="sc">:15s}</span><span class="ss"> </span><span class="sc">{</span>similarity<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Big - bigger + cold = colder</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.most_similar(positive<span class="op">=</span>[<span class="st">'bigger'</span>, <span class="st">'cold'</span>], negative<span class="op">=</span>[<span class="st">'big'</span>], topn<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">bigger - big + cold ="</span>)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, similarity <span class="kw">in</span> result:</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>word<span class="sc">:15s}</span><span class="ss"> </span><span class="sc">{</span>similarity<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Output</strong>:</p>
<pre><code>Paris - France + Germany =
  Berlin          0.735
  Munich          0.652
  Hamburg         0.618

swimming - swim + run =
  running         0.681
  runs            0.632
  jogging         0.598

bigger - big + cold =
  colder          0.708
  warmer          0.673
  hotter          0.649</code></pre>
<p>These examples show that Word2vec captures: - <strong>Geographic relationships</strong>: capital cities - <strong>Grammatical relationships</strong>: verb forms, comparatives - <strong>Semantic relationships</strong>: gender, magnitude</p>
<p>All from statistical patterns in text!</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why Vector Arithmetic Works
</div>
</div>
<div class="callout-body-container callout-body">
<p>Word2vec embeddings organize words so that semantic relationships correspond to geometric directions in vector space. The “gender” direction is roughly king - queen, the “capital-of” direction is roughly Paris - France.</p>
<p>This emergent structure wasn’t programmed—it arises naturally from the training objective.</p>
</div>
</div>
</section>
</section>
<section id="visualizing-word2vec-embeddings" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="visualizing-word2vec-embeddings"><span class="header-section-number">1.5</span> Visualizing Word2vec Embeddings</h2>
<p>Let’s visualize embeddings for scientific terms in 2D.</p>
<div id="22a55313" class="cell" data-fig-width="10" data-fig-height="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Scientific vocabulary</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> [</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Network science</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"network"</span>, <span class="st">"graph"</span>, <span class="st">"node"</span>, <span class="st">"edge"</span>, <span class="st">"community"</span>, <span class="st">"clustering"</span>,</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Biology</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"protein"</span>, <span class="st">"gene"</span>, <span class="st">"cell"</span>, <span class="st">"DNA"</span>, <span class="st">"molecule"</span>, <span class="st">"organism"</span>,</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Physics</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"quantum"</span>, <span class="st">"particle"</span>, <span class="st">"energy"</span>, <span class="st">"force"</span>, <span class="st">"electron"</span>, <span class="st">"photon"</span>,</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Math</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"theorem"</span>, <span class="st">"proof"</span>, <span class="st">"equation"</span>, <span class="st">"algebra"</span>, <span class="st">"calculus"</span>, <span class="st">"geometry"</span>,</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Computing</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"algorithm"</span>, <span class="st">"computer"</span>, <span class="st">"software"</span>, <span class="st">"data"</span>, <span class="st">"program"</span>, <span class="st">"code"</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Get embeddings</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>word_vectors <span class="op">=</span> np.array([model[word] <span class="cf">for</span> word <span class="kw">in</span> words <span class="cf">if</span> word <span class="kw">in</span> model])</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>valid_words <span class="op">=</span> [word <span class="cf">for</span> word <span class="kw">in</span> words <span class="cf">if</span> word <span class="kw">in</span> model]</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Reduce to 2D with t-SNE</span></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>tsne <span class="op">=</span> TSNE(n_components<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">42</span>, perplexity<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>word_2d <span class="op">=</span> tsne.fit_transform(word_vectors)</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">"white"</span>)</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Color by category</span></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>categories <span class="op">=</span> {</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Network Science'</span>: [<span class="st">'network'</span>, <span class="st">'graph'</span>, <span class="st">'node'</span>, <span class="st">'edge'</span>, <span class="st">'community'</span>, <span class="st">'clustering'</span>],</span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Biology'</span>: [<span class="st">'protein'</span>, <span class="st">'gene'</span>, <span class="st">'cell'</span>, <span class="st">'DNA'</span>, <span class="st">'molecule'</span>, <span class="st">'organism'</span>],</span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Physics'</span>: [<span class="st">'quantum'</span>, <span class="st">'particle'</span>, <span class="st">'energy'</span>, <span class="st">'force'</span>, <span class="st">'electron'</span>, <span class="st">'photon'</span>],</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Mathematics'</span>: [<span class="st">'theorem'</span>, <span class="st">'proof'</span>, <span class="st">'equation'</span>, <span class="st">'algebra'</span>, <span class="st">'calculus'</span>, <span class="st">'geometry'</span>],</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Computing'</span>: [<span class="st">'algorithm'</span>, <span class="st">'computer'</span>, <span class="st">'software'</span>, <span class="st">'data'</span>, <span class="st">'program'</span>, <span class="st">'code'</span>]</span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> {<span class="st">'Network Science'</span>: <span class="st">'#e74c3c'</span>, <span class="st">'Biology'</span>: <span class="st">'#2ecc71'</span>, <span class="st">'Physics'</span>: <span class="st">'#f39c12'</span>,</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a>          <span class="st">'Mathematics'</span>: <span class="st">'#9b59b6'</span>, <span class="st">'Computing'</span>: <span class="st">'#3498db'</span>}</span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> category, category_words <span class="kw">in</span> categories.items():</span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>    indices <span class="op">=</span> [valid_words.index(w) <span class="cf">for</span> w <span class="kw">in</span> category_words <span class="cf">if</span> w <span class="kw">in</span> valid_words]</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> indices:</span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a>        ax.scatter(word_2d[indices, <span class="dv">0</span>], word_2d[indices, <span class="dv">1</span>],</span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>                  c<span class="op">=</span>colors[category], label<span class="op">=</span>category, s<span class="op">=</span><span class="dv">200</span>, alpha<span class="op">=</span><span class="fl">0.7</span>,</span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a>                  edgecolors<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>)</span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx <span class="kw">in</span> indices:</span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a>            ax.annotate(valid_words[idx], (word_2d[idx, <span class="dv">0</span>], word_2d[idx, <span class="dv">1</span>]),</span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a>                       fontsize<span class="op">=</span><span class="dv">9</span>, ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Dimension 1"</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Dimension 2"</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Word2vec: Scientific Vocabulary Space"</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a>ax.legend(loc<span class="op">=</span><span class="st">'best'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a>ax.grid(alpha<span class="op">=</span><span class="fl">0.3</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a>sns.despine()</span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Observations</strong>: - <strong>Clusters form</strong>: Biology terms group together, physics terms group together - <strong>Overlap zones</strong>: Computing and math terms are nearby (both abstract/technical) - <strong>Distinct regions</strong>: Biology is far from physics (different domains)</p>
<p>The model discovered these relationships purely from word co-occurrence statistics.</p>
</section>
<section id="application-tracking-concept-evolution" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="application-tracking-concept-evolution"><span class="header-section-number">1.6</span> Application: Tracking Concept Evolution</h2>
<p>One powerful use of Word2vec: analyzing how scientific concepts change over time.</p>
<section id="example-network-in-different-decades" class="level3">
<h3 class="anchored" data-anchor-id="example-network-in-different-decades">Example: “Network” in Different Decades</h3>
<div id="fcdc7f35" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate training Word2vec on papers from different decades</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="co"># In practice, you'd train separate models on historical corpora</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># For illustration, we'll show conceptually how this works</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>decades <span class="op">=</span> {</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"1950s"</span>: [<span class="st">"electrical"</span>, <span class="st">"circuit"</span>, <span class="st">"television"</span>, <span class="st">"radio"</span>, <span class="st">"broadcasting"</span>],</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"1980s"</span>: [<span class="st">"computer"</span>, <span class="st">"telecommunications"</span>, <span class="st">"protocol"</span>, <span class="st">"LAN"</span>, <span class="st">"topology"</span>],</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"2010s"</span>: [<span class="st">"social"</span>, <span class="st">"online"</span>, <span class="st">"Twitter"</span>, <span class="st">"Facebook"</span>, <span class="st">"community"</span>, <span class="st">"graph"</span>]</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Evolution of 'network' neighbors over time:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> decade, neighbors <span class="kw">in</span> decades.items():</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>decade<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word <span class="kw">in</span> neighbors:</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> word <span class="kw">in</span> model:</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>            sim <span class="op">=</span> model.similarity(<span class="st">"network"</span>, word)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"  network ↔ </span><span class="sc">{</span>word<span class="sc">:20s}</span><span class="ss"> similarity: </span><span class="sc">{</span>sim<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"  network ↔ </span><span class="sc">{</span>word<span class="sc">:20s}</span><span class="ss"> similarity: N/A"</span>)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Real research application</strong>: Train Word2vec on scientific papers from different time periods, then measure how “network” embeddings shift. This reveals how the concept evolved from electrical networks → computer networks → social networks.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Historical Text Analysis
</div>
</div>
<div class="callout-body-container callout-body">
<p>Train Word2vec models on text from different eras (decades, centuries) and compare embeddings. You can track: - Semantic drift (how meanings change) - Emerging concepts (new words in vocabulary) - Shifting associations (changes in word neighbors)</p>
<p>This is a powerful tool for cultural evolution and history of science research.</p>
</div>
</div>
</section>
</section>
<section id="static-vs.-contextual-embeddings" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="static-vs.-contextual-embeddings"><span class="header-section-number">1.7</span> Static vs.&nbsp;Contextual Embeddings</h2>
<p>Now that you’ve seen both Word2vec (static) and transformer embeddings (contextual), let’s compare.</p>
<section id="static-embeddings-word2vec-glove" class="level3">
<h3 class="anchored" data-anchor-id="static-embeddings-word2vec-glove">Static Embeddings (Word2vec, GloVe)</h3>
<p><strong>One embedding per word</strong>:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">"bank"</span> → [<span class="fl">0.23</span>, <span class="op">-</span><span class="fl">0.45</span>, <span class="fl">0.67</span>, ...]  (always the same)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Example</strong>: - “I went to the <strong>bank</strong>” → [0.23, -0.45, 0.67, …] - “The river <strong>bank</strong>” → [0.23, -0.45, 0.67, …] (identical!)</p>
<p><strong>Strengths</strong>: - Fast to train and use - Small model size - Explicit semantic relationships (word algebra) - Good for word-level analysis</p>
<p><strong>Weaknesses</strong>: - Can’t handle polysemy (multiple meanings) - Ignores context - Struggles with rare words</p>
</section>
<section id="contextual-embeddings-bert-gpt-sentence-transformers" class="level3">
<h3 class="anchored" data-anchor-id="contextual-embeddings-bert-gpt-sentence-transformers">Contextual Embeddings (BERT, GPT, Sentence-Transformers)</h3>
<p><strong>Different embedding depending on context</strong>:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co">"I went to the bank"</span> → <span class="st">"bank"</span> gets embedding1</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="co">"The river bank"</span>      → <span class="st">"bank"</span> gets embedding2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Strengths</strong>: - Handles polysemy correctly - Context-aware meaning - Better for sentence/document tasks - State-of-the-art performance</p>
<p><strong>Weaknesses</strong>: - Computationally expensive - Large model size (GBs) - Less interpretable - Overkill for simple tasks</p>
</section>
<section id="when-to-use-which" class="level3">
<h3 class="anchored" data-anchor-id="when-to-use-which">When to Use Which?</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Task</th>
<th>Recommended Approach</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Word similarity, analogies</td>
<td>Word2vec</td>
</tr>
<tr class="even">
<td>Tracking semantic change over time</td>
<td>Word2vec (train per era)</td>
</tr>
<tr class="odd">
<td>Document classification</td>
<td>Contextual (sentence-transformers)</td>
</tr>
<tr class="even">
<td>Semantic search</td>
<td>Contextual (sentence-transformers)</td>
</tr>
<tr class="odd">
<td>Named entity recognition</td>
<td>Contextual (BERT)</td>
</tr>
<tr class="even">
<td>Text generation</td>
<td>Contextual (GPT)</td>
</tr>
<tr class="odd">
<td>Quick prototyping on a laptop</td>
<td>Word2vec</td>
</tr>
<tr class="even">
<td>Production system with accuracy priority</td>
<td>Contextual</td>
</tr>
</tbody>
</table>
<p><strong>Rule of thumb</strong>: Start simple (Word2vec). Upgrade to contextual embeddings only if you need the extra performance and can afford the computational cost.</p>
</section>
</section>
<section id="training-your-own-word2vec-model" class="level2" data-number="1.8">
<h2 data-number="1.8" class="anchored" data-anchor-id="training-your-own-word2vec-model"><span class="header-section-number">1.8</span> Training Your Own Word2vec Model</h2>
<p>For specialized domains (medical, legal, scientific subfields), pre-trained models might not have the right vocabulary. You can train your own Word2vec model.</p>
<div id="a34beb0d" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Scientific abstracts (simulated)</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>sentences <span class="op">=</span> [</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"community"</span>, <span class="st">"detection"</span>, <span class="st">"in"</span>, <span class="st">"networks"</span>, <span class="st">"using"</span>, <span class="st">"modularity"</span>],</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"graph"</span>, <span class="st">"clustering"</span>, <span class="st">"algorithms"</span>, <span class="st">"for"</span>, <span class="st">"large"</span>, <span class="st">"networks"</span>],</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"social"</span>, <span class="st">"network"</span>, <span class="st">"analysis"</span>, <span class="st">"with"</span>, <span class="st">"centrality"</span>, <span class="st">"measures"</span>],</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"protein"</span>, <span class="st">"interaction"</span>, <span class="st">"networks"</span>, <span class="st">"in"</span>, <span class="st">"systems"</span>, <span class="st">"biology"</span>],</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># In practice, you'd have thousands or millions of sentences</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Train Word2vec</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>model_custom <span class="op">=</span> Word2Vec(</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    sentences<span class="op">=</span>sentences,</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>    vector_size<span class="op">=</span><span class="dv">100</span>,      <span class="co"># Embedding dimensionality</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>    window<span class="op">=</span><span class="dv">5</span>,             <span class="co"># Context window size</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>    min_count<span class="op">=</span><span class="dv">1</span>,          <span class="co"># Minimum word frequency</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>    workers<span class="op">=</span><span class="dv">4</span>,            <span class="co"># Parallel processing</span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>    sg<span class="op">=</span><span class="dv">1</span>                  <span class="co"># Skip-gram (1) or CBOW (0)</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Trained custom Word2vec model"</span>)</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Vocabulary size: </span><span class="sc">{</span><span class="bu">len</span>(model_custom.wv)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Embedding size: </span><span class="sc">{</span>model_custom<span class="sc">.</span>wv<span class="sc">.</span>vector_size<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Most similar to "network" in our small corpus</span></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="st">"network"</span> <span class="kw">in</span> model_custom.wv:</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>    similar <span class="op">=</span> model_custom.wv.most_similar(<span class="st">"network"</span>, topn<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Most similar to 'network':"</span>)</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word, sim <span class="kw">in</span> similar:</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>word<span class="sc">:15s}</span><span class="ss"> </span><span class="sc">{</span>sim<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Output</strong>:</p>
<pre><code>Trained custom Word2vec model
Vocabulary size: 24
Embedding size: 100

Most similar to 'network':
  networks        0.892
  community       0.715
  clustering      0.687</code></pre>
<p>Even with this tiny dataset, the model learns that “networks,” “community,” and “clustering” are related concepts.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Training Considerations
</div>
</div>
<div class="callout-body-container callout-body">
<p>For good embeddings, you need: - <strong>Large corpus</strong>: Millions of words minimum, billions ideal - <strong>Clean preprocessing</strong>: Tokenization, lowercasing, removing noise - <strong>Hyperparameter tuning</strong>: vector_size, window, min_count - <strong>Domain-specific data</strong>: Train on text from your research domain</p>
<p>For most research purposes, pre-trained models (Word2vec, GloVe) are sufficient. Train custom models only when your domain vocabulary is poorly covered.</p>
</div>
</div>
</section>
<section id="limitations-and-biases" class="level2" data-number="1.9">
<h2 data-number="1.9" class="anchored" data-anchor-id="limitations-and-biases"><span class="header-section-number">1.9</span> Limitations and Biases</h2>
<p>Word2vec learns from data, which means it also learns human biases present in text.</p>
<section id="gender-bias-example" class="level3">
<h3 class="anchored" data-anchor-id="gender-bias-example">Gender Bias Example</h3>
<div id="e3927ce2" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Explore gender associations</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>male_professions <span class="op">=</span> model.most_similar(positive<span class="op">=</span>[<span class="st">'doctor'</span>, <span class="st">'man'</span>], negative<span class="op">=</span>[<span class="st">'woman'</span>], topn<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>female_professions <span class="op">=</span> model.most_similar(positive<span class="op">=</span>[<span class="st">'nurse'</span>, <span class="st">'woman'</span>], negative<span class="op">=</span>[<span class="st">'man'</span>], topn<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Male-associated professions:"</span>)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, sim <span class="kw">in</span> male_professions:</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Female-associated professions:"</span>)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, sim <span class="kw">in</span> female_professions:</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The model might associate “doctor” with male and “nurse” with female, reflecting biases in training data (news articles, books, web pages). These biases can propagate into downstream applications.</p>
<p><strong>Implications for research</strong>: - Be aware of biases in embeddings - Don’t use embeddings for sensitive applications without auditing - Consider debiasing techniques if needed - Embeddings can also be used to <em>measure</em> bias in text corpora</p>
<p>We’ll explore bias measurement with <strong>semantic axes</strong> in the final section.</p>
</section>
</section>
<section id="the-bigger-picture" class="level2" data-number="1.10">
<h2 data-number="1.10" class="anchored" data-anchor-id="the-bigger-picture"><span class="header-section-number">1.10</span> The Bigger Picture</h2>
<p>You’ve now seen the <strong>original approach to embeddings</strong>—Word2vec—and understand: - The distributional hypothesis (context determines meaning) - How Word2vec learns from skip-gram prediction - Word algebra and semantic relationships - When static embeddings are sufficient vs.&nbsp;when contextual embeddings are necessary</p>
<p>Word2vec was revolutionary in 2013. It enabled NLP to move from hand-crafted features to learned representations. But it had limitations (no context, polysemy), which transformers addressed.</p>
<p><strong>Now let’s go full circle: back to the basics.</strong> Before Word2vec, before embeddings, there was the simplest possible representation of text—counting words. These fundamental methods are still relevant, and understanding them completes the picture.</p>
<hr>
<p><strong>Next</strong>: <a href="../m03-text/text-fundamentals.html">Text Fundamentals: The Full Picture →</a></p>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../m03-text/transformers.html" class="pagination-link" aria-label="Transformers: The Architecture Behind the Magic">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Transformers: The Architecture Behind the Magic</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../m03-text/text-fundamentals.html" class="pagination-link" aria-label="Text Fundamentals: The Full Picture">
        <span class="nav-page-text">Text Fundamentals: The Full Picture</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb19" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Word Embeddings: Where It Started"</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="fu"># Before Transformers, We Had Word2vec</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>You've seen modern contextual embeddings from transformers. They're powerful, capturing nuanced meaning that depends on context. But they're also complex, computationally expensive, and sometimes overkill for simple tasks.</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>Before BERT and GPT, there was **Word2vec**—introduced in 2013 by Tomas Mikolov at Google. It's simpler, faster, and produces **static embeddings**: each word gets one fixed vector, regardless of context.</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>Word2vec might seem outdated compared to transformers, but it's still widely used because:</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Fast**: Train on millions of documents in minutes</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Lightweight**: Small models (~100MB) vs. gigabytes for transformers</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Interpretable**: Captures explicit semantic relationships ("king" - "man" + "woman" = "queen")</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Good enough**: For many tasks, static embeddings work just fine</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>This section explains where embeddings came from, how Word2vec works intuitively, and when to use static vs. contextual embeddings.</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Distributional Hypothesis</span></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>Word2vec is built on a simple but profound idea:</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **"You shall know a word by the company it keeps."**</span></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; — J.R. Firth, 1957</span></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>Words that appear in similar contexts tend to have similar meanings.</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a><span class="fu">### Example</span></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>Consider these sentences:</span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"The **cat** sat on the mat."</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"The **dog** sat on the mat."</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"The **cat** chased the mouse."</span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"The **dog** chased the rabbit."</span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a>"Cat" and "dog" appear in similar contexts ("sat on the mat", "chased..."). Therefore, they should have similar embeddings.</span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a>Now consider:</span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"The **theorem** was proved in 1995."</span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"The **conjecture** was proved in 1995."</span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a>"Theorem" and "conjecture" also appear in similar contexts, so they should have similar embeddings—even though they're very different from "cat" and "dog."</span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a>**Key insight**: We don't need to manually encode that "cat" is an animal or "theorem" is a mathematical statement. The model learns these relationships automatically from context.</span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a><span class="fu">## Word2vec: The Core Idea</span></span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a>Word2vec learns embeddings by training a simple neural network to predict:</span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Skip-gram**: Given a word, predict its context</span>
<span id="cb19-51"><a href="#cb19-51" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**CBOW (Continuous Bag-of-Words)**: Given context, predict the word</span>
<span id="cb19-52"><a href="#cb19-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-53"><a href="#cb19-53" aria-hidden="true" tabindex="-1"></a>Both approaches lead to similar embeddings. We'll focus on **Skip-gram** because it's more intuitive.</span>
<span id="cb19-54"><a href="#cb19-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-55"><a href="#cb19-55" aria-hidden="true" tabindex="-1"></a><span class="fu">### Skip-Gram Objective</span></span>
<span id="cb19-56"><a href="#cb19-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-57"><a href="#cb19-57" aria-hidden="true" tabindex="-1"></a>**Training setup**:</span>
<span id="cb19-58"><a href="#cb19-58" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-59"><a href="#cb19-59" aria-hidden="true" tabindex="-1"></a><span class="in">Sentence: "The cat sat on the mat"</span></span>
<span id="cb19-60"><a href="#cb19-60" aria-hidden="true" tabindex="-1"></a><span class="in">Target word: "cat"</span></span>
<span id="cb19-61"><a href="#cb19-61" aria-hidden="true" tabindex="-1"></a><span class="in">Context window (size=2): ["The", "sat"]</span></span>
<span id="cb19-62"><a href="#cb19-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-63"><a href="#cb19-63" aria-hidden="true" tabindex="-1"></a><span class="in">Task: Given "cat", predict you'll see "The" and "sat" nearby</span></span>
<span id="cb19-64"><a href="#cb19-64" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-65"><a href="#cb19-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-66"><a href="#cb19-66" aria-hidden="true" tabindex="-1"></a>The model learns embeddings such that:</span>
<span id="cb19-67"><a href="#cb19-67" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Words with similar contexts get similar embeddings</span>
<span id="cb19-68"><a href="#cb19-68" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Embeddings encode semantic relationships</span>
<span id="cb19-69"><a href="#cb19-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-70"><a href="#cb19-70" aria-hidden="true" tabindex="-1"></a><span class="fu">### Training Process (Simplified)</span></span>
<span id="cb19-71"><a href="#cb19-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-72"><a href="#cb19-72" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Initialize**: Random vectors for each word</span>
<span id="cb19-73"><a href="#cb19-73" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Sample**: Pick a word and its context from training data</span>
<span id="cb19-74"><a href="#cb19-74" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Predict**: Use the word's embedding to predict context words</span>
<span id="cb19-75"><a href="#cb19-75" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Update**: Adjust embeddings to improve predictions</span>
<span id="cb19-76"><a href="#cb19-76" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Repeat**: Millions of times across billions of words</span>
<span id="cb19-77"><a href="#cb19-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-78"><a href="#cb19-78" aria-hidden="true" tabindex="-1"></a>After training, embeddings capture semantic structure without anyone explicitly defining it.</span>
<span id="cb19-79"><a href="#cb19-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-80"><a href="#cb19-80" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb19-81"><a href="#cb19-81" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why This Works</span></span>
<span id="cb19-82"><a href="#cb19-82" aria-hidden="true" tabindex="-1"></a>If "cat" often appears near "furry," "pet," and "meow," its embedding learns to activate for animal-related contexts. If "dog" appears in similar contexts, its embedding will be similar to "cat's."</span>
<span id="cb19-83"><a href="#cb19-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-84"><a href="#cb19-84" aria-hidden="true" tabindex="-1"></a>The model discovers that "cat" and "dog" are related not because we told it, but because they share statistical patterns in text.</span>
<span id="cb19-85"><a href="#cb19-85" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-86"><a href="#cb19-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-87"><a href="#cb19-87" aria-hidden="true" tabindex="-1"></a><span class="fu">## Using Word2vec with Gensim</span></span>
<span id="cb19-88"><a href="#cb19-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-89"><a href="#cb19-89" aria-hidden="true" tabindex="-1"></a>Let's work with pre-trained Word2vec embeddings using the <span class="in">`gensim`</span> library.</span>
<span id="cb19-90"><a href="#cb19-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-93"><a href="#cb19-93" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-94"><a href="#cb19-94" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb19-95"><a href="#cb19-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-96"><a href="#cb19-96" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim.downloader <span class="im">as</span> api</span>
<span id="cb19-97"><a href="#cb19-97" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb19-98"><a href="#cb19-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-99"><a href="#cb19-99" aria-hidden="true" tabindex="-1"></a><span class="co"># Load pre-trained Word2vec embeddings (Google News corpus, ~100B words)</span></span>
<span id="cb19-100"><a href="#cb19-100" aria-hidden="true" tabindex="-1"></a><span class="co"># This is a large download (~1.6GB), so it may take a minute</span></span>
<span id="cb19-101"><a href="#cb19-101" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loading Word2vec model (this may take a moment)..."</span>)</span>
<span id="cb19-102"><a href="#cb19-102" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> api.load(<span class="st">"word2vec-google-news-300"</span>)</span>
<span id="cb19-103"><a href="#cb19-103" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Loaded embeddings for </span><span class="sc">{</span><span class="bu">len</span>(model)<span class="sc">}</span><span class="ss"> words, each with </span><span class="sc">{</span>model<span class="sc">.</span>vector_size<span class="sc">}</span><span class="ss"> dimensions"</span>)</span>
<span id="cb19-104"><a href="#cb19-104" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-105"><a href="#cb19-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-106"><a href="#cb19-106" aria-hidden="true" tabindex="-1"></a>**Output**:</span>
<span id="cb19-107"><a href="#cb19-107" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-108"><a href="#cb19-108" aria-hidden="true" tabindex="-1"></a><span class="in">Loading Word2vec model (this may take a moment)...</span></span>
<span id="cb19-109"><a href="#cb19-109" aria-hidden="true" tabindex="-1"></a><span class="in">Loaded embeddings for 3000000 words, each with 300 dimensions</span></span>
<span id="cb19-110"><a href="#cb19-110" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-111"><a href="#cb19-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-112"><a href="#cb19-112" aria-hidden="true" tabindex="-1"></a>This model has embeddings for 3 million words, each represented as a 300-dimensional vector.</span>
<span id="cb19-113"><a href="#cb19-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-114"><a href="#cb19-114" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exploring Word Similarities</span></span>
<span id="cb19-115"><a href="#cb19-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-118"><a href="#cb19-118" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-119"><a href="#cb19-119" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb19-120"><a href="#cb19-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-121"><a href="#cb19-121" aria-hidden="true" tabindex="-1"></a><span class="co"># Find words most similar to "network"</span></span>
<span id="cb19-122"><a href="#cb19-122" aria-hidden="true" tabindex="-1"></a>similar_to_network <span class="op">=</span> model.most_similar(<span class="st">"network"</span>, topn<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb19-123"><a href="#cb19-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-124"><a href="#cb19-124" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Words most similar to 'network':"</span>)</span>
<span id="cb19-125"><a href="#cb19-125" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, similarity <span class="kw">in</span> similar_to_network:</span>
<span id="cb19-126"><a href="#cb19-126" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>word<span class="sc">:20s}</span><span class="ss"> </span><span class="sc">{</span>similarity<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb19-127"><a href="#cb19-127" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-128"><a href="#cb19-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-129"><a href="#cb19-129" aria-hidden="true" tabindex="-1"></a>**Output**:</span>
<span id="cb19-130"><a href="#cb19-130" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-131"><a href="#cb19-131" aria-hidden="true" tabindex="-1"></a><span class="in">Words most similar to 'network':</span></span>
<span id="cb19-132"><a href="#cb19-132" aria-hidden="true" tabindex="-1"></a><span class="in">  networks             0.732</span></span>
<span id="cb19-133"><a href="#cb19-133" aria-hidden="true" tabindex="-1"></a><span class="in">  cable_network        0.682</span></span>
<span id="cb19-134"><a href="#cb19-134" aria-hidden="true" tabindex="-1"></a><span class="in">  television_network   0.654</span></span>
<span id="cb19-135"><a href="#cb19-135" aria-hidden="true" tabindex="-1"></a><span class="in">  broadcasting         0.623</span></span>
<span id="cb19-136"><a href="#cb19-136" aria-hidden="true" tabindex="-1"></a><span class="in">  cable_television     0.612</span></span>
<span id="cb19-137"><a href="#cb19-137" aria-hidden="true" tabindex="-1"></a><span class="in">  radio_network        0.598</span></span>
<span id="cb19-138"><a href="#cb19-138" aria-hidden="true" tabindex="-1"></a><span class="in">  telecoms             0.587</span></span>
<span id="cb19-139"><a href="#cb19-139" aria-hidden="true" tabindex="-1"></a><span class="in">  broadcaster          0.579</span></span>
<span id="cb19-140"><a href="#cb19-140" aria-hidden="true" tabindex="-1"></a><span class="in">  TV_network           0.571</span></span>
<span id="cb19-141"><a href="#cb19-141" aria-hidden="true" tabindex="-1"></a><span class="in">  communications       0.563</span></span>
<span id="cb19-142"><a href="#cb19-142" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-143"><a href="#cb19-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-144"><a href="#cb19-144" aria-hidden="true" tabindex="-1"></a>The model learned that "network" is related to broadcasting, telecommunications, and media—despite never being told these definitions.</span>
<span id="cb19-145"><a href="#cb19-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-146"><a href="#cb19-146" aria-hidden="true" tabindex="-1"></a><span class="fu">### Semantic Similarity</span></span>
<span id="cb19-147"><a href="#cb19-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-148"><a href="#cb19-148" aria-hidden="true" tabindex="-1"></a>Let's compare similarities across different domains:</span>
<span id="cb19-149"><a href="#cb19-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-152"><a href="#cb19-152" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-153"><a href="#cb19-153" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb19-154"><a href="#cb19-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-155"><a href="#cb19-155" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> [<span class="st">"network"</span>, <span class="st">"graph"</span>, <span class="st">"community"</span>, <span class="st">"theorem"</span>, <span class="st">"protein"</span>, <span class="st">"cat"</span>]</span>
<span id="cb19-156"><a href="#cb19-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-157"><a href="#cb19-157" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Pairwise similarities:"</span>)</span>
<span id="cb19-158"><a href="#cb19-158" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">''</span><span class="sc">:12s}</span><span class="ss">"</span>, end<span class="op">=</span><span class="st">""</span>)</span>
<span id="cb19-159"><a href="#cb19-159" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> w <span class="kw">in</span> words:</span>
<span id="cb19-160"><a href="#cb19-160" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>w<span class="sc">:12s}</span><span class="ss">"</span>, end<span class="op">=</span><span class="st">""</span>)</span>
<span id="cb19-161"><a href="#cb19-161" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb19-162"><a href="#cb19-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-163"><a href="#cb19-163" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> w1 <span class="kw">in</span> words:</span>
<span id="cb19-164"><a href="#cb19-164" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>w1<span class="sc">:12s}</span><span class="ss">"</span>, end<span class="op">=</span><span class="st">""</span>)</span>
<span id="cb19-165"><a href="#cb19-165" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> w2 <span class="kw">in</span> words:</span>
<span id="cb19-166"><a href="#cb19-166" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> w1 <span class="kw">in</span> model <span class="kw">and</span> w2 <span class="kw">in</span> model:</span>
<span id="cb19-167"><a href="#cb19-167" aria-hidden="true" tabindex="-1"></a>            sim <span class="op">=</span> model.similarity(w1, w2)</span>
<span id="cb19-168"><a href="#cb19-168" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>sim<span class="sc">:12.3f}</span><span class="ss">"</span>, end<span class="op">=</span><span class="st">""</span>)</span>
<span id="cb19-169"><a href="#cb19-169" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb19-170"><a href="#cb19-170" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'N/A'</span><span class="sc">:12s}</span><span class="ss">"</span>, end<span class="op">=</span><span class="st">""</span>)</span>
<span id="cb19-171"><a href="#cb19-171" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb19-172"><a href="#cb19-172" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-173"><a href="#cb19-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-174"><a href="#cb19-174" aria-hidden="true" tabindex="-1"></a>**Output**:</span>
<span id="cb19-175"><a href="#cb19-175" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-176"><a href="#cb19-176" aria-hidden="true" tabindex="-1"></a><span class="in">            network     graph       community   theorem     protein     cat</span></span>
<span id="cb19-177"><a href="#cb19-177" aria-hidden="true" tabindex="-1"></a><span class="in">network        1.000       0.312       0.385       0.187       0.143       0.089</span></span>
<span id="cb19-178"><a href="#cb19-178" aria-hidden="true" tabindex="-1"></a><span class="in">graph          0.312       1.000       0.245       0.298       0.112       0.076</span></span>
<span id="cb19-179"><a href="#cb19-179" aria-hidden="true" tabindex="-1"></a><span class="in">community      0.385       0.245       1.000       0.156       0.134       0.098</span></span>
<span id="cb19-180"><a href="#cb19-180" aria-hidden="true" tabindex="-1"></a><span class="in">theorem        0.187       0.298       0.156       1.000       0.198       0.065</span></span>
<span id="cb19-181"><a href="#cb19-181" aria-hidden="true" tabindex="-1"></a><span class="in">protein        0.143       0.112       0.134       0.198       1.000       0.102</span></span>
<span id="cb19-182"><a href="#cb19-182" aria-hidden="true" tabindex="-1"></a><span class="in">cat            0.089       0.076       0.098       0.065       0.102       1.000</span></span>
<span id="cb19-183"><a href="#cb19-183" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-184"><a href="#cb19-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-185"><a href="#cb19-185" aria-hidden="true" tabindex="-1"></a>**Observations**:</span>
<span id="cb19-186"><a href="#cb19-186" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"network" and "community" are moderately similar (0.385) — both social concepts</span>
<span id="cb19-187"><a href="#cb19-187" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"graph" and "theorem" have some similarity (0.298) — both mathematical</span>
<span id="cb19-188"><a href="#cb19-188" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"cat" is dissimilar to everything else — different domain entirely</span>
<span id="cb19-189"><a href="#cb19-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-190"><a href="#cb19-190" aria-hidden="true" tabindex="-1"></a><span class="fu">## Word Algebra: The Famous Examples</span></span>
<span id="cb19-191"><a href="#cb19-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-192"><a href="#cb19-192" aria-hidden="true" tabindex="-1"></a>One of Word2vec's most striking properties: **semantic relationships become vector arithmetic**.</span>
<span id="cb19-193"><a href="#cb19-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-194"><a href="#cb19-194" aria-hidden="true" tabindex="-1"></a><span class="fu">### The "King - Man + Woman = Queen" Example</span></span>
<span id="cb19-195"><a href="#cb19-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-198"><a href="#cb19-198" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-199"><a href="#cb19-199" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb19-200"><a href="#cb19-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-201"><a href="#cb19-201" aria-hidden="true" tabindex="-1"></a><span class="co"># Vector arithmetic</span></span>
<span id="cb19-202"><a href="#cb19-202" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.most_similar(positive<span class="op">=</span>[<span class="st">'king'</span>, <span class="st">'woman'</span>], negative<span class="op">=</span>[<span class="st">'man'</span>], topn<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb19-203"><a href="#cb19-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-204"><a href="#cb19-204" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"king - man + woman ="</span>)</span>
<span id="cb19-205"><a href="#cb19-205" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, similarity <span class="kw">in</span> result:</span>
<span id="cb19-206"><a href="#cb19-206" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>word<span class="sc">:15s}</span><span class="ss"> </span><span class="sc">{</span>similarity<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb19-207"><a href="#cb19-207" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-208"><a href="#cb19-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-209"><a href="#cb19-209" aria-hidden="true" tabindex="-1"></a>**Output**:</span>
<span id="cb19-210"><a href="#cb19-210" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-211"><a href="#cb19-211" aria-hidden="true" tabindex="-1"></a><span class="in">king - man + woman =</span></span>
<span id="cb19-212"><a href="#cb19-212" aria-hidden="true" tabindex="-1"></a><span class="in">  queen           0.711</span></span>
<span id="cb19-213"><a href="#cb19-213" aria-hidden="true" tabindex="-1"></a><span class="in">  monarch         0.619</span></span>
<span id="cb19-214"><a href="#cb19-214" aria-hidden="true" tabindex="-1"></a><span class="in">  princess        0.590</span></span>
<span id="cb19-215"><a href="#cb19-215" aria-hidden="true" tabindex="-1"></a><span class="in">  crown_prince    0.567</span></span>
<span id="cb19-216"><a href="#cb19-216" aria-hidden="true" tabindex="-1"></a><span class="in">  prince          0.561</span></span>
<span id="cb19-217"><a href="#cb19-217" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-218"><a href="#cb19-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-219"><a href="#cb19-219" aria-hidden="true" tabindex="-1"></a>The model learned that "king" relates to "man" as "queen" relates to "woman"—a relationship captured by vector subtraction and addition!</span>
<span id="cb19-220"><a href="#cb19-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-221"><a href="#cb19-221" aria-hidden="true" tabindex="-1"></a><span class="fu">### More Examples</span></span>
<span id="cb19-222"><a href="#cb19-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-225"><a href="#cb19-225" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-226"><a href="#cb19-226" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb19-227"><a href="#cb19-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-228"><a href="#cb19-228" aria-hidden="true" tabindex="-1"></a><span class="co"># Paris - France + Germany = Berlin</span></span>
<span id="cb19-229"><a href="#cb19-229" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.most_similar(positive<span class="op">=</span>[<span class="st">'Paris'</span>, <span class="st">'Germany'</span>], negative<span class="op">=</span>[<span class="st">'France'</span>], topn<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb19-230"><a href="#cb19-230" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Paris - France + Germany ="</span>)</span>
<span id="cb19-231"><a href="#cb19-231" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, similarity <span class="kw">in</span> result:</span>
<span id="cb19-232"><a href="#cb19-232" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>word<span class="sc">:15s}</span><span class="ss"> </span><span class="sc">{</span>similarity<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb19-233"><a href="#cb19-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-234"><a href="#cb19-234" aria-hidden="true" tabindex="-1"></a><span class="co"># Swimming - swim + run = running</span></span>
<span id="cb19-235"><a href="#cb19-235" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.most_similar(positive<span class="op">=</span>[<span class="st">'swimming'</span>, <span class="st">'run'</span>], negative<span class="op">=</span>[<span class="st">'swim'</span>], topn<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb19-236"><a href="#cb19-236" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">swimming - swim + run ="</span>)</span>
<span id="cb19-237"><a href="#cb19-237" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, similarity <span class="kw">in</span> result:</span>
<span id="cb19-238"><a href="#cb19-238" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>word<span class="sc">:15s}</span><span class="ss"> </span><span class="sc">{</span>similarity<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb19-239"><a href="#cb19-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-240"><a href="#cb19-240" aria-hidden="true" tabindex="-1"></a><span class="co"># Big - bigger + cold = colder</span></span>
<span id="cb19-241"><a href="#cb19-241" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.most_similar(positive<span class="op">=</span>[<span class="st">'bigger'</span>, <span class="st">'cold'</span>], negative<span class="op">=</span>[<span class="st">'big'</span>], topn<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb19-242"><a href="#cb19-242" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">bigger - big + cold ="</span>)</span>
<span id="cb19-243"><a href="#cb19-243" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, similarity <span class="kw">in</span> result:</span>
<span id="cb19-244"><a href="#cb19-244" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>word<span class="sc">:15s}</span><span class="ss"> </span><span class="sc">{</span>similarity<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb19-245"><a href="#cb19-245" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-246"><a href="#cb19-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-247"><a href="#cb19-247" aria-hidden="true" tabindex="-1"></a>**Output**:</span>
<span id="cb19-248"><a href="#cb19-248" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-249"><a href="#cb19-249" aria-hidden="true" tabindex="-1"></a><span class="in">Paris - France + Germany =</span></span>
<span id="cb19-250"><a href="#cb19-250" aria-hidden="true" tabindex="-1"></a><span class="in">  Berlin          0.735</span></span>
<span id="cb19-251"><a href="#cb19-251" aria-hidden="true" tabindex="-1"></a><span class="in">  Munich          0.652</span></span>
<span id="cb19-252"><a href="#cb19-252" aria-hidden="true" tabindex="-1"></a><span class="in">  Hamburg         0.618</span></span>
<span id="cb19-253"><a href="#cb19-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-254"><a href="#cb19-254" aria-hidden="true" tabindex="-1"></a><span class="in">swimming - swim + run =</span></span>
<span id="cb19-255"><a href="#cb19-255" aria-hidden="true" tabindex="-1"></a><span class="in">  running         0.681</span></span>
<span id="cb19-256"><a href="#cb19-256" aria-hidden="true" tabindex="-1"></a><span class="in">  runs            0.632</span></span>
<span id="cb19-257"><a href="#cb19-257" aria-hidden="true" tabindex="-1"></a><span class="in">  jogging         0.598</span></span>
<span id="cb19-258"><a href="#cb19-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-259"><a href="#cb19-259" aria-hidden="true" tabindex="-1"></a><span class="in">bigger - big + cold =</span></span>
<span id="cb19-260"><a href="#cb19-260" aria-hidden="true" tabindex="-1"></a><span class="in">  colder          0.708</span></span>
<span id="cb19-261"><a href="#cb19-261" aria-hidden="true" tabindex="-1"></a><span class="in">  warmer          0.673</span></span>
<span id="cb19-262"><a href="#cb19-262" aria-hidden="true" tabindex="-1"></a><span class="in">  hotter          0.649</span></span>
<span id="cb19-263"><a href="#cb19-263" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-264"><a href="#cb19-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-265"><a href="#cb19-265" aria-hidden="true" tabindex="-1"></a>These examples show that Word2vec captures:</span>
<span id="cb19-266"><a href="#cb19-266" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Geographic relationships**: capital cities</span>
<span id="cb19-267"><a href="#cb19-267" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Grammatical relationships**: verb forms, comparatives</span>
<span id="cb19-268"><a href="#cb19-268" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Semantic relationships**: gender, magnitude</span>
<span id="cb19-269"><a href="#cb19-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-270"><a href="#cb19-270" aria-hidden="true" tabindex="-1"></a>All from statistical patterns in text!</span>
<span id="cb19-271"><a href="#cb19-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-272"><a href="#cb19-272" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb19-273"><a href="#cb19-273" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why Vector Arithmetic Works</span></span>
<span id="cb19-274"><a href="#cb19-274" aria-hidden="true" tabindex="-1"></a>Word2vec embeddings organize words so that semantic relationships correspond to geometric directions in vector space. The "gender" direction is roughly king - queen, the "capital-of" direction is roughly Paris - France.</span>
<span id="cb19-275"><a href="#cb19-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-276"><a href="#cb19-276" aria-hidden="true" tabindex="-1"></a>This emergent structure wasn't programmed—it arises naturally from the training objective.</span>
<span id="cb19-277"><a href="#cb19-277" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-278"><a href="#cb19-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-279"><a href="#cb19-279" aria-hidden="true" tabindex="-1"></a><span class="fu">## Visualizing Word2vec Embeddings</span></span>
<span id="cb19-280"><a href="#cb19-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-281"><a href="#cb19-281" aria-hidden="true" tabindex="-1"></a>Let's visualize embeddings for scientific terms in 2D.</span>
<span id="cb19-282"><a href="#cb19-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-285"><a href="#cb19-285" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-286"><a href="#cb19-286" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb19-287"><a href="#cb19-287" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Word2vec embeddings of scientific terms projected to 2D. Similar concepts cluster together. The geometry encodes semantic relationships."</span></span>
<span id="cb19-288"><a href="#cb19-288" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 10</span></span>
<span id="cb19-289"><a href="#cb19-289" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 8</span></span>
<span id="cb19-290"><a href="#cb19-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-291"><a href="#cb19-291" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE</span>
<span id="cb19-292"><a href="#cb19-292" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb19-293"><a href="#cb19-293" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb19-294"><a href="#cb19-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-295"><a href="#cb19-295" aria-hidden="true" tabindex="-1"></a><span class="co"># Scientific vocabulary</span></span>
<span id="cb19-296"><a href="#cb19-296" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> [</span>
<span id="cb19-297"><a href="#cb19-297" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Network science</span></span>
<span id="cb19-298"><a href="#cb19-298" aria-hidden="true" tabindex="-1"></a>    <span class="st">"network"</span>, <span class="st">"graph"</span>, <span class="st">"node"</span>, <span class="st">"edge"</span>, <span class="st">"community"</span>, <span class="st">"clustering"</span>,</span>
<span id="cb19-299"><a href="#cb19-299" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Biology</span></span>
<span id="cb19-300"><a href="#cb19-300" aria-hidden="true" tabindex="-1"></a>    <span class="st">"protein"</span>, <span class="st">"gene"</span>, <span class="st">"cell"</span>, <span class="st">"DNA"</span>, <span class="st">"molecule"</span>, <span class="st">"organism"</span>,</span>
<span id="cb19-301"><a href="#cb19-301" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Physics</span></span>
<span id="cb19-302"><a href="#cb19-302" aria-hidden="true" tabindex="-1"></a>    <span class="st">"quantum"</span>, <span class="st">"particle"</span>, <span class="st">"energy"</span>, <span class="st">"force"</span>, <span class="st">"electron"</span>, <span class="st">"photon"</span>,</span>
<span id="cb19-303"><a href="#cb19-303" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Math</span></span>
<span id="cb19-304"><a href="#cb19-304" aria-hidden="true" tabindex="-1"></a>    <span class="st">"theorem"</span>, <span class="st">"proof"</span>, <span class="st">"equation"</span>, <span class="st">"algebra"</span>, <span class="st">"calculus"</span>, <span class="st">"geometry"</span>,</span>
<span id="cb19-305"><a href="#cb19-305" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Computing</span></span>
<span id="cb19-306"><a href="#cb19-306" aria-hidden="true" tabindex="-1"></a>    <span class="st">"algorithm"</span>, <span class="st">"computer"</span>, <span class="st">"software"</span>, <span class="st">"data"</span>, <span class="st">"program"</span>, <span class="st">"code"</span></span>
<span id="cb19-307"><a href="#cb19-307" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb19-308"><a href="#cb19-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-309"><a href="#cb19-309" aria-hidden="true" tabindex="-1"></a><span class="co"># Get embeddings</span></span>
<span id="cb19-310"><a href="#cb19-310" aria-hidden="true" tabindex="-1"></a>word_vectors <span class="op">=</span> np.array([model[word] <span class="cf">for</span> word <span class="kw">in</span> words <span class="cf">if</span> word <span class="kw">in</span> model])</span>
<span id="cb19-311"><a href="#cb19-311" aria-hidden="true" tabindex="-1"></a>valid_words <span class="op">=</span> [word <span class="cf">for</span> word <span class="kw">in</span> words <span class="cf">if</span> word <span class="kw">in</span> model]</span>
<span id="cb19-312"><a href="#cb19-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-313"><a href="#cb19-313" aria-hidden="true" tabindex="-1"></a><span class="co"># Reduce to 2D with t-SNE</span></span>
<span id="cb19-314"><a href="#cb19-314" aria-hidden="true" tabindex="-1"></a>tsne <span class="op">=</span> TSNE(n_components<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">42</span>, perplexity<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb19-315"><a href="#cb19-315" aria-hidden="true" tabindex="-1"></a>word_2d <span class="op">=</span> tsne.fit_transform(word_vectors)</span>
<span id="cb19-316"><a href="#cb19-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-317"><a href="#cb19-317" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb19-318"><a href="#cb19-318" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">"white"</span>)</span>
<span id="cb19-319"><a href="#cb19-319" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb19-320"><a href="#cb19-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-321"><a href="#cb19-321" aria-hidden="true" tabindex="-1"></a><span class="co"># Color by category</span></span>
<span id="cb19-322"><a href="#cb19-322" aria-hidden="true" tabindex="-1"></a>categories <span class="op">=</span> {</span>
<span id="cb19-323"><a href="#cb19-323" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Network Science'</span>: [<span class="st">'network'</span>, <span class="st">'graph'</span>, <span class="st">'node'</span>, <span class="st">'edge'</span>, <span class="st">'community'</span>, <span class="st">'clustering'</span>],</span>
<span id="cb19-324"><a href="#cb19-324" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Biology'</span>: [<span class="st">'protein'</span>, <span class="st">'gene'</span>, <span class="st">'cell'</span>, <span class="st">'DNA'</span>, <span class="st">'molecule'</span>, <span class="st">'organism'</span>],</span>
<span id="cb19-325"><a href="#cb19-325" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Physics'</span>: [<span class="st">'quantum'</span>, <span class="st">'particle'</span>, <span class="st">'energy'</span>, <span class="st">'force'</span>, <span class="st">'electron'</span>, <span class="st">'photon'</span>],</span>
<span id="cb19-326"><a href="#cb19-326" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Mathematics'</span>: [<span class="st">'theorem'</span>, <span class="st">'proof'</span>, <span class="st">'equation'</span>, <span class="st">'algebra'</span>, <span class="st">'calculus'</span>, <span class="st">'geometry'</span>],</span>
<span id="cb19-327"><a href="#cb19-327" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Computing'</span>: [<span class="st">'algorithm'</span>, <span class="st">'computer'</span>, <span class="st">'software'</span>, <span class="st">'data'</span>, <span class="st">'program'</span>, <span class="st">'code'</span>]</span>
<span id="cb19-328"><a href="#cb19-328" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb19-329"><a href="#cb19-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-330"><a href="#cb19-330" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> {<span class="st">'Network Science'</span>: <span class="st">'#e74c3c'</span>, <span class="st">'Biology'</span>: <span class="st">'#2ecc71'</span>, <span class="st">'Physics'</span>: <span class="st">'#f39c12'</span>,</span>
<span id="cb19-331"><a href="#cb19-331" aria-hidden="true" tabindex="-1"></a>          <span class="st">'Mathematics'</span>: <span class="st">'#9b59b6'</span>, <span class="st">'Computing'</span>: <span class="st">'#3498db'</span>}</span>
<span id="cb19-332"><a href="#cb19-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-333"><a href="#cb19-333" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> category, category_words <span class="kw">in</span> categories.items():</span>
<span id="cb19-334"><a href="#cb19-334" aria-hidden="true" tabindex="-1"></a>    indices <span class="op">=</span> [valid_words.index(w) <span class="cf">for</span> w <span class="kw">in</span> category_words <span class="cf">if</span> w <span class="kw">in</span> valid_words]</span>
<span id="cb19-335"><a href="#cb19-335" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> indices:</span>
<span id="cb19-336"><a href="#cb19-336" aria-hidden="true" tabindex="-1"></a>        ax.scatter(word_2d[indices, <span class="dv">0</span>], word_2d[indices, <span class="dv">1</span>],</span>
<span id="cb19-337"><a href="#cb19-337" aria-hidden="true" tabindex="-1"></a>                  c<span class="op">=</span>colors[category], label<span class="op">=</span>category, s<span class="op">=</span><span class="dv">200</span>, alpha<span class="op">=</span><span class="fl">0.7</span>,</span>
<span id="cb19-338"><a href="#cb19-338" aria-hidden="true" tabindex="-1"></a>                  edgecolors<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>)</span>
<span id="cb19-339"><a href="#cb19-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-340"><a href="#cb19-340" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx <span class="kw">in</span> indices:</span>
<span id="cb19-341"><a href="#cb19-341" aria-hidden="true" tabindex="-1"></a>            ax.annotate(valid_words[idx], (word_2d[idx, <span class="dv">0</span>], word_2d[idx, <span class="dv">1</span>]),</span>
<span id="cb19-342"><a href="#cb19-342" aria-hidden="true" tabindex="-1"></a>                       fontsize<span class="op">=</span><span class="dv">9</span>, ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb19-343"><a href="#cb19-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-344"><a href="#cb19-344" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Dimension 1"</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb19-345"><a href="#cb19-345" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Dimension 2"</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb19-346"><a href="#cb19-346" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Word2vec: Scientific Vocabulary Space"</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb19-347"><a href="#cb19-347" aria-hidden="true" tabindex="-1"></a>ax.legend(loc<span class="op">=</span><span class="st">'best'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb19-348"><a href="#cb19-348" aria-hidden="true" tabindex="-1"></a>ax.grid(alpha<span class="op">=</span><span class="fl">0.3</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb19-349"><a href="#cb19-349" aria-hidden="true" tabindex="-1"></a>sns.despine()</span>
<span id="cb19-350"><a href="#cb19-350" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb19-351"><a href="#cb19-351" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb19-352"><a href="#cb19-352" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-353"><a href="#cb19-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-354"><a href="#cb19-354" aria-hidden="true" tabindex="-1"></a>**Observations**:</span>
<span id="cb19-355"><a href="#cb19-355" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Clusters form**: Biology terms group together, physics terms group together</span>
<span id="cb19-356"><a href="#cb19-356" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Overlap zones**: Computing and math terms are nearby (both abstract/technical)</span>
<span id="cb19-357"><a href="#cb19-357" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Distinct regions**: Biology is far from physics (different domains)</span>
<span id="cb19-358"><a href="#cb19-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-359"><a href="#cb19-359" aria-hidden="true" tabindex="-1"></a>The model discovered these relationships purely from word co-occurrence statistics.</span>
<span id="cb19-360"><a href="#cb19-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-361"><a href="#cb19-361" aria-hidden="true" tabindex="-1"></a><span class="fu">## Application: Tracking Concept Evolution</span></span>
<span id="cb19-362"><a href="#cb19-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-363"><a href="#cb19-363" aria-hidden="true" tabindex="-1"></a>One powerful use of Word2vec: analyzing how scientific concepts change over time.</span>
<span id="cb19-364"><a href="#cb19-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-365"><a href="#cb19-365" aria-hidden="true" tabindex="-1"></a><span class="fu">### Example: "Network" in Different Decades</span></span>
<span id="cb19-366"><a href="#cb19-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-369"><a href="#cb19-369" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-370"><a href="#cb19-370" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb19-371"><a href="#cb19-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-372"><a href="#cb19-372" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate training Word2vec on papers from different decades</span></span>
<span id="cb19-373"><a href="#cb19-373" aria-hidden="true" tabindex="-1"></a><span class="co"># In practice, you'd train separate models on historical corpora</span></span>
<span id="cb19-374"><a href="#cb19-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-375"><a href="#cb19-375" aria-hidden="true" tabindex="-1"></a><span class="co"># For illustration, we'll show conceptually how this works</span></span>
<span id="cb19-376"><a href="#cb19-376" aria-hidden="true" tabindex="-1"></a>decades <span class="op">=</span> {</span>
<span id="cb19-377"><a href="#cb19-377" aria-hidden="true" tabindex="-1"></a>    <span class="st">"1950s"</span>: [<span class="st">"electrical"</span>, <span class="st">"circuit"</span>, <span class="st">"television"</span>, <span class="st">"radio"</span>, <span class="st">"broadcasting"</span>],</span>
<span id="cb19-378"><a href="#cb19-378" aria-hidden="true" tabindex="-1"></a>    <span class="st">"1980s"</span>: [<span class="st">"computer"</span>, <span class="st">"telecommunications"</span>, <span class="st">"protocol"</span>, <span class="st">"LAN"</span>, <span class="st">"topology"</span>],</span>
<span id="cb19-379"><a href="#cb19-379" aria-hidden="true" tabindex="-1"></a>    <span class="st">"2010s"</span>: [<span class="st">"social"</span>, <span class="st">"online"</span>, <span class="st">"Twitter"</span>, <span class="st">"Facebook"</span>, <span class="st">"community"</span>, <span class="st">"graph"</span>]</span>
<span id="cb19-380"><a href="#cb19-380" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb19-381"><a href="#cb19-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-382"><a href="#cb19-382" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Evolution of 'network' neighbors over time:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb19-383"><a href="#cb19-383" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> decade, neighbors <span class="kw">in</span> decades.items():</span>
<span id="cb19-384"><a href="#cb19-384" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>decade<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb19-385"><a href="#cb19-385" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word <span class="kw">in</span> neighbors:</span>
<span id="cb19-386"><a href="#cb19-386" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> word <span class="kw">in</span> model:</span>
<span id="cb19-387"><a href="#cb19-387" aria-hidden="true" tabindex="-1"></a>            sim <span class="op">=</span> model.similarity(<span class="st">"network"</span>, word)</span>
<span id="cb19-388"><a href="#cb19-388" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"  network ↔ </span><span class="sc">{</span>word<span class="sc">:20s}</span><span class="ss"> similarity: </span><span class="sc">{</span>sim<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb19-389"><a href="#cb19-389" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb19-390"><a href="#cb19-390" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"  network ↔ </span><span class="sc">{</span>word<span class="sc">:20s}</span><span class="ss"> similarity: N/A"</span>)</span>
<span id="cb19-391"><a href="#cb19-391" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb19-392"><a href="#cb19-392" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-393"><a href="#cb19-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-394"><a href="#cb19-394" aria-hidden="true" tabindex="-1"></a>**Real research application**:</span>
<span id="cb19-395"><a href="#cb19-395" aria-hidden="true" tabindex="-1"></a>Train Word2vec on scientific papers from different time periods, then measure how "network" embeddings shift. This reveals how the concept evolved from electrical networks → computer networks → social networks.</span>
<span id="cb19-396"><a href="#cb19-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-397"><a href="#cb19-397" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb19-398"><a href="#cb19-398" aria-hidden="true" tabindex="-1"></a><span class="fu">## Historical Text Analysis</span></span>
<span id="cb19-399"><a href="#cb19-399" aria-hidden="true" tabindex="-1"></a>Train Word2vec models on text from different eras (decades, centuries) and compare embeddings. You can track:</span>
<span id="cb19-400"><a href="#cb19-400" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Semantic drift (how meanings change)</span>
<span id="cb19-401"><a href="#cb19-401" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Emerging concepts (new words in vocabulary)</span>
<span id="cb19-402"><a href="#cb19-402" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Shifting associations (changes in word neighbors)</span>
<span id="cb19-403"><a href="#cb19-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-404"><a href="#cb19-404" aria-hidden="true" tabindex="-1"></a>This is a powerful tool for cultural evolution and history of science research.</span>
<span id="cb19-405"><a href="#cb19-405" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-406"><a href="#cb19-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-407"><a href="#cb19-407" aria-hidden="true" tabindex="-1"></a><span class="fu">## Static vs. Contextual Embeddings</span></span>
<span id="cb19-408"><a href="#cb19-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-409"><a href="#cb19-409" aria-hidden="true" tabindex="-1"></a>Now that you've seen both Word2vec (static) and transformer embeddings (contextual), let's compare.</span>
<span id="cb19-410"><a href="#cb19-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-411"><a href="#cb19-411" aria-hidden="true" tabindex="-1"></a><span class="fu">### Static Embeddings (Word2vec, GloVe)</span></span>
<span id="cb19-412"><a href="#cb19-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-413"><a href="#cb19-413" aria-hidden="true" tabindex="-1"></a>**One embedding per word**:</span>
<span id="cb19-414"><a href="#cb19-414" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb19-415"><a href="#cb19-415" aria-hidden="true" tabindex="-1"></a><span class="co">"bank"</span> → [<span class="fl">0.23</span>, <span class="op">-</span><span class="fl">0.45</span>, <span class="fl">0.67</span>, ...]  (always the same)</span>
<span id="cb19-416"><a href="#cb19-416" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-417"><a href="#cb19-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-418"><a href="#cb19-418" aria-hidden="true" tabindex="-1"></a>**Example**:</span>
<span id="cb19-419"><a href="#cb19-419" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"I went to the **bank**" → <span class="co">[</span><span class="ot">0.23, -0.45, 0.67, ...</span><span class="co">]</span></span>
<span id="cb19-420"><a href="#cb19-420" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"The river **bank**" → <span class="co">[</span><span class="ot">0.23, -0.45, 0.67, ...</span><span class="co">]</span> (identical!)</span>
<span id="cb19-421"><a href="#cb19-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-422"><a href="#cb19-422" aria-hidden="true" tabindex="-1"></a>**Strengths**:</span>
<span id="cb19-423"><a href="#cb19-423" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fast to train and use</span>
<span id="cb19-424"><a href="#cb19-424" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Small model size</span>
<span id="cb19-425"><a href="#cb19-425" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Explicit semantic relationships (word algebra)</span>
<span id="cb19-426"><a href="#cb19-426" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Good for word-level analysis</span>
<span id="cb19-427"><a href="#cb19-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-428"><a href="#cb19-428" aria-hidden="true" tabindex="-1"></a>**Weaknesses**:</span>
<span id="cb19-429"><a href="#cb19-429" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Can't handle polysemy (multiple meanings)</span>
<span id="cb19-430"><a href="#cb19-430" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Ignores context</span>
<span id="cb19-431"><a href="#cb19-431" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Struggles with rare words</span>
<span id="cb19-432"><a href="#cb19-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-433"><a href="#cb19-433" aria-hidden="true" tabindex="-1"></a><span class="fu">### Contextual Embeddings (BERT, GPT, Sentence-Transformers)</span></span>
<span id="cb19-434"><a href="#cb19-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-435"><a href="#cb19-435" aria-hidden="true" tabindex="-1"></a>**Different embedding depending on context**:</span>
<span id="cb19-436"><a href="#cb19-436" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb19-437"><a href="#cb19-437" aria-hidden="true" tabindex="-1"></a><span class="co">"I went to the bank"</span> → <span class="st">"bank"</span> gets embedding1</span>
<span id="cb19-438"><a href="#cb19-438" aria-hidden="true" tabindex="-1"></a><span class="co">"The river bank"</span>      → <span class="st">"bank"</span> gets embedding2</span>
<span id="cb19-439"><a href="#cb19-439" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-440"><a href="#cb19-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-441"><a href="#cb19-441" aria-hidden="true" tabindex="-1"></a>**Strengths**:</span>
<span id="cb19-442"><a href="#cb19-442" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Handles polysemy correctly</span>
<span id="cb19-443"><a href="#cb19-443" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Context-aware meaning</span>
<span id="cb19-444"><a href="#cb19-444" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Better for sentence/document tasks</span>
<span id="cb19-445"><a href="#cb19-445" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>State-of-the-art performance</span>
<span id="cb19-446"><a href="#cb19-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-447"><a href="#cb19-447" aria-hidden="true" tabindex="-1"></a>**Weaknesses**:</span>
<span id="cb19-448"><a href="#cb19-448" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Computationally expensive</span>
<span id="cb19-449"><a href="#cb19-449" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Large model size (GBs)</span>
<span id="cb19-450"><a href="#cb19-450" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Less interpretable</span>
<span id="cb19-451"><a href="#cb19-451" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Overkill for simple tasks</span>
<span id="cb19-452"><a href="#cb19-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-453"><a href="#cb19-453" aria-hidden="true" tabindex="-1"></a><span class="fu">### When to Use Which?</span></span>
<span id="cb19-454"><a href="#cb19-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-455"><a href="#cb19-455" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Task <span class="pp">|</span> Recommended Approach <span class="pp">|</span></span>
<span id="cb19-456"><a href="#cb19-456" aria-hidden="true" tabindex="-1"></a><span class="pp">|------|---------------------|</span></span>
<span id="cb19-457"><a href="#cb19-457" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Word similarity, analogies <span class="pp">|</span> Word2vec <span class="pp">|</span></span>
<span id="cb19-458"><a href="#cb19-458" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Tracking semantic change over time <span class="pp">|</span> Word2vec (train per era) <span class="pp">|</span></span>
<span id="cb19-459"><a href="#cb19-459" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Document classification <span class="pp">|</span> Contextual (sentence-transformers) <span class="pp">|</span></span>
<span id="cb19-460"><a href="#cb19-460" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Semantic search <span class="pp">|</span> Contextual (sentence-transformers) <span class="pp">|</span></span>
<span id="cb19-461"><a href="#cb19-461" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Named entity recognition <span class="pp">|</span> Contextual (BERT) <span class="pp">|</span></span>
<span id="cb19-462"><a href="#cb19-462" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Text generation <span class="pp">|</span> Contextual (GPT) <span class="pp">|</span></span>
<span id="cb19-463"><a href="#cb19-463" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Quick prototyping on a laptop <span class="pp">|</span> Word2vec <span class="pp">|</span></span>
<span id="cb19-464"><a href="#cb19-464" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Production system with accuracy priority <span class="pp">|</span> Contextual <span class="pp">|</span></span>
<span id="cb19-465"><a href="#cb19-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-466"><a href="#cb19-466" aria-hidden="true" tabindex="-1"></a>**Rule of thumb**: Start simple (Word2vec). Upgrade to contextual embeddings only if you need the extra performance and can afford the computational cost.</span>
<span id="cb19-467"><a href="#cb19-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-468"><a href="#cb19-468" aria-hidden="true" tabindex="-1"></a><span class="fu">## Training Your Own Word2vec Model</span></span>
<span id="cb19-469"><a href="#cb19-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-470"><a href="#cb19-470" aria-hidden="true" tabindex="-1"></a>For specialized domains (medical, legal, scientific subfields), pre-trained models might not have the right vocabulary. You can train your own Word2vec model.</span>
<span id="cb19-471"><a href="#cb19-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-474"><a href="#cb19-474" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-475"><a href="#cb19-475" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb19-476"><a href="#cb19-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-477"><a href="#cb19-477" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb19-478"><a href="#cb19-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-479"><a href="#cb19-479" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Scientific abstracts (simulated)</span></span>
<span id="cb19-480"><a href="#cb19-480" aria-hidden="true" tabindex="-1"></a>sentences <span class="op">=</span> [</span>
<span id="cb19-481"><a href="#cb19-481" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"community"</span>, <span class="st">"detection"</span>, <span class="st">"in"</span>, <span class="st">"networks"</span>, <span class="st">"using"</span>, <span class="st">"modularity"</span>],</span>
<span id="cb19-482"><a href="#cb19-482" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"graph"</span>, <span class="st">"clustering"</span>, <span class="st">"algorithms"</span>, <span class="st">"for"</span>, <span class="st">"large"</span>, <span class="st">"networks"</span>],</span>
<span id="cb19-483"><a href="#cb19-483" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"social"</span>, <span class="st">"network"</span>, <span class="st">"analysis"</span>, <span class="st">"with"</span>, <span class="st">"centrality"</span>, <span class="st">"measures"</span>],</span>
<span id="cb19-484"><a href="#cb19-484" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"protein"</span>, <span class="st">"interaction"</span>, <span class="st">"networks"</span>, <span class="st">"in"</span>, <span class="st">"systems"</span>, <span class="st">"biology"</span>],</span>
<span id="cb19-485"><a href="#cb19-485" aria-hidden="true" tabindex="-1"></a>    <span class="co"># In practice, you'd have thousands or millions of sentences</span></span>
<span id="cb19-486"><a href="#cb19-486" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb19-487"><a href="#cb19-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-488"><a href="#cb19-488" aria-hidden="true" tabindex="-1"></a><span class="co"># Train Word2vec</span></span>
<span id="cb19-489"><a href="#cb19-489" aria-hidden="true" tabindex="-1"></a>model_custom <span class="op">=</span> Word2Vec(</span>
<span id="cb19-490"><a href="#cb19-490" aria-hidden="true" tabindex="-1"></a>    sentences<span class="op">=</span>sentences,</span>
<span id="cb19-491"><a href="#cb19-491" aria-hidden="true" tabindex="-1"></a>    vector_size<span class="op">=</span><span class="dv">100</span>,      <span class="co"># Embedding dimensionality</span></span>
<span id="cb19-492"><a href="#cb19-492" aria-hidden="true" tabindex="-1"></a>    window<span class="op">=</span><span class="dv">5</span>,             <span class="co"># Context window size</span></span>
<span id="cb19-493"><a href="#cb19-493" aria-hidden="true" tabindex="-1"></a>    min_count<span class="op">=</span><span class="dv">1</span>,          <span class="co"># Minimum word frequency</span></span>
<span id="cb19-494"><a href="#cb19-494" aria-hidden="true" tabindex="-1"></a>    workers<span class="op">=</span><span class="dv">4</span>,            <span class="co"># Parallel processing</span></span>
<span id="cb19-495"><a href="#cb19-495" aria-hidden="true" tabindex="-1"></a>    sg<span class="op">=</span><span class="dv">1</span>                  <span class="co"># Skip-gram (1) or CBOW (0)</span></span>
<span id="cb19-496"><a href="#cb19-496" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb19-497"><a href="#cb19-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-498"><a href="#cb19-498" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Trained custom Word2vec model"</span>)</span>
<span id="cb19-499"><a href="#cb19-499" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Vocabulary size: </span><span class="sc">{</span><span class="bu">len</span>(model_custom.wv)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-500"><a href="#cb19-500" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Embedding size: </span><span class="sc">{</span>model_custom<span class="sc">.</span>wv<span class="sc">.</span>vector_size<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-501"><a href="#cb19-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-502"><a href="#cb19-502" aria-hidden="true" tabindex="-1"></a><span class="co"># Most similar to "network" in our small corpus</span></span>
<span id="cb19-503"><a href="#cb19-503" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="st">"network"</span> <span class="kw">in</span> model_custom.wv:</span>
<span id="cb19-504"><a href="#cb19-504" aria-hidden="true" tabindex="-1"></a>    similar <span class="op">=</span> model_custom.wv.most_similar(<span class="st">"network"</span>, topn<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb19-505"><a href="#cb19-505" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Most similar to 'network':"</span>)</span>
<span id="cb19-506"><a href="#cb19-506" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word, sim <span class="kw">in</span> similar:</span>
<span id="cb19-507"><a href="#cb19-507" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>word<span class="sc">:15s}</span><span class="ss"> </span><span class="sc">{</span>sim<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb19-508"><a href="#cb19-508" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-509"><a href="#cb19-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-510"><a href="#cb19-510" aria-hidden="true" tabindex="-1"></a>**Output**:</span>
<span id="cb19-511"><a href="#cb19-511" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-512"><a href="#cb19-512" aria-hidden="true" tabindex="-1"></a><span class="in">Trained custom Word2vec model</span></span>
<span id="cb19-513"><a href="#cb19-513" aria-hidden="true" tabindex="-1"></a><span class="in">Vocabulary size: 24</span></span>
<span id="cb19-514"><a href="#cb19-514" aria-hidden="true" tabindex="-1"></a><span class="in">Embedding size: 100</span></span>
<span id="cb19-515"><a href="#cb19-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-516"><a href="#cb19-516" aria-hidden="true" tabindex="-1"></a><span class="in">Most similar to 'network':</span></span>
<span id="cb19-517"><a href="#cb19-517" aria-hidden="true" tabindex="-1"></a><span class="in">  networks        0.892</span></span>
<span id="cb19-518"><a href="#cb19-518" aria-hidden="true" tabindex="-1"></a><span class="in">  community       0.715</span></span>
<span id="cb19-519"><a href="#cb19-519" aria-hidden="true" tabindex="-1"></a><span class="in">  clustering      0.687</span></span>
<span id="cb19-520"><a href="#cb19-520" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-521"><a href="#cb19-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-522"><a href="#cb19-522" aria-hidden="true" tabindex="-1"></a>Even with this tiny dataset, the model learns that "networks," "community," and "clustering" are related concepts.</span>
<span id="cb19-523"><a href="#cb19-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-524"><a href="#cb19-524" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb19-525"><a href="#cb19-525" aria-hidden="true" tabindex="-1"></a><span class="fu">## Training Considerations</span></span>
<span id="cb19-526"><a href="#cb19-526" aria-hidden="true" tabindex="-1"></a>For good embeddings, you need:</span>
<span id="cb19-527"><a href="#cb19-527" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Large corpus**: Millions of words minimum, billions ideal</span>
<span id="cb19-528"><a href="#cb19-528" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Clean preprocessing**: Tokenization, lowercasing, removing noise</span>
<span id="cb19-529"><a href="#cb19-529" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hyperparameter tuning**: vector_size, window, min_count</span>
<span id="cb19-530"><a href="#cb19-530" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Domain-specific data**: Train on text from your research domain</span>
<span id="cb19-531"><a href="#cb19-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-532"><a href="#cb19-532" aria-hidden="true" tabindex="-1"></a>For most research purposes, pre-trained models (Word2vec, GloVe) are sufficient. Train custom models only when your domain vocabulary is poorly covered.</span>
<span id="cb19-533"><a href="#cb19-533" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-534"><a href="#cb19-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-535"><a href="#cb19-535" aria-hidden="true" tabindex="-1"></a><span class="fu">## Limitations and Biases</span></span>
<span id="cb19-536"><a href="#cb19-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-537"><a href="#cb19-537" aria-hidden="true" tabindex="-1"></a>Word2vec learns from data, which means it also learns human biases present in text.</span>
<span id="cb19-538"><a href="#cb19-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-539"><a href="#cb19-539" aria-hidden="true" tabindex="-1"></a><span class="fu">### Gender Bias Example</span></span>
<span id="cb19-540"><a href="#cb19-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-543"><a href="#cb19-543" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-544"><a href="#cb19-544" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb19-545"><a href="#cb19-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-546"><a href="#cb19-546" aria-hidden="true" tabindex="-1"></a><span class="co"># Explore gender associations</span></span>
<span id="cb19-547"><a href="#cb19-547" aria-hidden="true" tabindex="-1"></a>male_professions <span class="op">=</span> model.most_similar(positive<span class="op">=</span>[<span class="st">'doctor'</span>, <span class="st">'man'</span>], negative<span class="op">=</span>[<span class="st">'woman'</span>], topn<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb19-548"><a href="#cb19-548" aria-hidden="true" tabindex="-1"></a>female_professions <span class="op">=</span> model.most_similar(positive<span class="op">=</span>[<span class="st">'nurse'</span>, <span class="st">'woman'</span>], negative<span class="op">=</span>[<span class="st">'man'</span>], topn<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb19-549"><a href="#cb19-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-550"><a href="#cb19-550" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Male-associated professions:"</span>)</span>
<span id="cb19-551"><a href="#cb19-551" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, sim <span class="kw">in</span> male_professions:</span>
<span id="cb19-552"><a href="#cb19-552" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-553"><a href="#cb19-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-554"><a href="#cb19-554" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Female-associated professions:"</span>)</span>
<span id="cb19-555"><a href="#cb19-555" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, sim <span class="kw">in</span> female_professions:</span>
<span id="cb19-556"><a href="#cb19-556" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-557"><a href="#cb19-557" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-558"><a href="#cb19-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-559"><a href="#cb19-559" aria-hidden="true" tabindex="-1"></a>The model might associate "doctor" with male and "nurse" with female, reflecting biases in training data (news articles, books, web pages). These biases can propagate into downstream applications.</span>
<span id="cb19-560"><a href="#cb19-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-561"><a href="#cb19-561" aria-hidden="true" tabindex="-1"></a>**Implications for research**:</span>
<span id="cb19-562"><a href="#cb19-562" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Be aware of biases in embeddings</span>
<span id="cb19-563"><a href="#cb19-563" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Don't use embeddings for sensitive applications without auditing</span>
<span id="cb19-564"><a href="#cb19-564" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Consider debiasing techniques if needed</span>
<span id="cb19-565"><a href="#cb19-565" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Embeddings can also be used to *measure* bias in text corpora</span>
<span id="cb19-566"><a href="#cb19-566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-567"><a href="#cb19-567" aria-hidden="true" tabindex="-1"></a>We'll explore bias measurement with **semantic axes** in the final section.</span>
<span id="cb19-568"><a href="#cb19-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-569"><a href="#cb19-569" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Bigger Picture</span></span>
<span id="cb19-570"><a href="#cb19-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-571"><a href="#cb19-571" aria-hidden="true" tabindex="-1"></a>You've now seen the **original approach to embeddings**—Word2vec—and understand:</span>
<span id="cb19-572"><a href="#cb19-572" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The distributional hypothesis (context determines meaning)</span>
<span id="cb19-573"><a href="#cb19-573" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>How Word2vec learns from skip-gram prediction</span>
<span id="cb19-574"><a href="#cb19-574" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Word algebra and semantic relationships</span>
<span id="cb19-575"><a href="#cb19-575" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>When static embeddings are sufficient vs. when contextual embeddings are necessary</span>
<span id="cb19-576"><a href="#cb19-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-577"><a href="#cb19-577" aria-hidden="true" tabindex="-1"></a>Word2vec was revolutionary in 2013. It enabled NLP to move from hand-crafted features to learned representations. But it had limitations (no context, polysemy), which transformers addressed.</span>
<span id="cb19-578"><a href="#cb19-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-579"><a href="#cb19-579" aria-hidden="true" tabindex="-1"></a>**Now let's go full circle: back to the basics.** Before Word2vec, before embeddings, there was the simplest possible representation of text—counting words. These fundamental methods are still relevant, and understanding them completes the picture.</span>
<span id="cb19-580"><a href="#cb19-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-581"><a href="#cb19-581" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb19-582"><a href="#cb19-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-583"><a href="#cb19-583" aria-hidden="true" tabindex="-1"></a>**Next**: <span class="co">[</span><span class="ot">Text Fundamentals: The Full Picture →</span><span class="co">](text-fundamentals.qmd)</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2025, Sadamori Kojaku</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions"><ul><li><a href="https://github.com/skojaku/applied-soft-comp/edit/main/m03-text/word-embeddings.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/skojaku/applied-soft-comp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/skojaku/applied-soft-comp">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>
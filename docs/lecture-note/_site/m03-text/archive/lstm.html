<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Sadamori Kojaku">
<meta name="dcterms.date" content="2025-10-19">

<title>Applied Soft Computing: Modeling Complex Systems with Deep Learning ‚Äì Applied Soft Computing</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-0348920b7671f696dc9078d39bff215e.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-fee01c958fd55f7b3b50896185ea610a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "|"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../assets/custom.css">
</head>

<body class="nav-sidebar docked nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../logo.jpg" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Applied Soft Computing</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../../index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-toolkit--workflow" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Toolkit &amp; Workflow</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-toolkit--workflow">    
        <li class="dropdown-header">‚îÄ‚îÄ‚îÄ Module 1 ‚îÄ‚îÄ‚îÄ</li>
        <li>
    <a class="dropdown-item" href="../../m01-toolkit/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m01-toolkit/git-github.html">
 <span class="dropdown-text">Git &amp; GitHub</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m01-toolkit/tidy-data.html">
 <span class="dropdown-text">Tidy Data</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m01-toolkit/data-provenance.html">
 <span class="dropdown-text">Data Provenance</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m01-toolkit/environments.qmd">
 <span class="dropdown-text">Environments</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-visualization" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Visualization</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-visualization">    
        <li class="dropdown-header">‚îÄ‚îÄ‚îÄ Module 2 ‚îÄ‚îÄ‚îÄ</li>
        <li>
    <a class="dropdown-item" href="../../m02-visualization/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m02-visualization/principles.html">
 <span class="dropdown-text">Principles</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m02-visualization/dimensionality-reduction.html">
 <span class="dropdown-text">High-Dimensional Data</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m02-visualization/networks.html">
 <span class="dropdown-text">Networks</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m02-visualization/time-series.html">
 <span class="dropdown-text">Time-Series</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-deep-learning" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Deep Learning</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-deep-learning">    
        <li class="dropdown-header">‚îÄ‚îÄ‚îÄ Module 3: Text ‚îÄ‚îÄ‚îÄ</li>
        <li>
    <a class="dropdown-item" href="../../m03-text/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m03-text/word2vec.md">
 <span class="dropdown-text">Word2Vec</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m03-text/lstm.md">
 <span class="dropdown-text">RNNs &amp; LSTMs</span></a>
  </li>  
        <li class="dropdown-header">‚îÄ‚îÄ‚îÄ Module 4: Images ‚îÄ‚îÄ‚îÄ</li>
        <li>
    <a class="dropdown-item" href="../../m04-images/overview.qmd">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m04-images/cnn.md">
 <span class="dropdown-text">CNNs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m04-images/resnet.md">
 <span class="dropdown-text">ResNet</span></a>
  </li>  
        <li class="dropdown-header">‚îÄ‚îÄ‚îÄ Module 5: Graphs ‚îÄ‚îÄ‚îÄ</li>
        <li>
    <a class="dropdown-item" href="../../m05-graphs/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m05-graphs/graph-embedding-w-word2vec.html">
 <span class="dropdown-text">Graph Embeddings</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m05-graphs/graph-convolutional-network.html">
 <span class="dropdown-text">GNNs</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-advanced-topics" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Advanced Topics</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-advanced-topics">    
        <li class="dropdown-header">‚îÄ‚îÄ‚îÄ Module 6: LLMs ‚îÄ‚îÄ‚îÄ</li>
        <li>
    <a class="dropdown-item" href="../../m06-llms/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m06-llms/transformers.md">
 <span class="dropdown-text">Transformers</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m06-llms/scaling-emergence.html">
 <span class="dropdown-text">Scaling &amp; Emergence</span></a>
  </li>  
        <li class="dropdown-header">‚îÄ‚îÄ‚îÄ Module 7: Self-Supervised ‚îÄ‚îÄ‚îÄ</li>
        <li>
    <a class="dropdown-item" href="../../m07-self-supervised/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m07-self-supervised/contrastive-learning.html">
 <span class="dropdown-text">Contrastive Learning</span></a>
  </li>  
        <li class="dropdown-header">‚îÄ‚îÄ‚îÄ Module 8: Explainability ‚îÄ‚îÄ‚îÄ</li>
        <li>
    <a class="dropdown-item" href="../../m08-explainability/overview.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../m08-explainability/fairness.html">
 <span class="dropdown-text">Fairness &amp; Ethics</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Applied Soft Computing: Modeling Complex Systems with Deep Learning</li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Course Information</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/welcome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About Us</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/why-applied-soft-computing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Why applied soft computing?</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/discord.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Discord</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/minidora-usage.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Using Minidora</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/how-to-submit-assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">How to submit assignment</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/deliverables.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deliverables</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 1: The Data Scientist‚Äôs Toolkit</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m01-toolkit/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m01-toolkit/git-github.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Version Control with Git &amp; GitHub</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m01-toolkit/tidy-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Tidy Data Philosophy</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m01-toolkit/data-provenance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Provenance</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m01-toolkit/reproduceability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Reproducibility</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 2: Visualizing Complexity</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m02-visualization/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m02-visualization/principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Principles of Effective Visualization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m02-visualization/1d-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing 1D Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m02-visualization/2d-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing 2D Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m02-visualization/highd-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing High-Dimensional Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m02-visualization/networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m02-visualization/time-series.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing Time-Series</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 3: Deep Learning for Text</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m03-text/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m03-text/llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Large Language Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m03-text/prompt-engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prompt Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m03-text/transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Transformer Modules</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m03-text/embedding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Embedding</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 4: Deep Learning for Images</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/overview.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/image-processing.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Processing Fundamentals</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/cnn.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/lenet.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LeNet Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/alexnet.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AlexNet: Deep CNN Revolution</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/vgg.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">VGG Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/inception.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Inception &amp; Multi-Scale Features</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/batch-normalization.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Batch Normalization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/resnet.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ResNet &amp; Skip Connections</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 5: Deep Learning for Graphs</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m05-graphs/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m05-graphs/spectral-embedding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Spectral Graph Embedding</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m05-graphs/graph-embedding-w-word2vec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Graph Embeddings with Word2Vec</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m05-graphs/spectral-vs-neural-embedding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Spectral vs.&nbsp;Neural Embeddings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m05-graphs/from-image-to-graph.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">From Images to Graphs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m05-graphs/graph-convolutional-network.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Graph Convolutional Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m05-graphs/popular-gnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Popular GNN Architectures</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m05-graphs/software.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">GNN Software &amp; Tools</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 6: Large Language Models &amp; Emergent Behavior</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m06-llms/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m06-llms/transformers.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Transformer Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m06-llms/bert.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">BERT &amp; Contextual Embeddings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m06-llms/sentence-bert.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sentence-BERT for Semantic Similarity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m06-llms/gpt.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">GPT &amp; Generative Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m06-llms/from-language-model-to-instruction-following.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">From Language Models to Instruction Following</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m06-llms/prompt-tuning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prompt Engineering &amp; In-Context Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m06-llms/scaling-emergence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Scaling Laws &amp; Emergent Abilities</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m06-llms/llms-as-complex-systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LLMs as Complex Systems</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 7: Self-Supervised Learning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m07-self-supervised/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m07-self-supervised/paradigm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Self-Supervised Paradigm</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m07-self-supervised/contrastive-learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Contrastive Learning (SimCLR)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m07-self-supervised/graphs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Self-Supervised Learning for Graphs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m07-self-supervised/time-series.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Self-Supervised Learning for Time-Series</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">Module 8: Explainability &amp; Ethics</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m08-explainability/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m08-explainability/need.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Need for Explainability</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m08-explainability/attention.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Attention Visualization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m08-explainability/lime-shap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LIME &amp; SHAP</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m08-explainability/fairness.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Algorithmic Fairness &amp; Bias</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m08-explainability/causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Causality vs.&nbsp;Correlation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false">
 <span class="menu-text">Legacy Materials</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m03-text/what-to-learn.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Word &amp; Document Embeddings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m03-text/what-to-learn.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Recurrent Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../m04-images/what-to-learn.md" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Processing (CNNs)</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Applied Soft Computing: Modeling Complex Systems with Deep Learning</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Sadamori Kojaku </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 19, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="long-short-term-memory-lstm" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Long Short-Term Memory (LSTM)</h1>
<p>While the RNN model is able to handle the sequence data, it struggles with the long-term dependencies. Long Short-Term Memory (LSTM) model {footcite}<code>hochreiter1997long</code> is designed to overcome this limitation by introducing a ‚Äúcontrolled‚Äù memory cell that can maintain information over long periods.</p>
<section id="lstm-architecture" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="lstm-architecture"><span class="header-section-number">1.1</span> LSTM Architecture</h2>
</section>
<section id="brncgzot-..figslstm.jpg" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="brncgzot-..figslstm.jpg"><span class="header-section-number">1.2</span> ```brncgzot ../figs/lstm.jpg</h2>
</section>
<section id="name-lstm" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="name-lstm"><span class="header-section-number">1.3</span> name: lstm</h2>
<p>LSTM architecture showing the cell state (horizontal line at top) and the three gates: forget gate, input gate, and output gate. The cell state acts as a conveyor belt carrying information forward, while gates control information flow.</p>
<pre><code>
The input and output of LSTM is fundamentally the same as the simple RNN we have seen before. The only difference is that LSTM has two kinds of hidden states: the hidden state $h_t$ and the cell state (or memory cell) $c_t$.
The hidden state $h_t$ is the output of the LSTM, and it is used to predict the next state. The cell state $c_t$ is the internal state of the LSTM, and it is used to maintain the memory of the LSTM.
Think of this cell as a conveyor belt that runs straight through the network, allowing information to flow forward largely unchanged. This cell state forms the backbone of the LSTM's memory system.

### Deep Dive into LSTM

Internally, LSTM controls the flow of information through the cell state by using three gates: the forget gate, the input gate, and the output gate. Let us break down each gate and see how they work.



#### Forget Gate

```{figure} ../figs/lstm-forget-gate.jpg
---
width: 400px
name: lstm-01
align: center
---

Forget gate. $\sigma(x_t, h_t)$ decides how much of the previous cell state $c_{t-1}$ to keep. For example, if $\sigma(x_t, h_t) = 0$, the forget gate will completely forget the previous cell state. If $\sigma(x_t, h_t) = 1$, the forget gate will keep the previous cell state. $\sigma$ is the sigmoid function which is bounded between 0 and 1.</code></pre>
<p>The <em>forget gate</em> examines the current input and the previous hidden state to decide what information to remove from the cell state. Like a selective eraser, it outputs values between 0 and 1 for each number in the cell state, where 0 means ‚Äúcompletely forget this‚Äù and 1 means ‚Äúkeep this entirely.‚Äù</p>
<section id="input-gate" class="level4">
<h4 class="anchored" data-anchor-id="input-gate">Input Gate</h4>
</section>
</section>
<section id="brncgzot-..figslstm-input-gate.jpg" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="brncgzot-..figslstm-input-gate.jpg"><span class="header-section-number">1.4</span> ```brncgzot ../figs/lstm-input-gate.jpg</h2>
<p>width: 400px name: lstm-02 align: center ‚Äî</p>
<p>Input gate. <span class="math inline">\sigma(x_t, h_t)</span> decides how much of the new information (that passes through the tanh function) to add to the cell state. For example, if <span class="math inline">\sigma(x_t, h_t) = 0</span>, the input gate will completely ignore the new candidate information. If <span class="math inline">\sigma(x_t, h_t) = 1</span>, the input gate will add the new candidate information to the cell state.</p>
<pre><code>
The input gate works together with a candidate memory generator to decide what new information to store. The input gate determines how much of the new candidate values should be added to the cell state, while the candidate memory proposes new values that could be added. This mechanism allows the network to selectively update its memory with new information.


#### Output Gate

```{figure} ../figs/lstm-output-gate.jpg
---
width: 400px
name: lstm-03
align: center
---
Output gate. $\sigma(x_t, h_t)$ decides how much of the cell state to reveal as output. For example, if $\sigma(x_t, h_t) = 0$, the output gate will completely hide the cell state. If $\sigma(x_t, h_t) = 1$, the output gate will reveal the cell state.</code></pre>
<p>The output gate controls what parts of the cell state should be revealed as output. It applies a filtered version of the cell state to produce the hidden state, which serves as both the output for the current timestep and part of the input for the next timestep.</p>
<pre class="{note}"><code>The key innovation of LSTMs is not just having memory, but having controlled memory. The network learns what to remember and what to forget, rather than trying to remember everything.</code></pre>
<section id="mathematical-framework" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-framework">Mathematical Framework</h3>
<p>The LSTM‚Äôs operation can be described through a series of equations that work together to process sequential data. The cell state <span class="math inline">C_t</span> evolves according to:</p>
<p><span class="math display"> C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t </span></p>
<p>where <span class="math inline">f_t</span> is the forget gate, <span class="math inline">i_t</span> is the input gate, and <span class="math inline">\tilde{C}_t</span> is the candidate memory. The <span class="math inline">\odot</span> symbol represents element-wise multiplication, allowing the gates to control information flow by scaling values between 0 and 1.</p>
<p>The gates themselves are neural networks that take the current input <span class="math inline">x_t</span> and previous hidden state <span class="math inline">h_{t-1}</span> as inputs:</p>
<p><span class="math display"> f_t = \sigma(W_f[h_{t-1}, x_t] + b_f) </span> <span class="math display"> i_t = \sigma(W_i[h_{t-1}, x_t] + b_i) </span> <span class="math display"> o_t = \sigma(W_o[h_{t-1}, x_t] + b_o) </span></p>
<p>The candidate memory is generated similarly:</p>
<p><span class="math display"> \tilde{C}_t = \tanh(W_c[h_{t-1}, x_t] + b_c) </span></p>
<p>Finally, the hidden state is produced by:</p>
<p><span class="math display"> h_t = o_t \odot \tanh(C_t) </span></p>
<p>```wxkrabaobtfw Memory Challenge Game üëæ :class: tip</p>
<p>Let us learn how LSTM works by playing <a href="memory-challenge.md">a memory challenge game üéÆ</a>. Given a sequence of numbers and possible questions, your job is to manage a limited memory to compress the sequence into three numbers üßÆ.</p>
<pre><code>

## Hands on

We will train an LSTM model to identify a wrapped character in a sequence. The task is to predict which character is enclosed in `&lt;&gt;` tags within a sequence of randomly ordered uppercase letters. For example,

- Input: `ABCDEFGHIJKLMNOPQRST&lt;U&gt;VWXYZ`
- Output: `U`

This requires a selective memory that can remember the wrapped character and forget the rest of the characters, which is exactly what LSTM is designed for.

Let us first import the necessary libraries.

```{code-cell} ipython
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
import random
import string</code></pre>
<p>Then, we define the data generation function.</p>
<p>```gvraboblvtg ipython :tags: [hide-input]</p>
<p>def generate_wrapped_char_data(n_samples=1000, seq_length=26): ‚Äú‚Äú‚Äù Generate training data where one random character in a sequence is wrapped with &lt;&gt;.</p>
<pre><code>Args:
    n_samples (int): Number of sequences to generate
    seq_length (int): Length of each sequence (default 26 for A-Z)

Returns:
    list: List of input sequences
    list: List of target characters (the wrapped characters)
"""
sequences = []
targets = []

for _ in range(n_samples):
    # Generate a random permutation of A-Z
    chars = list(string.ascii_uppercase)
    random.shuffle(chars)

    # Choose a random position for the wrapped character
    wrap_pos = random.randint(0, seq_length - 1)
    target_char = chars[wrap_pos]

    # Create the sequence with wrapped character
    chars.insert(wrap_pos, "&lt;")
    chars.insert(wrap_pos + 2, "&gt;")
    sequence = "".join(chars)

    sequences.append(sequence)
    targets.append(target_char)

vocab = list(string.ascii_uppercase) + ["&lt;", "&gt;"]

return sequences, targets, vocab</code></pre>
<p>sequences, targets, vocab = generate_wrapped_char_data(n_samples = 3)</p>
<p>for seq, target in zip(sequences, targets): print(f‚ÄùSequence: {seq}, Target: {target}‚Äú)</p>
<pre><code>
This function generates our training data by creating n_samples sequences, where each sequence is a random permutation of A-Z letters. In each sequence, one random character is wrapped with &lt;&gt; tags. The function returns both the generated sequences and their corresponding target characters (the wrapped ones) as separate lists.

The next step is to convert the sequences into tokenized representations that can be fed into the LSTM model.

```{code-cell} ipython
:tags: [hide-input]


def tokenize(sequences, vocab):
    retval = []
    for seq in sequences:
        r = []
        for char in seq:
            r.append(vocab.index(char))
        retval.append(r)
    return torch.tensor(retval)

X = tokenize(['ABCDEFGHIJKLMNOPQRST&lt;U&gt;VWXYZ', 'ABCDEFGHIJKLMNOPQRSTU&lt;V&gt;WXYZ'], vocab)
print("X:", X)
print("Shape of X:", X.shape)</code></pre>
<p>The output tensor <code>X</code> is of shape <code>(2, 28)</code>, where <code>2</code> is the number of samples, and <code>28</code> is the sequence length.</p>
<p>Now, let‚Äôs prepare the data and train the LSTM model. As before, we will use PyTorch‚Äôs <code>TensorDataset</code> and <code>DataLoader</code> to handle the data.</p>
<p>```gvraboblvtg ipython from torch.utils.data import Dataset</p>
</section>
</section>
</section>
<section id="generate-data" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Generate data</h1>
<p>sequences, targets, vocab = generate_wrapped_char_data(n_samples=1000)</p>
</section>
<section id="tokenize-data" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Tokenize data</h1>
<p>sequences = tokenize(sequences, vocab) targets = tokenize(targets, vocab)</p>
</section>
<section id="create-dataset" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Create dataset</h1>
<p>dataset = TensorDataset(sequences, targets)</p>
</section>
<section id="split-dataset-into-train-and-validation" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Split dataset into train and validation</h1>
<p>train_frac = 0.8 batch_size = 128 train_size = int(train_frac * len(dataset)) val_size = len(dataset) - train_size train_dataset, val_dataset = torch.utils.data.random_split( dataset, [train_size, val_size] )</p>
</section>
<section id="create-dataloaders" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Create dataloaders</h1>
<p>train_dataloader = DataLoader( train_dataset, batch_size=batch_size, shuffle=True, ) val_dataloader = DataLoader( val_dataset, batch_size=batch_size, shuffle=False, )</p>
<pre><code>
This creates an efficient data loading pipeline that combines our features and targets into a unified dataset structure. The data loader then handles batching the data, with a batch size of 128 samples which is a common choice that balances between training speed and memory usage. The loader also shuffles the data between epochs, which helps prevent the model from learning any unintended patterns based on the order of samples and improves generalization.

Now, let's define the model parameters and initialize the LSTM and output layer.

```{code-cell} ipython
:tags: [hide-input]

import pytorch_lightning as pyl

class CharDecoder(pyl.LightningModule):
    def __init__(self, vocab_size, output_size, hidden_size, num_layers):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=vocab_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
        )
        self.fc = nn.Linear(hidden_size, output_size)
        self.embedding = nn.Embedding(vocab_size, vocab_size)

        # One-hot encoding
        self.embedding.weight.data = torch.eye(vocab_size)
        self.embedding.weight.requires_grad = False

        # Validation loss
        self.val_losses = []

    def forward(self, x):

        # x is a tensor of shape (batch_size, seq_len)
        batch_size, seq_len = x.shape

        # To token index to one-hot encoding
        x = self.embedding(x)

        # To sentnece to sequence of chars
        hidden = self.init_hidden(batch_size)
        x, _ = self.lstm(x, hidden)
        x = x[:, -1, :]
        x = self.fc(x)
        return x

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = torch.nn.functional.cross_entropy(y_hat, y.reshape(-1))
        self.log("train_loss", loss)
        return loss

    def validation_step(self, batch, batch_idx):
        with torch.no_grad():
            x, y = batch
            y_hat = self(x)
            loss = torch.nn.functional.cross_entropy(y_hat, y.reshape(-1))
            self.log("val_loss", loss)
            self.val_losses.append(loss.cpu().item())
        return loss

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.01)

    def init_hidden(self, batch_size):
        return (
            torch.zeros(self.lstm.num_layers, batch_size, self.lstm.hidden_size, device=self.device),
            torch.zeros(self.lstm.num_layers, batch_size, self.lstm.hidden_size, device=self.device),
        )</code></pre>
<p><code>gvraboblvtg ipython model = CharDecoder(     vocab_size=28,     output_size=28,     hidden_size=32,     num_layers=1, )</code></p>
<ul>
<li><code>num_layers=1</code>: The model uses a single-layer LSTM architecture for sequence processing.</li>
<li><code>vocab_size=28</code>: The input dimension matches the vocabulary size to handle the one-hot encoded characters.</li>
<li><code>hidden_size=32</code>: The LSTM contains 32 hidden units per layer to learn complex sequential patterns in the data.</li>
<li><code>output_size=28</code>: The output from the LSTM feeds into a final linear layer that performs classification over the vocabulary space.</li>
</ul>
<pre class="{tip}"><code>The LSTM model can be stacked with multiple layers to learn more complex patterns {footcite:p}`irsoy2014opinion`. For example, `num_layers=2` will stack two LSTM layers on top of each other. The first layer will take the input and produce a hidden state, which will be used as the input for the second layer. The second layer will then produce the final hidden state and output. By stacking multiple layers, the model can learn more complex patterns in the data.

```{figure} https://i.sstatic.net/QxzoG.png
---
width: 400px
name: lstm-04
align: center
---

LSTM with multiple layers.</code></pre>
<pre class="{tip}"><code>`torch.nn.Embedding` is a convenient way to convert token indices to vectors. By default, it uses random initialization, but we can use one-hot encoding by setting `self.embedding.weight.data = torch.eye(vocab_size)`, and fix the weights by setting `self.embedding.weight.requires_grad = False`.</code></pre>
<pre class="{tip}"><code>In PyTorch lightning, `configure_optimizers` is a method that returns the optimizer and the learning rate scheduler.
We use ADAM {footcite}`kingma2014adam` as the optimizer. It is a popular optimizer for deep learning. It is a variant of stochastic gradient descent that can adaptively adjust the learning rate for each parameter using the first and second moments of the gradients.</code></pre>
<p>Now, let‚Äôs train the model.</p>
<p><code>gvraboblvtg ipython trainer = pyl.Trainer(     max_epochs=200,     enable_progress_bar=False,     enable_model_summary=False, ) trainer.fit(model, train_dataloader, val_dataloader)</code></p>
<p>Let‚Äôs plot the training loss.</p>
<p><code>gvraboblvtg ipython import matplotlib.pyplot as plt fig, ax = plt.subplots(figsize = (5,5)) ax.plot(model.val_losses) ax.set_title("Validation Loss") ax.set_xlabel("Epoch") ax.set_ylabel("Loss") plt.show()</code></p>
<p>The loss did not decrease nicely. This is a sign of the struggle of the LSTM model to learn the data. If you see this, you might need to decrease the learning rate, increase the number of epochs untile the loss becomes stable, or re-design the model architecture.</p>
<p>Nevertheless, let‚Äôs test the model.</p>
<p>```gvraboblvtg ipython eval_seq, eval_target, vocab = generate_wrapped_char_data(n_samples=5) X_eval = tokenize(eval_seq, vocab) y_eval = tokenize(eval_target, vocab)</p>
<p>model.eval() with torch.no_grad(): y_hat = model(X_eval) predicted_idx = torch.argmax(y_hat, dim=1) predicted_char = [vocab[idx] for idx in predicted_idx]</p>
<pre><code>for i in range(len(eval_seq)):
    print(f"Sequence: {eval_seq[i]}, Target: {eval_target[i]}, Predicted: {predicted_char[i]}")
accuracy = (predicted_idx == y_eval).sum() / len(y_eval)
print(f"Accuracy: {accuracy}")</code></pre>
<pre><code>
We see that the validation loss increased as the training progressed. This implies overfitting, i.e. the model is too expressive and memorizes the training data but lacks the understanding of underlying patterns and thereby generalizes poorly to unseen data. This is a common problem in deep learning.

```{admonition} Regularization
:class: tip

Regularization is a technique to prevent a model from overfitting. A traditional way to regularize a model is to add a penalty term to the loss function, e.g., $L = L_{\text{data}} + \lambda L_{\text{reg}}$, where $L_{\text{data}}$ is the data loss and $L_{\text{reg}}$ is the regularization loss.
Another popular regularization technique is dropout {footcite}`srivastava2014dropout`, which randomly drops out some neurons during training to prevent the model from relying too heavily on specific features.


```{figure} https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/1IrdJ5PghD9YoOyVAQ73MJw.gif
:alt: Dropout
:width: 50%
:align: center

Dropout.</code></pre>
<section id="exercise" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="exercise"><span class="header-section-number">6.1</span> üî• Exercise üî•</h2>
<p>Let‚Äôs fix the model by doing the following:</p>
<ol type="1">
<li>Try increasing the number of hidden units in the LSTM model.</li>
<li>Bring back to the original number of hidden units, and try increasing the number of layers in the LSTM model.</li>
<li>Add dropout to the model by using <code>torch.nn.Dropout</code> on the output of the LSTM layer.</li>
<li>Try increasing the learning rate.</li>
<li>Play with other hyperparameters, e.g., the number of epochs, batch size, etc.</li>
<li>Change the model to <code>nn.RNN</code> instead of <code>nn.LSTM</code>. You should replace <code>(h_n, c_n)</code> with <code>hidden</code> in the training and evaluation since <code>nn.RNN</code> does not have a cell state.</li>
</ol>
<p>You should be able to see the model to correctly predict the wrapped character.</p>
<pre class="{footbibliography}"><code>:style: unsrt</code></pre>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb14" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="an">jupytext:</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co">  formats: md:myst</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co">  text_representation:</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co">    extension: .md</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co">    format_name: myst</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="an">kernelspec:</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="co">  display_name: Python 3</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="co">  language: python</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co">  name: python3</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="fu"># Long Short-Term Memory (LSTM)</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>While the RNN model is able to handle the sequence data, it struggles with the long-term dependencies. Long Short-Term Memory (LSTM) model {footcite}<span class="in">`hochreiter1997long`</span> is designed to overcome this limitation by introducing a "controlled" memory cell that can maintain information over long periods.</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="fu">## LSTM Architecture</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a><span class="in">```{figure} ../figs/lstm.jpg</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a><span class="in">---</span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a><span class="in">name: lstm</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a><span class="in">---</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a><span class="in">LSTM architecture showing the cell state (horizontal line at top) and the three gates: forget gate, input gate, and output gate. The cell state acts as a conveyor belt carrying information forward, while gates control information flow.</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>The input and output of LSTM is fundamentally the same as the simple RNN we have seen before. The only difference is that LSTM has two kinds of hidden states: the hidden state $h_t$ and the cell state (or memory cell) $c_t$.</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>The hidden state $h_t$ is the output of the LSTM, and it is used to predict the next state. The cell state $c_t$ is the internal state of the LSTM, and it is used to maintain the memory of the LSTM.</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>Think of this cell as a conveyor belt that runs straight through the network, allowing information to flow forward largely unchanged. This cell state forms the backbone of the LSTM's memory system.</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a><span class="fu">### Deep Dive into LSTM</span></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>Internally, LSTM controls the flow of information through the cell state by using three gates: the forget gate, the input gate, and the output gate. Let us break down each gate and see how they work.</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Forget Gate</span></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a><span class="in">```{figure} ../figs/lstm-forget-gate.jpg</span></span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a><span class="in">---</span></span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a><span class="in">width: 400px</span></span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a><span class="in">name: lstm-01</span></span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a><span class="in">align: center</span></span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a><span class="in">---</span></span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a><span class="in">Forget gate. $\sigma(x_t, h_t)$ decides how much of the previous cell state $c_{t-1}$ to keep. For example, if $\sigma(x_t, h_t) = 0$, the forget gate will completely forget the previous cell state. If $\sigma(x_t, h_t) = 1$, the forget gate will keep the previous cell state. $\sigma$ is the sigmoid function which is bounded between 0 and 1.</span></span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a>The *forget gate* examines the current input and the previous hidden state to decide what information to remove from the cell state. Like a selective eraser, it outputs values between 0 and 1 for each number in the cell state, where 0 means "completely forget this" and 1 means "keep this entirely."</span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Input Gate</span></span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a><span class="in">```{figure} ../figs/lstm-input-gate.jpg</span></span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a><span class="in">---</span></span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a><span class="in">width: 400px</span></span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a><span class="in">name: lstm-02</span></span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a><span class="in">align: center</span></span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a><span class="in">---</span></span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a><span class="in">Input gate. $\sigma(x_t, h_t)$ decides how much of the new information (that passes through the tanh function) to add to the cell state. For example, if $\sigma(x_t, h_t) = 0$, the input gate will completely ignore the new candidate information. If $\sigma(x_t, h_t) = 1$, the input gate will add the new candidate information to the cell state.</span></span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-64"><a href="#cb14-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-65"><a href="#cb14-65" aria-hidden="true" tabindex="-1"></a>The input gate works together with a candidate memory generator to decide what new information to store. The input gate determines how much of the new candidate values should be added to the cell state, while the candidate memory proposes new values that could be added. This mechanism allows the network to selectively update its memory with new information.</span>
<span id="cb14-66"><a href="#cb14-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-67"><a href="#cb14-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-68"><a href="#cb14-68" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Output Gate</span></span>
<span id="cb14-69"><a href="#cb14-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-70"><a href="#cb14-70" aria-hidden="true" tabindex="-1"></a><span class="in">```{figure} ../figs/lstm-output-gate.jpg</span></span>
<span id="cb14-71"><a href="#cb14-71" aria-hidden="true" tabindex="-1"></a><span class="in">---</span></span>
<span id="cb14-72"><a href="#cb14-72" aria-hidden="true" tabindex="-1"></a><span class="in">width: 400px</span></span>
<span id="cb14-73"><a href="#cb14-73" aria-hidden="true" tabindex="-1"></a><span class="in">name: lstm-03</span></span>
<span id="cb14-74"><a href="#cb14-74" aria-hidden="true" tabindex="-1"></a><span class="in">align: center</span></span>
<span id="cb14-75"><a href="#cb14-75" aria-hidden="true" tabindex="-1"></a><span class="in">---</span></span>
<span id="cb14-76"><a href="#cb14-76" aria-hidden="true" tabindex="-1"></a><span class="in">Output gate. $\sigma(x_t, h_t)$ decides how much of the cell state to reveal as output. For example, if $\sigma(x_t, h_t) = 0$, the output gate will completely hide the cell state. If $\sigma(x_t, h_t) = 1$, the output gate will reveal the cell state.</span></span>
<span id="cb14-77"><a href="#cb14-77" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-78"><a href="#cb14-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-79"><a href="#cb14-79" aria-hidden="true" tabindex="-1"></a>The output gate controls what parts of the cell state should be revealed as output. It applies a filtered version of the cell state to produce the hidden state, which serves as both the output for the current timestep and part of the input for the next timestep.</span>
<span id="cb14-80"><a href="#cb14-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-83"><a href="#cb14-83" aria-hidden="true" tabindex="-1"></a><span class="in">```{note}</span></span>
<span id="cb14-84"><a href="#cb14-84" aria-hidden="true" tabindex="-1"></a><span class="in">The key innovation of LSTMs is not just having memory, but having controlled memory. The network learns what to remember and what to forget, rather than trying to remember everything.</span></span>
<span id="cb14-85"><a href="#cb14-85" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-86"><a href="#cb14-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-87"><a href="#cb14-87" aria-hidden="true" tabindex="-1"></a><span class="fu">### Mathematical Framework</span></span>
<span id="cb14-88"><a href="#cb14-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-89"><a href="#cb14-89" aria-hidden="true" tabindex="-1"></a>The LSTM's operation can be described through a series of equations that work together to process sequential data. The cell state $C_t$ evolves according to:</span>
<span id="cb14-90"><a href="#cb14-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-91"><a href="#cb14-91" aria-hidden="true" tabindex="-1"></a>$$ C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t $$</span>
<span id="cb14-92"><a href="#cb14-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-93"><a href="#cb14-93" aria-hidden="true" tabindex="-1"></a>where $f_t$ is the forget gate, $i_t$ is the input gate, and $\tilde{C}_t$ is the candidate memory. The $\odot$ symbol represents element-wise multiplication, allowing the gates to control information flow by scaling values between 0 and 1.</span>
<span id="cb14-94"><a href="#cb14-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-95"><a href="#cb14-95" aria-hidden="true" tabindex="-1"></a>The gates themselves are neural networks that take the current input $x_t$ and previous hidden state $h_{t-1}$ as inputs:</span>
<span id="cb14-96"><a href="#cb14-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-97"><a href="#cb14-97" aria-hidden="true" tabindex="-1"></a>$$ f_t = \sigma(W_f<span class="co">[</span><span class="ot">h_{t-1}, x_t</span><span class="co">]</span> + b_f) $$</span>
<span id="cb14-98"><a href="#cb14-98" aria-hidden="true" tabindex="-1"></a>$$ i_t = \sigma(W_i<span class="co">[</span><span class="ot">h_{t-1}, x_t</span><span class="co">]</span> + b_i) $$</span>
<span id="cb14-99"><a href="#cb14-99" aria-hidden="true" tabindex="-1"></a>$$ o_t = \sigma(W_o<span class="co">[</span><span class="ot">h_{t-1}, x_t</span><span class="co">]</span> + b_o) $$</span>
<span id="cb14-100"><a href="#cb14-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-101"><a href="#cb14-101" aria-hidden="true" tabindex="-1"></a>The candidate memory is generated similarly:</span>
<span id="cb14-102"><a href="#cb14-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-103"><a href="#cb14-103" aria-hidden="true" tabindex="-1"></a>$$ \tilde{C}_t = \tanh(W_c[h_{t-1}, x_t] + b_c) $$</span>
<span id="cb14-104"><a href="#cb14-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-105"><a href="#cb14-105" aria-hidden="true" tabindex="-1"></a>Finally, the hidden state is produced by:</span>
<span id="cb14-106"><a href="#cb14-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-107"><a href="#cb14-107" aria-hidden="true" tabindex="-1"></a>$$ h_t = o_t \odot \tanh(C_t) $$</span>
<span id="cb14-108"><a href="#cb14-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-109"><a href="#cb14-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-110"><a href="#cb14-110" aria-hidden="true" tabindex="-1"></a><span class="in">```{admonition} Memory Challenge Game üëæ</span></span>
<span id="cb14-111"><a href="#cb14-111" aria-hidden="true" tabindex="-1"></a><span class="in">:class: tip</span></span>
<span id="cb14-112"><a href="#cb14-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-113"><a href="#cb14-113" aria-hidden="true" tabindex="-1"></a><span class="in">Let us learn how LSTM works by playing [a memory challenge game üéÆ](memory-challenge.md). Given a sequence of numbers and possible questions, your job is to manage a limited memory to compress the sequence into three numbers üßÆ.</span></span>
<span id="cb14-114"><a href="#cb14-114" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-115"><a href="#cb14-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-116"><a href="#cb14-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-117"><a href="#cb14-117" aria-hidden="true" tabindex="-1"></a><span class="fu">## Hands on</span></span>
<span id="cb14-118"><a href="#cb14-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-119"><a href="#cb14-119" aria-hidden="true" tabindex="-1"></a>We will train an LSTM model to identify a wrapped character in a sequence. The task is to predict which character is enclosed in <span class="in">`&lt;&gt;`</span> tags within a sequence of randomly ordered uppercase letters. For example,</span>
<span id="cb14-120"><a href="#cb14-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-121"><a href="#cb14-121" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Input: <span class="in">`ABCDEFGHIJKLMNOPQRST&lt;U&gt;VWXYZ`</span></span>
<span id="cb14-122"><a href="#cb14-122" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Output: <span class="in">`U`</span></span>
<span id="cb14-123"><a href="#cb14-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-124"><a href="#cb14-124" aria-hidden="true" tabindex="-1"></a>This requires a selective memory that can remember the wrapped character and forget the rest of the characters, which is exactly what LSTM is designed for.</span>
<span id="cb14-125"><a href="#cb14-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-126"><a href="#cb14-126" aria-hidden="true" tabindex="-1"></a>Let us first import the necessary libraries.</span>
<span id="cb14-127"><a href="#cb14-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-128"><a href="#cb14-128" aria-hidden="true" tabindex="-1"></a><span class="in">```{code-cell} ipython</span></span>
<span id="cb14-129"><a href="#cb14-129" aria-hidden="true" tabindex="-1"></a>import torch</span>
<span id="cb14-130"><a href="#cb14-130" aria-hidden="true" tabindex="-1"></a>import torch<span class="op">.</span>nn as nn</span>
<span id="cb14-131"><a href="#cb14-131" aria-hidden="true" tabindex="-1"></a>from torch<span class="op">.</span>utils<span class="op">.</span>data import TensorDataset<span class="op">,</span> DataLoader</span>
<span id="cb14-132"><a href="#cb14-132" aria-hidden="true" tabindex="-1"></a>import random</span>
<span id="cb14-133"><a href="#cb14-133" aria-hidden="true" tabindex="-1"></a>import string</span>
<span id="cb14-134"><a href="#cb14-134" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-135"><a href="#cb14-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-136"><a href="#cb14-136" aria-hidden="true" tabindex="-1"></a>Then, we define the data generation function.</span>
<span id="cb14-137"><a href="#cb14-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-138"><a href="#cb14-138" aria-hidden="true" tabindex="-1"></a><span class="in">```{code-cell} ipython</span></span>
<span id="cb14-139"><a href="#cb14-139" aria-hidden="true" tabindex="-1"></a><span class="op">:</span>tags<span class="op">:</span> <span class="op">[</span>hide<span class="op">-</span>input<span class="op">]</span></span>
<span id="cb14-140"><a href="#cb14-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-141"><a href="#cb14-141" aria-hidden="true" tabindex="-1"></a>def generate_wrapped_char_data<span class="op">(</span>n_samples<span class="op">=</span><span class="dv">1000</span><span class="op">,</span> seq_length<span class="op">=</span><span class="dv">26</span><span class="op">):</span></span>
<span id="cb14-142"><a href="#cb14-142" aria-hidden="true" tabindex="-1"></a>    <span class="st">"""</span></span>
<span id="cb14-143"><a href="#cb14-143" aria-hidden="true" tabindex="-1"></a>    Generate training data where one random character in a sequence is wrapped with <span class="op">&lt;&gt;.</span></span>
<span id="cb14-144"><a href="#cb14-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-145"><a href="#cb14-145" aria-hidden="true" tabindex="-1"></a>    Args<span class="op">:</span></span>
<span id="cb14-146"><a href="#cb14-146" aria-hidden="true" tabindex="-1"></a>        n_samples <span class="op">(</span><span class="dt">int</span><span class="op">):</span> Number of sequences to generate</span>
<span id="cb14-147"><a href="#cb14-147" aria-hidden="true" tabindex="-1"></a>        seq_length <span class="op">(</span><span class="dt">int</span><span class="op">):</span> Length of each sequence <span class="op">(</span><span class="cf">default</span> <span class="dv">26</span> <span class="cf">for</span> A<span class="op">-</span>Z<span class="op">)</span></span>
<span id="cb14-148"><a href="#cb14-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-149"><a href="#cb14-149" aria-hidden="true" tabindex="-1"></a>    Returns<span class="op">:</span></span>
<span id="cb14-150"><a href="#cb14-150" aria-hidden="true" tabindex="-1"></a>        list<span class="op">:</span> List of input sequences</span>
<span id="cb14-151"><a href="#cb14-151" aria-hidden="true" tabindex="-1"></a>        list<span class="op">:</span> List of target characters <span class="op">(</span>the wrapped characters<span class="op">)</span></span>
<span id="cb14-152"><a href="#cb14-152" aria-hidden="true" tabindex="-1"></a>    <span class="st">"""</span></span>
<span id="cb14-153"><a href="#cb14-153" aria-hidden="true" tabindex="-1"></a>    sequences <span class="op">=</span> <span class="op">[]</span></span>
<span id="cb14-154"><a href="#cb14-154" aria-hidden="true" tabindex="-1"></a>    targets <span class="op">=</span> <span class="op">[]</span></span>
<span id="cb14-155"><a href="#cb14-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-156"><a href="#cb14-156" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ in range<span class="op">(</span>n_samples<span class="op">):</span></span>
<span id="cb14-157"><a href="#cb14-157" aria-hidden="true" tabindex="-1"></a>        <span class="pp"># </span><span class="er">Generate a random permutation of A-Z</span></span>
<span id="cb14-158"><a href="#cb14-158" aria-hidden="true" tabindex="-1"></a>        chars <span class="op">=</span> list<span class="op">(</span>string<span class="op">.</span>ascii_uppercase<span class="op">)</span></span>
<span id="cb14-159"><a href="#cb14-159" aria-hidden="true" tabindex="-1"></a>        random<span class="op">.</span>shuffle<span class="op">(</span>chars<span class="op">)</span></span>
<span id="cb14-160"><a href="#cb14-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-161"><a href="#cb14-161" aria-hidden="true" tabindex="-1"></a>        <span class="pp"># </span><span class="er">Choose a random position for the wrapped character</span></span>
<span id="cb14-162"><a href="#cb14-162" aria-hidden="true" tabindex="-1"></a>        wrap_pos <span class="op">=</span> random<span class="op">.</span>randint<span class="op">(</span><span class="dv">0</span><span class="op">,</span> seq_length <span class="op">-</span> <span class="dv">1</span><span class="op">)</span></span>
<span id="cb14-163"><a href="#cb14-163" aria-hidden="true" tabindex="-1"></a>        target_char <span class="op">=</span> chars<span class="op">[</span>wrap_pos<span class="op">]</span></span>
<span id="cb14-164"><a href="#cb14-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-165"><a href="#cb14-165" aria-hidden="true" tabindex="-1"></a>        <span class="pp"># </span><span class="er">Create the sequence with wrapped character</span></span>
<span id="cb14-166"><a href="#cb14-166" aria-hidden="true" tabindex="-1"></a>        chars<span class="op">.</span>insert<span class="op">(</span>wrap_pos<span class="op">,</span> <span class="st">"&lt;"</span><span class="op">)</span></span>
<span id="cb14-167"><a href="#cb14-167" aria-hidden="true" tabindex="-1"></a>        chars<span class="op">.</span>insert<span class="op">(</span>wrap_pos <span class="op">+</span> <span class="dv">2</span><span class="op">,</span> <span class="st">"&gt;"</span><span class="op">)</span></span>
<span id="cb14-168"><a href="#cb14-168" aria-hidden="true" tabindex="-1"></a>        sequence <span class="op">=</span> <span class="st">""</span><span class="op">.</span>join<span class="op">(</span>chars<span class="op">)</span></span>
<span id="cb14-169"><a href="#cb14-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-170"><a href="#cb14-170" aria-hidden="true" tabindex="-1"></a>        sequences<span class="op">.</span>append<span class="op">(</span>sequence<span class="op">)</span></span>
<span id="cb14-171"><a href="#cb14-171" aria-hidden="true" tabindex="-1"></a>        targets<span class="op">.</span>append<span class="op">(</span>target_char<span class="op">)</span></span>
<span id="cb14-172"><a href="#cb14-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-173"><a href="#cb14-173" aria-hidden="true" tabindex="-1"></a>    vocab <span class="op">=</span> list<span class="op">(</span>string<span class="op">.</span>ascii_uppercase<span class="op">)</span> <span class="op">+</span> <span class="op">[</span><span class="st">"&lt;"</span><span class="op">,</span> <span class="st">"&gt;"</span><span class="op">]</span></span>
<span id="cb14-174"><a href="#cb14-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-175"><a href="#cb14-175" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sequences<span class="op">,</span> targets<span class="op">,</span> vocab</span>
<span id="cb14-176"><a href="#cb14-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-177"><a href="#cb14-177" aria-hidden="true" tabindex="-1"></a>sequences<span class="op">,</span> targets<span class="op">,</span> vocab <span class="op">=</span> generate_wrapped_char_data<span class="op">(</span>n_samples <span class="op">=</span> <span class="dv">3</span><span class="op">)</span></span>
<span id="cb14-178"><a href="#cb14-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-179"><a href="#cb14-179" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> seq<span class="op">,</span> target in zip<span class="op">(</span>sequences<span class="op">,</span> targets<span class="op">):</span></span>
<span id="cb14-180"><a href="#cb14-180" aria-hidden="true" tabindex="-1"></a>    print<span class="op">(</span>f<span class="st">"Sequence: {seq}, Target: {target}"</span><span class="op">)</span></span>
<span id="cb14-181"><a href="#cb14-181" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-182"><a href="#cb14-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-183"><a href="#cb14-183" aria-hidden="true" tabindex="-1"></a>This function generates our training data by creating n_samples sequences, where each sequence is a random permutation of A-Z letters. In each sequence, one random character is wrapped with &lt;&gt; tags. The function returns both the generated sequences and their corresponding target characters (the wrapped ones) as separate lists.</span>
<span id="cb14-184"><a href="#cb14-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-185"><a href="#cb14-185" aria-hidden="true" tabindex="-1"></a>The next step is to convert the sequences into tokenized representations that can be fed into the LSTM model.</span>
<span id="cb14-186"><a href="#cb14-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-187"><a href="#cb14-187" aria-hidden="true" tabindex="-1"></a><span class="in">```{code-cell} ipython</span></span>
<span id="cb14-188"><a href="#cb14-188" aria-hidden="true" tabindex="-1"></a><span class="op">:</span>tags<span class="op">:</span> <span class="op">[</span>hide<span class="op">-</span>input<span class="op">]</span></span>
<span id="cb14-189"><a href="#cb14-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-190"><a href="#cb14-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-191"><a href="#cb14-191" aria-hidden="true" tabindex="-1"></a>def tokenize<span class="op">(</span>sequences<span class="op">,</span> vocab<span class="op">):</span></span>
<span id="cb14-192"><a href="#cb14-192" aria-hidden="true" tabindex="-1"></a>    retval <span class="op">=</span> <span class="op">[]</span></span>
<span id="cb14-193"><a href="#cb14-193" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> seq in sequences<span class="op">:</span></span>
<span id="cb14-194"><a href="#cb14-194" aria-hidden="true" tabindex="-1"></a>        r <span class="op">=</span> <span class="op">[]</span></span>
<span id="cb14-195"><a href="#cb14-195" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> <span class="dt">char</span> in seq<span class="op">:</span></span>
<span id="cb14-196"><a href="#cb14-196" aria-hidden="true" tabindex="-1"></a>            r<span class="op">.</span>append<span class="op">(</span>vocab<span class="op">.</span>index<span class="op">(</span><span class="dt">char</span><span class="op">))</span></span>
<span id="cb14-197"><a href="#cb14-197" aria-hidden="true" tabindex="-1"></a>        retval<span class="op">.</span>append<span class="op">(</span>r<span class="op">)</span></span>
<span id="cb14-198"><a href="#cb14-198" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch<span class="op">.</span>tensor<span class="op">(</span>retval<span class="op">)</span></span>
<span id="cb14-199"><a href="#cb14-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-200"><a href="#cb14-200" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> tokenize<span class="op">([</span><span class="ch">'A</span><span class="er">BCDEFGHIJKLMNOPQRST&lt;U&gt;VWXYZ</span><span class="ch">'</span><span class="op">,</span> <span class="ch">'A</span><span class="er">BCDEFGHIJKLMNOPQRSTU&lt;V&gt;WXYZ</span><span class="ch">'</span><span class="op">],</span> vocab<span class="op">)</span></span>
<span id="cb14-201"><a href="#cb14-201" aria-hidden="true" tabindex="-1"></a>print<span class="op">(</span><span class="st">"X:"</span><span class="op">,</span> X<span class="op">)</span></span>
<span id="cb14-202"><a href="#cb14-202" aria-hidden="true" tabindex="-1"></a>print<span class="op">(</span><span class="st">"Shape of X:"</span><span class="op">,</span> X<span class="op">.</span>shape<span class="op">)</span></span>
<span id="cb14-203"><a href="#cb14-203" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-204"><a href="#cb14-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-205"><a href="#cb14-205" aria-hidden="true" tabindex="-1"></a>The output tensor <span class="in">`X`</span> is of shape <span class="in">`(2, 28)`</span>, where <span class="in">`2`</span> is the number of samples, and <span class="in">`28`</span> is the sequence length.</span>
<span id="cb14-206"><a href="#cb14-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-207"><a href="#cb14-207" aria-hidden="true" tabindex="-1"></a>Now, let's prepare the data and train the LSTM model. As before, we will use PyTorch's <span class="in">`TensorDataset`</span> and <span class="in">`DataLoader`</span> to handle the data.</span>
<span id="cb14-208"><a href="#cb14-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-209"><a href="#cb14-209" aria-hidden="true" tabindex="-1"></a><span class="in">```{code-cell} ipython</span></span>
<span id="cb14-210"><a href="#cb14-210" aria-hidden="true" tabindex="-1"></a>from torch<span class="op">.</span>utils<span class="op">.</span>data import Dataset</span>
<span id="cb14-211"><a href="#cb14-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-212"><a href="#cb14-212" aria-hidden="true" tabindex="-1"></a><span class="pp"># </span><span class="er">Generate data</span></span>
<span id="cb14-213"><a href="#cb14-213" aria-hidden="true" tabindex="-1"></a>sequences<span class="op">,</span> targets<span class="op">,</span> vocab <span class="op">=</span> generate_wrapped_char_data<span class="op">(</span>n_samples<span class="op">=</span><span class="dv">1000</span><span class="op">)</span></span>
<span id="cb14-214"><a href="#cb14-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-215"><a href="#cb14-215" aria-hidden="true" tabindex="-1"></a><span class="pp"># </span><span class="er">Tokenize data</span></span>
<span id="cb14-216"><a href="#cb14-216" aria-hidden="true" tabindex="-1"></a>sequences <span class="op">=</span> tokenize<span class="op">(</span>sequences<span class="op">,</span> vocab<span class="op">)</span></span>
<span id="cb14-217"><a href="#cb14-217" aria-hidden="true" tabindex="-1"></a>targets <span class="op">=</span> tokenize<span class="op">(</span>targets<span class="op">,</span> vocab<span class="op">)</span></span>
<span id="cb14-218"><a href="#cb14-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-219"><a href="#cb14-219" aria-hidden="true" tabindex="-1"></a><span class="pp"># </span><span class="er">Create dataset</span></span>
<span id="cb14-220"><a href="#cb14-220" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> TensorDataset<span class="op">(</span>sequences<span class="op">,</span> targets<span class="op">)</span></span>
<span id="cb14-221"><a href="#cb14-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-222"><a href="#cb14-222" aria-hidden="true" tabindex="-1"></a><span class="pp"># </span><span class="er">Split dataset into train and validation</span></span>
<span id="cb14-223"><a href="#cb14-223" aria-hidden="true" tabindex="-1"></a>train_frac <span class="op">=</span> <span class="fl">0.8</span></span>
<span id="cb14-224"><a href="#cb14-224" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb14-225"><a href="#cb14-225" aria-hidden="true" tabindex="-1"></a>train_size <span class="op">=</span> <span class="dt">int</span><span class="op">(</span>train_frac <span class="op">*</span> len<span class="op">(</span>dataset<span class="op">))</span></span>
<span id="cb14-226"><a href="#cb14-226" aria-hidden="true" tabindex="-1"></a>val_size <span class="op">=</span> len<span class="op">(</span>dataset<span class="op">)</span> <span class="op">-</span> train_size</span>
<span id="cb14-227"><a href="#cb14-227" aria-hidden="true" tabindex="-1"></a>train_dataset<span class="op">,</span> val_dataset <span class="op">=</span> torch<span class="op">.</span>utils<span class="op">.</span>data<span class="op">.</span>random_split<span class="op">(</span></span>
<span id="cb14-228"><a href="#cb14-228" aria-hidden="true" tabindex="-1"></a>    dataset<span class="op">,</span> <span class="op">[</span>train_size<span class="op">,</span> val_size<span class="op">]</span></span>
<span id="cb14-229"><a href="#cb14-229" aria-hidden="true" tabindex="-1"></a><span class="op">)</span></span>
<span id="cb14-230"><a href="#cb14-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-231"><a href="#cb14-231" aria-hidden="true" tabindex="-1"></a><span class="pp"># </span><span class="er">Create dataloaders</span></span>
<span id="cb14-232"><a href="#cb14-232" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> DataLoader<span class="op">(</span></span>
<span id="cb14-233"><a href="#cb14-233" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">,</span></span>
<span id="cb14-234"><a href="#cb14-234" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>batch_size<span class="op">,</span></span>
<span id="cb14-235"><a href="#cb14-235" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span>True<span class="op">,</span></span>
<span id="cb14-236"><a href="#cb14-236" aria-hidden="true" tabindex="-1"></a><span class="op">)</span></span>
<span id="cb14-237"><a href="#cb14-237" aria-hidden="true" tabindex="-1"></a>val_dataloader <span class="op">=</span> DataLoader<span class="op">(</span></span>
<span id="cb14-238"><a href="#cb14-238" aria-hidden="true" tabindex="-1"></a>    val_dataset<span class="op">,</span></span>
<span id="cb14-239"><a href="#cb14-239" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>batch_size<span class="op">,</span></span>
<span id="cb14-240"><a href="#cb14-240" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span>False<span class="op">,</span></span>
<span id="cb14-241"><a href="#cb14-241" aria-hidden="true" tabindex="-1"></a><span class="op">)</span></span>
<span id="cb14-242"><a href="#cb14-242" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-243"><a href="#cb14-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-244"><a href="#cb14-244" aria-hidden="true" tabindex="-1"></a>This creates an efficient data loading pipeline that combines our features and targets into a unified dataset structure. The data loader then handles batching the data, with a batch size of 128 samples which is a common choice that balances between training speed and memory usage. The loader also shuffles the data between epochs, which helps prevent the model from learning any unintended patterns based on the order of samples and improves generalization.</span>
<span id="cb14-245"><a href="#cb14-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-246"><a href="#cb14-246" aria-hidden="true" tabindex="-1"></a>Now, let's define the model parameters and initialize the LSTM and output layer.</span>
<span id="cb14-247"><a href="#cb14-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-248"><a href="#cb14-248" aria-hidden="true" tabindex="-1"></a><span class="in">```{code-cell} ipython</span></span>
<span id="cb14-249"><a href="#cb14-249" aria-hidden="true" tabindex="-1"></a><span class="op">:</span>tags<span class="op">:</span> <span class="op">[</span>hide<span class="op">-</span>input<span class="op">]</span></span>
<span id="cb14-250"><a href="#cb14-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-251"><a href="#cb14-251" aria-hidden="true" tabindex="-1"></a>import pytorch_lightning as pyl</span>
<span id="cb14-252"><a href="#cb14-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-253"><a href="#cb14-253" aria-hidden="true" tabindex="-1"></a>class CharDecoder<span class="op">(</span>pyl<span class="op">.</span>LightningModule<span class="op">):</span></span>
<span id="cb14-254"><a href="#cb14-254" aria-hidden="true" tabindex="-1"></a>    def __init__<span class="op">(</span>self<span class="op">,</span> vocab_size<span class="op">,</span> output_size<span class="op">,</span> hidden_size<span class="op">,</span> num_layers<span class="op">):</span></span>
<span id="cb14-255"><a href="#cb14-255" aria-hidden="true" tabindex="-1"></a>        super<span class="op">().</span>__init__<span class="op">()</span></span>
<span id="cb14-256"><a href="#cb14-256" aria-hidden="true" tabindex="-1"></a>        self<span class="op">.</span>lstm <span class="op">=</span> nn<span class="op">.</span>LSTM<span class="op">(</span></span>
<span id="cb14-257"><a href="#cb14-257" aria-hidden="true" tabindex="-1"></a>            input_size<span class="op">=</span>vocab_size<span class="op">,</span></span>
<span id="cb14-258"><a href="#cb14-258" aria-hidden="true" tabindex="-1"></a>            hidden_size<span class="op">=</span>hidden_size<span class="op">,</span></span>
<span id="cb14-259"><a href="#cb14-259" aria-hidden="true" tabindex="-1"></a>            num_layers<span class="op">=</span>num_layers<span class="op">,</span></span>
<span id="cb14-260"><a href="#cb14-260" aria-hidden="true" tabindex="-1"></a>            batch_first<span class="op">=</span>True<span class="op">,</span></span>
<span id="cb14-261"><a href="#cb14-261" aria-hidden="true" tabindex="-1"></a>        <span class="op">)</span></span>
<span id="cb14-262"><a href="#cb14-262" aria-hidden="true" tabindex="-1"></a>        self<span class="op">.</span>fc <span class="op">=</span> nn<span class="op">.</span>Linear<span class="op">(</span>hidden_size<span class="op">,</span> output_size<span class="op">)</span></span>
<span id="cb14-263"><a href="#cb14-263" aria-hidden="true" tabindex="-1"></a>        self<span class="op">.</span>embedding <span class="op">=</span> nn<span class="op">.</span>Embedding<span class="op">(</span>vocab_size<span class="op">,</span> vocab_size<span class="op">)</span></span>
<span id="cb14-264"><a href="#cb14-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-265"><a href="#cb14-265" aria-hidden="true" tabindex="-1"></a>        <span class="pp"># </span><span class="er">One-hot encoding</span></span>
<span id="cb14-266"><a href="#cb14-266" aria-hidden="true" tabindex="-1"></a>        self<span class="op">.</span>embedding<span class="op">.</span>weight<span class="op">.</span>data <span class="op">=</span> torch<span class="op">.</span>eye<span class="op">(</span>vocab_size<span class="op">)</span></span>
<span id="cb14-267"><a href="#cb14-267" aria-hidden="true" tabindex="-1"></a>        self<span class="op">.</span>embedding<span class="op">.</span>weight<span class="op">.</span>requires_grad <span class="op">=</span> False</span>
<span id="cb14-268"><a href="#cb14-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-269"><a href="#cb14-269" aria-hidden="true" tabindex="-1"></a>        <span class="pp"># </span><span class="er">Validation loss</span></span>
<span id="cb14-270"><a href="#cb14-270" aria-hidden="true" tabindex="-1"></a>        self<span class="op">.</span>val_losses <span class="op">=</span> <span class="op">[]</span></span>
<span id="cb14-271"><a href="#cb14-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-272"><a href="#cb14-272" aria-hidden="true" tabindex="-1"></a>    def forward<span class="op">(</span>self<span class="op">,</span> x<span class="op">):</span></span>
<span id="cb14-273"><a href="#cb14-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-274"><a href="#cb14-274" aria-hidden="true" tabindex="-1"></a>        <span class="pp"># </span><span class="er">x is a tensor of shape (batch_size, seq_len)</span></span>
<span id="cb14-275"><a href="#cb14-275" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">,</span> seq_len <span class="op">=</span> x<span class="op">.</span>shape</span>
<span id="cb14-276"><a href="#cb14-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-277"><a href="#cb14-277" aria-hidden="true" tabindex="-1"></a>        <span class="pp"># </span><span class="er">To token index to one-hot encoding</span></span>
<span id="cb14-278"><a href="#cb14-278" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> self<span class="op">.</span>embedding<span class="op">(</span>x<span class="op">)</span></span>
<span id="cb14-279"><a href="#cb14-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-280"><a href="#cb14-280" aria-hidden="true" tabindex="-1"></a>        <span class="pp"># </span><span class="er">To sentnece to sequence of chars</span></span>
<span id="cb14-281"><a href="#cb14-281" aria-hidden="true" tabindex="-1"></a>        hidden <span class="op">=</span> self<span class="op">.</span>init_hidden<span class="op">(</span>batch_size<span class="op">)</span></span>
<span id="cb14-282"><a href="#cb14-282" aria-hidden="true" tabindex="-1"></a>        x<span class="op">,</span> _ <span class="op">=</span> self<span class="op">.</span>lstm<span class="op">(</span>x<span class="op">,</span> hidden<span class="op">)</span></span>
<span id="cb14-283"><a href="#cb14-283" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x<span class="op">[:,</span> <span class="op">-</span><span class="dv">1</span><span class="op">,</span> <span class="op">:]</span></span>
<span id="cb14-284"><a href="#cb14-284" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> self<span class="op">.</span>fc<span class="op">(</span>x<span class="op">)</span></span>
<span id="cb14-285"><a href="#cb14-285" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb14-286"><a href="#cb14-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-287"><a href="#cb14-287" aria-hidden="true" tabindex="-1"></a>    def training_step<span class="op">(</span>self<span class="op">,</span> batch<span class="op">,</span> batch_idx<span class="op">):</span></span>
<span id="cb14-288"><a href="#cb14-288" aria-hidden="true" tabindex="-1"></a>        x<span class="op">,</span> y <span class="op">=</span> batch</span>
<span id="cb14-289"><a href="#cb14-289" aria-hidden="true" tabindex="-1"></a>        y_hat <span class="op">=</span> self<span class="op">(</span>x<span class="op">)</span></span>
<span id="cb14-290"><a href="#cb14-290" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> torch<span class="op">.</span>nn<span class="op">.</span>functional<span class="op">.</span>cross_entropy<span class="op">(</span>y_hat<span class="op">,</span> y<span class="op">.</span>reshape<span class="op">(-</span><span class="dv">1</span><span class="op">))</span></span>
<span id="cb14-291"><a href="#cb14-291" aria-hidden="true" tabindex="-1"></a>        self<span class="op">.</span>log<span class="op">(</span><span class="st">"train_loss"</span><span class="op">,</span> loss<span class="op">)</span></span>
<span id="cb14-292"><a href="#cb14-292" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss</span>
<span id="cb14-293"><a href="#cb14-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-294"><a href="#cb14-294" aria-hidden="true" tabindex="-1"></a>    def validation_step<span class="op">(</span>self<span class="op">,</span> batch<span class="op">,</span> batch_idx<span class="op">):</span></span>
<span id="cb14-295"><a href="#cb14-295" aria-hidden="true" tabindex="-1"></a>        with torch<span class="op">.</span>no_grad<span class="op">():</span></span>
<span id="cb14-296"><a href="#cb14-296" aria-hidden="true" tabindex="-1"></a>            x<span class="op">,</span> y <span class="op">=</span> batch</span>
<span id="cb14-297"><a href="#cb14-297" aria-hidden="true" tabindex="-1"></a>            y_hat <span class="op">=</span> self<span class="op">(</span>x<span class="op">)</span></span>
<span id="cb14-298"><a href="#cb14-298" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> torch<span class="op">.</span>nn<span class="op">.</span>functional<span class="op">.</span>cross_entropy<span class="op">(</span>y_hat<span class="op">,</span> y<span class="op">.</span>reshape<span class="op">(-</span><span class="dv">1</span><span class="op">))</span></span>
<span id="cb14-299"><a href="#cb14-299" aria-hidden="true" tabindex="-1"></a>            self<span class="op">.</span>log<span class="op">(</span><span class="st">"val_loss"</span><span class="op">,</span> loss<span class="op">)</span></span>
<span id="cb14-300"><a href="#cb14-300" aria-hidden="true" tabindex="-1"></a>            self<span class="op">.</span>val_losses<span class="op">.</span>append<span class="op">(</span>loss<span class="op">.</span>cpu<span class="op">().</span>item<span class="op">())</span></span>
<span id="cb14-301"><a href="#cb14-301" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss</span>
<span id="cb14-302"><a href="#cb14-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-303"><a href="#cb14-303" aria-hidden="true" tabindex="-1"></a>    def configure_optimizers<span class="op">(</span>self<span class="op">):</span></span>
<span id="cb14-304"><a href="#cb14-304" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch<span class="op">.</span>optim<span class="op">.</span>Adam<span class="op">(</span>self<span class="op">.</span>parameters<span class="op">(),</span> lr<span class="op">=</span><span class="fl">0.01</span><span class="op">)</span></span>
<span id="cb14-305"><a href="#cb14-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-306"><a href="#cb14-306" aria-hidden="true" tabindex="-1"></a>    def init_hidden<span class="op">(</span>self<span class="op">,</span> batch_size<span class="op">):</span></span>
<span id="cb14-307"><a href="#cb14-307" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="op">(</span></span>
<span id="cb14-308"><a href="#cb14-308" aria-hidden="true" tabindex="-1"></a>            torch<span class="op">.</span>zeros<span class="op">(</span>self<span class="op">.</span>lstm<span class="op">.</span>num_layers<span class="op">,</span> batch_size<span class="op">,</span> self<span class="op">.</span>lstm<span class="op">.</span>hidden_size<span class="op">,</span> device<span class="op">=</span>self<span class="op">.</span>device<span class="op">),</span></span>
<span id="cb14-309"><a href="#cb14-309" aria-hidden="true" tabindex="-1"></a>            torch<span class="op">.</span>zeros<span class="op">(</span>self<span class="op">.</span>lstm<span class="op">.</span>num_layers<span class="op">,</span> batch_size<span class="op">,</span> self<span class="op">.</span>lstm<span class="op">.</span>hidden_size<span class="op">,</span> device<span class="op">=</span>self<span class="op">.</span>device<span class="op">),</span></span>
<span id="cb14-310"><a href="#cb14-310" aria-hidden="true" tabindex="-1"></a>        <span class="op">)</span></span>
<span id="cb14-311"><a href="#cb14-311" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-312"><a href="#cb14-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-313"><a href="#cb14-313" aria-hidden="true" tabindex="-1"></a><span class="in">```{code-cell} ipython</span></span>
<span id="cb14-314"><a href="#cb14-314" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> CharDecoder<span class="op">(</span></span>
<span id="cb14-315"><a href="#cb14-315" aria-hidden="true" tabindex="-1"></a>    vocab_size<span class="op">=</span><span class="dv">28</span><span class="op">,</span></span>
<span id="cb14-316"><a href="#cb14-316" aria-hidden="true" tabindex="-1"></a>    output_size<span class="op">=</span><span class="dv">28</span><span class="op">,</span></span>
<span id="cb14-317"><a href="#cb14-317" aria-hidden="true" tabindex="-1"></a>    hidden_size<span class="op">=</span><span class="dv">32</span><span class="op">,</span></span>
<span id="cb14-318"><a href="#cb14-318" aria-hidden="true" tabindex="-1"></a>    num_layers<span class="op">=</span><span class="dv">1</span><span class="op">,</span></span>
<span id="cb14-319"><a href="#cb14-319" aria-hidden="true" tabindex="-1"></a><span class="op">)</span></span>
<span id="cb14-320"><a href="#cb14-320" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-321"><a href="#cb14-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-322"><a href="#cb14-322" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`num_layers=1`</span>: The model uses a single-layer LSTM architecture for sequence processing.</span>
<span id="cb14-323"><a href="#cb14-323" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`vocab_size=28`</span>: The input dimension matches the vocabulary size to handle the one-hot encoded characters.</span>
<span id="cb14-324"><a href="#cb14-324" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`hidden_size=32`</span>: The LSTM contains 32 hidden units per layer to learn complex sequential patterns in the data.</span>
<span id="cb14-325"><a href="#cb14-325" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`output_size=28`</span>: The output from the LSTM feeds into a final linear layer that performs classification over the vocabulary space.</span>
<span id="cb14-326"><a href="#cb14-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-327"><a href="#cb14-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-330"><a href="#cb14-330" aria-hidden="true" tabindex="-1"></a><span class="in">```{tip}</span></span>
<span id="cb14-331"><a href="#cb14-331" aria-hidden="true" tabindex="-1"></a><span class="in">The LSTM model can be stacked with multiple layers to learn more complex patterns {footcite:p}`irsoy2014opinion`. For example, `num_layers=2` will stack two LSTM layers on top of each other. The first layer will take the input and produce a hidden state, which will be used as the input for the second layer. The second layer will then produce the final hidden state and output. By stacking multiple layers, the model can learn more complex patterns in the data.</span></span>
<span id="cb14-332"><a href="#cb14-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-333"><a href="#cb14-333" aria-hidden="true" tabindex="-1"></a><span class="in">```{figure} https://i.sstatic.net/QxzoG.png</span></span>
<span id="cb14-334"><a href="#cb14-334" aria-hidden="true" tabindex="-1"></a><span class="in">---</span></span>
<span id="cb14-335"><a href="#cb14-335" aria-hidden="true" tabindex="-1"></a><span class="in">width: 400px</span></span>
<span id="cb14-336"><a href="#cb14-336" aria-hidden="true" tabindex="-1"></a><span class="in">name: lstm-04</span></span>
<span id="cb14-337"><a href="#cb14-337" aria-hidden="true" tabindex="-1"></a><span class="in">align: center</span></span>
<span id="cb14-338"><a href="#cb14-338" aria-hidden="true" tabindex="-1"></a><span class="in">---</span></span>
<span id="cb14-339"><a href="#cb14-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-340"><a href="#cb14-340" aria-hidden="true" tabindex="-1"></a><span class="in">LSTM with multiple layers.</span></span>
<span id="cb14-341"><a href="#cb14-341" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-342"><a href="#cb14-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-345"><a href="#cb14-345" aria-hidden="true" tabindex="-1"></a><span class="in">```{tip}</span></span>
<span id="cb14-346"><a href="#cb14-346" aria-hidden="true" tabindex="-1"></a><span class="in">`torch.nn.Embedding` is a convenient way to convert token indices to vectors. By default, it uses random initialization, but we can use one-hot encoding by setting `self.embedding.weight.data = torch.eye(vocab_size)`, and fix the weights by setting `self.embedding.weight.requires_grad = False`.</span></span>
<span id="cb14-347"><a href="#cb14-347" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-348"><a href="#cb14-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-351"><a href="#cb14-351" aria-hidden="true" tabindex="-1"></a><span class="in">```{tip}</span></span>
<span id="cb14-352"><a href="#cb14-352" aria-hidden="true" tabindex="-1"></a><span class="in">In PyTorch lightning, `configure_optimizers` is a method that returns the optimizer and the learning rate scheduler.</span></span>
<span id="cb14-353"><a href="#cb14-353" aria-hidden="true" tabindex="-1"></a><span class="in">We use ADAM {footcite}`kingma2014adam` as the optimizer. It is a popular optimizer for deep learning. It is a variant of stochastic gradient descent that can adaptively adjust the learning rate for each parameter using the first and second moments of the gradients.</span></span>
<span id="cb14-354"><a href="#cb14-354" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-355"><a href="#cb14-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-356"><a href="#cb14-356" aria-hidden="true" tabindex="-1"></a>Now, let's train the model.</span>
<span id="cb14-357"><a href="#cb14-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-358"><a href="#cb14-358" aria-hidden="true" tabindex="-1"></a><span class="in">```{code-cell} ipython</span></span>
<span id="cb14-359"><a href="#cb14-359" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> pyl<span class="op">.</span>Trainer<span class="op">(</span></span>
<span id="cb14-360"><a href="#cb14-360" aria-hidden="true" tabindex="-1"></a>    max_epochs<span class="op">=</span><span class="dv">200</span><span class="op">,</span></span>
<span id="cb14-361"><a href="#cb14-361" aria-hidden="true" tabindex="-1"></a>    enable_progress_bar<span class="op">=</span>False<span class="op">,</span></span>
<span id="cb14-362"><a href="#cb14-362" aria-hidden="true" tabindex="-1"></a>    enable_model_summary<span class="op">=</span>False<span class="op">,</span></span>
<span id="cb14-363"><a href="#cb14-363" aria-hidden="true" tabindex="-1"></a><span class="op">)</span></span>
<span id="cb14-364"><a href="#cb14-364" aria-hidden="true" tabindex="-1"></a>trainer<span class="op">.</span>fit<span class="op">(</span>model<span class="op">,</span> train_dataloader<span class="op">,</span> val_dataloader<span class="op">)</span></span>
<span id="cb14-365"><a href="#cb14-365" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-366"><a href="#cb14-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-367"><a href="#cb14-367" aria-hidden="true" tabindex="-1"></a>Let's plot the training loss.</span>
<span id="cb14-368"><a href="#cb14-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-369"><a href="#cb14-369" aria-hidden="true" tabindex="-1"></a><span class="in">```{code-cell} ipython</span></span>
<span id="cb14-370"><a href="#cb14-370" aria-hidden="true" tabindex="-1"></a>import matplotlib<span class="op">.</span>pyplot as plt</span>
<span id="cb14-371"><a href="#cb14-371" aria-hidden="true" tabindex="-1"></a>fig<span class="op">,</span> ax <span class="op">=</span> plt<span class="op">.</span>subplots<span class="op">(</span>figsize <span class="op">=</span> <span class="op">(</span><span class="dv">5</span><span class="op">,</span><span class="dv">5</span><span class="op">))</span></span>
<span id="cb14-372"><a href="#cb14-372" aria-hidden="true" tabindex="-1"></a>ax<span class="op">.</span>plot<span class="op">(</span>model<span class="op">.</span>val_losses<span class="op">)</span></span>
<span id="cb14-373"><a href="#cb14-373" aria-hidden="true" tabindex="-1"></a>ax<span class="op">.</span>set_title<span class="op">(</span><span class="st">"Validation Loss"</span><span class="op">)</span></span>
<span id="cb14-374"><a href="#cb14-374" aria-hidden="true" tabindex="-1"></a>ax<span class="op">.</span>set_xlabel<span class="op">(</span><span class="st">"Epoch"</span><span class="op">)</span></span>
<span id="cb14-375"><a href="#cb14-375" aria-hidden="true" tabindex="-1"></a>ax<span class="op">.</span>set_ylabel<span class="op">(</span><span class="st">"Loss"</span><span class="op">)</span></span>
<span id="cb14-376"><a href="#cb14-376" aria-hidden="true" tabindex="-1"></a>plt<span class="op">.</span>show<span class="op">()</span></span>
<span id="cb14-377"><a href="#cb14-377" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-378"><a href="#cb14-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-379"><a href="#cb14-379" aria-hidden="true" tabindex="-1"></a>The loss did not decrease nicely. This is a sign of the struggle of the LSTM model to learn the data. If you see this, you might need to decrease the learning rate, increase the number of epochs untile the loss becomes stable, or re-design the model architecture.</span>
<span id="cb14-380"><a href="#cb14-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-381"><a href="#cb14-381" aria-hidden="true" tabindex="-1"></a>Nevertheless, let's test the model.</span>
<span id="cb14-382"><a href="#cb14-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-383"><a href="#cb14-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-384"><a href="#cb14-384" aria-hidden="true" tabindex="-1"></a><span class="in">```{code-cell} ipython</span></span>
<span id="cb14-385"><a href="#cb14-385" aria-hidden="true" tabindex="-1"></a>eval_seq<span class="op">,</span> eval_target<span class="op">,</span> vocab <span class="op">=</span> generate_wrapped_char_data<span class="op">(</span>n_samples<span class="op">=</span><span class="dv">5</span><span class="op">)</span></span>
<span id="cb14-386"><a href="#cb14-386" aria-hidden="true" tabindex="-1"></a>X_eval <span class="op">=</span> tokenize<span class="op">(</span>eval_seq<span class="op">,</span> vocab<span class="op">)</span></span>
<span id="cb14-387"><a href="#cb14-387" aria-hidden="true" tabindex="-1"></a>y_eval <span class="op">=</span> tokenize<span class="op">(</span>eval_target<span class="op">,</span> vocab<span class="op">)</span></span>
<span id="cb14-388"><a href="#cb14-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-389"><a href="#cb14-389" aria-hidden="true" tabindex="-1"></a>model<span class="op">.</span>eval<span class="op">()</span></span>
<span id="cb14-390"><a href="#cb14-390" aria-hidden="true" tabindex="-1"></a>with torch<span class="op">.</span>no_grad<span class="op">():</span></span>
<span id="cb14-391"><a href="#cb14-391" aria-hidden="true" tabindex="-1"></a>    y_hat <span class="op">=</span> model<span class="op">(</span>X_eval<span class="op">)</span></span>
<span id="cb14-392"><a href="#cb14-392" aria-hidden="true" tabindex="-1"></a>    predicted_idx <span class="op">=</span> torch<span class="op">.</span>argmax<span class="op">(</span>y_hat<span class="op">,</span> dim<span class="op">=</span><span class="dv">1</span><span class="op">)</span></span>
<span id="cb14-393"><a href="#cb14-393" aria-hidden="true" tabindex="-1"></a>    predicted_char <span class="op">=</span> <span class="op">[</span>vocab<span class="op">[</span>idx<span class="op">]</span> <span class="cf">for</span> idx in predicted_idx<span class="op">]</span></span>
<span id="cb14-394"><a href="#cb14-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-395"><a href="#cb14-395" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i in range<span class="op">(</span>len<span class="op">(</span>eval_seq<span class="op">)):</span></span>
<span id="cb14-396"><a href="#cb14-396" aria-hidden="true" tabindex="-1"></a>        print<span class="op">(</span>f<span class="st">"Sequence: {eval_seq[i]}, Target: {eval_target[i]}, Predicted: {predicted_char[i]}"</span><span class="op">)</span></span>
<span id="cb14-397"><a href="#cb14-397" aria-hidden="true" tabindex="-1"></a>    accuracy <span class="op">=</span> <span class="op">(</span>predicted_idx <span class="op">==</span> y_eval<span class="op">).</span>sum<span class="op">()</span> <span class="op">/</span> len<span class="op">(</span>y_eval<span class="op">)</span></span>
<span id="cb14-398"><a href="#cb14-398" aria-hidden="true" tabindex="-1"></a>    print<span class="op">(</span>f<span class="st">"Accuracy: {accuracy}"</span><span class="op">)</span></span>
<span id="cb14-399"><a href="#cb14-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-400"><a href="#cb14-400" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-401"><a href="#cb14-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-402"><a href="#cb14-402" aria-hidden="true" tabindex="-1"></a>We see that the validation loss increased as the training progressed. This implies overfitting, i.e. the model is too expressive and memorizes the training data but lacks the understanding of underlying patterns and thereby generalizes poorly to unseen data. This is a common problem in deep learning.</span>
<span id="cb14-403"><a href="#cb14-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-404"><a href="#cb14-404" aria-hidden="true" tabindex="-1"></a><span class="in">```{admonition} Regularization</span></span>
<span id="cb14-405"><a href="#cb14-405" aria-hidden="true" tabindex="-1"></a><span class="in">:class: tip</span></span>
<span id="cb14-406"><a href="#cb14-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-407"><a href="#cb14-407" aria-hidden="true" tabindex="-1"></a><span class="in">Regularization is a technique to prevent a model from overfitting. A traditional way to regularize a model is to add a penalty term to the loss function, e.g., $L = L_{\text{data}} + \lambda L_{\text{reg}}$, where $L_{\text{data}}$ is the data loss and $L_{\text{reg}}$ is the regularization loss.</span></span>
<span id="cb14-408"><a href="#cb14-408" aria-hidden="true" tabindex="-1"></a><span class="in">Another popular regularization technique is dropout {footcite}`srivastava2014dropout`, which randomly drops out some neurons during training to prevent the model from relying too heavily on specific features.</span></span>
<span id="cb14-409"><a href="#cb14-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-410"><a href="#cb14-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-411"><a href="#cb14-411" aria-hidden="true" tabindex="-1"></a><span class="in">```{figure} https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/1IrdJ5PghD9YoOyVAQ73MJw.gif</span></span>
<span id="cb14-412"><a href="#cb14-412" aria-hidden="true" tabindex="-1"></a><span class="in">:alt: Dropout</span></span>
<span id="cb14-413"><a href="#cb14-413" aria-hidden="true" tabindex="-1"></a><span class="in">:width: 50%</span></span>
<span id="cb14-414"><a href="#cb14-414" aria-hidden="true" tabindex="-1"></a><span class="in">:align: center</span></span>
<span id="cb14-415"><a href="#cb14-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-416"><a href="#cb14-416" aria-hidden="true" tabindex="-1"></a><span class="in">Dropout.</span></span>
<span id="cb14-417"><a href="#cb14-417" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-418"><a href="#cb14-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-419"><a href="#cb14-419" aria-hidden="true" tabindex="-1"></a><span class="fu">## üî• Exercise üî•</span></span>
<span id="cb14-420"><a href="#cb14-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-421"><a href="#cb14-421" aria-hidden="true" tabindex="-1"></a>Let's fix the model by doing the following:</span>
<span id="cb14-422"><a href="#cb14-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-423"><a href="#cb14-423" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Try increasing the number of hidden units in the LSTM model.</span>
<span id="cb14-424"><a href="#cb14-424" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Bring back to the original number of hidden units, and try increasing the number of layers in the LSTM model.</span>
<span id="cb14-425"><a href="#cb14-425" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Add dropout to the model by using <span class="in">`torch.nn.Dropout`</span> on the output of the LSTM layer.</span>
<span id="cb14-426"><a href="#cb14-426" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Try increasing the learning rate.</span>
<span id="cb14-427"><a href="#cb14-427" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Play with other hyperparameters, e.g., the number of epochs, batch size, etc.</span>
<span id="cb14-428"><a href="#cb14-428" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Change the model to <span class="in">`nn.RNN`</span> instead of <span class="in">`nn.LSTM`</span>. You should replace <span class="in">`(h_n, c_n)`</span> with <span class="in">`hidden`</span> in the training and evaluation since <span class="in">`nn.RNN`</span> does not have a cell state.</span>
<span id="cb14-429"><a href="#cb14-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-430"><a href="#cb14-430" aria-hidden="true" tabindex="-1"></a>You should be able to see the model to correctly predict the wrapped character.</span>
<span id="cb14-431"><a href="#cb14-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-434"><a href="#cb14-434" aria-hidden="true" tabindex="-1"></a><span class="in">```{footbibliography}</span></span>
<span id="cb14-435"><a href="#cb14-435" aria-hidden="true" tabindex="-1"></a><span class="in">:style: unsrt</span></span>
<span id="cb14-436"><a href="#cb14-436" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2025, Sadamori Kojaku</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions"><ul><li><a href="https://github.com/skojaku/applied-soft-comp/edit/main/m03-text/archive/lstm.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/skojaku/applied-soft-comp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/skojaku/applied-soft-comp">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>
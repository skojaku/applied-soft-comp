
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Teaching computers how to understand words &#8212; Applied Soft Computing</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'm01-word-embedding/tf-idf';</script>
    <script src="../_static/js/custom.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="word2vec: a small model with a big idea" href="word2vec.html" />
    <link rel="prev" title="Pen and Paper Exercise" href="pen-and-paper.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../home.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.jpg" class="logo__image only-light" alt="Applied Soft Computing - Home"/>
    <script>document.write(`<img src="../_static/logo.jpg" class="logo__image only-dark" alt="Applied Soft Computing - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../home.html">
                    Welcome to SSIE 541/441 Applied Soft Computing
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Intro</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../intro/why-applied-soft-computing.html">Why applied soft computing?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/setup.html">Set up</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/deliverables.html">Deliverables</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Word and Document Embeddings</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="what-to-learn.html">Module 1: Word Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="pen-and-paper.html">Pen and Paper Exercise</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Teaching computers how to understand words</a></li>
<li class="toctree-l1"><a class="reference internal" href="word2vec.html">word2vec: a small model with a big idea</a></li>
<li class="toctree-l1"><a class="reference internal" href="word2vec_plus.html">GloVe and FastText: Building on Word2Vec’s Foundation</a></li>
<li class="toctree-l1"><a class="reference internal" href="sem-axis.html">Understanding SemAxis: Semantic Axes in Word Vector Spaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="bias-in-embedding.html">Bias in Word Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="doc2vec.html">Doc2Vec: From Words to Documents</a></li>
<li class="toctree-l1"><a class="reference internal" href="summary.html">Summary</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Recurrent Neural Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/what-to-learn.html">Module 2: Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/rnn-interactive.html">🧠 Learn RNNs Through Physics!</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/reccurrent-neural-net.html">Recurrent Neural Network (RNN)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/lstm.html">Long Short-Term Memory (LSTM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/elmo.html">Embedding from Language Models (ELMo)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/seq2seq.html">Sequence-to-Sequence Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/summary.html">Summary</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Transformers</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/what-to-learn.html">Module 3: Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/transformers.html">Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/bert.html">Bidirectional Encoder Representations from Transformers (BERT)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/sentence-bert.html">Sentence-BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/gpt.html">Generative Pre-trained Transformer (GPT)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/from-language-model-to-instruction-following.html">From Language Model to Instruction-Following</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/prompt-tuning.html">Prompt Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/prompt-tuning-exercise.html">Prompt Tuning Exercise</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/summary.html">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/appendix-t5.html">Appendix: Text-to-Text Transfer Transformer (T5)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Teaching Machine How to Understand Images</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m04-image-processing/what-to-learn.html">Module 4: Image Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m04-image-processing/pen-and-paper.html">Pen and paper exercises</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m04-image-processing/image-processing.html">Preliminaries: Image Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m04-image-processing/cnn.html">From Traditional Image Processing to Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m04-image-processing/lenet.html">LeNet and LeNet-5: Pioneering CNN Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m04-image-processing/alexnet.html">AlexNet: A Breakthrough in Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m04-image-processing/vgg.html">VGGNet - A Deep Convolutional Neural Network for Image Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m04-image-processing/inception.html">GoogleNet and the Inception Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m04-image-processing/resnet.html">ResNet (Residual Neural Networks)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Teaching Machine How to Understand Graphs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/what-to-learn.html">Module 4: Graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/pen-and-paper.html">Pen and paper exercises</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/spectral-embedding.html">Spectral Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/graph-embedding-w-word2vec.html">Graph embedding with word2vec</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/spectral-vs-neural-embedding.html">Spectral vs Neural Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/from-image-to-graph.html">From Image to Graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/graph-convolutional-network.html">Graph Convolutional Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/popular-gnn.html">Popular Graph Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/appendix.html">Appendix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/software.html">Software for Network Embedding</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/skojaku/applied-soft-comp/gh-pages?urlpath=tree/docs/lecture-note/m01-word-embedding/tf-idf.md" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/skojaku/applied-soft-comp" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/skojaku/applied-soft-comp/issues/new?title=Issue%20on%20page%20%2Fm01-word-embedding/tf-idf.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/m01-word-embedding/tf-idf.ipynb" target="_blank"
   class="btn btn-sm btn-download-notebook-button dropdown-item"
   title="Download notebook file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li><a href="../_sources/m01-word-embedding/tf-idf.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Teaching computers how to understand words</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-encoding-the-first-step">One-Hot Encoding: The First Step</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distributional-hypothesis">Distributional Hypothesis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tf-idf-a-simple-but-powerful-word-embedding-technique">TF-IDF: A Simple but Powerful Word Embedding Technique</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Distributional Hypothesis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tf-idf-words-as-patterns-of-usage">TF-IDF: Words as Patterns of Usage</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#first-attempt-word-document-count-matrix">First Attempt: Word-Document Count Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-problem-with-raw-counts">The Problem with Raw Counts</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-need-for-normalization">The Need for Normalization</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="teaching-computers-how-to-understand-words">
<h1>Teaching computers how to understand words<a class="headerlink" href="#teaching-computers-how-to-understand-words" title="Link to this heading">#</a></h1>
<p>Imagine trying to explain the meaning of words to someone who only understands numbers. This is exactly the challenge we face when teaching computers to process text. Just as we need to translate between languages, we need to translate between the world of human language and the world of computer numbers. This translation process has evolved dramatically over time, becoming increasingly sophisticated in its ability to capture the nuances of meaning.</p>
<section id="one-hot-encoding-the-first-step">
<h2>One-Hot Encoding: The First Step<a class="headerlink" href="#one-hot-encoding-the-first-step" title="Link to this heading">#</a></h2>
<p>The simplest approach to this translation challenge is one-hot encoding, akin to giving each word its own unique light switch in a vast room of switches. When representing a word, we turn on its switch and leave all others off. For example, in a tiny vocabulary of just three words {cat, dog, fish}:</p>
<ul class="simple">
<li><p>‘cat’ becomes [1, 0, 0]</p></li>
<li><p>‘dog’ becomes [0, 1, 0]</p></li>
<li><p>‘fish’ becomes [0, 0, 1]</p></li>
</ul>
<p>While simple, this approach has a fundamental flaw: it suggests that all words are equally different from each other. In this representation, ‘cat’ is just as different from ‘dog’ as it is from ‘algorithm’ - something we know isn’t true in real language use.</p>
</section>
<section id="distributional-hypothesis">
<h2>Distributional Hypothesis<a class="headerlink" href="#distributional-hypothesis" title="Link to this heading">#</a></h2>
<p>What is missing in one-hot encoding is the notion of <em>context</em>.
One associates <code class="docutils literal notranslate"><span class="pre">cat</span></code> with <code class="docutils literal notranslate"><span class="pre">dog</span></code> because they have similar context, while <code class="docutils literal notranslate"><span class="pre">cat</span></code> is more different from <code class="docutils literal notranslate"><span class="pre">fish</span></code> than <code class="docutils literal notranslate"><span class="pre">dog</span></code> because they are in different contexts.
This is the core idea of the <strong>distributional hypothesis</strong>.</p>
<p>In a nutshell, the distributional hypothesis states that:</p>
<ul class="simple">
<li><p>Words that frequently appear together in text (co-occur) are likely to be semantically related</p></li>
<li><p>The meaning of a word can be inferred by examining the distribution of other words around it</p></li>
<li><p>Similar words will have similar distributions of surrounding context words</p></li>
</ul>
<p>This hypothesis forms the theoretical foundation for many modern word embedding techniques.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The idea that words can be understood by their context is captured by the famous linguistic principle: “You shall know a word by the company it keeps” <a class="footnote-reference brackets" href="#footcite-firth1957synopsis" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>. This principle suggests that the meaning of a word is not inherent to the word itself, but rather emerges from how it is used alongside other words.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The ancient Buddhist concept of Apoha, developed by Dignāga in the 5th-6th century CE, shares similarities with modern distributional semantics. According to Apoha theory, we understand concepts by distinguishing what they are not - for example, we know what a “cow” is by recognizing everything that is not a cow. This mirrors how modern word embeddings define words through their relationships and contrasts with other words, showing how both ancient philosophy and contemporary linguistics recognize that meaning emerges from relationships between concepts.</p>
<p><img alt="" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQbWHl7Npbub7nDC9GvfUneConFZbjoHkPHuMPh3PXpGakxdDrv_0JmWt7Fpg63lo_XhhqZqFzOs6YUsVEbPyHBMVexnaqPLWzDQJ-CXAjFCoe7PzNrKlm474QDo14LiqOjrfr1zMt6As/s1600/cnononcow.jpg" /></p>
</div>
</section>
<section id="tf-idf-a-simple-but-powerful-word-embedding-technique">
<h2>TF-IDF: A Simple but Powerful Word Embedding Technique<a class="headerlink" href="#tf-idf-a-simple-but-powerful-word-embedding-technique" title="Link to this heading">#</a></h2>
</section>
<section id="id2">
<h2>Distributional Hypothesis<a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<p>What is missing in one-hot encoding is the notion of <em>context</em>. One associates <code class="docutils literal notranslate"><span class="pre">cat</span></code> with <code class="docutils literal notranslate"><span class="pre">dog</span></code> because they have similar context, while <code class="docutils literal notranslate"><span class="pre">cat</span></code> is more different from <code class="docutils literal notranslate"><span class="pre">fish</span></code> than <code class="docutils literal notranslate"><span class="pre">dog</span></code> because they are in different contexts. This is the core idea of the <strong>distributional hypothesis</strong>.</p>
<p>In a nutshell, the distributional hypothesis states that we can understand the meaning of a word by examining the context in which it appears. Just as you might understand a person by the company they keep, we can understand a word by the words that surround it. This principle suggests that words appearing in similar contexts likely have similar meanings.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The idea that words can be understood by their context is captured by the famous linguistic principle: “You shall know a word by the company it keeps” <a class="footnote-reference brackets" href="#footcite-firth1957synopsis" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>. This principle suggests that the meaning of a word is not inherent to the word itself, but rather emerges from how it is used alongside other words.</p>
</div>
</section>
<section id="tf-idf-words-as-patterns-of-usage">
<h2>TF-IDF: Words as Patterns of Usage<a class="headerlink" href="#tf-idf-words-as-patterns-of-usage" title="Link to this heading">#</a></h2>
<p>The distributional hypothesis leads us to an important question: How can we capture these contextual patterns mathematically?
More specifically, what is a good unit of “context”?
A natural choice is to let the <em>document</em> be the unit of context.
The distributional hypothesis suggests that words that frequently appear in the same documents are likely to be semantically related.</p>
<section id="first-attempt-word-document-count-matrix">
<h3>First Attempt: Word-Document Count Matrix<a class="headerlink" href="#first-attempt-word-document-count-matrix" title="Link to this heading">#</a></h3>
<p>Let’s try to organize this information systematically. Imagine creating a giant table where:</p>
<ul class="simple">
<li><p>Each row represents a word</p></li>
<li><p>Each column represents a document</p></li>
<li><p>Each cell contains the count of how often that word appears in that document</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Sample documents</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;The cat chases mice in the garden&quot;</span><span class="p">,</span>
    <span class="s2">&quot;The dog chases cats in the park&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Mice eat cheese in the house&quot;</span><span class="p">,</span>
    <span class="s2">&quot;The cat and dog play in the garden&quot;</span>
<span class="p">]</span>

<span class="c1"># Create word count matrix</span>
<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="n">count_matrix</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
<span class="n">words</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">()</span>

<span class="c1"># Display as a table</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">count_matrix</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="n">words</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">background_gradient</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;cividis&#39;</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span><span class="o">.</span><span class="n">set_caption</span><span class="p">(</span><span class="s2">&quot;Word-Document Count Matrix&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style type="text/css">
#T_b51f0_row0_col0, #T_b51f0_row0_col2, #T_b51f0_row0_col4, #T_b51f0_row0_col5, #T_b51f0_row0_col6, #T_b51f0_row0_col8, #T_b51f0_row0_col11, #T_b51f0_row0_col12, #T_b51f0_row1_col0, #T_b51f0_row1_col1, #T_b51f0_row1_col4, #T_b51f0_row1_col6, #T_b51f0_row1_col7, #T_b51f0_row1_col8, #T_b51f0_row1_col10, #T_b51f0_row1_col12, #T_b51f0_row2_col0, #T_b51f0_row2_col1, #T_b51f0_row2_col2, #T_b51f0_row2_col3, #T_b51f0_row2_col5, #T_b51f0_row2_col7, #T_b51f0_row2_col11, #T_b51f0_row2_col12, #T_b51f0_row3_col2, #T_b51f0_row3_col3, #T_b51f0_row3_col4, #T_b51f0_row3_col6, #T_b51f0_row3_col8, #T_b51f0_row3_col10, #T_b51f0_row3_col11 {
  background-color: #00224e;
  color: #f1f1f1;
}
#T_b51f0_row0_col1, #T_b51f0_row0_col3, #T_b51f0_row0_col7, #T_b51f0_row0_col9, #T_b51f0_row0_col10, #T_b51f0_row1_col2, #T_b51f0_row1_col3, #T_b51f0_row1_col5, #T_b51f0_row1_col9, #T_b51f0_row1_col11, #T_b51f0_row2_col4, #T_b51f0_row2_col6, #T_b51f0_row2_col8, #T_b51f0_row2_col9, #T_b51f0_row2_col10, #T_b51f0_row2_col13, #T_b51f0_row3_col0, #T_b51f0_row3_col1, #T_b51f0_row3_col5, #T_b51f0_row3_col7, #T_b51f0_row3_col9, #T_b51f0_row3_col12 {
  background-color: #7d7c78;
  color: #f1f1f1;
}
#T_b51f0_row0_col13, #T_b51f0_row1_col13, #T_b51f0_row3_col13 {
  background-color: #fee838;
  color: #000000;
}
</style>
<table id="T_b51f0">
  <caption>Word-Document Count Matrix</caption>
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_b51f0_level0_col0" class="col_heading level0 col0" >and</th>
      <th id="T_b51f0_level0_col1" class="col_heading level0 col1" >cat</th>
      <th id="T_b51f0_level0_col2" class="col_heading level0 col2" >cats</th>
      <th id="T_b51f0_level0_col3" class="col_heading level0 col3" >chases</th>
      <th id="T_b51f0_level0_col4" class="col_heading level0 col4" >cheese</th>
      <th id="T_b51f0_level0_col5" class="col_heading level0 col5" >dog</th>
      <th id="T_b51f0_level0_col6" class="col_heading level0 col6" >eat</th>
      <th id="T_b51f0_level0_col7" class="col_heading level0 col7" >garden</th>
      <th id="T_b51f0_level0_col8" class="col_heading level0 col8" >house</th>
      <th id="T_b51f0_level0_col9" class="col_heading level0 col9" >in</th>
      <th id="T_b51f0_level0_col10" class="col_heading level0 col10" >mice</th>
      <th id="T_b51f0_level0_col11" class="col_heading level0 col11" >park</th>
      <th id="T_b51f0_level0_col12" class="col_heading level0 col12" >play</th>
      <th id="T_b51f0_level0_col13" class="col_heading level0 col13" >the</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_b51f0_level0_row0" class="row_heading level0 row0" >0</th>
      <td id="T_b51f0_row0_col0" class="data row0 col0" >0</td>
      <td id="T_b51f0_row0_col1" class="data row0 col1" >1</td>
      <td id="T_b51f0_row0_col2" class="data row0 col2" >0</td>
      <td id="T_b51f0_row0_col3" class="data row0 col3" >1</td>
      <td id="T_b51f0_row0_col4" class="data row0 col4" >0</td>
      <td id="T_b51f0_row0_col5" class="data row0 col5" >0</td>
      <td id="T_b51f0_row0_col6" class="data row0 col6" >0</td>
      <td id="T_b51f0_row0_col7" class="data row0 col7" >1</td>
      <td id="T_b51f0_row0_col8" class="data row0 col8" >0</td>
      <td id="T_b51f0_row0_col9" class="data row0 col9" >1</td>
      <td id="T_b51f0_row0_col10" class="data row0 col10" >1</td>
      <td id="T_b51f0_row0_col11" class="data row0 col11" >0</td>
      <td id="T_b51f0_row0_col12" class="data row0 col12" >0</td>
      <td id="T_b51f0_row0_col13" class="data row0 col13" >2</td>
    </tr>
    <tr>
      <th id="T_b51f0_level0_row1" class="row_heading level0 row1" >1</th>
      <td id="T_b51f0_row1_col0" class="data row1 col0" >0</td>
      <td id="T_b51f0_row1_col1" class="data row1 col1" >0</td>
      <td id="T_b51f0_row1_col2" class="data row1 col2" >1</td>
      <td id="T_b51f0_row1_col3" class="data row1 col3" >1</td>
      <td id="T_b51f0_row1_col4" class="data row1 col4" >0</td>
      <td id="T_b51f0_row1_col5" class="data row1 col5" >1</td>
      <td id="T_b51f0_row1_col6" class="data row1 col6" >0</td>
      <td id="T_b51f0_row1_col7" class="data row1 col7" >0</td>
      <td id="T_b51f0_row1_col8" class="data row1 col8" >0</td>
      <td id="T_b51f0_row1_col9" class="data row1 col9" >1</td>
      <td id="T_b51f0_row1_col10" class="data row1 col10" >0</td>
      <td id="T_b51f0_row1_col11" class="data row1 col11" >1</td>
      <td id="T_b51f0_row1_col12" class="data row1 col12" >0</td>
      <td id="T_b51f0_row1_col13" class="data row1 col13" >2</td>
    </tr>
    <tr>
      <th id="T_b51f0_level0_row2" class="row_heading level0 row2" >2</th>
      <td id="T_b51f0_row2_col0" class="data row2 col0" >0</td>
      <td id="T_b51f0_row2_col1" class="data row2 col1" >0</td>
      <td id="T_b51f0_row2_col2" class="data row2 col2" >0</td>
      <td id="T_b51f0_row2_col3" class="data row2 col3" >0</td>
      <td id="T_b51f0_row2_col4" class="data row2 col4" >1</td>
      <td id="T_b51f0_row2_col5" class="data row2 col5" >0</td>
      <td id="T_b51f0_row2_col6" class="data row2 col6" >1</td>
      <td id="T_b51f0_row2_col7" class="data row2 col7" >0</td>
      <td id="T_b51f0_row2_col8" class="data row2 col8" >1</td>
      <td id="T_b51f0_row2_col9" class="data row2 col9" >1</td>
      <td id="T_b51f0_row2_col10" class="data row2 col10" >1</td>
      <td id="T_b51f0_row2_col11" class="data row2 col11" >0</td>
      <td id="T_b51f0_row2_col12" class="data row2 col12" >0</td>
      <td id="T_b51f0_row2_col13" class="data row2 col13" >1</td>
    </tr>
    <tr>
      <th id="T_b51f0_level0_row3" class="row_heading level0 row3" >3</th>
      <td id="T_b51f0_row3_col0" class="data row3 col0" >1</td>
      <td id="T_b51f0_row3_col1" class="data row3 col1" >1</td>
      <td id="T_b51f0_row3_col2" class="data row3 col2" >0</td>
      <td id="T_b51f0_row3_col3" class="data row3 col3" >0</td>
      <td id="T_b51f0_row3_col4" class="data row3 col4" >0</td>
      <td id="T_b51f0_row3_col5" class="data row3 col5" >1</td>
      <td id="T_b51f0_row3_col6" class="data row3 col6" >0</td>
      <td id="T_b51f0_row3_col7" class="data row3 col7" >1</td>
      <td id="T_b51f0_row3_col8" class="data row3 col8" >0</td>
      <td id="T_b51f0_row3_col9" class="data row3 col9" >1</td>
      <td id="T_b51f0_row3_col10" class="data row3 col10" >0</td>
      <td id="T_b51f0_row3_col11" class="data row3 col11" >0</td>
      <td id="T_b51f0_row3_col12" class="data row3 col12" >1</td>
      <td id="T_b51f0_row3_col13" class="data row3 col13" >2</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This matrix is our first attempt at a distributed representation - each word is represented not by a single number, but by its pattern of appearances across all documents.</p>
</div>
</section>
<section id="the-problem-with-raw-counts">
<h3>The Problem with Raw Counts<a class="headerlink" href="#the-problem-with-raw-counts" title="Link to this heading">#</a></h3>
<p>Looking at our word-document matrix, something seems off. Words like “the” and “in” appear frequently in almost every document. Are these words really the most important for understanding document meaning? Let’s see what happens if we count how often each word appears across all documents:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">word_counts</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">word_counts_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">word_counts</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;count&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
<span class="n">word_counts_df</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">background_gradient</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;cividis&#39;</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span><span class="o">.</span><span class="n">set_caption</span><span class="p">(</span><span class="s2">&quot;Total appearances of each word&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style type="text/css">
#T_0cc0c_row0_col0, #T_0cc0c_row0_col2, #T_0cc0c_row0_col4, #T_0cc0c_row0_col6, #T_0cc0c_row0_col8, #T_0cc0c_row0_col11, #T_0cc0c_row0_col12 {
  background-color: #00224e;
  color: #f1f1f1;
}
#T_0cc0c_row0_col1, #T_0cc0c_row0_col3, #T_0cc0c_row0_col5, #T_0cc0c_row0_col7, #T_0cc0c_row0_col10 {
  background-color: #2a3f6d;
  color: #f1f1f1;
}
#T_0cc0c_row0_col9 {
  background-color: #7d7c78;
  color: #f1f1f1;
}
#T_0cc0c_row0_col13 {
  background-color: #fee838;
  color: #000000;
}
</style>
<table id="T_0cc0c">
  <caption>Total appearances of each word</caption>
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_0cc0c_level0_col0" class="col_heading level0 col0" >and</th>
      <th id="T_0cc0c_level0_col1" class="col_heading level0 col1" >cat</th>
      <th id="T_0cc0c_level0_col2" class="col_heading level0 col2" >cats</th>
      <th id="T_0cc0c_level0_col3" class="col_heading level0 col3" >chases</th>
      <th id="T_0cc0c_level0_col4" class="col_heading level0 col4" >cheese</th>
      <th id="T_0cc0c_level0_col5" class="col_heading level0 col5" >dog</th>
      <th id="T_0cc0c_level0_col6" class="col_heading level0 col6" >eat</th>
      <th id="T_0cc0c_level0_col7" class="col_heading level0 col7" >garden</th>
      <th id="T_0cc0c_level0_col8" class="col_heading level0 col8" >house</th>
      <th id="T_0cc0c_level0_col9" class="col_heading level0 col9" >in</th>
      <th id="T_0cc0c_level0_col10" class="col_heading level0 col10" >mice</th>
      <th id="T_0cc0c_level0_col11" class="col_heading level0 col11" >park</th>
      <th id="T_0cc0c_level0_col12" class="col_heading level0 col12" >play</th>
      <th id="T_0cc0c_level0_col13" class="col_heading level0 col13" >the</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_0cc0c_level0_row0" class="row_heading level0 row0" >count</th>
      <td id="T_0cc0c_row0_col0" class="data row0 col0" >1</td>
      <td id="T_0cc0c_row0_col1" class="data row0 col1" >2</td>
      <td id="T_0cc0c_row0_col2" class="data row0 col2" >1</td>
      <td id="T_0cc0c_row0_col3" class="data row0 col3" >2</td>
      <td id="T_0cc0c_row0_col4" class="data row0 col4" >1</td>
      <td id="T_0cc0c_row0_col5" class="data row0 col5" >2</td>
      <td id="T_0cc0c_row0_col6" class="data row0 col6" >1</td>
      <td id="T_0cc0c_row0_col7" class="data row0 col7" >2</td>
      <td id="T_0cc0c_row0_col8" class="data row0 col8" >1</td>
      <td id="T_0cc0c_row0_col9" class="data row0 col9" >4</td>
      <td id="T_0cc0c_row0_col10" class="data row0 col10" >2</td>
      <td id="T_0cc0c_row0_col11" class="data row0 col11" >1</td>
      <td id="T_0cc0c_row0_col12" class="data row0 col12" >1</td>
      <td id="T_0cc0c_row0_col13" class="data row0 col13" >7</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This pattern, where a few words appear very frequently while most words appear rarely, is known as Zipf’s Law. It’s a fundamental property of natural language.
<img alt="" src="https://miro.medium.com/v2/resize:fit:1400/1*GTpckiHyFLe04pUMeYDYOg.png" /></p>
</div>
<p>We’ve discovered two problems with raw word counts:</p>
<ol class="arabic simple">
<li><p>Common words like “the” and “in” dominate the counts, but they tell us little about document content</p></li>
<li><p>Raw frequencies don’t tell us how unique or informative a word is across documents</p></li>
</ol>
<p>Think about it: if a word appears frequently in one document but rarely in others (like “cheese” in our example), it’s probably more informative about that document’s content than a word that appears equally frequently in all documents (like “the”).</p>
<p>This realization leads us to two important questions:</p>
<ol class="arabic simple">
<li><p>How can we normalize word frequencies within each document to account for document length?</p></li>
<li><p>How can we adjust these frequencies to give more weight to words that are unique to specific documents?</p></li>
</ol>
<p><strong>TF-IDF (Term Frequency-Inverse Document Frequency)</strong> provides elegant solutions to both questions. Let’s see how it works in the next section.</p>
</section>
</section>
<section id="the-need-for-normalization">
<h2>The Need for Normalization<a class="headerlink" href="#the-need-for-normalization" title="Link to this heading">#</a></h2>
<p>TF-IDF (Term Frequency-Inverse Document Frequency) offers our first practical glimpse into representing words as distributed patterns rather than isolated units.</p>
<p>The TF-IDF score for a word <span class="math notranslate nohighlight">\(t\)</span> in document <span class="math notranslate nohighlight">\(d\)</span> combines two components:</p>
<p><span class="math notranslate nohighlight">\(\text{TF-IDF}(t,d) = \text{TF}(t,d) \times \text{IDF}(t)\)</span></p>
<p>where:</p>
<p><span class="math notranslate nohighlight">\(\text{TF}(t,d) = \dfrac{\text{count of term }t\text{ in document }d}{\text{total number of terms in document }d}\)</span></p>
<p><span class="math notranslate nohighlight">\(\text{IDF}(t) = \log\left(\dfrac{\text{total number of documents}}{\text{number of documents containing term }t}\right)\)</span></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Unlike one-hot encoding where each word is represented by a single position, TF-IDF represents each word through its pattern of occurrence across all documents. This distributed nature allows TF-IDF to capture semantic relationships: words that appear in similar documents will have similar patterns of TF-IDF scores.</p>
</div>
<p>Let’s see this distributed representation in action:
First, let us consider a simple example with 5 documents about animals.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Sample documents about animals</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;Cats chase mice in the garden&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Dogs chase cats in the park&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Mice eat cheese in the house&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Pets like dogs and cats need care&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Wild animals hunt in nature&quot;</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Second, we will split each document into words and create a vocabulary.
This process is called <strong>tokenization</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">:</span>
    <span class="n">tokens_in_doc</span> <span class="o">=</span> <span class="n">doc</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="n">tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tokens_in_doc</span><span class="p">)</span>
    <span class="n">vocab</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">tokens_in_doc</span><span class="p">)</span>

<span class="c1"># create a dictionary that maps each word to a unique index</span>
<span class="n">word_to_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>
</pre></div>
</div>
</div>
</div>
<p>Third, we will count how many times each word appears in each document. We begin by creating a placeholder matrix <code class="docutils literal notranslate"><span class="pre">tf_matrix</span></code> to store the counts.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate term frequencies for each document</span>
<span class="n">n_docs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span> <span class="c1"># number of documents</span>
<span class="n">n_terms</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span> <span class="c1"># number of words</span>

<span class="c1"># This is a matrix of zeros</span>
<span class="c1"># with the number of rows equal to the number of words</span>
<span class="c1"># and the number of columns equal to the number of documents</span>
<span class="n">tf_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_terms</span><span class="p">,</span> <span class="n">n_docs</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">tf_matrix</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]]
</pre></div>
</div>
</div>
</div>
<p>And then we count how many times each word appears in each document.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">doc_idx</span><span class="p">,</span> <span class="n">tokens_in_doc</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tokens</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tokens_in_doc</span><span class="p">:</span>
        <span class="n">term_idx</span> <span class="o">=</span> <span class="n">word_to_idx</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
        <span class="n">tf_matrix</span><span class="p">[</span><span class="n">term_idx</span><span class="p">,</span> <span class="n">doc_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="nb">print</span><span class="p">(</span><span class="n">tf_matrix</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[1. 1. 1. 0. 1.]
 [0. 1. 0. 1. 0.]
 [0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1.]
 [0. 0. 1. 0. 0.]
 [0. 0. 0. 1. 0.]
 [1. 0. 1. 0. 0.]
 [0. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 1.]
 [0. 0. 1. 0. 0.]
 [1. 1. 0. 0. 0.]
 [1. 1. 0. 1. 0.]
 [1. 1. 1. 0. 0.]
 [0. 0. 0. 1. 0.]
 [0. 0. 0. 1. 0.]
 [0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 1.]
 [0. 0. 0. 1. 0.]
 [1. 0. 0. 0. 0.]]
</pre></div>
</div>
</div>
</div>
<p>Fourth, we calculate the IDF for each word.
IDF is defined as the logarithm of the inverse document frequency.
Document frequency is the number of documents that contain the word.
Note that, if a word appears multiple times in the same document, it should only be counted once!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate IDF for each term</span>
<span class="c1"># let&#39;s use tf_matrix to calculate the document frequency</span>
<span class="n">doc_freq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_terms</span><span class="p">)</span>

<span class="c1"># Go through each word</span>
<span class="k">for</span> <span class="n">term_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_terms</span><span class="p">):</span>

    <span class="c1"># For each word, go through each document</span>
    <span class="k">for</span> <span class="n">doc_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_docs</span><span class="p">):</span>
        <span class="c1"># If the word appears in the document, increment the document frequency</span>
        <span class="k">if</span> <span class="n">tf_matrix</span><span class="p">[</span><span class="n">term_idx</span><span class="p">,</span> <span class="n">doc_idx</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">doc_freq</span><span class="p">[</span><span class="n">term_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="n">idf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">n_docs</span> <span class="o">/</span> <span class="n">doc_freq</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we calculate the TF-IDF matrix. <code class="docutils literal notranslate"><span class="pre">tf_matrix</span></code> is a matrix of <code class="docutils literal notranslate"><span class="pre">n_terms</span></code> by <code class="docutils literal notranslate"><span class="pre">n_docs</span></code>, and <code class="docutils literal notranslate"><span class="pre">idf</span></code> is a vector of length <code class="docutils literal notranslate"><span class="pre">n_terms</span></code>. Remind that the tf-idf is given by</p>
<p><span class="math notranslate nohighlight">\(
\text{TF-IDF}(t,d) = \text{TF}(t,d) \times \text{IDF}(t)
\)</span></p>
<p>A naive way to do this (albeit not efficient) is to perform this using for loops</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate TF-IDF matrix</span>

<span class="n">tfidf_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_terms</span><span class="p">,</span> <span class="n">n_docs</span><span class="p">))</span>
<span class="k">for</span> <span class="n">term_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_terms</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">doc_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_docs</span><span class="p">):</span>
        <span class="n">tfidf_matrix</span><span class="p">[</span><span class="n">term_idx</span><span class="p">,</span> <span class="n">doc_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf_matrix</span><span class="p">[</span><span class="n">term_idx</span><span class="p">,</span> <span class="n">doc_idx</span><span class="p">]</span> <span class="o">*</span> <span class="n">idf</span><span class="p">[</span><span class="n">term_idx</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>A more efficient way is to use matrix multiplication.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tfidf_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">idf</span><span class="p">)</span> <span class="o">@</span> <span class="n">tf_matrix</span>
</pre></div>
</div>
</div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">np.diag(idf)</span></code> creates a diagonal matrix with <code class="docutils literal notranslate"><span class="pre">idf</span></code> on the diagonal.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Consider a diagonal matrix as a tool to scale the rows/columns of a matrix.
If we multiple a diagonal matrix from the left, we scale the rows of the matrix.
If we multiple a diagonal matrix from the right, we scale the columns of the matrix.</p>
<p><img alt="" src="https://allisonhorst.github.io/EDS_212_essential-math/slides/slide_images/diagonal_matrix_scaling.png" /></p>
</div>
<p>A more efficient way is to use einsum, which is a powerful function for performing Einstein summation convention on arrays.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tfidf_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ij,i-&gt;ij&#39;</span><span class="p">,</span> <span class="n">tf_matrix</span><span class="p">,</span> <span class="n">idf</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><code class="docutils literal notranslate"><span class="pre">einsum</span></code> provides a concise way to express complex array manipulations and can often lead to more efficient computations. Here is a concise description of how <code class="docutils literal notranslate"><span class="pre">einsum</span></code> works <a class="reference external" href="https://ajcr.net/Basic-guide-to-einsum/">A basic introduction to NumPy’s einsum</a></p>
<p>The general form of <code class="docutils literal notranslate"><span class="pre">einsum</span></code> is <code class="docutils literal notranslate"><span class="pre">np.einsum(subscripts,</span> <span class="pre">*operands)</span></code>, where:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">subscripts</span></code> is a string specifying the subscripts for summation as comma-separated list of subscript labels</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">operands</span></code> are the input arrays</p></li>
</ul>
<p>In our case, <code class="docutils literal notranslate"><span class="pre">'ij,i-&gt;ij'</span></code> means:</p>
<ol class="arabic simple">
<li><p>Take the first array <code class="docutils literal notranslate"><span class="pre">tf_matrix</span></code> with dimensions <code class="docutils literal notranslate"><span class="pre">i,j</span></code></p></li>
<li><p>Take the second array <code class="docutils literal notranslate"><span class="pre">idf</span></code> with dimension <code class="docutils literal notranslate"><span class="pre">i</span></code></p></li>
<li><p>Multiply each row <code class="docutils literal notranslate"><span class="pre">j</span></code> of <code class="docutils literal notranslate"><span class="pre">tf_matrix</span></code> by the corresponding element <code class="docutils literal notranslate"><span class="pre">i</span></code> of <code class="docutils literal notranslate"><span class="pre">idf</span></code></p></li>
<li><p>Output has same dimensions <code class="docutils literal notranslate"><span class="pre">i,j</span></code> as input</p></li>
</ol>
</div>
<p>Now, we have the TF-IDF matrix as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">tfidf_matrix</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0.22314355 0.22314355 0.22314355 0.         0.22314355]
 [0.         0.91629073 0.         0.91629073 0.        ]
 [0.         0.         0.         0.         1.60943791]
 [0.         0.         0.         0.         1.60943791]
 [0.         0.         1.60943791 0.         0.        ]
 [0.         0.         0.         1.60943791 0.        ]
 [0.91629073 0.         0.91629073 0.         0.        ]
 [0.         1.60943791 0.         0.         0.        ]
 [0.         0.         0.         1.60943791 0.        ]
 [0.         0.         0.         0.         1.60943791]
 [0.         0.         1.60943791 0.         0.        ]
 [0.91629073 0.91629073 0.         0.         0.        ]
 [0.51082562 0.51082562 0.         0.51082562 0.        ]
 [0.51082562 0.51082562 0.51082562 0.         0.        ]
 [0.         0.         0.         1.60943791 0.        ]
 [0.         0.         0.         1.60943791 0.        ]
 [0.         0.         1.60943791 0.         0.        ]
 [0.         0.         0.         0.         1.60943791]
 [0.         0.         0.         1.60943791 0.        ]
 [1.60943791 0.         0.         0.         0.        ]]
</pre></div>
</div>
</div>
</div>
<p>Each row of the TF-IDF matrix is our first attempt at a distributed representation of a word.
Words that appear together in the same documents frequently will have similar rows.</p>
<p>Just like one-hot encoding, this representation can be high-dimensional, e.g., if we have 10000 documents, each word is represented by a vector of length 10000.
One can reduce the dimensionality of the representation using dimensionality reduction techniques (e.g., PCA, SVD) while maintaining the structure of the data.
This is possible because tf-idf matrix is often low-rank in practice.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>TF-IDF matrices are often low-rank because documents naturally group into clusters based on topics or themes. This clustering creates redundancy in the term-document matrix, as many terms are specific to certain clusters and rare in others. Consequently, the TF-IDF matrix can be approximated by a lower-rank matrix. This low-rank nature is useful for applications like dimensionality reduction and topic modeling, as it simplifies data visualization and analysis.</p>
</div>
<p>Let us apply PCA to reduce the dimensionality of the tf-idf matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">reducer</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">xy</span> <span class="o">=</span> <span class="n">reducer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">tfidf_matrix</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s visualize the result using Bokeh.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">bokeh.plotting</span> <span class="kn">import</span> <span class="n">figure</span><span class="p">,</span> <span class="n">show</span><span class="p">,</span> <span class="n">output_notebook</span>
<span class="kn">from</span> <span class="nn">bokeh.models</span> <span class="kn">import</span> <span class="n">ColumnDataSource</span>
<span class="kn">from</span> <span class="nn">bokeh.io</span> <span class="kn">import</span> <span class="n">push_notebook</span>
<span class="kn">from</span> <span class="nn">bokeh.plotting</span> <span class="kn">import</span> <span class="n">figure</span><span class="p">,</span> <span class="n">show</span>
<span class="kn">from</span> <span class="nn">bokeh.io</span> <span class="kn">import</span> <span class="n">output_notebook</span>
<span class="kn">from</span> <span class="nn">bokeh.models</span> <span class="kn">import</span> <span class="n">ColumnDataSource</span><span class="p">,</span> <span class="n">HoverTool</span>

<span class="n">output_notebook</span><span class="p">()</span>

<span class="c1"># Prepare data for Bokeh</span>
<span class="n">source</span> <span class="o">=</span> <span class="n">ColumnDataSource</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">xy</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">y</span><span class="o">=</span><span class="n">xy</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="n">text_x</span><span class="o">=</span><span class="n">xy</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_terms</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.02</span><span class="p">,</span>
    <span class="n">text_y</span><span class="o">=</span><span class="n">xy</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_terms</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.02</span><span class="p">,</span>
    <span class="n">term</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">word_to_idx</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="p">))</span>

<span class="n">p</span> <span class="o">=</span> <span class="n">figure</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&quot;Node Embeddings from Word2Vec&quot;</span><span class="p">,</span> <span class="n">x_axis_label</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">,</span> <span class="n">y_axis_label</span><span class="o">=</span><span class="s2">&quot;Y&quot;</span><span class="p">)</span>

<span class="n">p</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">source</span><span class="o">=</span><span class="n">source</span><span class="p">,</span> <span class="n">line_color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">30</span><span class="p">)</span>

<span class="c1"># Add labels to the points</span>
<span class="n">p</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;text_x&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;text_y&#39;</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="s1">&#39;term&#39;</span><span class="p">,</span> <span class="n">source</span><span class="o">=</span><span class="n">source</span><span class="p">,</span> <span class="n">text_font_size</span><span class="o">=</span><span class="s2">&quot;10pt&quot;</span><span class="p">,</span> <span class="n">text_baseline</span><span class="o">=</span><span class="s2">&quot;middle&quot;</span><span class="p">,</span> <span class="n">text_align</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">)</span>

<span class="n">show</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">13</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="kn">from</span> <span class="nn">bokeh.plotting</span> <span class="kn">import</span> <span class="n">figure</span><span class="p">,</span> <span class="n">show</span><span class="p">,</span> <span class="n">output_notebook</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="kn">from</span> <span class="nn">bokeh.models</span> <span class="kn">import</span> <span class="n">ColumnDataSource</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="kn">from</span> <span class="nn">bokeh.io</span> <span class="kn">import</span> <span class="n">push_notebook</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;bokeh&#39;
</pre></div>
</div>
</div>
</div>
<p>The power of TF-IDF lies in its ability to transform the distributional hypothesis into a practical mathematical framework. By representing words through their patterns of usage across documents, TF-IDF creates a distributed representation where semantic relationships emerge naturally from the data.</p>
<p>However, TF-IDF has its limitations. It only captures word-document relationships, missing out on the rich word-word relationships that occur within documents. This limitation led researchers to develop more sophisticated techniques like word2vec, which we’ll explore in the next section.</p>
<div class="docutils container" id="id4">
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="footcite-firth1957synopsis" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id3">2</a>)</span>
<p>J. Firth. A synopsis of linguistic theory, 1930-55. In <em>Studies in Linguistic Analysis</em>, pages 1–31. Blackwell, Oxford, 1957.</p>
</aside>
</aside>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "skojaku/applied-soft-comp",
            ref: "gh-pages",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./m01-word-embedding"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="pen-and-paper.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Pen and Paper Exercise</p>
      </div>
    </a>
    <a class="right-next"
       href="word2vec.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">word2vec: a small model with a big idea</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-encoding-the-first-step">One-Hot Encoding: The First Step</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distributional-hypothesis">Distributional Hypothesis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tf-idf-a-simple-but-powerful-word-embedding-technique">TF-IDF: A Simple but Powerful Word Embedding Technique</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Distributional Hypothesis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tf-idf-words-as-patterns-of-usage">TF-IDF: Words as Patterns of Usage</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#first-attempt-word-document-count-matrix">First Attempt: Word-Document Count Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-problem-with-raw-counts">The Problem with Raw Counts</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-need-for-normalization">The Need for Normalization</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sadamori Kojaku
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
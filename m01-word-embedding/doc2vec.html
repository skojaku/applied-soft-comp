
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Doc2Vec: From Words to Documents &#8212; Applied Soft Computing</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'm01-word-embedding/doc2vec';</script>
    <script src="../_static/js/custom.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Summary" href="summary.html" />
    <link rel="prev" title="Bias in Word Embeddings" href="bias-in-embedding.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../home.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.jpg" class="logo__image only-light" alt="Applied Soft Computing - Home"/>
    <script>document.write(`<img src="../_static/logo.jpg" class="logo__image only-dark" alt="Applied Soft Computing - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../home.html">
                    Welcome to SSIE 541/441 Applied Soft Computing
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Intro</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../intro/why-applied-soft-computing.html">Why applied soft computing?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/setup.html">Set up</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/deliverables.html">Deliverables</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Word and Document Embeddings</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="what-to-learn.html">Module 1: Word Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="pen-and-paper.html">Pen and Paper Exercise</a></li>
<li class="toctree-l1"><a class="reference internal" href="tf-idf.html">Teaching computers how to understand words</a></li>
<li class="toctree-l1"><a class="reference internal" href="word2vec.html">word2vec: a small model with a big idea</a></li>
<li class="toctree-l1"><a class="reference internal" href="word2vec_plus.html">GloVe and FastText: Building on Word2Vec’s Foundation</a></li>
<li class="toctree-l1"><a class="reference internal" href="sem-axis.html">Understanding SemAxis: Semantic Axes in Word Vector Spaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="bias-in-embedding.html">Bias in Word Embeddings</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Doc2Vec: From Words to Documents</a></li>
<li class="toctree-l1"><a class="reference internal" href="summary.html">Summary</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Recurrent Neural Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/what-to-learn.html">Module 2: Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/rnn-interactive.html">🧠 Learn RNNs Through Physics!</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/reccurrent-neural-net.html">Recurrent Neural Network (RNN)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/lstm.html">Long Short-Term Memory (LSTM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/elmo.html">Embedding from Language Models (ELMo)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/seq2seq.html">Sequence-to-Sequence Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/summary.html">Summary</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Transformers</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/what-to-learn.html">Module 3: Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/transformers.html">Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/bert.html">Bidirectional Encoder Representations from Transformers (BERT)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/sentence-bert.html">Sentence-BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/gpt.html">Generative Pre-trained Transformer (GPT)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/from-language-model-to-instruction-following.html">From Language Model to Instruction-Following</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/prompt-tuning.html">Prompt Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/prompt-tuning-exercise.html">Prompt Tuning Exercise</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/summary.html">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/appendix-t5.html">Appendix: Text-to-Text Transfer Transformer (T5)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Teaching Machine How to Understand Images</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m04-image-processing/what-to-learn.html">Module 4: Image Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m04-image-processing/pen-and-paper.html">Pen and paper exercises</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m04-image-processing/image-processing.html">Preliminaries: Image Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m04-image-processing/cnn.html">From Traditional Image Processing to Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m04-image-processing/lenet.html">LeNet and LeNet-5: Pioneering CNN Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m04-image-processing/alexnet.html">AlexNet: A Breakthrough in Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m04-image-processing/vgg.html">VGGNet - A Deep Convolutional Neural Network for Image Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m04-image-processing/inception.html">GoogleNet and the Inception Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m04-image-processing/batch-normalization.html">Batch Normalization Explained</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m04-image-processing/resnet.html">ResNet (Residual Neural Networks)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Teaching Machine How to Understand Graphs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/what-to-learn.html">Module 4: Graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/pen-and-paper.html">Pen and paper exercises</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/spectral-embedding.html">Spectral Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/graph-embedding-w-word2vec.html">Graph embedding with word2vec</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/spectral-vs-neural-embedding.html">Spectral vs Neural Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/from-image-to-graph.html">From Image to Graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/graph-convolutional-network.html">Graph Convolutional Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/popular-gnn.html">Popular Graph Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/appendix.html">Appendix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/software.html">Software for Network Embedding</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/skojaku/applied-soft-comp/gh-pages?urlpath=tree/docs/lecture-note/m01-word-embedding/doc2vec.md" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/skojaku/applied-soft-comp" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/skojaku/applied-soft-comp/issues/new?title=Issue%20on%20page%20%2Fm01-word-embedding/doc2vec.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/m01-word-embedding/doc2vec.ipynb" target="_blank"
   class="btn btn-sm btn-download-notebook-button dropdown-item"
   title="Download notebook file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li><a href="../_sources/m01-word-embedding/doc2vec.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Doc2Vec: From Words to Documents</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#doc2vec-model">Doc2Vec Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-memory-pv-dm">Distributed Memory (PV-DM)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-bag-of-words-pv-dbow">Distributed Bag of Words (PV-DBOW)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hands-on-implementation">Hands-on Implementation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">🔥 Exercises 🔥</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="doc2vec-from-words-to-documents">
<h1>Doc2Vec: From Words to Documents<a class="headerlink" href="#doc2vec-from-words-to-documents" title="Link to this heading">#</a></h1>
<section id="doc2vec-model">
<h2>Doc2Vec Model<a class="headerlink" href="#doc2vec-model" title="Link to this heading">#</a></h2>
<p>Doc2Vec <a class="footnote-reference brackets" href="#footcite-le2014distributed" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> extends word2vec by learning document vectors alongside word vectors. For a document <span class="math notranslate nohighlight">\(d\)</span> with words <span class="math notranslate nohighlight">\(w_1, w_2, ..., w_n\)</span>, it learns:</p>
<ul class="simple">
<li><p>Document vector <span class="math notranslate nohighlight">\(v_d \in \mathbb{R}^m\)</span></p></li>
<li><p>Word vectors <span class="math notranslate nohighlight">\(v_w \in \mathbb{R}^m\)</span></p></li>
</ul>
<p>There are two types of Doc2Vec:</p>
<ul class="simple">
<li><p>Distributed Memory (PV-DM)</p></li>
<li><p>Distributed Bag of Words (PV-DBOW)</p></li>
</ul>
<p>where PV-DM corresponds to the CBOW model, and PV-DBOW corresponds to the Skip-Gram model of word2vec.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <a class="reference internal" href="word2vec.html"><span class="std std-doc">the lecture note of word2vec</span></a> for more details on CBOW and Skip-Gram.</p>
</div>
<section id="distributed-memory-pv-dm">
<h3>Distributed Memory (PV-DM)<a class="headerlink" href="#distributed-memory-pv-dm" title="Link to this heading">#</a></h3>
<figure class="align-center" id="pv-dm">
<a class="reference internal image-reference" href="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JPetbQHmG0NAbdQ08JSiMQ.png"><img alt="PV-DM" src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JPetbQHmG0NAbdQ08JSiMQ.png" style="width: 500px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 1 </span><span class="caption-text">PV-DM predicts the center word based on the <em>average</em> or <em>concatenated</em> vector of the context words. Image taken from <a class="reference external" href="https://heartbeat.comet.ml/getting-started-with-doc2vec-2645e3e9f137">https://heartbeat.comet.ml/getting-started-with-doc2vec-2645e3e9f137</a></span><a class="headerlink" href="#pv-dm" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>CBOW word2vec predicts the center word based on the <em>average</em> or <em>concatenated</em> vector of the context words.
In PV-DM, the document vector is added to the average or concatenation.
More specifically, the probability of a word <span class="math notranslate nohighlight">\(w_i\)</span> given the document <span class="math notranslate nohighlight">\(d\)</span> and the context <span class="math notranslate nohighlight">\(w_{i-k},...,w_{i-1}\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[P(w_i|w_{i-k},...,w_{i-1},d) = \frac{\exp(u_{w_i}^T h)}{\sum_{w \in V} \exp(u_w^T h)}\]</div>
<p>where <span class="math notranslate nohighlight">\(h\)</span> is the context vector, which is either the average:</p>
<div class="math notranslate nohighlight">
\[
h = \frac{1}{k\textcolor{red}{+1}}\left(\textcolor{red}{v_d} + \sum_{j=i-k}^{i-1}v_{w_j}\right)
\]</div>
<p>or the concatenation:</p>
<div class="math notranslate nohighlight">
\[
h = \left(v_d, \sum_{j=i-k}^{i-1}v_{w_j}\right) U, \quad U \in \mathbb{R}^{(d+kd) \times d}
\]</div>
<p>where <span class="math notranslate nohighlight">\(U\)</span> is a matrix that maps the concatenated vector (of dimension <span class="math notranslate nohighlight">\(d+kd\)</span>) back to dimension <span class="math notranslate nohighlight">\(d\)</span> to match the word vector space. Here, <span class="math notranslate nohighlight">\(d\)</span> is the embedding dimension and <span class="math notranslate nohighlight">\(k\)</span> is the context window size.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The choice between concatenation and average affects how the document and context vectors are combined:</p>
<ul class="simple">
<li><p><strong>Average</strong>: Treats document vector and context word vectors equally by taking their mean. This is simpler but may neglect the influence of individual context words. No additional parameters needed, making it computationally efficient.</p></li>
<li><p><strong>Concatenation</strong>: Keeps document and context information separate before combining through the U matrix. This preserves more distinct information but requires learning additional parameters (the U matrix). Though more computationally intensive, it allows the model to learn different weights for document and word contexts.
The original paper used concatenation, arguing it allows the model to treat document and word vectors differently.</p></li>
</ul>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The softmax computation over the entire vocabulary V can be computationally expensive for large vocabularies. In practice, optimization techniques like negative sampling or hierarchical softmax are commonly used to approximate this computation more efficiently.</p>
</div>
</section>
<section id="distributed-bag-of-words-pv-dbow">
<h3>Distributed Bag of Words (PV-DBOW)<a class="headerlink" href="#distributed-bag-of-words-pv-dbow" title="Link to this heading">#</a></h3>
<figure class="align-center" id="pv-dbow">
<a class="reference internal image-reference" href="https://miro.medium.com/v2/resize:fit:1400/1*ALpuAo7uv0V8PlrVgSzMsg.png"><img alt="PV-DBOW" src="https://miro.medium.com/v2/resize:fit:1400/1*ALpuAo7uv0V8PlrVgSzMsg.png" style="width: 500px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2 </span><span class="caption-text">PV-DBOW predicts context words using only the document vector, similar to Skip-Gram predicting context words from a center word. Image taken from <a class="reference external" href="https://heartbeat.comet.ml/getting-started-with-doc2vec-2645e3e9f137">https://heartbeat.comet.ml/getting-started-with-doc2vec-2645e3e9f137</a></span><a class="headerlink" href="#pv-dbow" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>PV-DBOW is similar to Skip-Gram. The probability of a word <span class="math notranslate nohighlight">\(w_i\)</span> given the document <span class="math notranslate nohighlight">\(d\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[P(w_i|d) = \frac{\exp(u_{w_i}^T v_d)}{\sum_{w \in V} \exp(u_w^T v_d)}\]</div>
<p>This is analogous to the skip-gram model, where the document vector <span class="math notranslate nohighlight">\(v_d\)</span> is used to predict the context words.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Which mode, PV-DM or PV-DBOW, is better? The original paper <a class="footnote-reference brackets" href="#footcite-le2014distributed" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> suggests that PV-DM is better, since it can distinguish the order of words within a document.
Yet, <a class="footnote-reference brackets" href="#footcite-le2016empirical" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a> found that PV-DBOW, despite being more simple, is better overall for document similarity tasks, when properly tuned. This highlights the importance of hyperparameter optimization in practice.</p>
<p>Key considerations for choosing between PV-DM and PV-DBOW:</p>
<ul class="simple">
<li><p>PV-DM: Better for tasks requiring word order sensitivity</p></li>
<li><p>PV-DBOW: More efficient training, often better for similarity tasks</p></li>
<li><p>Hybrid approach: Some implementations combine both methods</p></li>
</ul>
</div>
</section>
</section>
<section id="hands-on-implementation">
<h2>Hands-on Implementation<a class="headerlink" href="#hands-on-implementation" title="Link to this heading">#</a></h2>
<p>Let us have a hands-on implementation of Doc2Vec using the <code class="docutils literal notranslate"><span class="pre">gensim</span></code> library.
Our sample documents are:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Sample documents</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;Machine learning is a subset of artificial intelligence&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Deep learning uses neural networks with multiple layers&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Natural language processing deals with text and speech&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Computer vision focuses on image and video analysis&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Reinforcement learning involves agents making decisions&quot;</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>We will first import the necessary libraries.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">gensim.models.doc2vec</span> <span class="kn">import</span> <span class="n">Doc2Vec</span><span class="p">,</span> <span class="n">TaggedDocument</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span>
</pre></div>
</div>
</div>
</div>
<p>In gensim doc2vec, we need to prepare the documents in the form of <code class="docutils literal notranslate"><span class="pre">TaggedDocument</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Prepare documents</span>
<span class="n">tagged_docs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">doc</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">documents</span><span class="p">):</span>
    <span class="n">tagged_doc</span> <span class="o">=</span> <span class="n">TaggedDocument</span><span class="p">(</span>
        <span class="n">words</span><span class="o">=</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">lower</span><span class="p">()),</span> <span class="c1"># tokenize the document</span>
        <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="c1"># tag the document with its index</span>
    <span class="p">)</span>
    <span class="n">tagged_docs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tagged_doc</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We added “tags” along with the words. The “tag” is used to identify the document.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><code class="docutils literal notranslate"><span class="pre">word_tokenize</span></code> is a function from the <code class="docutils literal notranslate"><span class="pre">nltk</span></code> library that tokenizes the document into words.
For example, “Machine learning is a subset of artificial intelligence” is tokenized into <code class="docutils literal notranslate"><span class="pre">['machine',</span> <span class="pre">'learning',</span> <span class="pre">'is',</span> <span class="pre">'a',</span> <span class="pre">'subset',</span> <span class="pre">'of',</span> <span class="pre">'artificial',</span> <span class="pre">'intelligence']</span></code>.</p>
</div>
<p>Second, we need to train the Doc2Vec model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train Doc2Vec model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Doc2Vec</span><span class="p">(</span><span class="n">tagged_docs</span><span class="p">,</span>
                <span class="n">vector_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="c1"># dimension of the document vector</span>
                <span class="n">window</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="c1"># context window size</span>
                <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># ignore words that appear less than this</span>
                <span class="n">epochs</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span>
                <span class="n">dm</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># 0: PV-DBOW, 1: PV-DM</span>
                <span class="p">)</span>

<span class="c1"># Common hyperparameters to tune:</span>
<span class="c1"># - vector_size: Higher dimensions can capture more complex relationships but need more data</span>
<span class="c1"># - window: Larger windows capture broader context but increase computation</span>
<span class="c1"># - dm_concat: 1 for concatenation, 0 for averaging in PV-DM</span>
<span class="c1"># - negative: Number of negative samples (default: 5)</span>
<span class="c1"># - alpha: Initial learning rate</span>
</pre></div>
</div>
</div>
</div>
<p>This generates the model along with the word and document vectors.</p>
<p>One of the interesting features of Doc2Vec is that it can generate an embedding for a new unseen document.
This can be done by using the <code class="docutils literal notranslate"><span class="pre">infer_vector</span></code> method.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>A new document vector is generated by fixing the word vectors and optimizing the document vector through gradient descent.
The inference process:</p>
<ol class="arabic simple">
<li><p>Initialize a random document vector</p></li>
<li><p>Perform gradient updates using the pre-trained word vectors</p></li>
<li><p>Return the optimized document vector
See <a class="footnote-reference brackets" href="#footcite-le2014distributed" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> for more details.</p></li>
</ol>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Find similar documents</span>
<span class="n">test_doc</span> <span class="o">=</span> <span class="s2">&quot;AI systems use machine learning algorithms&quot;</span>
<span class="n">test_vector</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">infer_vector</span><span class="p">(</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">test_doc</span><span class="o">.</span><span class="n">lower</span><span class="p">()))</span>
</pre></div>
</div>
</div>
</div>
<p>Now, let us find the most similar documents to the test document.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">similar_docs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">dv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">([</span><span class="n">test_vector</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Similar documents to:&quot;</span><span class="p">,</span> <span class="n">test_doc</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">doc_id</span><span class="p">,</span> <span class="n">similarity</span> <span class="ow">in</span> <span class="n">similar_docs</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Document </span><span class="si">{</span><span class="n">doc_id</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">documents</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">doc_id</span><span class="p">)]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Similarity: </span><span class="si">{</span><span class="n">similarity</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Similar documents to: AI systems use machine learning algorithms 

Document 0: Machine learning is a subset of artificial intelligence
Similarity: 0.9842

Document 2: Natural language processing deals with text and speech
Similarity: 0.9803

Document 1: Deep learning uses neural networks with multiple layers
Similarity: 0.9698

Document 3: Computer vision focuses on image and video analysis
Similarity: 0.9667

Document 4: Reinforcement learning involves agents making decisions
Similarity: 0.9577
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercises">
<h2>🔥 Exercises 🔥<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Create a search engine using Doc2Vec. This search engine takes a query document and returns the most similar documents.</p></li>
<li><p>Perform topic classification based on the document vectors.</p></li>
<li><p>Visualization Challenge</p>
<ul class="simple">
<li><p>Create t-SNE/UMAP/PCA visualization of the document vectors.</p></li>
<li><p>Color-code by topic</p></li>
<li><p>Analyze clustering patterns</p></li>
</ul>
</li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_documents</span> <span class="o">=</span> <span class="p">[</span>
    <span class="c1"># Technology</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Artificial intelligence is transforming the way we interact with computers&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;technology&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Machine learning algorithms can identify patterns in complex datasets&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;technology&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Deep learning models have achieved human-level performance in image recognition&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;technology&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Neural networks are inspired by biological brain structures&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;technology&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Natural language processing enables machines to understand human text&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;technology&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Computer vision systems can detect objects in real-time video streams&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;technology&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Reinforcement learning agents learn through interaction with environments&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;technology&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Edge computing brings AI processing closer to data sources&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;technology&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Transfer learning reduces the need for large training datasets&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;technology&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Automated machine learning optimizes model architecture search&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;technology&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Explainable AI helps understand model decision-making processes&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;technology&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Federated learning enables privacy-preserving model training&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;technology&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Quantum computing promises breakthroughs in optimization problems&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;technology&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Cloud platforms provide scalable computing resources&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;technology&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;GPUs accelerate deep learning model training significantly&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;technology&quot;</span><span class="p">},</span>

    <span class="c1"># Science</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Quantum mechanics explains behavior at the atomic scale&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;science&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;DNA sequencing reveals genetic variations between organisms&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;science&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Chemical reactions transfer energy between molecules&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;science&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Gravitational waves provide insights into cosmic events&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;science&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Particle accelerators probe fundamental physics laws&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;science&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Cell biology studies the basic units of life&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;science&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Evolutionary theory explains species adaptation&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;science&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Neuroscience investigates brain structure and function&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;science&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Climate models predict long-term weather patterns&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;science&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Geological processes shape Earth&#39;s surface features&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;science&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Conservation biology aims to protect endangered species&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;science&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Astronomy studies celestial objects and phenomena&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;science&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Biochemistry examines cellular metabolic processes&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;science&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Physics laws describe fundamental force interactions&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;science&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Scientific method tests hypotheses through experiments&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;science&quot;</span><span class="p">},</span>

    <span class="c1"># Business</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Market analysis guides investment decisions&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;business&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Strategic planning sets long-term company goals&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;business&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Financial reports track business performance metrics&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;business&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Supply chain optimization reduces operational costs&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;business&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Customer relationship management builds loyalty&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;business&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Digital marketing reaches targeted audiences online&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;business&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Product development responds to market demands&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;business&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Risk management protects business assets&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;business&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Human resources develops employee talent&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;business&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Sales strategies drive revenue growth&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;business&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Competitive analysis identifies market opportunities&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;business&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Business analytics inform decision-making processes&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;business&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Brand management builds company reputation&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;business&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Operations management streamlines production processes&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;business&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Innovation strategy drives business transformation&quot;</span><span class="p">,</span> <span class="s2">&quot;topic&quot;</span><span class="p">:</span> <span class="s2">&quot;business&quot;</span><span class="p">}</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="docutils container" id="id5">
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="footcite-le2014distributed" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id2">2</a>,<a role="doc-backlink" href="#id4">3</a>)</span>
<p>Quoc Le and Tomas Mikolov. Distributed representations of sentences and documents. In <em>International conference on machine learning</em>, 1188–1196. PMLR, 2014.</p>
</aside>
<aside class="footnote brackets" id="footcite-le2016empirical" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">2</a><span class="fn-bracket">]</span></span>
<p>Quoc Le and Tomas Mikolov. An empirical evaluation of doc2vec with practical insights into document embedding generation. In Phil Blunsom, Kyunghyun Cho, Shay Cohen, Edward Grefenstette, Karl Moritz Hermann, Laura Rimell, Jason Weston, and Scott Wen-tau Yih, editors, <em>Proceedings of the 1st Workshop on Representation Learning for NLP</em>, 78–86. Berlin, Germany, August 2016. Association for Computational Linguistics. URL: <a class="reference external" href="https://aclanthology.org/W16-1609">https://aclanthology.org/W16-1609</a>, <a class="reference external" href="https://doi.org/10.18653/v1/W16-1609">doi:10.18653/v1/W16-1609</a>.</p>
</aside>
</aside>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "skojaku/applied-soft-comp",
            ref: "gh-pages",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./m01-word-embedding"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="bias-in-embedding.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Bias in Word Embeddings</p>
      </div>
    </a>
    <a class="right-next"
       href="summary.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Summary</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#doc2vec-model">Doc2Vec Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-memory-pv-dm">Distributed Memory (PV-DM)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-bag-of-words-pv-dbow">Distributed Bag of Words (PV-DBOW)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hands-on-implementation">Hands-on Implementation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">🔥 Exercises 🔥</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sadamori Kojaku
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
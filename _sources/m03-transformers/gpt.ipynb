{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "540b2d83",
   "metadata": {},
   "source": [
    "# Generative Pre-trained Transformer (GPT)\n",
    "\n",
    "The Generative Pre-trained Transformer (GPT) {footcite}`radford2018language` represents a significant evolution in transformer-based language models, focusing on powerful text generation capabilities through a decoder-only architecture. While BERT uses bidirectional attention to understand context, GPT employs unidirectional (causal) attention to generate coherent text by predicting one token at a time.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "Like in BERT, GPT also uses a transformer architecture. The main difference is that BERT uses an encoder transformer, while GPT uses a decoder transformer with some modifications.\n",
    "\n",
    "```{figure} https://heidloff.net/assets/img/2023/02/transformers.png\n",
    ":name: gpt-architecture\n",
    ":alt: GPT architecture\n",
    ":align: center\n",
    ":width: 80%\n",
    "\n",
    "GPT architecture.\n",
    "```\n",
    "\n",
    "\n",
    "```{tip}\n",
    "\n",
    "The GPT model family has evolved through several iterations, starting with GPT-1 in 2018 which introduced the basic architecture with 117M parameters and transfer learning capabilities. GPT-2 followed in 2019 with 1.5B parameters and zero-shot abilities, while GPT-3 in 2020 dramatically scaled up to 175B parameters, enabling few-shot learning. The latest GPT-4 (2023) features multimodal capabilities, improved reasoning, and a 32K token context window. Throughout these iterations, the core decoder-only transformer architecture remained unchanged, with improvements coming primarily from increased scale that enabled emergent capabilities.\n",
    "\n",
    "```{figure} https://miro.medium.com/v2/resize:fit:1400/1*Wnn0e8B-_IiTvmpv-1P7Iw.png\n",
    ":name: gpt-evolution\n",
    ":alt: GPT evolution\n",
    ":align: center\n",
    ":width: 80%\n",
    "\n",
    "```\n",
    "\n",
    "### Core Components\n",
    "\n",
    "```{figure} https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7536a59a-5326-4a8b-ab12-cebe49acde31_1438x936.png\n",
    ":name: gpt-causal-attention\n",
    ":alt: GPT causal attention\n",
    ":align: center\n",
    ":width: 80%\n",
    "\n",
    "Causal attention in GPT.\n",
    "```\n",
    "\n",
    "Like BERT, GPT uses learned token embeddings to convert input tokens into continuous vector representations. The model also employs learned positional embeddings that are added to the token embeddings to encode position information. A key difference from BERT is that GPT uses a *causal attention mechanism*, which means each position can only attend to previous positions in the sequence, enabling the model to generate text in a left-to-right fashion by predicting one token at a time.\n",
    "\n",
    "\n",
    "\n",
    "### Causal Language Modeling\n",
    "\n",
    "Causal (autoregressive) language modeling is the pre-training objective of GPT, where the model learns to predict the next token given all previous tokens in the sequence. More formally, given a sequence of tokens $(x_1, x_2, ..., x_n)$, the model is trained to maximize the likelihood:\n",
    "\n",
    "$$\n",
    "P(x_1, ..., x_n) = \\prod_{i=1}^n P(x_i|x_1, ..., x_{i-1})\n",
    "$$\n",
    "\n",
    "For example, given the partial sentence \"The cat sat on\", the model learns to predict the next word by calculating probability distributions over its entire vocabulary. During training, it might learn that \"mat\" has a high probability in this context, while \"laptop\" has a lower probability.\n",
    "\n",
    "```{note}\n",
    "While BERT uses bidirectional attention and sees the entire sequence at once (making it powerful for understanding), GPT's unidirectional approach more naturally models how humans write text, i.e., one word at a time, with each word influenced by all previous words.\n",
    "The bidirectional nature of BERT is more powerful for understanding, but it is less suitable for text generation.\n",
    "```\n",
    "\n",
    "```{tip}\n",
    "The autoregressive nature of GPT means it's particularly sensitive to the initial tokens (prompt) it receives. Well-crafted prompts that establish clear patterns or constraints can significantly improve generation quality.\n",
    "```\n",
    "\n",
    "The next-token prediction objective has remained unchanged across all GPT versions due to its remarkable effectiveness. Rather than modifying this core approach, improvements have come from increasing model size and refining the architecture. This simple yet powerful training method has become fundamental to modern language models.\n",
    "\n",
    "```{admonition} Scaling Laws\n",
    ":class: tip\n",
    ":name: scaling-laws\n",
    "\n",
    "Language model performance improves *predictably* as models get larger, following simple mathematical relationships (power laws). The larger the model, the better it performs - and this improvement is reliable and measurable. This predictability was crucial for the development of models like GPT-3 and Claude, as it gave researchers confidence that investing in larger models would yield better results.\n",
    "Importantly, larger models are more efficient learners - they need proportionally less training data and fewer training steps to achieve good performance.\n",
    "\n",
    "These findings revolutionized AI development by showing that better AI systems could be reliably built simply by scaling up model size, compute, and data in the right proportions. This insight led directly to the development of increasingly powerful models, as researchers could confidently invest in building larger and larger systems knowing they would see improved performance.\n",
    "\n",
    "See the paper [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361) for more details.\n",
    "\n",
    "```{figure} https://miro.medium.com/v2/resize:fit:1400/1*5fsJPwvFjS7fo8g8NwsxNA.png\n",
    ":name: scaling-laws-figure\n",
    ":alt: Scaling laws figure\n",
    ":align: center\n",
    ":width: 80%\n",
    "```\n",
    "\n",
    "## Inference Strategies\n",
    "\n",
    "GPT does not generate text in one go. Instead, it predicts the next token repeatedly to generate text. GPT does not pick a specific token but provides a *probability distribution* over the next token. It is our job to *sample* a token from the distribution. There are several strategies to sample a token from the distribution as we will see below.\n",
    "\n",
    "```{figure} https://media.licdn.com/dms/image/v2/D4E22AQFZFRSwwzCSqQ/feedshare-shrink_2048_1536/feedshare-shrink_2048_1536/0/1725003016027?e=2147483647&v=beta&t=oBH1s4V8N0wKCOJQakA_wrwgFrixs56S0s_QafZOvbA\n",
    ":name: gpt-inference\n",
    ":alt: GPT inference\n",
    ":align: center\n",
    ":width: 50%\n",
    "\n",
    "GPT predicts the next token repeatedly to generate text.\n",
    "```\n",
    "\n",
    "### Greedy and Beam Search\n",
    "\n",
    "When generating text, language models assign probabilities to possible next tokens.\n",
    "Sampling a token from the distribution is not as easy as it might seem. This is because the distribution is high-dimensional. Namely, we need to sample a single token from millions of possible tokens, and thus, sampling a token can be computationally very expensive.\n",
    "\n",
    "**Greedy sampling** always picks the highest probability token, which is deterministic but can lead to repetitive or trapped text. For example, if the model predicts \"the\" with high probability, it will always predict \"the\" again.\n",
    "\n",
    "```{figure} https://huggingface.co/blog/assets/02_how-to-generate/greedy_search.png\n",
    ":name: gpt-greedy-search\n",
    ":alt: GPT greedy search\n",
    ":align: center\n",
    ":width: 50%\n",
    "\n",
    "GPT greedy search.\n",
    "```\n",
    "\n",
    "**Beam search** alleviates this problem by taking into account the high-order dependencies between tokens. For example, in generating \"The cat ran across the ___\", beam search might preserve a path containing \"mat\" even if \"floor\" or \"room\" have higher individual probabilities at that position. This is because the complete sequence like \"mat quickly\" could be more probable when considering the token next after \"mat\".  \"The cat ran across the mat quickly\" is a more natural phrase than \"The cat ran across the floor quickly\" when considering the full flow and common linguistic patterns.\n",
    "\n",
    "\n",
    " ```{figure} https://huggingface.co/blog/assets/02_how-to-generate/beam_search.png\n",
    " :name: gpt-beam-search\n",
    " :alt: GPT beam search\n",
    " :align: center\n",
    " :width: 50%\n",
    "\n",
    " GPT beam search.\n",
    " ```\n",
    "\n",
    "Beam search maintains multiple possible sequences (beams) in parallel, exploring different paths simultaneously. At each step, it expands all current beams, scores the resulting sequences, and keeps only the top-k highest scoring ones. For instance, with a beam width of 3:\n",
    "- First beams might be: [\"The cat ran\", \"The cat walked\", \"The cat jumped\"]\n",
    "- Next step: [\"The cat ran across\", \"The cat ran through\", \"The cat walked across\"]\n",
    "- And so on, keeping the 3 most promising complete sequences at each step\n",
    "\n",
    "This process continues until reaching the end, finally selecting the sequence with highest overall probability. The beam search can be combined with top-k sampling or nucleus sampling. For example, one can sample a token based on the top-k sampling or nucleus sampling to form the next beam.\n",
    "\n",
    "While beam search often produces high-quality outputs since it considers longer-term coherence, it can still suffer from the problem of repetitive or trapped text.\n",
    "\n",
    "### From Deterministic to Stochastic Sampling\n",
    "\n",
    "Both greedy and beam search are deterministic. They pick the most likely token at each step. However, this creates a loop where the model always predicts the same tokens repeatedly. A simple way to alleviate this problem is to sample a token from the distribution.\n",
    "\n",
    "**Top-k Sampling** relaxes the deterministic nature of greedy sampling by selecting randomly from the k most likely next tokens at each generation step. While this introduces some diversity compared to greedy sampling, choosing a fixed k can be problematic. Value of $k$ might be too large for some distribution tails (including many poor options) or too small for others (excluding reasonable options).\n",
    "\n",
    "**Nucleus Sampling**~{footcite}`holtzman2019curious` addresses this limitation by dynamically selecting tokens based on cumulative probability. It samples from the smallest set of tokens whose cumulative probability exceeds a threshold p (e.g. 0.9). This adapts naturally to different probability distributions, i.e., selecting few tokens when the distribution is concentrated and more when it's spread out. This approach often provides a good balance between quality and diversity.\n",
    "\n",
    "```{figure} https://storage.googleapis.com/zenn-user-upload/8p2r9urhtn5nztdg6mnia3toibhl\n",
    ":name: gpt-top-k-top-p\n",
    ":alt: GPT top-k top-p\n",
    ":align: center\n",
    ":width: 80%\n",
    "\n",
    "Nucleus sampling. The image is taken from [this blog](https://zenn.dev/hellorusk/articles/1c0bef15057b1d).\n",
    "```\n",
    "\n",
    "**Temperature Control**\n",
    "Temperature ($\\tau$) modifies how \"concentrated\" the probability distribution is for sampling by scaling the logits before applying softmax:\n",
    "\n",
    "$$\n",
    "p_i = \\frac{\\exp(z_i/\\tau)}{\\sum_j \\exp(z_j/\\tau)}\n",
    "$$\n",
    "\n",
    "where $z_i$ are the logits and $\\tau$ is the temperature parameter. Lower temperatures ($\\tau < 1.0$) make the distribution more peaked, making high probability tokens even more likely to be chosen, leading to more focused and conservative outputs. Higher temperatures ($\\tau > 1.0$) flatten the distribution by making the logits more similar, increasing the chances of selecting lower probability tokens and producing more diverse but potentially less coherent text. As $\\tau \\to 0$, the distribution approaches a one-hot vector (equivalent to greedy search), while as $\\tau \\to \\infty$, it approaches a uniform distribution.\n",
    "\n",
    "\n",
    "```{figure} https://cdn.prod.website-files.com/618399cd49d125734c8dec95/6639e35ce91c16b3b9564b2f_mxaIPcROZcBFYta1I0nzWjlGTgs-LxzUOE3p6Kbvf9qPpZzBh5AAZG7ciRtgVquhLTtrM8ToJdNd-ubXvuz8tRfrqBwSozWHCj457pm378buxz2-XrMfWzfSv3b793QP61kLxRKT299WP1gbas_E118.png\n",
    ":name: gpt-temperature\n",
    ":alt: GPT temperature\n",
    ":align: center\n",
    ":width: 80%\n",
    "\n",
    "Temperature controls the concentration of the probability distribution. Lower temperature makes the distribution more peaked, while higher temperature makes the distribution more flat.\n",
    "```\n",
    "\n",
    "\n",
    "## Hands-on\n",
    "\n",
    "Let us learn how to generate text with GPT-2. We will use the `transformers` library to load the model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "177d4459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch_device = \"cpu\" # \"cpu\" or \"cuda\" or \"mps\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch_device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    torch_device = \"mps\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(torch_device)\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab602aa0",
   "metadata": {},
   "source": [
    "Let's generate text using the greedy search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a72cd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI enjoy walking with my cute dog\u001b[39m\u001b[38;5;124m'\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch_device)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# generate 40 new tokens\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m greedy_output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(greedy_output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m~/miniforge3/envs/advnetsci/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/advnetsci/lib/python3.11/site-packages/transformers/generation/utils.py:1828\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1825\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m inputs_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1827\u001b[0m device \u001b[38;5;241m=\u001b[39m inputs_tensor\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m-> 1828\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_special_tokens(generation_config, kwargs_has_attention_mask, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m   1830\u001b[0m \u001b[38;5;66;03m# decoder-only models must use left-padding for batched generation.\u001b[39;00m\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;66;03m# If `input_ids` was given, check if the last id in any sequence is `pad_token_id`\u001b[39;00m\n\u001b[1;32m   1833\u001b[0m     \u001b[38;5;66;03m# Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/advnetsci/lib/python3.11/site-packages/transformers/generation/utils.py:1677\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_special_tokens\u001b[0;34m(self, generation_config, kwargs_has_attention_mask, device)\u001b[0m\n\u001b[1;32m   1671\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1672\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`decoder_start_token_id` or `bos_token_id` has to be defined for encoder-decoder generation.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1673\u001b[0m     )\n\u001b[1;32m   1674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():  \u001b[38;5;66;03m# Checks that depend on tensor-dependent control flow\u001b[39;00m\n\u001b[1;32m   1675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1676\u001b[0m         eos_token_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1677\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m isin_mps_friendly(elements\u001b[38;5;241m=\u001b[39meos_token_tensor, test_elements\u001b[38;5;241m=\u001b[39mpad_token_tensor)\u001b[38;5;241m.\u001b[39many()\n\u001b[1;32m   1678\u001b[0m     ):\n\u001b[1;32m   1679\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m kwargs_has_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs_has_attention_mask:\n\u001b[1;32m   1680\u001b[0m             logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   1681\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe attention mask is not set and cannot be inferred from input because pad token is same as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1682\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meos token. As a consequence, you may observe unexpected behavior. Please pass your input\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1683\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`attention_mask` to obtain reliable results.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1684\u001b[0m             )\n",
      "File \u001b[0;32m~/miniforge3/envs/advnetsci/lib/python3.11/site-packages/transformers/pytorch_utils.py:325\u001b[0m, in \u001b[0;36misin_mps_friendly\u001b[0;34m(elements, test_elements)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;124;03mSame as `torch.isin` without flags, but MPS-friendly. We can remove this function when we stop supporting\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03mtorch <= 2.3. See https://github.com/pytorch/pytorch/issues/77764#issuecomment-2067838075\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m    and False otherwise\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m elements\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_greater_or_equal_than_2_4:\n\u001b[0;32m--> 325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elements\u001b[38;5;241m.\u001b[39mtile(test_elements\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39meq(test_elements\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mbool()\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;66;03m# Note: don't use named arguments in `torch.isin`, see https://github.com/pytorch/pytorch/issues/126045\u001b[39;00m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misin(elements, test_elements)\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# We encode the input text into tokens.\n",
    "model_inputs = tokenizer('I enjoy walking with my cute dog', return_tensors='pt').to(torch_device)\n",
    "\n",
    "# generate 40 new tokens\n",
    "greedy_output = model.generate(**model_inputs, max_new_tokens=40)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ee8fdd",
   "metadata": {},
   "source": [
    "The greedy search approach often falls into repetitive patterns. This limitation occurs because greedy search always selects the most probable next token, leading to predictable and sometimes monotonous outputs.\n",
    "Interested readers can refer to these papers {footcite}`vijayakumar2016diverse` and {footcite}`shao2017generating` for more details.\n",
    "\n",
    "Now, let us showcase how to generate text with beam search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eff800",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    num_beams=5, # Number of beams\n",
    "    early_stopping=True # Stop when the model generates the EOS token\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1720f27e",
   "metadata": {},
   "source": [
    "Beam search produces more polished text compared to greedy search by exploring multiple possible sequences simultaneously. However, it still tends to generate repetitive content since it focuses on the most probable paths.\n",
    "\n",
    "Beam search generates multiple sequences in parallel, and we can retrieve the top-k sequences with the highest probability as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7388ce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    num_beams=5, # Number of beams\n",
    "    num_return_sequences=5, # Number of output sequences\n",
    "    early_stopping=True # Stop when the model generates the EOS token\n",
    ")\n",
    "\n",
    "# now we have 3 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9752fec6",
   "metadata": {},
   "source": [
    "Notice how similar these generated sequences are to each other. This similarity stems from beam search's fundamental approach: it still favors highly probable sequences, which often share common patterns and phrasings. This behavior reveals a key limitation of probability-based selection strategies.\n",
    "\n",
    "These results highlight a crucial insight: the most probable sequences aren't necessarily the most engaging or human-like. Research has shown that human-written text actually exhibits more variety and unpredictability than the highest-probability sequences generated by language models {footcite}`holtzman2019curious`.\n",
    "\n",
    "This observation can be interpreted in two ways: either our models aren't yet sophisticated enough to capture the true distribution of human text {footcite}`welleck2020unlikelihood`, or our generation strategies need to incorporate some randomness to better mimic human writing patterns {footcite}`holtzman2019curious`. Let's explore the second hypothesis by introducing controlled randomness into our generation process.\n",
    "\n",
    "In transformers, we set `do_sample=True` to activate sampling. And we set `top_k=50` to perform the top-k sampling, with $k=50$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23627ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42)\n",
    "\n",
    "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf81776",
   "metadata": {},
   "source": [
    "Now the generated text becomes very different from the greedy search. We can control the \"randomness\" of the generated text by setting the temperature. For example, by default, the temperature is 1.0. But we can reduce it to make the generated text to have a higher probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabf2ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af76dcf",
   "metadata": {},
   "source": [
    "Now, let us try nucleus sampling. We set `top_p=0.95` to perform the nucleus sampling, with $p=0.95$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c977494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    top_k=0, # deactivate top_k.\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f33c2b0",
   "metadata": {},
   "source": [
    "It is hard to say which method is better. In practice, both methods often work well. You can combine both methods by setting `top_k=50` and `top_p=0.95`, for instance. This leaves the candidate set of tokens that are more likely than those in the top-k sampling or nucleus sampling. So let's sample multiple sequences using the nucleus sampling and top-k sampling being toggled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae5362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=3,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd469e2d",
   "metadata": {},
   "source": [
    "## ðŸ”¥ Exercise ðŸ”¥\n",
    "\n",
    "Write an interesting story using only GPT-2. Write the first prompt and let GPT-2 generate the rest.\n",
    "\n",
    "Change the temperature, top-k, top-p, and other parameters to make the story natural and interesting."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe1ddeea",
   "metadata": {},
   "source": [
    "# Bidirectional Encoder Representations from Transformers (BERT)\n",
    "\n",
    "We learned about ELMo's approach to the problem of polysemy using bidirectional LSTMs. BERT builds upon its bidirectional approach by using a self-attention mechanism in transformers.\n",
    "BERT has become the leading transformer model for natural language processing tasks like question answering and text classification. Its effectiveness led Google to incorporate it into their search engine to improve query understanding. In this section, we will explore BERT's architecture and mechanisms.\n",
    "\n",
    "```{figure} https://cdn.botpenguin.com/assets/website/BERT_c35709b509.webp\n",
    ":name: bert_mlm\n",
    ":alt: BERT MLM\n",
    ":width: 50%\n",
    ":align: center\n",
    "\n",
    "```\n",
    "\n",
    "```{admonition} BERT in interactive mode:\n",
    ":class: tip\n",
    "\n",
    "[Here is a demo notebook for BERT](https://static.marimo.app/static/bert-ux7g)\n",
    "\n",
    "To run the notebook, download the notebook as a `.py` file and run it with:\n",
    "\n",
    "> marimo edit --sandbox bert.py\n",
    "\n",
    "You will need to install `marimo` and `uv` to run the notebook. But other packages will be installed automatically in uv's virtual environment.\n",
    "```\n",
    "\n",
    "## Architecture\n",
    "\n",
    "BERT consists of a stack of encoder transformer layers. Each layer is composed of a self-attention mechanism, a feed-forward neural network, and layer normalization, wired together with residual connections.\n",
    "The output of each layer is fed into the next layer, and as we go through the layers, the token embeddings get more and more contextualized, reflecting the context more and more, thanks to the self-attention mechanism.\n",
    "\n",
    "```{figure} https://www.researchgate.net/publication/372906672/figure/fig2/AS:11431281179224913@1691164535766/BERT-model-architecture.ppm\n",
    ":name: bert_architecture\n",
    ":alt: BERT architecture\n",
    ":width: 80%\n",
    ":align: center\n",
    "\n",
    "BERT consists of a stack of encoder transformer layers. The position embeddings are added to the token embeddings to provide the model with information about the position of the tokens in the sequence.\n",
    "```\n",
    "\n",
    "```{admonition} Which layer of BERT should we use?\n",
    ":class: tip\n",
    "\n",
    "BERT internally generates multiple hierarchical representations of the input sentence. The higher layers of the model capture more abstract and context-sensitive information, while the lower layers capture more local and surface-level information. Which layer to use depends on the task. For example, if we want to do text classification, we should use the output of the last layer. If we are interested in word-level representations, we should use the output of the first layer.\n",
    "```\n",
    "\n",
    "\n",
    "## Special tokens\n",
    "\n",
    "BERT uses several special tokens to represent the input sentence.\n",
    "\n",
    "- [CLS] is used to represent the start of the sentence.\n",
    "- [SEP] is used to represent the end of the sentence.\n",
    "- [MASK] is used to represent the masked words.\n",
    "- [UNK] is used to represent the unknown words.\n",
    "\n",
    "For example, the sentence \"The cat sat on the mat. It then went to sleep.\" is represented as \"[CLS] The cat sat on the mat [SEP] It then went to sleep [SEP]\".\n",
    "\n",
    "In BERT, [CLS] token is used to classify the input sentences, as we will see later. As a result, the model learns to encode a summary of the input sentence into the [CLS] token, which is particularly useful when we want the embedding of the whole input text, instead of the token level embeddings. {footcite}`reimers2019sentence`\n",
    "\n",
    "## Position and Segment embeddings\n",
    "\n",
    "BERT uses *position* and *segment* embeddings to provide the model with information about the position of the tokens in the sequence.\n",
    "\n",
    "- Position embeddings are used to provide the model with information about the position of the tokens in the sequence. Unlike the sinusoidal position embedding used in the original transformer paper {footcite}`vaswani2017attention`, BERT uses learnable position embeddings.\n",
    "\n",
    "\n",
    "- The segment embeddings are used to distinguish the sentences in the input. For example, for the sentence \"The cat sat on the mat. It then went to sleep.\", the tokens in the first sentence are added with segment embedding 0, and the tokens in the second sentence are added with segment embedding 1. These segmend embeddings are also learned during the pre-training process.\n",
    "\n",
    "```{figure} https://lh3.googleusercontent.com/stK9CWIWiSuF_aq75q7_6wUqyqfePKzeLxqVet9IVNqrcyJqqg9hXkhuFXBXXbIjaGY15gSF9Yr7kyjceVXs5HbDMpmkhet49fhbtLsm9-4E4iCYckzGTsYSxOqRaVGNTkkhWykg\n",
    ":name: bert_position_segment_embeddings\n",
    ":alt: BERT position and segment embeddings\n",
    ":width: 80%\n",
    ":align: center\n",
    "\n",
    "Position and segment embeddings in BERT. Position embeddings, which are learnable, are added to the token embeddings. Segment embeddings indicate the sentence that the token belongs to (e.g., $E_A$ and $E_B$).\n",
    "```\n",
    "\n",
    "```{tip}\n",
    ":class: tip\n",
    "\n",
    "Position embeddings can be either absolute or relative:\n",
    "\n",
    "Absolute position embeddings (like in BERT) directly encode the position of each token as a fixed index (1st, 2nd, 3rd position etc). Each position gets its own unique embedding vector that is learned during training.\n",
    "\n",
    "Relative position embeddings (like sinusoidal embeddings in the original Transformer) encode the relative distance between tokens rather than their absolute positions. For example, they can encode that token A is 2 positions away from token B, regardless of their absolute positions in the sequence. This makes them more flexible for handling sequences of varying lengths.\n",
    "\n",
    "For interested readers, you can read more about the difference between absolute and relative position embeddings in [The Use Case for Relative Position Embeddings – Ofir Press](https://ofir.io/The-Use-Case-for-Relative-Position-Embeddings/) and [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409).\n",
    "```\n",
    "\n",
    "### Pre-training\n",
    "\n",
    "A key aspect of BERT is its pre-training process, which involves two main objectives:\n",
    "\n",
    "- Masked Language Modeling (MLM)\n",
    "- Next Sentence Prediction (NSP)\n",
    "\n",
    "Both objectives are designed to learn the language structure, such as the relationship between words and sentences.\n",
    "\n",
    "#### Masked Language Modeling (MLM)\n",
    "\n",
    "In MLM, the model is trained to predict the original words that are masked in the input sentence. The masked words are replaced with a special token, [MASK], and the model is trained to predict the original words. For example, the sentence \"The cat [MASK] on the mat\" is transformed into \"The cat [MASK] on the mat\". The model is trained to predict the original word \"sat\" in the sentence.\n",
    "\n",
    "```{figure} https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/MLM.png\n",
    ":name: bert_mlm\n",
    ":alt: BERT MLM\n",
    ":width: 80%\n",
    ":align: center\n",
    "\n",
    "Masked Language Modeling (MLM). A token is randomly masked and the model is trained to predict the original word.\n",
    "```\n",
    "\n",
    "To generate training data for MLM, BERT randomly masks 15% of the tokens in each sequence. However, the masking process is not as straightforward as simply replacing words with [MASK] tokens. For the 15% of tokens chosen for masking:\n",
    "\n",
    "- 80% of the time, replace the word with the [MASK] token\n",
    "  - Example: \"the cat sat on the mat\" → \"the cat [MASK] on the mat\"\n",
    "\n",
    "- 10% of the time, replace the word with a random word\n",
    "  - Example: \"the cat sat on the mat\" → \"the cat dog on the mat\"\n",
    "\n",
    "- 10% of the time, keep the word unchanged\n",
    "  - Example: \"the cat sat on the mat\" → \"the cat sat on the mat\"\n",
    "\n",
    "The model must predict the original token for all selected positions, regardless of whether they were masked, replaced, or left unchanged. This helps prevent the model from simply learning to detect replaced tokens.\n",
    "\n",
    "During training, the model processes the modified input sequence through its transformer layers and predicts the original token at each masked position using the contextual representations.\n",
    "\n",
    "```{tip}\n",
    "While replacing words with random tokens or leaving them unchanged may seem counterintuitive, research has shown this approach is effective {footcite}`raffel2020exploring`. It has become an essential component of BERT's pre-training process.\n",
    "```\n",
    "\n",
    "#### Next Sentence Prediction (NSP)\n",
    "\n",
    "\n",
    "```{figure} https://amitness.com/posts/images/bert-nsp.png\n",
    ":name: bert_nsp\n",
    ":alt: BERT NSP\n",
    ":width: 80%\n",
    ":align: center\n",
    "\n",
    "Next Sentence Prediction (NSP). The model is trained to predict whether two sentences are consecutive or not.\n",
    "```\n",
    "\n",
    "Next Sentence Prediction (NSP) trains BERT to understand relationships between sentences. The model learns to predict whether two sentences naturally follow each other in text. During training, half of the sentence pairs are consecutive sentences from documents (labeled as IsNext), while the other half are random sentence pairs (labeled as NotNext).\n",
    "\n",
    "The input format uses special tokens to structure the sentence pairs: a [CLS] token at the start, the first sentence, a [SEP] token, the second sentence, and a final [SEP] token. For instance:\n",
    "\n",
    "$$\n",
    "\\text{``[CLS] }\\underbrace{\\text{I went to the store}}_{\\text{Sentence 1}}\\text{ [SEP] }\\underbrace{\\text{They were out of milk}}_{\\text{Sentence 2}}\\text{ [SEP]}\".\n",
    "$$\n",
    "\n",
    "BERT uses the final hidden state of the [CLS] token to classify whether the sentences are consecutive or not. This helps the model develop a broader understanding of language context and relationships between sentences.\n",
    "\n",
    "These two objectives help BERT learn the structure of language, such as the relationship between words and sentences.\n",
    "\n",
    "\n",
    "## Fine-tuning\n",
    "\n",
    "A powerful aspect of BERT is its ability to be fine-tuned on a wide range of tasks with minimal changes to the model architecture. This is achieved through transfer learning, where the pre-trained BERT model is used as a starting point for specific tasks.\n",
    "\n",
    "Consider a hospital that wants to classify patient reviews. Due to privacy concerns, collecting enough data to train a deep learning model from scratch would be difficult. This is where BERT shines - since it's already pre-trained on vast amounts of text data and understands language structure, it can be fine-tuned effectively even with a small dataset of patient reviews. The pre-trained BERT model can be adapted to this specific classification task with only minor architectural changes.\n",
    "\n",
    "```{tip}\n",
    ":class: tip\n",
    "\n",
    "You can find many fine-tuned and pre-trained models for various tasks by searching the [Hugging Face model hub](https://huggingface.co/models), with the keyword \"BERT\".\n",
    "```\n",
    "\n",
    "## Variants and improvements\n",
    "\n",
    "**RoBERTa (Robustly Optimized BERT Approach)* {footcite}`liu2019roberta`* improved upon BERT through several optimizations: removing the Next Sentence Prediction task, using dynamic masking that changes the mask patterns across training epochs, training with larger batches, and using a larger dataset. These changes led to significant performance improvements while maintaining BERT's core architecture.\n",
    "\n",
    "**DistilBERT** {footcite}`sanh2019distilbert` focused on making BERT more efficient through knowledge distillation, where a smaller student model learns from the larger BERT teacher model. It achieves 95% of BERT's performance while being 40% smaller and 60% faster, making it more suitable for resource-constrained environments and real-world applications.\n",
    "\n",
    "**ALBERT** {footcite}`lan2019albert` introduced parameter reduction techniques to address BERT's memory limitations. It uses factorized embedding parameterization and cross-layer parameter sharing to dramatically reduce parameters while maintaining performance. ALBERT also replaced Next Sentence Prediction with Sentence Order Prediction, where the model must determine if two consecutive sentences are in the correct order.\n",
    "\n",
    "Domain-specific BERT models have been trained on specialized corpora to better handle specific fields. Examples include **BioBERT** {footcite}`lee2020biobert` for biomedical text, **SciBERT** {footcite}`reimers2019sentence` for scientific papers, and **FinBERT** {footcite}`araci2019finbert` for financial documents. These models demonstrate superior performance in their respective domains compared to the general-purpose BERT.\n",
    "\n",
    "**Multilingual BERT (mBERT)** {footcite}`liu2019roberta` was trained on Wikipedia data from 104 languages, using a shared vocabulary across all languages. Despite not having explicit cross-lingual objectives during training, mBERT shows remarkable zero-shot cross-lingual transfer abilities, allowing it to perform tasks in languages it wasn't explicitly aligned on. This has made it a valuable resource for low-resource languages and cross-lingual applications.\n",
    "\n",
    "\n",
    "## Hands on\n",
    "\n",
    "Let us load a pre-trained BERT model and see how it works using a sense disambiguation task. The sense disambiguation task is a task that involves identifying the correct sense of a word in a sentence. For example, given a sentence with word \"apple\", we need to identify whether it refers to the fruit or the technology company.\n",
    "\n",
    "Let us first load the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "292bb4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/skojaku-admin/miniforge3/envs/applsoftcomp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2466a1eb",
   "metadata": {},
   "source": [
    "We will use [CoarseWSD-20](https://github.com/danlou/bert-disambiguation/tree/master/data/CoarseWSD-20). The dataset contains sentences with polysemous words and their sense labels. We will see how to use BERT to disambiguate the word senses. Read the [README](https://github.com/danlou/bert-disambiguation/blob/master/data/CoarseWSD-20/README.txt) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "613e3d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_pos</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2239</th>\n",
       "      <td>6</td>\n",
       "      <td>the main crops are pear and apple .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>42</td>\n",
       "      <td>then sony pictures syndicated the series in a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2250</th>\n",
       "      <td>28</td>\n",
       "      <td>in 2011 , animax designed and developed beanie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384</th>\n",
       "      <td>9</td>\n",
       "      <td>the larvae feed on birch , tilia , and apple .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>9</td>\n",
       "      <td>'' fruits , sometimes categorized with vegetab...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1885</th>\n",
       "      <td>7</td>\n",
       "      <td>they were traditionally cooked with bits of ap...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>17</td>\n",
       "      <td>it is prevalent in the southern united states ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>18</td>\n",
       "      <td>nichicon received particular infamy because of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356</th>\n",
       "      <td>17</td>\n",
       "      <td>the rather absurd legal precedent in digidyne ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>18</td>\n",
       "      <td>alcatel ot-908s ot-918s ot-981a ot-990s ot-991...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_pos                                           sentence  label\n",
       "2239         6                the main crops are pear and apple .      1\n",
       "915         42  then sony pictures syndicated the series in a ...      0\n",
       "2250        28  in 2011 , animax designed and developed beanie...      0\n",
       "1384         9     the larvae feed on birch , tilia , and apple .      1\n",
       "207          9  '' fruits , sometimes categorized with vegetab...      1\n",
       "1885         7  they were traditionally cooked with bits of ap...      1\n",
       "1653        17  it is prevalent in the southern united states ...      1\n",
       "496         18  nichicon received particular infamy because of...      0\n",
       "1356        17  the rather absurd legal precedent in digidyne ...      0\n",
       "8           18  alcatel ot-908s ot-918s ot-981a ot-990s ot-991...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data(focal_word, is_train, n_samples=100):\n",
    "    data_type = \"train\" if is_train else \"test\"\n",
    "    data_file = f\"https://raw.githubusercontent.com/danlou/bert-disambiguation/master/data/CoarseWSD-20/{focal_word}/{data_type}.data.txt\"\n",
    "    label_file = f\"https://raw.githubusercontent.com/danlou/bert-disambiguation/master/data/CoarseWSD-20/{focal_word}/{data_type}.gold.txt\"\n",
    "\n",
    "    data_table = pd.read_csv(\n",
    "        data_file,\n",
    "        sep=\"\\t\",\n",
    "        header=None,\n",
    "        dtype={\"word_pos\": int, \"sentence\": str},\n",
    "        names=[\"word_pos\", \"sentence\"],\n",
    "    )\n",
    "    label_table = pd.read_csv(\n",
    "        label_file,\n",
    "        sep=\"\\t\",\n",
    "        header=None,\n",
    "        dtype={\"label\": int},\n",
    "        names=[\"label\"],\n",
    "    )\n",
    "    combined_table = pd.concat([data_table, label_table], axis=1)\n",
    "    return combined_table.sample(n_samples)\n",
    "\n",
    "\n",
    "focal_word = \"apple\"\n",
    "\n",
    "train_data = load_data(focal_word, is_train=True)\n",
    "\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317a6301",
   "metadata": {},
   "source": [
    "We will use transformers library developed by Hugging Face to define the BERT model. To use the model, we will need:\n",
    "\n",
    "- BERT tokenizer that converts the text into tokens.\n",
    "- BERT model that computes the embeddings of the tokens.\n",
    "\n",
    "We will use the bert-base-uncased model and tokenizer. Let's define the model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a7d7db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = transformers.AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "model.eval() # set the model to evaluation mode\n",
    "print(model) # Print the model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197a77c0",
   "metadata": {},
   "source": [
    "This prints the model architecture, which shows:\n",
    "\n",
    "1. BertEmbeddings layer that converts tokens into embeddings using:\n",
    "   - Word embeddings (30522 vocab size, 768 dimensions)\n",
    "   - Position embeddings (512 positions, 768 dimensions)\n",
    "   - Token type embeddings (2 types, 768 dimensions)\n",
    "   - Layer normalization and dropout\n",
    "\n",
    "2. BertEncoder with 12 identical BertLayers, each containing:\n",
    "   - Self-attention mechanism with query/key/value projections\n",
    "   - Intermediate layer with GELU activation\n",
    "   - Output layer with layer normalization\n",
    "\n",
    "3. BertPooler that processes the [CLS] token embedding with:\n",
    "   - Dense layer (768 dimensions)\n",
    "   - Tanh activation\n",
    "\n",
    "All layers maintain the 768-dimensional hidden size, except the intermediate layer which expands to 3072 dimensions.\n",
    "\n",
    "With BERT, we need to prepare text in ways that BERT can understand. Specifically, we prepend it with [CLS] and append [SEP]. We will then convert the text to a tensor of token ids, which is ready to be fed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "058c87da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(text):\n",
    "    text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    segments_ids = torch.ones((1, len(indexed_tokens)), dtype=torch.long)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensor = segments_ids.clone()\n",
    "    return tokenized_text, tokens_tensor, segments_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfcaa62",
   "metadata": {},
   "source": [
    "Let's get the BERT embeddings for the sentence \"Bank is located in the city of London\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dea55776",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Bank is located in the city of London\"\n",
    "tokenized_text, tokens_tensor, segments_tensor = prepare_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba61ba0",
   "metadata": {},
   "source": [
    "This produces the following output.\n",
    "**Tokenized text**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "916cc085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'bank', 'is', 'located', 'in', 'the', 'city', 'of', 'london', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d88067",
   "metadata": {},
   "source": [
    "**Token IDs**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b7cf0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 2924, 2003, 2284, 1999, 1996, 2103, 1997, 2414,  102]])\n"
     ]
    }
   ],
   "source": [
    "print(tokens_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7d06d8",
   "metadata": {},
   "source": [
    "**Segment IDs**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99087a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "print(segments_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f7365c",
   "metadata": {},
   "source": [
    "Then, let's get the BERT embeddings for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58cc1a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model to return hidden states\n",
    "model.config.output_hidden_states = True\n",
    "\n",
    "outputs = model(tokens_tensor, segments_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec1ccd",
   "metadata": {},
   "source": [
    "The output includes `loss`, `logits`, and `hidden_states`. We will use `hidden_states`, which contains the embeddings of the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fb15080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many layers?  13\n",
      "Shape?  torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "hidden_states = outputs.hidden_states\n",
    "\n",
    "print(\"how many layers? \", len(hidden_states))\n",
    "print(\"Shape? \", hidden_states[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c0f60d",
   "metadata": {},
   "source": [
    "The hidden states are a list of 13 tensors, where each tensor is of shape (batch_size, sequence_length, hidden_size). The first tensor is the input embeddings, and the subsequent tensors are the hidden states of the BERT layers.\n",
    "\n",
    "So, we have 13 choice of hidden states. Deep layers close to the output capture the context of the word from the previous layers.\n",
    "\n",
    "Here we will take the average over the last four hidden states for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53d838a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "last_four_layers = hidden_states[-4:]\n",
    "# Stack the layers and then calculate mean\n",
    "stacked_layers = torch.stack(last_four_layers)\n",
    "emb = torch.mean(stacked_layers, dim=0)\n",
    "\n",
    "print(emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20fdec6",
   "metadata": {},
   "source": [
    "emb is of shape (sequence_length, hidden_size). Let us summarize the embeddings of the tokens into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c4ddf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(text):\n",
    "    tokenized_text, tokens_tensor, segments_tensor = prepare_text(text)\n",
    "    outputs = model(tokens_tensor, segments_tensor)\n",
    "    hidden_states = outputs[2]  # Access hidden states from tuple output\n",
    "    # Stack the last 4 layers then take mean\n",
    "    stacked_layers = torch.stack(hidden_states[-4:])\n",
    "    emb = torch.mean(stacked_layers, dim=0)\n",
    "    return emb, tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dad13b",
   "metadata": {},
   "source": [
    "Now, let us embed text and get the embeddings of the focal token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9cc5ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []  # label\n",
    "emb = []  # embedding\n",
    "sentences = []  # sentence\n",
    "\n",
    "def get_focal_token_embedding(text, focal_word_idx):\n",
    "    emb, tokenized_text = get_bert_embeddings(text)\n",
    "    return emb[0][focal_word_idx]  # Access first batch dimension\n",
    "\n",
    "for index, row in train_data.iterrows():\n",
    "    text = row[\"sentence\"]\n",
    "    focal_word_idx = row[\"word_pos\"]\n",
    "    _emb = get_focal_token_embedding(text, focal_word_idx)\n",
    "    labels.append(row[\"label\"])\n",
    "    emb.append(_emb)\n",
    "    sentences.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a67b6a5",
   "metadata": {},
   "source": [
    "Finally, let us visualize the embeddings using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0bffe5c",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"d2d7d6db-1829-4976-8032-de6f05569a52\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "'use strict';\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    function drop(id) {\n",
       "      const view = Bokeh.index.get_by_id(id)\n",
       "      if (view != null) {\n",
       "        view.model.document.clear()\n",
       "        Bokeh.index.delete(view)\n",
       "      }\n",
       "    }\n",
       "\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null) {\n",
       "      drop(id)\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim()\n",
       "            drop(id)\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded(error = null) {\n",
       "    const el = document.getElementById(\"d2d7d6db-1829-4976-8032-de6f05569a52\");\n",
       "    if (el != null) {\n",
       "      const html = (() => {\n",
       "        if (typeof root.Bokeh === \"undefined\") {\n",
       "          if (error == null) {\n",
       "            return \"BokehJS is loading ...\";\n",
       "          } else {\n",
       "            return \"BokehJS failed to load.\";\n",
       "          }\n",
       "        } else {\n",
       "          const prefix = `BokehJS ${root.Bokeh.version}`;\n",
       "          if (error == null) {\n",
       "            return `${prefix} successfully loaded.`;\n",
       "          } else {\n",
       "            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n",
       "          }\n",
       "        }\n",
       "      })();\n",
       "      el.innerHTML = html;\n",
       "\n",
       "      if (error != null) {\n",
       "        const wrapper = document.createElement(\"div\");\n",
       "        wrapper.style.overflow = \"auto\";\n",
       "        wrapper.style.height = \"5em\";\n",
       "        wrapper.style.resize = \"vertical\";\n",
       "        const content = document.createElement(\"div\");\n",
       "        content.style.fontFamily = \"monospace\";\n",
       "        content.style.whiteSpace = \"pre-wrap\";\n",
       "        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n",
       "        content.textContent = error.stack ?? error.toString();\n",
       "        wrapper.append(content);\n",
       "        el.append(wrapper);\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(() => display_loaded(error), 100);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.6.2.min.js\"];\n",
       "  const css_urls = [];\n",
       "\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      try {\n",
       "            for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "\n",
       "      } catch (error) {display_loaded(error);throw error;\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"d2d7d6db-1829-4976-8032-de6f05569a52\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"d2d7d6db-1829-4976-8032-de6f05569a52\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.6.2.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"d2d7d6db-1829-4976-8032-de6f05569a52\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"e7c4d1c9-bd05-46dc-9290-9e11be497523\" data-root-id=\"p1004\" style=\"display: contents;\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "  const docs_json = {\"fa5e1eeb-57e2-43db-8192-3483329f0acf\":{\"version\":\"3.6.2\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1004\",\"attributes\":{\"width\":700,\"height\":500,\"x_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1005\"},\"y_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1006\"},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1014\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1015\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1007\",\"attributes\":{\"text\":\"Word Embeddings Visualization\"}},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1046\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1001\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1002\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1003\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"hVwAwFAHSME3gPs/4TMNQF7bQEBtfw9ArOeDQF3m9D+Ra8c96zJuv1oL6r6t3Oe+vFosQDLjdT936jnB8v8uP5h2ej9aCJtAw69SQMtVF0BcdT/Bd5AVPniRwj9Ap3dADbjFPY8RREAH4ba/t1siQNDr8MCTzoQ/MurkP983E0CZlCE/DEsqvxl1qj8CSStAa3xKQMe6AEAOjLlAszowv4mmQ8GY9JU+sNpjP3BCikB7XDA/ELsuwTrxgD+iHqtA8+wBQI5XCMHHi1DAM+CWP6oZhj7k4Jw+9/OqP3HRFD+UswpA9xZ0QIJP/z3mHQ1AseKNQHE5db6jNKg/jyYIPwinmUATZd4/HYs/QFEaBMHJa2I/XG16vwtwjUCNJwRAt4lpvr/06b9aGwzB+7HePmX+/j+ulOE/m29Vv/Hl1T936HM/XGgHQIAtW0CUbXY9bJ1xvqIsEMFzKSHAA6e8Py98Az4Pxz7BQzKpvvt0Z0D5Trg/Ubg4P5P9HT8K7pTACvzePqSBEL8Yxog/jwMJPw==\"},\"shape\":[100],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"4OCxQMnbH0DgVqDALPyLPeLsEL8oltlAVb4YQO8IPsDf3/W/4wCYvsJNfT+GrT6/TO4yQDJ9o0AHJRVA2hhSwJbLjMBqFYw/RvhGQBnJjL/nbyRAIdbQvqMb4b8ifmxAagV4wGzwtcCq/TnAGwqaQP3UwEADqqbAqM+8v0q5AEEJ+kzAzPgrwLKeJsBkNlxA6NqUQHHUN8AhGjVAmqNHwN9VUEA+oI++ts2RwIO3g0Af+pi/tMxAQHKa4MCbeJlAEfTXvs1jgb8g3ou/4LszwN90HcBd/3u/9u1SwAaQocBICFS/tKfgQPvUHECJwUPAlVa0QGzVVkDTqRs/BEwUvwm6yUCDT8TAZCk7QFT2cr///wRAJ49TwMpgrkAr6HW/KkZdwB/5VMBJkx8/sh/SwPiZwUCq4yXAH1pHwHyWHsCoiRrAHhsoQNyDm0BtIjLAXIYnwGTFHz9ITu6/TAIDwByc6b3AxxBAytk9wC1ORUAcZ2dA65RgwPJVmr93D6Y/K56yv1clmr+9BIvAxiOnQA==\"},\"shape\":[100],\"dtype\":\"float32\",\"order\":\"little\"}],[\"label\",[1,0,0,1,1,1,1,0,0,0,1,1,1,1,0,0,0,1,1,0,0,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,1,0,1,1,0,1,0,1,1,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,1,1,1,0,1,0,1,0,1,0,1,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,0,1]],[\"sentence\",[\"the main crops are pear and apple .\",\"then sony pictures syndicated the series in a multi-platform footprint including : youtube , hulu , the playstation network , google tv , the bravia network , animax , axn , at&t , sprint , etc. on october 19 , 2012 , apple releases an exclusive and innovative mobile app edition of urban wolf available on its 172 itunes app stores worldwide for iphone , ipod touch and ipad .\",\"in 2011 , animax designed and developed beanie ballz bounce for ty inc for ipad , iphone , and ipod touch and is free for download in the apple app store .\",\"the larvae feed on birch , tilia , and apple .\",\"'' fruits , sometimes categorized with vegetables , include apple , oranges , banana , berries and lemon .\",\"they were traditionally cooked with bits of apple ( \\u00e6ble ) or applesauce inside but these ingredients are very rarely included in modern danish forms of the dish .\",\"it is prevalent in the southern united states and elsewhere , and damages the leaves of infected apple trees .\",\"nichicon received particular infamy because of their use by major computer manufacturers including dell , hewlett-packard , and apple .\",\"the rather absurd legal precedent in digidyne v. data general regarding bundling has never been applied to apple , which might never have been as profitable as it is today had it been forced to license its macintosh operating systems to competitors ( although it did do so temporarily , voluntarily , on a limited scale and for a limited period of time ) .\",\"alcatel ot-908s ot-918s ot-981a ot-990s ot-991 ot-995 one touch fierce one touch scribe x one touch scribe hd-lte apple iphone 5 , model a1428 manufactured after april 12 , 2013 iphone 5c , models a1436 and a1532 iphone 5s , models a1453 and a1533 ipad air , model a1475 ipad mini with retina display , model a1490 asus google nexus 7 padfone 2 google nexus 7 ( 2013 ) fonepad 7 blackberry q5 ( only sqr100-1 ) q10 ( only sqn100-5 ) z10 ( only stl-100-3 ( rfk121lw ) ) bold 9700 bold 9780 bold 9790 bold 9900 curve 9300 curve 9320 curve 9360 curve 9380 pearl 9100 torch 9810 torch 9860 many of the above listed blackberry devices have multiple versions , only one of each being aws compatible .\",\"the iwaki river is the longest river in aomori prefecture , and is the source of irrigation for the large-scale rice and apple production of the prefecture .\",\"the town was first settled around 1763 by jean laframboise , who is also credited with introducing apple growing to the area .\",\"dabinett apples dabinett is a variety of apple , customarily used in somerset for making cider .\",\"michurin also proposed means for overcoming the genetic barrier of incompatibility during the process of hybridization , such as pollination of the young hybrids during their first florescence , preliminary vegetative crossing , use of a '' mediator '' , pollination with the mix of different kinds of pollen etc. the soviets began to cultivate michurin 's hybrids of apple , pear , cherry , rowan and others .\",\"he noted in 2012 that many of the largest companies in silicon valley , including microsoft , apple , and facebook , have also bought large patent portfolios to ` further their strategic game ' .\",\"something venture d follows the stories of the venture capitalists who worked with entrepreneurs to start and build companies like apple , intel , genentech , cisco , atari , tandem , and others .\",\"these features were frowned upon by the then board of directors , who ordered the xerox engineers to share them with apple technicians .\",\"the mcintosh apple was discovered and cultivated in south dundas near williamsburg .\",\"world 's rarest apple was discovered on bardsey island in 2000 .\",\"in 2005 and 2006 , recording artists gwen stefani appeared in advertisements related to a campaign in which codes printed underneath diet pepsi bottle caps could be redeemed for music downloads on the apple itunes music store .\",\"the company has affiliations with more than 3,000 online retailers including amazon , target , staples , best buy and apple .\",\"because userland jailbreaks exploit holes ( vulnerabilities ) in ios ( not the bootrom , iboot , ibss etc. ) they affect users who are both jailbroken and not jailbroken ( potentially compromising the security of users who have not jailbroken ) , apple patched the exploit on versions 4.0 and 3.2.1 .\",\"another prominent proponent of the voucher system was apple co-founder and ceo , steve jobs , who said : some proponents of school vouchers , including the sutherland institute and many supporters of the utah voucher effort , see it as a remedy for the negative cultural impact caused by under-performing public schools , which falls disproportionately on demographic minorities .\",\"ortley ( better known in the southern united states as the white bellflower ) is a variety of apple , medium in size , and light green to yellow in color .\",\"with the advent of apple 's ipod , several companies worked to enable mobile phones and other handheld and portable devices with the ability to access music ( and subsequently video ) over the network in a legal manner .\",\"industry response when surface was first announced , critics noted that the device represented a significant departure for microsoft , as the company had previously relied exclusively on third-party oems to produce devices running windows , and began shifting towards a first-party hardware model with similarities to that of apple .\",\"integrated iris pro graphics was adopted by apple for their late-2013 15-inch macbook pro laptops ( with retina display ) , which for the first time in the history of series did not have discrete graphics , although only for the low-end model .\",\"the larger domain comprises 220 hectares , with terraced garden , english park , and about 1500 varieties of old rose , 800 varieties of pear , and 550 varieties of apple trees .\",\"thus in many sub counties there are successful farmers engaged in the production of grape , apple , pear and peach .\",\"on apple macbook models , it is possible to plug a device in , close the laptop ( putting it into sleep mode ) and have the device continue to charge .\",\"world wide web browser software , such as microsoft 's internet explorer , mozilla firefox , opera , apple 's safari , and google chrome , lets users navigate from one web page to another via hyperlinks embedded in the documents .\",\"the caterpillars feed on orange fruit and other types of citrus fruit , apple and maize .\",\"as for micropayments , they appear to have finally become feasible via apple 's itunes store .\",\"sol republic 's main distribution channels at that time were through apple , radioshack , and best buy .\",\"granddad 's complaint made it to apple .\",\"the ralls genet , sometimes spelled ralls janet or rawls jennet , is a cultivar of apple which originated in virginia in the late 18th century .\",\"in 1995 , the fran\\u00e7ois rabelais university in tours created an orchard of pear and apple trees in one part of the park .\",\"apple did n't join this working group , and delayed w3c recommendation of its touch events specification by disclosing patents late in the recommendation process .\",\"beacon is a cultivar of apple created by university of minnesota in 1936 .\",\"hamton j . pig : hungry boy hamton ( a.k.a. '' junction '' ) - similar to konami 's loco-motion , hamton is walking along a blue path towards apple .\",\"the school has during its history educated many professionals in key positions around the world in such companies as honda , toyota , apple , marimekko , nokia and h&m .\",\"pear are the most susceptible , but apple , loquat , crabapple , quince , hawthorn , cotoneaster , pyracantha , raspberry and some other rosaceous plants are also vulnerable .\",\"as with the rest of kala , m.i.a. and co-composer switch relied heavily on logic pro , a digital audio workstation produced by apple , and were able to capture vocals and background sounds outside the traditional studio environment , using a microphone and a macbook pro . consisting of urumee drum percussion , trumpets , tambourine , electronic scratches and synths , the song 's instrumentation also consists of repetitive pumped-up synthesizer and 4/4 beats .\",\"structure of proanthocyanidins distribution in plants proanthocyanidins , including the lesser bioactive / bioavailable polymers ( 4 or more catechines ) represent a group of condensed flavan-3-ols , such as procyanidins , prodelphinidins and propelargonidins , that can be found in many plants , most notably apple , maritime pine bark , cinnamon , aronia fruit , cocoa bean , grape seed , grape skin ( procyanidins and prodelphinidin ) , and red wine of vitis vinifera ( the european wine grape ) .\",\"meanwhile , the office staff begins betting on various things , from counting the jelly beans in pam beesly 's ( jenna fischer ) candy dish to whether creed bratton ( portrayed by the actor of the same name ) will notice that his apple has been replaced with a potato .\",\"it contains branches of luxury firms such as beymen and atelier rebul , vakko , cos , moncler , bulgari , pomellato , louis vuitton , fendi , lanvin , dior , miu miu , burberry , tory burch , michael kors and valentino , and istanbul 's ( and the country 's ) first apple store .\",\"both of these names encompass both the instruction format and the software supporting it ; aat is included on apple operating system , while graphite is available for microsoft windows and linux - based systems .\",\"the stout larva is green with a reddish stripe and feeds on the flower of various rosaceae including apple , blackthorn , cherry , hawthorn , pear and quince .\",\"the permanent sliding calendar was invented by claussen-finks 24 years before apple presented its first iphone in 2008 , where sliding on the screen with the finger appeared in an electronic consumer product for the first time .\",\"examples of de facto standards that were not approved by any standards organizations ( or at least not approved until after they were in widespread de facto use ) include the hayes command set developed by hayes , apple 's truetype font design and the pcl protocol used by hewlett-packard in the computer printer they produced .\",\"the final version took four months of post-production , using off-the-shelf software from electric image , adobe , and apple , and about $ 3,000 in out-of-pocket costs .\",\"over the months of may and june in 2007 , colbert begged apple to give him a free iphone , and finally received one in july .\",\"iphone hacking brocious , under the pseudonym daeken , joined with a group of other hackers to reverse engineer the iphone , granting users the ability to use their phones in ways not intended by apple .\",\"'' personal life he married novelist mona simpson , the biological sister of apple founder steve jobs , in 1993 .\",\"in 1994 , chahil organized a meeting between apple ceo michael spindler and indian prime minister p.v . narasimha rao and finance minister manmohan singh ( later prime minister ) .\",\"mactracker is a freeware application containing a complete database of all apple hardware models and software versions , created and actively developed by ian page .\",\"however , apple has retained inetd for compatibility purposes .\",\"every autumn , the many different kind of apple - sorts that are represented in the almost hundred trees , becomes mature .\",\"for the mountain towns , the crops are summer fruits , mainly apple and pear .\",\"apple used a number of c t controllers in their powerbook line .\",\"fruits such as coconut , mango , fig , pomegranate , apple , peach , melon , banana , guava and papaya are also reportedly grown .\",\"melon dolma along with quince or apple dolma was one of the palace 's specialties ( raw melon stuffed with minced meat , onion , rice , almonds , cooked in an oven ) .\",\"it is made by crushing apple and squeezing out the liquid .\",\"as a location scout in hollywood , he has recently scouted for the hangover part iii , hawaii five-0 , burlesque , the fighter , castle and commercials for brands such as apple , exxonmobil , ford , hyundai , mountain dew , nike , pepsi , verizon and others .\",\"in season , they eat acorn and apple .\",\"the duo subsequently released the vilefault free software program which broke apple 's filevault security .\",\"the peninsula is also a productive fruit region growing apple and tart cherries .\",\"some of the clinical applications that medopad includes are patient clinical notes by voice recognition or typing , scheduling , lab results , image viewer to view x-ray to ct scans , support documents to digitalise all paper documents or to take photos , video conferencing , primary records , realtime vitals , demographics and contact information , apple healthkit integration , realtime data stream , traffic light to sort and prioritise patients , admission , and access to more third party applications integrated into medopad through the clinical app store .\",\") violet lathyrus tuberosus violet wild cress -- nasturtium officinalis '' biolair '' bitter vetch -- lathyrus tuberosus -- cairmeal bilberries fixed with alum yellow -- buidhe achlasan chaluim cille rhubarb yellow apple - tree , ash and buckthorn poplar and elm bog myrtle , roid ash roots teazle -- dipsacus fullonum -- l\\u00f9s-an-fh\\u00f9cadair / leadan bracken roots -- raineach mh\\u00f2r cow weed tops and flowers of heather , erica , fraoch wild mignonette , reseda luteola , '' lus buidhe m\\u00f2r '' , dried , reduced to powder and boiled .\",\"their album , glorious five year plan , was issued on the mgm label and appeared on apple 's itunes store on 10 january .\",\"pear - introduced may 2003 , discontinued february 2008 clementine - introduced february 2004 blueberry - introduced april 2004 pomegranate - introduced march 2005 apple - introduced april 2006 cranberry - introduced november 2006 , discontinued december 2006 , only available at starbucks .\",\"on 13 april 2010 , apple added support for audio out using mini displayport in their macbook pro product line .\",\"they decided to call their company apple and sell their machines for $ 666.66 .\",\"in 2010 the museum began with the collection of source code of important software , beginning with apple 's macpaint 1.3 , written in a combination of assembly and pascal and available as download for the public .\",\"the ccaf has raised objections to class action settlements involving the grand theft auto '' hot coffee '' minigame , honda civic hybrid , apple backdating , a.g. edwards , bluetooth headsets , and the cobell indian trust .\",\"stacks are a feature first found in apple 's operating system , mac os x v10.5 '' leopard '' .\",\"foods foods that are claimed to be negative in calories are mostly low-calorie fruits and vegetables such as celery , grapefruit , lemon , lime , apple , lettuce , broccoli , and cabbage .\",\"he performed at les siestes electroniques in toulouse , france , in the summer of 2006 ( with chicks on speed ) , and performs live periodically at apple stores in japan .\",\"apart from operating systems , mt xinu also produced apple - unix interoperability software , including an appleshare server for unix .\",\"user interface caps lock on an apple wireless keyboard .\",\"size comparison , from top to bottom , between : - a first generation ipod nano - a first generation iphone - a fourth generation ipod classic the apple ipod line has been upgraded many times , and each significant revision is called a '' generation '' .\",\"arboretum trees include apple , white ash , quaking aspen , gray birch , paper birch , yellow birch , american beech , black cherry , dogwood , eastern hemlock , hop-hornbeam , red maple , sugar maple , northern red oak , white oak , and eastern white pine .\",\"the much sought-after winter apple , and especially the glan plum were sent by the wagonload in the months of september and october as far as hamburg .\",\"the company is well known for their service and sale of apple products , including notebooks , desktops , and the ipod music player .\",\"markoff was one of a team of new york times reporters who won the 2013 pulitzer prize for explanatory reporting , for a series of 10 articles on the business practices of apple and other technology companies .\",\"it is available for linux , apple os x , unix and microsoft windows .\",\"as of early 2011 amazon 's kindle reading device is a significant force in the market , along with the apple ipad and the nook from barnes & noble .\",\"the main plot of the south park episode humancentipad was that kyle did not read the terms of service for his last itunes update and therefore had inadvertently agreed to have apple employees experiment upon him .\",\"they are mostly popular with tourists with apple ( elma \\u00e7ay\\u0131 ) , rose hip ( ku\\u015fburnu \\u00e7ay\\u0131 ) , and linden flower ( \\u0131hlamur \\u00e7ay\\u0131 ) being the most popular flavors .\",\"the board of directors includes representatives from eleven '' promoter '' companies : amd , american megatrends , apple , dell , hp , ibm , insyde software , intel , lenovo , microsoft , and phoenix technologies .\",\"at macworld expo in 2001 , not long after the dot-com bubble burst and amidst industry-wide angst over the future of the pc , apple founder and ceo steve jobs announced a strategy seeing a pc ( specifically , the macintosh ) serving as a '' digital hub '' for future mobile devices ( such as its ipod mp3 player ) .\",\"bunbury to walpole from bunbury , the highway goes through boyanup and on to donnybrook , the heart of wa 's apple country .\",\"its main products include apple , peach and vegetable .\",\"in january , 2012 the focus changed to economic development projects and the organization was rebranded as aythos , inc. projects in 2012 aythos was asked to consult for a grant to develop apple tree and kiwifruit plant farms in villages in helambu .\",\"the images had been demanded in exchange for interviews regarding an alleged leak of apple iphone and ipad user data from an fbi laptop .\",\"intel and microsoft built their first overseas research and development centers in israel , and other high-tech multi-national corporations , such as ibm , google , apple , hp , cisco systems , and motorola , have opened r & d facilities in the country .\",\"thus , matrix capital is not to be confused with ferri 's separate venture capital firm matrix partners , which has made early stage investments in young , private companies over the past 36 years that include apple , sandisk , gilt groupe , and others .\",\"funding key investors include former apple european ceo , pascal cagny , former bbc director-general john birt , co-founder of games workshop and life president of eidos , ian livingstone , and chairman of retail group kingfisher and former chief executive officer of carrefour , daniel bernard .\",\"she is also an internationally recognized digital media artist who began creating digital media with early apple computers , including the colorization of early macintosh educational software .\",\"the fragrance contains scents of apple , grapefruit , white raspberry , water lily , honeysuckle , jasmine , pear , and musk , which minaj commented adds '' a bit of dimension and playfulness to the original scent '' .\"]],[\"color\",[\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\"]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1047\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1048\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1043\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":12},\"line_color\":{\"type\":\"value\",\"value\":\"DarkSlateGrey\"},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1044\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":12},\"line_color\":{\"type\":\"value\",\"value\":\"DarkSlateGrey\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1045\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":12},\"line_color\":{\"type\":\"value\",\"value\":\"DarkSlateGrey\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1013\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1026\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1027\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1028\",\"attributes\":{\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1029\",\"attributes\":{\"syncable\":false,\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"handles\":{\"type\":\"object\",\"name\":\"BoxInteractionHandles\",\"id\":\"p1035\",\"attributes\":{\"all\":{\"type\":\"object\",\"name\":\"AreaVisuals\",\"id\":\"p1034\",\"attributes\":{\"fill_color\":\"white\",\"hover_fill_color\":\"lightgray\"}}}}}}}},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1036\"},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1037\"},{\"type\":\"object\",\"name\":\"HelpTool\",\"id\":\"p1038\"},{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1039\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":[[\"Label\",\"@label\"],[\"Sentence\",\"@sentence\"]]}}]}},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1021\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1022\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1023\"},\"axis_label\":\"PCA 2\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1024\"}}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1016\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1017\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1018\"},\"axis_label\":\"PCA 1\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1019\"}}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1020\",\"attributes\":{\"axis\":{\"id\":\"p1016\"}}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1025\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1021\"}}}]}}]}};\n",
       "  const render_items = [{\"docid\":\"fa5e1eeb-57e2-43db-8192-3483329f0acf\",\"roots\":{\"p1004\":\"e7c4d1c9-bd05-46dc-9290-9e11be497523\"},\"root_ids\":[\"p1004\"]}];\n",
       "  void root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    let attempts = 0;\n",
       "    const timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "p1004"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert list of tensors to numpy array\n",
    "emb_numpy = torch.stack(emb).detach().numpy()\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "xy = pca.fit_transform(emb_numpy)\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "# Create data source for Bokeh\n",
    "source = ColumnDataSource(data=dict(\n",
    "    x=xy[:, 0],\n",
    "    y=xy[:, 1],\n",
    "    label=labels,\n",
    "    sentence=sentences\n",
    "))\n",
    "\n",
    "# Create Bokeh figure\n",
    "p = figure(title=\"Word Embeddings Visualization\", x_axis_label=\"PCA 1\", y_axis_label=\"PCA 2\",\n",
    "           width=700, height=500)\n",
    "\n",
    "# Add hover tool\n",
    "hover = HoverTool(tooltips=[\n",
    "    ('Label', '@label'),\n",
    "    ('Sentence', '@sentence')\n",
    "])\n",
    "p.add_tools(hover)\n",
    "\n",
    "# Create color map for labels\n",
    "import seaborn as sns\n",
    "\n",
    "unique_labels = list(set(labels))\n",
    "color_map = sns.color_palette().as_hex()[0:len(unique_labels)]\n",
    "source.data['color'] = [color_map[label] for label in labels]\n",
    "\n",
    "# Add scatter plot\n",
    "p.scatter('x', 'y', size=12, line_color=\"DarkSlateGrey\", line_width=2,\n",
    "         fill_color='color', source=source)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51146d66",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "We have used the last 4 layers of BERT to generate the embeddings of the tokens. Now, let's use the last $k = 1, 2, 3$ layers of BERT to generate the embeddings of the tokens. Then plot the embeddings using PCA.\n",
    "\n",
    "## References\n",
    "\n",
    "```{footbibliography}\n",
    ":style: unsrt\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bceef7db",
   "metadata": {},
   "source": [
    "# Bidirectional Encoder Representations from Transformers (BERT)\n",
    "\n",
    "We learned about ELMo's approach to the problem of polysemy using bidirectional LSTMs. BERT builds upon its bidirectional approach by using a self-attention mechanism in transformers.\n",
    "BERT has become the leading transformer model for natural language processing tasks like question answering and text classification. Its effectiveness led Google to incorporate it into their search engine to improve query understanding. In this section, we will explore BERT's architecture and mechanisms.\n",
    "\n",
    "```{figure} https://cdn.botpenguin.com/assets/website/BERT_c35709b509.webp\n",
    ":name: bert_mlm\n",
    ":alt: BERT MLM\n",
    ":width: 50%\n",
    ":align: center\n",
    "\n",
    "```\n",
    "\n",
    "## Architecture\n",
    "\n",
    "BERT consists of a stack of encoder transformer layers. Each layer is composed of a self-attention mechanism, a feed-forward neural network, and layer normalization, wired together with residual connections.\n",
    "The output of each layer is fed into the next layer, and as we go through the layers, the token embeddings get more and more contextualized, reflecting the context more and more, thanks to the self-attention mechanism.\n",
    "\n",
    "```{figure} https://www.researchgate.net/publication/372906672/figure/fig2/AS:11431281179224913@1691164535766/BERT-model-architecture.ppm\n",
    ":name: bert_architecture\n",
    ":alt: BERT architecture\n",
    ":width: 80%\n",
    ":align: center\n",
    "\n",
    "BERT consists of a stack of encoder transformer layers. The position embeddings are added to the token embeddings to provide the model with information about the position of the tokens in the sequence.\n",
    "```\n",
    "\n",
    "```{admonition} Which layer of BERT should we use?\n",
    ":class: tip\n",
    "\n",
    "BERT internally generates multiple hierarchical representations of the input sentence. The higher layers of the model capture more abstract and context-sensitive information, while the lower layers capture more local and surface-level information. Which layer to use depends on the task. For example, if we want to do text classification, we should use the output of the last layer. If we are interested in word-level representations, we should use the output of the first layer.\n",
    "```\n",
    "\n",
    "\n",
    "## Special tokens\n",
    "\n",
    "BERT uses several special tokens to represent the input sentence.\n",
    "\n",
    "- [CLS] is used to represent the start of the sentence.\n",
    "- [SEP] is used to represent the end of the sentence.\n",
    "- [MASK] is used to represent the masked words.\n",
    "- [UNK] is used to represent the unknown words.\n",
    "\n",
    "For example, the sentence \"The cat sat on the mat. It then went to sleep.\" is represented as \"[CLS] The cat sat on the mat [SEP] It then went to sleep [SEP]\".\n",
    "\n",
    "In BERT, [CLS] token is used to classify the input sentences, as we will see later. As a result, the model learns to encode a summary of the input sentence into the [CLS] token, which is particularly useful when we want the embedding of the whole input text, instead of the token level embeddings. {footcite}`reimers2019sentence`\n",
    "\n",
    "## Position and Segment embeddings\n",
    "\n",
    "BERT uses *position* and *segment* embeddings to provide the model with information about the position of the tokens in the sequence.\n",
    "\n",
    "- Position embeddings are used to provide the model with information about the position of the tokens in the sequence. Unlike the sinusoidal position embedding used in the original transformer paper {footcite}`vaswani2017attention`, BERT uses learnable position embeddings.\n",
    "\n",
    "\n",
    "- The segment embeddings are used to distinguish the sentences in the input. For example, for the sentence \"The cat sat on the mat. It then went to sleep.\", the tokens in the first sentence are added with segment embedding 0, and the tokens in the second sentence are added with segment embedding 1. These segmend embeddings are also learned during the pre-training process.\n",
    "\n",
    "```{figure} https://i.sstatic.net/thmqC.png\n",
    ":name: bert_position_segment_embeddings\n",
    ":alt: BERT position and segment embeddings\n",
    ":width: 80%\n",
    ":align: center\n",
    "\n",
    "Position and segment embeddings in BERT. Position embeddings, which are learnable, are added to the token embeddings. Segment embeddings indicate the sentence that the token belongs to (e.g., $E_A$ and $E_B$).\n",
    "```\n",
    "\n",
    "```{tip}\n",
    ":class: tip\n",
    "\n",
    "Position embeddings can be either absolute or relative:\n",
    "\n",
    "Absolute position embeddings (like in BERT) directly encode the position of each token as a fixed index (1st, 2nd, 3rd position etc). Each position gets its own unique embedding vector that is learned during training.\n",
    "\n",
    "Relative position embeddings (like sinusoidal embeddings in the original Transformer) encode the relative distance between tokens rather than their absolute positions. For example, they can encode that token A is 2 positions away from token B, regardless of their absolute positions in the sequence. This makes them more flexible for handling sequences of varying lengths.\n",
    "\n",
    "For interested readers, you can read more about the difference between absolute and relative position embeddings in [The Use Case for Relative Position Embeddings – Ofir Press](https://ofir.io/The-Use-Case-for-Relative-Position-Embeddings/) and [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409).\n",
    "```\n",
    "\n",
    "### Pre-training\n",
    "\n",
    "A key aspect of BERT is its pre-training process, which involves two main objectives:\n",
    "\n",
    "- Masked Language Modeling (MLM)\n",
    "- Next Sentence Prediction (NSP)\n",
    "\n",
    "Both objectives are designed to learn the language structure, such as the relationship between words and sentences.\n",
    "\n",
    "#### Masked Language Modeling (MLM)\n",
    "\n",
    "In MLM, the model is trained to predict the original words that are masked in the input sentence. The masked words are replaced with a special token, [MASK], and the model is trained to predict the original words. For example, the sentence \"The cat [MASK] on the mat\" is transformed into \"The cat [MASK] on the mat\". The model is trained to predict the original word \"sat\" in the sentence.\n",
    "\n",
    "```{figure} https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/MLM.png\n",
    ":name: bert_mlm\n",
    ":alt: BERT MLM\n",
    ":width: 80%\n",
    ":align: center\n",
    "\n",
    "Masked Language Modeling (MLM). A token is randomly masked and the model is trained to predict the original word.\n",
    "```\n",
    "\n",
    "To generate training data for MLM, BERT randomly masks 15% of the tokens in each sequence. However, the masking process is not as straightforward as simply replacing words with [MASK] tokens. For the 15% of tokens chosen for masking:\n",
    "\n",
    "- 80% of the time, replace the word with the [MASK] token\n",
    "  - Example: \"the cat sat on the mat\" → \"the cat [MASK] on the mat\"\n",
    "\n",
    "- 10% of the time, replace the word with a random word\n",
    "  - Example: \"the cat sat on the mat\" → \"the cat dog on the mat\"\n",
    "\n",
    "- 10% of the time, keep the word unchanged\n",
    "  - Example: \"the cat sat on the mat\" → \"the cat sat on the mat\"\n",
    "\n",
    "The model must predict the original token for all selected positions, regardless of whether they were masked, replaced, or left unchanged. This helps prevent the model from simply learning to detect replaced tokens.\n",
    "\n",
    "During training, the model processes the modified input sequence through its transformer layers and predicts the original token at each masked position using the contextual representations.\n",
    "\n",
    "```{tip}\n",
    "While replacing words with random tokens or leaving them unchanged may seem counterintuitive, research has shown this approach is effective {footcite}`raffel2020exploring`. It has become an essential component of BERT's pre-training process.\n",
    "```\n",
    "\n",
    "#### Next Sentence Prediction (NSP)\n",
    "\n",
    "\n",
    "```{figure} https://amitness.com/posts/images/bert-nsp.png\n",
    ":name: bert_nsp\n",
    ":alt: BERT NSP\n",
    ":width: 80%\n",
    ":align: center\n",
    "\n",
    "Next Sentence Prediction (NSP). The model is trained to predict whether two sentences are consecutive or not.\n",
    "```\n",
    "\n",
    "Next Sentence Prediction (NSP) trains BERT to understand relationships between sentences. The model learns to predict whether two sentences naturally follow each other in text. During training, half of the sentence pairs are consecutive sentences from documents (labeled as IsNext), while the other half are random sentence pairs (labeled as NotNext).\n",
    "\n",
    "The input format uses special tokens to structure the sentence pairs: a [CLS] token at the start, the first sentence, a [SEP] token, the second sentence, and a final [SEP] token. For instance:\n",
    "\n",
    "$$\n",
    "\\text{``[CLS] }\\underbrace{\\text{I went to the store}}_{\\text{Sentence 1}}\\text{ [SEP] }\\underbrace{\\text{They were out of milk}}_{\\text{Sentence 2}}\\text{ [SEP]}\".\n",
    "$$\n",
    "\n",
    "BERT uses the final hidden state of the [CLS] token to classify whether the sentences are consecutive or not. This helps the model develop a broader understanding of language context and relationships between sentences.\n",
    "\n",
    "These two objectives help BERT learn the structure of language, such as the relationship between words and sentences.\n",
    "\n",
    "\n",
    "## Fine-tuning\n",
    "\n",
    "A powerful aspect of BERT is its ability to be fine-tuned on a wide range of tasks with minimal changes to the model architecture. This is achieved through transfer learning, where the pre-trained BERT model is used as a starting point for specific tasks.\n",
    "\n",
    "Consider a hospital that wants to classify patient reviews. Due to privacy concerns, collecting enough data to train a deep learning model from scratch would be difficult. This is where BERT shines - since it's already pre-trained on vast amounts of text data and understands language structure, it can be fine-tuned effectively even with a small dataset of patient reviews. The pre-trained BERT model can be adapted to this specific classification task with only minor architectural changes.\n",
    "\n",
    "```{tip}\n",
    ":class: tip\n",
    "\n",
    "You can find many fine-tuned and pre-trained models for various tasks by searching the [Hugging Face model hub](https://huggingface.co/models), with the keyword \"BERT\".\n",
    "```\n",
    "\n",
    "## Variants and improvements\n",
    "\n",
    "**RoBERTa (Robustly Optimized BERT Approach)* {footcite}`liu2019roberta`* improved upon BERT through several optimizations: removing the Next Sentence Prediction task, using dynamic masking that changes the mask patterns across training epochs, training with larger batches, and using a larger dataset. These changes led to significant performance improvements while maintaining BERT's core architecture.\n",
    "\n",
    "**DistilBERT** {footcite}`sanh2019distilbert` focused on making BERT more efficient through knowledge distillation, where a smaller student model learns from the larger BERT teacher model. It achieves 95% of BERT's performance while being 40% smaller and 60% faster, making it more suitable for resource-constrained environments and real-world applications.\n",
    "\n",
    "**ALBERT** {footcite}`lan2019albert` introduced parameter reduction techniques to address BERT's memory limitations. It uses factorized embedding parameterization and cross-layer parameter sharing to dramatically reduce parameters while maintaining performance. ALBERT also replaced Next Sentence Prediction with Sentence Order Prediction, where the model must determine if two consecutive sentences are in the correct order.\n",
    "\n",
    "Domain-specific BERT models have been trained on specialized corpora to better handle specific fields. Examples include **BioBERT** {footcite}`lee2020biobert` for biomedical text, **SciBERT** {footcite}`reimers2019sentence` for scientific papers, and **FinBERT** {footcite}`araci2019finbert` for financial documents. These models demonstrate superior performance in their respective domains compared to the general-purpose BERT.\n",
    "\n",
    "**Multilingual BERT (mBERT)** {footcite}`liu2019roberta` was trained on Wikipedia data from 104 languages, using a shared vocabulary across all languages. Despite not having explicit cross-lingual objectives during training, mBERT shows remarkable zero-shot cross-lingual transfer abilities, allowing it to perform tasks in languages it wasn't explicitly aligned on. This has made it a valuable resource for low-resource languages and cross-lingual applications.\n",
    "\n",
    "\n",
    "## Hands on\n",
    "\n",
    "Let us load a pre-trained BERT model and see how it works using a sense disambiguation task. The sense disambiguation task is a task that involves identifying the correct sense of a word in a sentence. For example, given a sentence with word \"apple\", we need to identify whether it refers to the fruit or the technology company.\n",
    "\n",
    "Let us first load the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c260adae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef78845",
   "metadata": {},
   "source": [
    "We will use [CoarseWSD-20](https://github.com/danlou/bert-disambiguation/tree/master/data/CoarseWSD-20). The dataset contains sentences with polysemous words and their sense labels. We will see how to use BERT to disambiguate the word senses. Read the [README](https://github.com/danlou/bert-disambiguation/blob/master/data/CoarseWSD-20/README.txt) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0749d6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_pos</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2171</th>\n",
       "      <td>5</td>\n",
       "      <td>esopus spitzenburg is an antique apple .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>15</td>\n",
       "      <td>a good portion of the population is engaged in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>4</td>\n",
       "      <td>in september 2011 , apple opened its 13th aust...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>23</td>\n",
       "      <td>origins the name is believed to derive from th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>5</td>\n",
       "      <td>he had been contacted by apple to see if he wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1676</th>\n",
       "      <td>5</td>\n",
       "      <td>the application is free on apple 's itunes , b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>13</td>\n",
       "      <td>he has made keynote addresses at the united na...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1847</th>\n",
       "      <td>40</td>\n",
       "      <td>it contains a home demonstration garden , vege...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>6</td>\n",
       "      <td>benzoic acid is also formed in apple after inf...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>13</td>\n",
       "      <td>rogers introduced service on the gsm wireless ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_pos                                           sentence  label\n",
       "2171         5           esopus spitzenburg is an antique apple .      1\n",
       "1581        15  a good portion of the population is engaged in...      1\n",
       "924          4  in september 2011 , apple opened its 13th aust...      0\n",
       "756         23  origins the name is believed to derive from th...      1\n",
       "1407         5  he had been contacted by apple to see if he wo...      0\n",
       "1676         5  the application is free on apple 's itunes , b...      0\n",
       "233         13  he has made keynote addresses at the united na...      0\n",
       "1847        40  it contains a home demonstration garden , vege...      1\n",
       "832          6  benzoic acid is also formed in apple after inf...      1\n",
       "1130        13  rogers introduced service on the gsm wireless ...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data(focal_word, is_train, n_samples=100):\n",
    "    data_type = \"train\" if is_train else \"test\"\n",
    "    data_file = f\"https://raw.githubusercontent.com/danlou/bert-disambiguation/master/data/CoarseWSD-20/{focal_word}/{data_type}.data.txt\"\n",
    "    label_file = f\"https://raw.githubusercontent.com/danlou/bert-disambiguation/master/data/CoarseWSD-20/{focal_word}/{data_type}.gold.txt\"\n",
    "\n",
    "    data_table = pd.read_csv(\n",
    "        data_file,\n",
    "        sep=\"\\t\",\n",
    "        header=None,\n",
    "        dtype={\"word_pos\": int, \"sentence\": str},\n",
    "        names=[\"word_pos\", \"sentence\"],\n",
    "    )\n",
    "    label_table = pd.read_csv(\n",
    "        label_file,\n",
    "        sep=\"\\t\",\n",
    "        header=None,\n",
    "        dtype={\"label\": int},\n",
    "        names=[\"label\"],\n",
    "    )\n",
    "    combined_table = pd.concat([data_table, label_table], axis=1)\n",
    "    return combined_table.sample(n_samples)\n",
    "\n",
    "\n",
    "focal_word = \"apple\"\n",
    "\n",
    "train_data = load_data(focal_word, is_train=True)\n",
    "\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d30924",
   "metadata": {},
   "source": [
    "We will use transformers library developed by Hugging Face to define the BERT model. To use the model, we will need:\n",
    "\n",
    "- BERT tokenizer that converts the text into tokens.\n",
    "- BERT model that computes the embeddings of the tokens.\n",
    "\n",
    "We will use the bert-base-uncased model and tokenizer. Let's define the model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2d32628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = transformers.AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "model.eval() # set the model to evaluation mode\n",
    "print(model) # Print the model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4771a4c2",
   "metadata": {},
   "source": [
    "This prints the model architecture, which shows:\n",
    "\n",
    "1. BertEmbeddings layer that converts tokens into embeddings using:\n",
    "   - Word embeddings (30522 vocab size, 768 dimensions)\n",
    "   - Position embeddings (512 positions, 768 dimensions)\n",
    "   - Token type embeddings (2 types, 768 dimensions)\n",
    "   - Layer normalization and dropout\n",
    "\n",
    "2. BertEncoder with 12 identical BertLayers, each containing:\n",
    "   - Self-attention mechanism with query/key/value projections\n",
    "   - Intermediate layer with GELU activation\n",
    "   - Output layer with layer normalization\n",
    "\n",
    "3. BertPooler that processes the [CLS] token embedding with:\n",
    "   - Dense layer (768 dimensions)\n",
    "   - Tanh activation\n",
    "\n",
    "All layers maintain the 768-dimensional hidden size, except the intermediate layer which expands to 3072 dimensions.\n",
    "\n",
    "With BERT, we need to prepare text in ways that BERT can understand. Specifically, we prepend it with [CLS] and append [SEP]. We will then convert the text to a tensor of token ids, which is ready to be fed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d189eda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(text):\n",
    "    text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    segments_ids = torch.ones((1, len(indexed_tokens)), dtype=torch.long)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensor = segments_ids.clone()\n",
    "    return tokenized_text, tokens_tensor, segments_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0673e06",
   "metadata": {},
   "source": [
    "Let's get the BERT embeddings for the sentence \"Bank is located in the city of London\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83636b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Bank is located in the city of London\"\n",
    "tokenized_text, tokens_tensor, segments_tensor = prepare_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bebd5a",
   "metadata": {},
   "source": [
    "This produces the following output.\n",
    "**Tokenized text**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fe799e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'bank', 'is', 'located', 'in', 'the', 'city', 'of', 'london', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095fc930",
   "metadata": {},
   "source": [
    "**Token IDs**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83303d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 2924, 2003, 2284, 1999, 1996, 2103, 1997, 2414,  102]])\n"
     ]
    }
   ],
   "source": [
    "print(tokens_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd2424d",
   "metadata": {},
   "source": [
    "**Segment IDs**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efe188e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "print(segments_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91057bea",
   "metadata": {},
   "source": [
    "Then, let's get the BERT embeddings for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07832de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model to return hidden states\n",
    "model.config.output_hidden_states = True\n",
    "\n",
    "outputs = model(tokens_tensor, segments_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dd4080",
   "metadata": {},
   "source": [
    "The output includes `loss`, `logits`, and `hidden_states`. We will use `hidden_states`, which contains the embeddings of the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aae3508b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many layers?  13\n",
      "Shape?  torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "hidden_states = outputs.hidden_states\n",
    "\n",
    "print(\"how many layers? \", len(hidden_states))\n",
    "print(\"Shape? \", hidden_states[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9040bbd",
   "metadata": {},
   "source": [
    "The hidden states are a list of 13 tensors, where each tensor is of shape (batch_size, sequence_length, hidden_size). The first tensor is the input embeddings, and the subsequent tensors are the hidden states of the BERT layers.\n",
    "\n",
    "So, we have 13 choice of hidden states. Deep layers close to the output capture the context of the word from the previous layers.\n",
    "\n",
    "Here we will take the average over the last four hidden states for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4c4e340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "last_four_layers = hidden_states[-4:]\n",
    "# Stack the layers and then calculate mean\n",
    "stacked_layers = torch.stack(last_four_layers)\n",
    "emb = torch.mean(stacked_layers, dim=0)\n",
    "\n",
    "print(emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de55c7e",
   "metadata": {},
   "source": [
    "emb is of shape (sequence_length, hidden_size). Let us summarize the embeddings of the tokens into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85d11506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(text):\n",
    "    tokenized_text, tokens_tensor, segments_tensor = prepare_text(text)\n",
    "    outputs = model(tokens_tensor, segments_tensor)\n",
    "    hidden_states = outputs[2]  # Access hidden states from tuple output\n",
    "    # Stack the last 4 layers then take mean\n",
    "    stacked_layers = torch.stack(hidden_states[-4:])\n",
    "    emb = torch.mean(stacked_layers, dim=0)\n",
    "    return emb, tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b885d780",
   "metadata": {},
   "source": [
    "Now, let us embed text and get the embeddings of the focal token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18e03810",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []  # label\n",
    "emb = []  # embedding\n",
    "sentences = []  # sentence\n",
    "\n",
    "def get_focal_token_embedding(text, focal_word_idx):\n",
    "    emb, tokenized_text = get_bert_embeddings(text)\n",
    "    return emb[0][focal_word_idx]  # Access first batch dimension\n",
    "\n",
    "for index, row in train_data.iterrows():\n",
    "    text = row[\"sentence\"]\n",
    "    focal_word_idx = row[\"word_pos\"]\n",
    "    _emb = get_focal_token_embedding(text, focal_word_idx)\n",
    "    labels.append(row[\"label\"])\n",
    "    emb.append(_emb)\n",
    "    sentences.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c7c069",
   "metadata": {},
   "source": [
    "Finally, let us visualize the embeddings using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b4a8f22",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"cab8d44f-4664-4e34-bb0d-79d5becdad14\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "'use strict';\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    function drop(id) {\n",
       "      const view = Bokeh.index.get_by_id(id)\n",
       "      if (view != null) {\n",
       "        view.model.document.clear()\n",
       "        Bokeh.index.delete(view)\n",
       "      }\n",
       "    }\n",
       "\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null) {\n",
       "      drop(id)\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim()\n",
       "            drop(id)\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded(error = null) {\n",
       "    const el = document.getElementById(\"cab8d44f-4664-4e34-bb0d-79d5becdad14\");\n",
       "    if (el != null) {\n",
       "      const html = (() => {\n",
       "        if (typeof root.Bokeh === \"undefined\") {\n",
       "          if (error == null) {\n",
       "            return \"BokehJS is loading ...\";\n",
       "          } else {\n",
       "            return \"BokehJS failed to load.\";\n",
       "          }\n",
       "        } else {\n",
       "          const prefix = `BokehJS ${root.Bokeh.version}`;\n",
       "          if (error == null) {\n",
       "            return `${prefix} successfully loaded.`;\n",
       "          } else {\n",
       "            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n",
       "          }\n",
       "        }\n",
       "      })();\n",
       "      el.innerHTML = html;\n",
       "\n",
       "      if (error != null) {\n",
       "        const wrapper = document.createElement(\"div\");\n",
       "        wrapper.style.overflow = \"auto\";\n",
       "        wrapper.style.height = \"5em\";\n",
       "        wrapper.style.resize = \"vertical\";\n",
       "        const content = document.createElement(\"div\");\n",
       "        content.style.fontFamily = \"monospace\";\n",
       "        content.style.whiteSpace = \"pre-wrap\";\n",
       "        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n",
       "        content.textContent = error.stack ?? error.toString();\n",
       "        wrapper.append(content);\n",
       "        el.append(wrapper);\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(() => display_loaded(error), 100);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.6.0.min.js\"];\n",
       "  const css_urls = [];\n",
       "\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      try {\n",
       "            for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "\n",
       "      } catch (error) {display_loaded(error);throw error;\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"cab8d44f-4664-4e34-bb0d-79d5becdad14\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"cab8d44f-4664-4e34-bb0d-79d5becdad14\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.6.0.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"cab8d44f-4664-4e34-bb0d-79d5becdad14\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"a3a7598e-e7f5-4537-a96f-eb4e4534bc19\" data-root-id=\"p1004\" style=\"display: contents;\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "  const docs_json = {\"196e5b55-3c27-4adf-b8f8-acae3f86b680\":{\"version\":\"3.6.0\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1004\",\"attributes\":{\"width\":700,\"height\":500,\"x_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1005\"},\"y_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1006\"},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1014\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1015\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1007\",\"attributes\":{\"text\":\"Word Embeddings Visualization\"}},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1046\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1001\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1002\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1003\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"KLseQLyHvL/DibRATqDIP3w2k8AmCbPAy00SwLgsKsBF+JE/kq4awMa8ZcBCIhvAWD+bv6C8tkB4PY7AP7bEQO+keMDSGhW+E/6OwNmxEsC0abVAGtQ9wB/dHMD2e5nA9fBtwBOXob8dxSJAT5qYwMZmlL/KoIG/E9dwwEpfQT8O40I/F5RNQICbd0B12Wi/0V6KQERor0AIRhFBhq2pvx1dwcDmeCzA6c1gvzn1nj6rFOy/0LRGQKh0sL+995W9jFWdQHAn/T8MMw3AD0Xcv1L8BcBD3949HRcGQBzPdD/3p99A0bQ3v8himEDnu7fAmcTfP2dkikACipa/fA9KQFH6tEC01kPANbBOP8hFg8C4BaXAY0DuvpThrz/v1ZY/Yn/IQBbxpsDwvfBAzRWXwCvgM8CtnMjADkUsQTE/HcBF0khBBDoCQUUkDcBQuJVA2RW+vywvQL/ElGfAntpDwI/k77/WlxNAN0GGQAM8f8BmlVvARMrAvwCTur9hvuy/n/57v1YgCkCgRri/+SwVwA==\"},\"shape\":[100],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"/JM0P6Pd7ECQLwXA1tGEP42u/b91+C/ABq/AQOtpJUEGvE8/vs8Gv/HhED+b7LW/He8vQfVn6r+ke4bA3qkbwOPr7r9NtGi+zTM7wJeWEz5SegfAZGF+wEhOa8Dn2q3AgQImwKcwBT+QBDnAOv2TPzz83r8c8Lo/z/A/P+eW3b4WFQi/avkTPwqDzT3SO5q/jGZJwFwclL/eEIk/jLoiwEDeWMDPhepAFuEVwNTegz6z0U3AaBTMP9IzYEA9da4/BCfgv5Dpvb9L0r2/dg8EQMlh6D4KJrS/5v4UwEuHBUDyOwDAElWSwF3kBMDfy96/fDJGwNePrr96tWdAM3bjPww+GMCwLcc/AP9RQKcLqMDphC3ArXDAQDNKUr+pSYHAWtPUP1uEeMC/bsc/vpFywDX/W8ApdHK/JSKQvW6fGUE0Nm4/Qfd4QFSta0AZg8O/8HtEQC+s2L+ZXrdAsSYkQb+sPsASwSpAKpE8wOypTsDSDUK/bbYVQfQPZMCWh13A/Dnmv6U6nr9aFNI/u/J2QA==\"},\"shape\":[100],\"dtype\":\"float32\",\"order\":\"little\"}],[\"label\",[1,1,0,1,0,0,0,1,1,0,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,1,1,0,1,0,0,1,1,0,0,1,0,1,0,1,1,1,0,0,0,1,1,0,0,1,0,0,0,0,0,1,1,1,0,1,1,0,0,1,1,0,1,0,1,0,0,0,1,1,1,1,1,0,1,0,0,1,0,1,0,0,0,1,0,0,0,0,1,1]],[\"sentence\",[\"esopus spitzenburg is an antique apple .\",\"a good portion of the population is engaged in agriculture , especially fruits such as apple , pear , plum , strawberries , and raspberries on farms and gardens in the village .\",\"in september 2011 , apple opened its 13th australian store in westfield hornsby .\",\"origins the name is believed to derive from the obsolete west country dialect term '' scrimp '' , meaning a small or withered apple , which also gave rise to the verb '' scrump '' , meaning to steal fruit .\",\"he had been contacted by apple to see if he would consider to bring revolution 's classic titles to the app store , and cecil on his turn had contacted dave gibbons to work on new editions of beneath a steel sky , broken sword : the shadow of the templars -- the director 's cut and broken sword : the smoking mirror -- remastered ( 2010 ) .\",\"the application is free on apple 's itunes , but an in-app purchase is necessary to enable translation capabilities .\",\"he has made keynote addresses at the united nations , , google , apple , ucla and the hammer museum .\",\"it contains a home demonstration garden , vegetable garden , herb garden , home fruit and nuts garden ( including black , persian and japanese walnuts , pecan , shellbark and shagbark hickory , chinese chestnut as well as dwarf apple cultivar , an american persimmon and native pawpaw trees ) , the all america selection trials garden , perennial collection , ground cover demonstration , woody plant collection , and a '' walk across kentucky '' that simulates kentucky 's seven regional landscapes : bluegrass , knobs , appalachian plateaus , cumberland mountains , mississippian plateaus and outer nashville basin ( pennyroyal ) , shawnee hills , mississippi embayment and alluvial basin ( jackson purchase ) .\",\"benzoic acid is also formed in apple after infection with the fungus nectria galligena .\",\"rogers introduced service on the gsm wireless network in 2002 , and brought apple 's iphone to canada in 2008 .\",\"phytochemicals lychees have moderate amounts of polyphenols , shown in one french study to be higher than several other fruits analyzed , such as grape and apple .\",\"his video scratching and live video manipulation performances received attention and acclaim from the likes of apple and wired magazine .\",\"the cultivated fruit include : mulberry , peach , apricot , apple , pomegranate , pear , fig , walnut , as well as grape , watermelon and muskmelon .\",\"france telecom , facebook , at&t , apple , cisco , sprint are some of the members of the maawg .\",\") the dtp market exploded in 1985 with the introduction in january of the apple laserwriter printer , and later in july with the introduction of pagemaker software from aldus which rapidly became the dtp industry standard software .\",\"in early february 2006 , apple confirmed reports of video display problems on the new intel-based imacs .\",\"the iconic image of big brother ( played by david graham ) played a key role in apple 's 1984 television commercial introducing the macintosh .\",\"ogier was fowling in his fields one afternoon , when he surprised a russian soldier stealing apple from his orchard .\",\"the company is well known for their service and sale of apple products , including notebooks , desktops , and the ipod music player .\",\"the selectric composer was accorded respect and affection among small publishers , unrivaled until the appearance of the apple macintosh , laser printer , and desktop publishing software .\",\"in january 2008 , apple featured her song '' new soul '' in its debut commercial for the macbook air laptop .\",\"for example , when apple introduced the ipad , it took sales away from the original macintosh , but ultimately led to an expanded market for consumer computing hardware .\",\"the n95 's main competitors during its lifetime were the lg prada , apple 's iphone and the sony ericsson w950i .\",\"it is a developer tool included in apple mac os x v10.5 and later versions of mac os x , built on top of the dtrace tracing framework from opensolaris and ported to mac os x .\",\"dr. arnold kim ( often referred to as '' arn '' online ) is the owner of macrumors , a popular apple rumors website .\",\"tentation \\u00ae delblush is a new commercial apple variety ( also known as delblush ) , that was created in france in 1979 by georges delbard as the result of a crossing of grifer ( blushing golden ) golden delicious .\",\"fotopedia was launched in june 2009 by five former apple employees : jean-marie hullot , bertrand guiheneuf , manuel colom , s\\u00e9bastien maury and olivier gutknecht .\",\"as such , it competes with commercially developed operating systems such as apple 's ios , google 's android , microsoft 's windows phone and jolla 's sailfish os as well as other community-based open source systems such as ubuntu touch .\",\"kodak sold many of its patents for approximately $ 525,000,000 to a group of companies ( including apple , google , facebook , amazon , microsoft , samsung , adobe systems and htc ) under the name intellectual ventures and rpx corporation .\",\"ambrosia is a cultivar of apple originating in british columbia in the early 1990s .\",\"the technical committee was chaired by two microsoft employees and included members drawn from apple , canon , intel , nextpage , novell , pioneer , statoil asa , toshiba , the united states library of congress , the british library and the gnome foundation .\",\"the apple orchards produce the vasquez and king david varieties , which are grown only in oak glen , as well as antique varieties no longer commercially available such as ben davis , gravenstein , and pink pearl .\",\"huling planted an apple orchard on what was to become known as montgomery island .\",\"it runs in black-and-white on the apple macintosh line of computers .\",\"known as sagardoa , the cider in basque cuisine is produced at cider house in areas such as astigarraga , spain , an apple growing region .\",\"an ssd can also be completely integrated in the other circuitry of the device , as in the apple macbook air ( starting with the fall 2010 model ) .\",\"notable examples modern examples of flat design include microsoft 's metro , apple 's os x yosemite , and google 's material design .\",\"due to the seasonal nature of the meat in western culinary history , apple ( harvested in late summer and autumn ) have been a staple pairing to fresh pork .\",\"other fruits include nightshade , apple and pear .\",\"these features were frowned upon by the then board of directors , who ordered the xerox engineers to share them with apple technicians .\",\"this project was intended to provide competition for apple and hardly fit in with sinclair's focus on inexpensive consumer-oriented products .\",\"vimto is also available in a summer flavor , containing ingredients such as orange and apple both of which are cordial products .\",\"user interface caps lock on an apple wireless keyboard .\",\"she also owned an apple farm from 1977 to 1988 and was active with local agricultural organizations .\",\"wildfire has also been cited as '' a really early version of siri mostly geared for business execs '' and similarities have also been drawn between wildfire 's relaxed conversational style and that of the apple assistant .\",\"the flower of kent is a green variety of cooking apple .\",\"simple crafting involves enchanting ammunition and weapons , making a fishing rod ( fishing pole + rope ) , keyring ( key + ring ) , dough ( flour + water ) , pie ( dough + rolling pin + optional apple + optional bottle of wine ) , powder ( bone / plant + pestle and mortar ) , and potion ( powder + empty bottle + still ) .\",\"the jellies are manufactured in circular , half-moon , teardrop and diamond shapes and are flavored lemon , apple , strawberry , orange , blackcurrant and pear .\",\"in particular , it is known for the 6800 family and 68000 family of microprocessor used in atari st , commodore amiga , color computer , and apple macintosh personal computers .\",\"ironically , cowon would later be accused for ` stealing ' the ` i-prefix ' from apple , despite the iaudio brand being launched one year prior to the first ipod model and aimed exclusively at the korean market .\",\"brown 's patented techniques , such as voicemail-to-email , visual voicemail , enhanced caller id , were innovations later deployed by google voice and apple .\",\"a view of huonville , sometime between 1900-1949 today the huon valley is best known as one of tasmania 's primary apple growing areas .\",\"glomerella cingulata is a plant pathogenic fungus that causes disease on many different hosts including quince and apple bitter rot and anthracnose on many fruit and vegetable species such as mango or cultivated plants like st. john 's wort ( hypericum perforatum ) .\",\"john calhoun mentioned that he 'd like to create a successor to glider pro that went back to the roots of glider ( for example , by having fewer objects and all-indoor houses ) but that it would have to wait until he 's done with his work at apple .\",\"apple is an excellent example of using semiotics in their advertising campaign .\",\"weld was noted for numerous and prolific apple orchard .\",\"informix corporation was a leading developer of relational database software for computers using the unix , microsoft windows , and apple macintosh operating systems .\",\"xgrid is developed by apple .\",\"in the mid-1980s , apple considered buying xerox ; however , a deal was never reached .\",\"history iso base media file format is directly based on apple 's quicktime container format .\",\"icc profile specification version membership the eight founding members of the icc were adobe , agfa , apple , kodak , microsoft , silicon graphics , sun microsystems , and taligent .\",\"the area around alexandreia has the greatest production of peach in greece and apple , pear , tobacco , and cotton are also grown at large .\",\"in 1995 , the fran\\u00e7ois rabelais university in tours created an orchard of pear and apple trees in one part of the park .\",\"in 1959 , barclay hauled several wagonloads of apple to the roadside and set up a makeshift stand .\",\"in june 2005 , apple released itunes 4.9 with native support for podcasts .\",\"present day kingston remains as a major service center for the apple growing industry in the area of western kings county and has a growing retail district , owing to its access to highway 101 .\",\"the ripe fruits are tasty , reminiscent of a small apple .\",\"my singing monsters is a world builder simulation video game released for apple 's iphone , ipod and ipad , android , and amazon platforms , among others .\",\"the font is also apparently licensed to apple , who announced on october 16 , 2007 that their flagship operating system , mac os x v10.5 ( '' leopard '' ) , would be bundled with arial unicode .\",\"they were traditionally cooked with bits of apple ( \\u00e6ble ) or applesauce inside but these ingredients are very rarely included in modern danish forms of the dish .\",\"clobbler -- the clobblers are apple - shaped monsters that appear in .\",\"partners and resellers included apple , dell , emc , fujitsu-siemens , hp , ibm and sun .\",\"another release of tootsie roll pops , named tropical stormz , features six swirl-textured flavors : orange pineapple , lemon lime , strawberry banana , apple blueberry , citrus punch , and berry berry punch .\",\"the game was later ported to apple 's iphone and ipod touch on november 14 , 2009 .\",\"the larvae feed on birch , elm , alder , hazel , apple , crataegus , sorbus , willow , comptonia peregrina and quercus wislizenii .\",\"it mainly competed against windows 98 and windows 2000 by microsoft , plus mac os 9 by apple .\",\"examples examples of file managers that use a spatial metaphor by default include : amiga 's workbench apple 's finder 5 to 9 ( versions prior to mac os x ) beos 's tracker enlightenment 17 's file manager gnome 's nautilus from version 2.6 till 2.29 the os/2 workplace shell the risc os filer the rox desktop 's rox-filer windows explorer in windows 95 was also a spatial file manager in some aspects , but became a navigational file manager in later versions of the operating system ( when using the default settings ) other objects some file managers represent other objects , such as a trash can for unwanted files , or computer or floppy disk icons to represent storage media .\",\"in 2010 the museum began with the collection of source code of important software , beginning with apple 's macpaint 1.3 , written in a combination of assembly and pascal and available as download for the public .\",\"golden power golden power was a norwegian-produced sparkling fruit wine ( not to be confused with an energy drink of the same name ) , made of 70 % rhubarb , 20 % apple and 10 % grape juice , which was produced by ving\\u00e5rden ( the r\\u00f8ed farm in filtvet , in the tofte area , in hurum ) , associated with owner frantz michaelsen .\",\"biennial bearing is more common in certain fruit crop like mango , apple , pear , apricot and avocado , and almost nonexistent in grapes .\",\"greens are a staple , such as chickweed , seedy ( lawn type ) grass heads , dandelion , carrot , broccoli , brussels sprout , and apple .\",\"habitat the species feed on trees such as : alder ash beech birch blackthorn clematis mayweed tuft mistletoe oak poplar sallow sycamore sea-buckthorn willow they also feed on fruits such as apple , bramble , horse chestnut , lime , plum , sweet chestnut .\",\"the ripe fruit are edible but may be fairly tart , and taste like a giant cranberry , though some species have fruity flavors with overtones of strawberry or apple .\",\"in the report , they state that because social media sites have forbidden any content related to '' hate speech '' , companies including apple , google , facebook , myspace , and others are blocking content expressing anti-homosexual viewpoints .\",\"the term is derived from the words costard ( a now-extinct medieval variety of large , ribbed apple ) and monger ; i.e. , seller .\",\"popularity show of hands has been downloaded over 300,000 times across the apple and android app stores .\",\"hachette digital titles are available from the major uk ebook retailers , including the amazon 's kindle store , waterstones , whsmith and apple 's ibookstore .\",\"also it is a district that has a lot of valley of fruit like chirimoya , avocado , apple , tumbo , and other fruits .\",\"this venture was funded in part by jerome wiesner , richard leghorn of itek , and apple 's john sculley , and won contracts worth $ 7.8 million from the defense advanced research projects agency ( darpa ) .\",\"this other ingredient may vary ; examples include chocolate ; chocolate-flavoured dough ; a combination of chocolate and cinnamon ; and a combination of cinnamon , apple , and coconut .\",\"see also - , an extended article on troutwine and moberg comparing veritas prep to google and apple in the march 2009 issue of entrepreneur magazine .\",\"the magazine is produced on a monthly basis and contains information about all aspects of apple computers all the way from the apple-1 to the latest macintosh computers with mac os x .\",\"his many presentations include keynote addresses in china , australia , canada , and the us as well as consultancies in malaysia , singapore , hong kong , and throughout the us with companies including apple , hewlett-packard , pacific bell , the internal revenue service .\",\"crops grown include vegetables , apple , grape and stone fruit .\",\"additionally , all students and teachers have the latest apple laptops and are connected via a wireless network .\",\"steve jobs ( 1955 -- 2011 ) is shown here holding the first generation apple ipad .\",\"in the episode , it 's phil 's birthday and claire wants to buy him an apple ipad , excited about buying a good gift for the first time .\",\"competition in the global mobile advertising market , inmobi primarily competes against google 's admob millennial media and apple 's iad .\",\"pear are the most susceptible , but apple , loquat , crabapple , quince , hawthorn , cotoneaster , pyracantha , raspberry and some other rosaceous plants are also vulnerable .\",\"washington roussanne is often blended with viognier and is characterized by its fruit salad profile of notes that range from apple , lime , peach and citrus to cream and honey .\"]],[\"color\",[\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\"]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1047\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1048\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1043\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":12},\"line_color\":{\"type\":\"value\",\"value\":\"DarkSlateGrey\"},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1044\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":12},\"line_color\":{\"type\":\"value\",\"value\":\"DarkSlateGrey\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1045\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":12},\"line_color\":{\"type\":\"value\",\"value\":\"DarkSlateGrey\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1013\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1026\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1027\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1028\",\"attributes\":{\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1029\",\"attributes\":{\"syncable\":false,\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"handles\":{\"type\":\"object\",\"name\":\"BoxInteractionHandles\",\"id\":\"p1035\",\"attributes\":{\"all\":{\"type\":\"object\",\"name\":\"AreaVisuals\",\"id\":\"p1034\",\"attributes\":{\"fill_color\":\"white\",\"hover_fill_color\":\"lightgray\"}}}}}}}},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1036\"},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1037\"},{\"type\":\"object\",\"name\":\"HelpTool\",\"id\":\"p1038\"},{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1039\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":[[\"Label\",\"@label\"],[\"Sentence\",\"@sentence\"]]}}]}},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1021\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1022\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1023\"},\"axis_label\":\"PCA 2\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1024\"}}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1016\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1017\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1018\"},\"axis_label\":\"PCA 1\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1019\"}}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1020\",\"attributes\":{\"axis\":{\"id\":\"p1016\"}}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1025\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1021\"}}}]}}]}};\n",
       "  const render_items = [{\"docid\":\"196e5b55-3c27-4adf-b8f8-acae3f86b680\",\"roots\":{\"p1004\":\"a3a7598e-e7f5-4537-a96f-eb4e4534bc19\"},\"root_ids\":[\"p1004\"]}];\n",
       "  void root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    let attempts = 0;\n",
       "    const timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "p1004"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert list of tensors to numpy array\n",
    "emb_numpy = torch.stack(emb).detach().numpy()\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "xy = pca.fit_transform(emb_numpy)\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "# Create data source for Bokeh\n",
    "source = ColumnDataSource(data=dict(\n",
    "    x=xy[:, 0],\n",
    "    y=xy[:, 1],\n",
    "    label=labels,\n",
    "    sentence=sentences\n",
    "))\n",
    "\n",
    "# Create Bokeh figure\n",
    "p = figure(title=\"Word Embeddings Visualization\", x_axis_label=\"PCA 1\", y_axis_label=\"PCA 2\",\n",
    "           width=700, height=500)\n",
    "\n",
    "# Add hover tool\n",
    "hover = HoverTool(tooltips=[\n",
    "    ('Label', '@label'),\n",
    "    ('Sentence', '@sentence')\n",
    "])\n",
    "p.add_tools(hover)\n",
    "\n",
    "# Create color map for labels\n",
    "import seaborn as sns\n",
    "\n",
    "unique_labels = list(set(labels))\n",
    "color_map = sns.color_palette().as_hex()[0:len(unique_labels)]\n",
    "source.data['color'] = [color_map[label] for label in labels]\n",
    "\n",
    "# Add scatter plot\n",
    "p.scatter('x', 'y', size=12, line_color=\"DarkSlateGrey\", line_width=2,\n",
    "         fill_color='color', source=source)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dc2e04",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "We have used the last 4 layers of BERT to generate the embeddings of the tokens. Now, let's use the last $k = 1, 2, 3$ layers of BERT to generate the embeddings of the tokens. Then plot the embeddings using PCA.\n",
    "\n",
    "## References\n",
    "\n",
    "```{footbibliography}\n",
    ":style: unsrt\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
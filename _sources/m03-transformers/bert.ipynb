{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3a6be94",
   "metadata": {},
   "source": [
    "# Bidirectional Encoder Representations from Transformers (BERT)\n",
    "\n",
    "We learned about ELMo's approach to the problem of polysemy using bidirectional LSTMs. BERT builds upon its bidirectional approach by using a self-attention mechanism in transformers.\n",
    "BERT has become the leading transformer model for natural language processing tasks like question answering and text classification. Its effectiveness led Google to incorporate it into their search engine to improve query understanding. In this section, we will explore BERT's architecture and mechanisms.\n",
    "\n",
    "```{figure} https://cdn.botpenguin.com/assets/website/BERT_c35709b509.webp\n",
    ":name: bert_mlm\n",
    ":alt: BERT MLM\n",
    ":width: 50%\n",
    ":align: center\n",
    "\n",
    "```\n",
    "\n",
    "## Architecture\n",
    "\n",
    "BERT consists of a stack of encoder transformer layers. Each layer is composed of a self-attention mechanism, a feed-forward neural network, and layer normalization, wired together with residual connections.\n",
    "The output of each layer is fed into the next layer, and as we go through the layers, the token embeddings get more and more contextualized, reflecting the context more and more, thanks to the self-attention mechanism.\n",
    "\n",
    "```{figure} https://www.researchgate.net/publication/372906672/figure/fig2/AS:11431281179224913@1691164535766/BERT-model-architecture.ppm\n",
    ":name: bert_architecture\n",
    ":alt: BERT architecture\n",
    ":width: 80%\n",
    ":align: center\n",
    "\n",
    "BERT consists of a stack of encoder transformer layers. The position embeddings are added to the token embeddings to provide the model with information about the position of the tokens in the sequence.\n",
    "```\n",
    "\n",
    "```{admonition} Which layer of BERT should we use?\n",
    ":class: tip\n",
    "\n",
    "BERT internally generates multiple hierarchical representations of the input sentence. The higher layers of the model capture more abstract and context-sensitive information, while the lower layers capture more local and surface-level information. Which layer to use depends on the task. For example, if we want to do text classification, we should use the output of the last layer. If we are interested in word-level representations, we should use the output of the first layer.\n",
    "```\n",
    "\n",
    "\n",
    "## Special tokens\n",
    "\n",
    "BERT uses several special tokens to represent the input sentence.\n",
    "\n",
    "- [CLS] is used to represent the start of the sentence.\n",
    "- [SEP] is used to represent the end of the sentence.\n",
    "- [MASK] is used to represent the masked words.\n",
    "- [UNK] is used to represent the unknown words.\n",
    "\n",
    "For example, the sentence \"The cat sat on the mat. It then went to sleep.\" is represented as \"[CLS] The cat sat on the mat [SEP] It then went to sleep [SEP]\".\n",
    "\n",
    "In BERT, [CLS] token is used to classify the input sentences, as we will see later. As a result, the model learns to encode a summary of the input sentence into the [CLS] token, which is particularly useful when we want the embedding of the whole input text, instead of the token level embeddings. {footcite}`reimers2019sentence`\n",
    "\n",
    "## Position and Segment embeddings\n",
    "\n",
    "BERT uses *position* and *segment* embeddings to provide the model with information about the position of the tokens in the sequence.\n",
    "\n",
    "- Position embeddings are used to provide the model with information about the position of the tokens in the sequence. Unlike the sinusoidal position embedding used in the original transformer paper {footcite}`vaswani2017attention`, BERT uses learnable position embeddings.\n",
    "\n",
    "\n",
    "- The segment embeddings are used to distinguish the sentences in the input. For example, for the sentence \"The cat sat on the mat. It then went to sleep.\", the tokens in the first sentence are added with segment embedding 0, and the tokens in the second sentence are added with segment embedding 1. These segmend embeddings are also learned during the pre-training process.\n",
    "\n",
    "```{figure} https://i.sstatic.net/thmqC.png\n",
    ":name: bert_position_segment_embeddings\n",
    ":alt: BERT position and segment embeddings\n",
    ":width: 80%\n",
    ":align: center\n",
    "\n",
    "Position and segment embeddings in BERT. Position embeddings, which are learnable, are added to the token embeddings. Segment embeddings indicate the sentence that the token belongs to (e.g., $E_A$ and $E_B$).\n",
    "```\n",
    "\n",
    "```{tip}\n",
    ":class: tip\n",
    "\n",
    "Position embeddings can be either absolute or relative:\n",
    "\n",
    "Absolute position embeddings (like in BERT) directly encode the position of each token as a fixed index (1st, 2nd, 3rd position etc). Each position gets its own unique embedding vector that is learned during training.\n",
    "\n",
    "Relative position embeddings (like sinusoidal embeddings in the original Transformer) encode the relative distance between tokens rather than their absolute positions. For example, they can encode that token A is 2 positions away from token B, regardless of their absolute positions in the sequence. This makes them more flexible for handling sequences of varying lengths.\n",
    "\n",
    "For interested readers, you can read more about the difference between absolute and relative position embeddings in [The Use Case for Relative Position Embeddings – Ofir Press](https://ofir.io/The-Use-Case-for-Relative-Position-Embeddings/) and [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409).\n",
    "```\n",
    "\n",
    "### Pre-training\n",
    "\n",
    "A key aspect of BERT is its pre-training process, which involves two main objectives:\n",
    "\n",
    "- Masked Language Modeling (MLM)\n",
    "- Next Sentence Prediction (NSP)\n",
    "\n",
    "Both objectives are designed to learn the language structure, such as the relationship between words and sentences.\n",
    "\n",
    "#### Masked Language Modeling (MLM)\n",
    "\n",
    "In MLM, the model is trained to predict the original words that are masked in the input sentence. The masked words are replaced with a special token, [MASK], and the model is trained to predict the original words. For example, the sentence \"The cat [MASK] on the mat\" is transformed into \"The cat [MASK] on the mat\". The model is trained to predict the original word \"sat\" in the sentence.\n",
    "\n",
    "```{figure} https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/MLM.png\n",
    ":name: bert_mlm\n",
    ":alt: BERT MLM\n",
    ":width: 80%\n",
    ":align: center\n",
    "\n",
    "Masked Language Modeling (MLM). A token is randomly masked and the model is trained to predict the original word.\n",
    "```\n",
    "\n",
    "To generate training data for MLM, BERT randomly masks 15% of the tokens in each sequence. However, the masking process is not as straightforward as simply replacing words with [MASK] tokens. For the 15% of tokens chosen for masking:\n",
    "\n",
    "- 80% of the time, replace the word with the [MASK] token\n",
    "  - Example: \"the cat sat on the mat\" → \"the cat [MASK] on the mat\"\n",
    "\n",
    "- 10% of the time, replace the word with a random word\n",
    "  - Example: \"the cat sat on the mat\" → \"the cat dog on the mat\"\n",
    "\n",
    "- 10% of the time, keep the word unchanged\n",
    "  - Example: \"the cat sat on the mat\" → \"the cat sat on the mat\"\n",
    "\n",
    "The model must predict the original token for all selected positions, regardless of whether they were masked, replaced, or left unchanged. This helps prevent the model from simply learning to detect replaced tokens.\n",
    "\n",
    "During training, the model processes the modified input sequence through its transformer layers and predicts the original token at each masked position using the contextual representations.\n",
    "\n",
    "```{tip}\n",
    "While replacing words with random tokens or leaving them unchanged may seem counterintuitive, research has shown this approach is effective {footcite}`raffel2020exploring`. It has become an essential component of BERT's pre-training process.\n",
    "```\n",
    "\n",
    "#### Next Sentence Prediction (NSP)\n",
    "\n",
    "\n",
    "```{figure} https://amitness.com/posts/images/bert-nsp.png\n",
    ":name: bert_nsp\n",
    ":alt: BERT NSP\n",
    ":width: 80%\n",
    ":align: center\n",
    "\n",
    "Next Sentence Prediction (NSP). The model is trained to predict whether two sentences are consecutive or not.\n",
    "```\n",
    "\n",
    "Next Sentence Prediction (NSP) trains BERT to understand relationships between sentences. The model learns to predict whether two sentences naturally follow each other in text. During training, half of the sentence pairs are consecutive sentences from documents (labeled as IsNext), while the other half are random sentence pairs (labeled as NotNext).\n",
    "\n",
    "The input format uses special tokens to structure the sentence pairs: a [CLS] token at the start, the first sentence, a [SEP] token, the second sentence, and a final [SEP] token. For instance:\n",
    "\n",
    "$$\n",
    "\\text{``[CLS] }\\underbrace{\\text{I went to the store}}_{\\text{Sentence 1}}\\text{ [SEP] }\\underbrace{\\text{They were out of milk}}_{\\text{Sentence 2}}\\text{ [SEP]}\".\n",
    "$$\n",
    "\n",
    "BERT uses the final hidden state of the [CLS] token to classify whether the sentences are consecutive or not. This helps the model develop a broader understanding of language context and relationships between sentences.\n",
    "\n",
    "These two objectives help BERT learn the structure of language, such as the relationship between words and sentences.\n",
    "\n",
    "\n",
    "## Fine-tuning\n",
    "\n",
    "A powerful aspect of BERT is its ability to be fine-tuned on a wide range of tasks with minimal changes to the model architecture. This is achieved through transfer learning, where the pre-trained BERT model is used as a starting point for specific tasks.\n",
    "\n",
    "Consider a hospital that wants to classify patient reviews. Due to privacy concerns, collecting enough data to train a deep learning model from scratch would be difficult. This is where BERT shines - since it's already pre-trained on vast amounts of text data and understands language structure, it can be fine-tuned effectively even with a small dataset of patient reviews. The pre-trained BERT model can be adapted to this specific classification task with only minor architectural changes.\n",
    "\n",
    "```{tip}\n",
    ":class: tip\n",
    "\n",
    "You can find many fine-tuned and pre-trained models for various tasks by searching the [Hugging Face model hub](https://huggingface.co/models), with the keyword \"BERT\".\n",
    "```\n",
    "\n",
    "## Variants and improvements\n",
    "\n",
    "**RoBERTa (Robustly Optimized BERT Approach)* {footcite}`liu2019roberta`* improved upon BERT through several optimizations: removing the Next Sentence Prediction task, using dynamic masking that changes the mask patterns across training epochs, training with larger batches, and using a larger dataset. These changes led to significant performance improvements while maintaining BERT's core architecture.\n",
    "\n",
    "**DistilBERT** {footcite}`sanh2019distilbert` focused on making BERT more efficient through knowledge distillation, where a smaller student model learns from the larger BERT teacher model. It achieves 95% of BERT's performance while being 40% smaller and 60% faster, making it more suitable for resource-constrained environments and real-world applications.\n",
    "\n",
    "**ALBERT** {footcite}`lan2019albert` introduced parameter reduction techniques to address BERT's memory limitations. It uses factorized embedding parameterization and cross-layer parameter sharing to dramatically reduce parameters while maintaining performance. ALBERT also replaced Next Sentence Prediction with Sentence Order Prediction, where the model must determine if two consecutive sentences are in the correct order.\n",
    "\n",
    "Domain-specific BERT models have been trained on specialized corpora to better handle specific fields. Examples include **BioBERT** {footcite}`lee2020biobert` for biomedical text, **SciBERT** {footcite}`reimers2019sentence` for scientific papers, and **FinBERT** {footcite}`araci2019finbert` for financial documents. These models demonstrate superior performance in their respective domains compared to the general-purpose BERT.\n",
    "\n",
    "**Multilingual BERT (mBERT)** {footcite}`liu2019roberta` was trained on Wikipedia data from 104 languages, using a shared vocabulary across all languages. Despite not having explicit cross-lingual objectives during training, mBERT shows remarkable zero-shot cross-lingual transfer abilities, allowing it to perform tasks in languages it wasn't explicitly aligned on. This has made it a valuable resource for low-resource languages and cross-lingual applications.\n",
    "\n",
    "\n",
    "## Hands on\n",
    "\n",
    "Let us load a pre-trained BERT model and see how it works using a sense disambiguation task. The sense disambiguation task is a task that involves identifying the correct sense of a word in a sentence. For example, given a sentence with word \"apple\", we need to identify whether it refers to the fruit or the technology company.\n",
    "\n",
    "Let us first load the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eb0ae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb2ac1e",
   "metadata": {},
   "source": [
    "We will use [CoarseWSD-20](https://github.com/danlou/bert-disambiguation/tree/master/data/CoarseWSD-20). The dataset contains sentences with polysemous words and their sense labels. We will see how to use BERT to disambiguate the word senses. Read the [README](https://github.com/danlou/bert-disambiguation/blob/master/data/CoarseWSD-20/README.txt) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba28720e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_pos</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>9</td>\n",
       "      <td>'' fruits , sometimes categorized with vegetab...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>2</td>\n",
       "      <td>history since apple release more and more macs...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>24</td>\n",
       "      <td>original version the original formulation cont...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>29</td>\n",
       "      <td>in 2005 , north korea was ranked by the fao as...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>11</td>\n",
       "      <td>the permanent sliding calendar was invented by...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>13</td>\n",
       "      <td>arbitrary marks include preexisting words used...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1841</th>\n",
       "      <td>12</td>\n",
       "      <td>renderware was widely cross-platform : it ran ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>6</td>\n",
       "      <td>the town is the center of apple production in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>6</td>\n",
       "      <td>in 2001 , hcps began distributing apple ibook ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>15</td>\n",
       "      <td>it consists of two hollow parts : a handle and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_pos                                           sentence  label\n",
       "207          9  '' fruits , sometimes categorized with vegetab...      1\n",
       "431          2  history since apple release more and more macs...      0\n",
       "19          24  original version the original formulation cont...      1\n",
       "1769        29  in 2005 , north korea was ranked by the fao as...      1\n",
       "1193        11  the permanent sliding calendar was invented by...      0\n",
       "144         13  arbitrary marks include preexisting words used...      0\n",
       "1841        12  renderware was widely cross-platform : it ran ...      0\n",
       "724          6  the town is the center of apple production in ...      1\n",
       "1369         6  in 2001 , hcps began distributing apple ibook ...      0\n",
       "335         15  it consists of two hollow parts : a handle and...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data(focal_word, is_train, n_samples=100):\n",
    "    data_type = \"train\" if is_train else \"test\"\n",
    "    data_file = f\"https://raw.githubusercontent.com/danlou/bert-disambiguation/master/data/CoarseWSD-20/{focal_word}/{data_type}.data.txt\"\n",
    "    label_file = f\"https://raw.githubusercontent.com/danlou/bert-disambiguation/master/data/CoarseWSD-20/{focal_word}/{data_type}.gold.txt\"\n",
    "\n",
    "    data_table = pd.read_csv(\n",
    "        data_file,\n",
    "        sep=\"\\t\",\n",
    "        header=None,\n",
    "        dtype={\"word_pos\": int, \"sentence\": str},\n",
    "        names=[\"word_pos\", \"sentence\"],\n",
    "    )\n",
    "    label_table = pd.read_csv(\n",
    "        label_file,\n",
    "        sep=\"\\t\",\n",
    "        header=None,\n",
    "        dtype={\"label\": int},\n",
    "        names=[\"label\"],\n",
    "    )\n",
    "    combined_table = pd.concat([data_table, label_table], axis=1)\n",
    "    return combined_table.sample(n_samples)\n",
    "\n",
    "\n",
    "focal_word = \"apple\"\n",
    "\n",
    "train_data = load_data(focal_word, is_train=True)\n",
    "\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8088a8f2",
   "metadata": {},
   "source": [
    "We will use transformers library developed by Hugging Face to define the BERT model. To use the model, we will need:\n",
    "\n",
    "- BERT tokenizer that converts the text into tokens.\n",
    "- BERT model that computes the embeddings of the tokens.\n",
    "\n",
    "We will use the bert-base-uncased model and tokenizer. Let's define the model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7c9c62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = transformers.AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "model.eval() # set the model to evaluation mode\n",
    "print(model) # Print the model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997cb992",
   "metadata": {},
   "source": [
    "This prints the model architecture, which shows:\n",
    "\n",
    "1. BertEmbeddings layer that converts tokens into embeddings using:\n",
    "   - Word embeddings (30522 vocab size, 768 dimensions)\n",
    "   - Position embeddings (512 positions, 768 dimensions)\n",
    "   - Token type embeddings (2 types, 768 dimensions)\n",
    "   - Layer normalization and dropout\n",
    "\n",
    "2. BertEncoder with 12 identical BertLayers, each containing:\n",
    "   - Self-attention mechanism with query/key/value projections\n",
    "   - Intermediate layer with GELU activation\n",
    "   - Output layer with layer normalization\n",
    "\n",
    "3. BertPooler that processes the [CLS] token embedding with:\n",
    "   - Dense layer (768 dimensions)\n",
    "   - Tanh activation\n",
    "\n",
    "All layers maintain the 768-dimensional hidden size, except the intermediate layer which expands to 3072 dimensions.\n",
    "\n",
    "With BERT, we need to prepare text in ways that BERT can understand. Specifically, we prepend it with [CLS] and append [SEP]. We will then convert the text to a tensor of token ids, which is ready to be fed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fae1ae9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(text):\n",
    "    text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    segments_ids = torch.ones((1, len(indexed_tokens)), dtype=torch.long)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensor = segments_ids.clone()\n",
    "    return tokenized_text, tokens_tensor, segments_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055d396c",
   "metadata": {},
   "source": [
    "Let's get the BERT embeddings for the sentence \"Bank is located in the city of London\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea45dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Bank is located in the city of London\"\n",
    "tokenized_text, tokens_tensor, segments_tensor = prepare_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d838b58f",
   "metadata": {},
   "source": [
    "This produces the following output.\n",
    "**Tokenized text**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beb981bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'bank', 'is', 'located', 'in', 'the', 'city', 'of', 'london', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716dcfc7",
   "metadata": {},
   "source": [
    "**Token IDs**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7655f6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 2924, 2003, 2284, 1999, 1996, 2103, 1997, 2414,  102]])\n"
     ]
    }
   ],
   "source": [
    "print(tokens_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef41bd65",
   "metadata": {},
   "source": [
    "**Segment IDs**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1439b8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "print(segments_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c46219",
   "metadata": {},
   "source": [
    "Then, let's get the BERT embeddings for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a88f7caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model to return hidden states\n",
    "model.config.output_hidden_states = True\n",
    "\n",
    "outputs = model(tokens_tensor, segments_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e6fc40",
   "metadata": {},
   "source": [
    "The output includes `loss`, `logits`, and `hidden_states`. We will use `hidden_states`, which contains the embeddings of the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c84f199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many layers?  13\n",
      "Shape?  torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "hidden_states = outputs.hidden_states\n",
    "\n",
    "print(\"how many layers? \", len(hidden_states))\n",
    "print(\"Shape? \", hidden_states[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951daff0",
   "metadata": {},
   "source": [
    "The hidden states are a list of 13 tensors, where each tensor is of shape (batch_size, sequence_length, hidden_size). The first tensor is the input embeddings, and the subsequent tensors are the hidden states of the BERT layers.\n",
    "\n",
    "So, we have 13 choice of hidden states. Deep layers close to the output capture the context of the word from the previous layers.\n",
    "\n",
    "Here we will take the average over the last four hidden states for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa785c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "last_four_layers = hidden_states[-4:]\n",
    "# Stack the layers and then calculate mean\n",
    "stacked_layers = torch.stack(last_four_layers)\n",
    "emb = torch.mean(stacked_layers, dim=0)\n",
    "\n",
    "print(emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776cddd0",
   "metadata": {},
   "source": [
    "emb is of shape (sequence_length, hidden_size). Let us summarize the embeddings of the tokens into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d815918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(text):\n",
    "    tokenized_text, tokens_tensor, segments_tensor = prepare_text(text)\n",
    "    outputs = model(tokens_tensor, segments_tensor)\n",
    "    hidden_states = outputs[2]  # Access hidden states from tuple output\n",
    "    # Stack the last 4 layers then take mean\n",
    "    stacked_layers = torch.stack(hidden_states[-4:])\n",
    "    emb = torch.mean(stacked_layers, dim=0)\n",
    "    return emb, tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5126e7d3",
   "metadata": {},
   "source": [
    "Now, let us embed text and get the embeddings of the focal token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8e0a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []  # label\n",
    "emb = []  # embedding\n",
    "sentences = []  # sentence\n",
    "\n",
    "def get_focal_token_embedding(text, focal_word_idx):\n",
    "    emb, tokenized_text = get_bert_embeddings(text)\n",
    "    return emb[0][focal_word_idx]  # Access first batch dimension\n",
    "\n",
    "for index, row in train_data.iterrows():\n",
    "    text = row[\"sentence\"]\n",
    "    focal_word_idx = row[\"word_pos\"]\n",
    "    _emb = get_focal_token_embedding(text, focal_word_idx)\n",
    "    labels.append(row[\"label\"])\n",
    "    emb.append(_emb)\n",
    "    sentences.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8f1f2a",
   "metadata": {},
   "source": [
    "Finally, let us visualize the embeddings using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7bcf7b3",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"eb028173-c8ae-48b4-962d-a0bdb31c3dfe\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "'use strict';\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    function drop(id) {\n",
       "      const view = Bokeh.index.get_by_id(id)\n",
       "      if (view != null) {\n",
       "        view.model.document.clear()\n",
       "        Bokeh.index.delete(view)\n",
       "      }\n",
       "    }\n",
       "\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null) {\n",
       "      drop(id)\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim()\n",
       "            drop(id)\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded(error = null) {\n",
       "    const el = document.getElementById(\"eb028173-c8ae-48b4-962d-a0bdb31c3dfe\");\n",
       "    if (el != null) {\n",
       "      const html = (() => {\n",
       "        if (typeof root.Bokeh === \"undefined\") {\n",
       "          if (error == null) {\n",
       "            return \"BokehJS is loading ...\";\n",
       "          } else {\n",
       "            return \"BokehJS failed to load.\";\n",
       "          }\n",
       "        } else {\n",
       "          const prefix = `BokehJS ${root.Bokeh.version}`;\n",
       "          if (error == null) {\n",
       "            return `${prefix} successfully loaded.`;\n",
       "          } else {\n",
       "            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n",
       "          }\n",
       "        }\n",
       "      })();\n",
       "      el.innerHTML = html;\n",
       "\n",
       "      if (error != null) {\n",
       "        const wrapper = document.createElement(\"div\");\n",
       "        wrapper.style.overflow = \"auto\";\n",
       "        wrapper.style.height = \"5em\";\n",
       "        wrapper.style.resize = \"vertical\";\n",
       "        const content = document.createElement(\"div\");\n",
       "        content.style.fontFamily = \"monospace\";\n",
       "        content.style.whiteSpace = \"pre-wrap\";\n",
       "        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n",
       "        content.textContent = error.stack ?? error.toString();\n",
       "        wrapper.append(content);\n",
       "        el.append(wrapper);\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(() => display_loaded(error), 100);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.6.0.min.js\"];\n",
       "  const css_urls = [];\n",
       "\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      try {\n",
       "            for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "\n",
       "      } catch (error) {display_loaded(error);throw error;\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"eb028173-c8ae-48b4-962d-a0bdb31c3dfe\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"eb028173-c8ae-48b4-962d-a0bdb31c3dfe\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.6.0.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"eb028173-c8ae-48b4-962d-a0bdb31c3dfe\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"cb5ef5db-3210-4283-8219-2ad822cec95d\" data-root-id=\"p1004\" style=\"display: contents;\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "  const docs_json = {\"e98ed28a-8b96-4b03-9ead-2035a5c013bd\":{\"version\":\"3.6.0\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1004\",\"attributes\":{\"width\":700,\"height\":500,\"x_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1005\"},\"y_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1006\"},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1014\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1015\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1007\",\"attributes\":{\"text\":\"Word Embeddings Visualization\"}},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1046\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1001\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1002\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1003\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"NU1dwJ8dyT0YDs+/RnrTP8VFecBkAfy/b97kwJNjTECE0BW+9fKfv0V/6D8TF6i+rzmxP/PE1b5n5RG/iZcXwMqtTMDbU/O/SN3cv/a/VL9TmaQ/VhOEwHd/t0Ay9W/A1tHevy47nz//S/2/FvzePvD6hb7kMVHAeEmIwBL4YT9SvATBHykpQA7vnL4phjI/zc62P7+EkECWMXtAZO7LP/0Mwb56gVe/KvUXQebFycDX8BJB4iXvwFC+fz1YyP9A+7X6P6p9A8H6W7W/4YL/QEyUDUDnfPpAUftVQGh2V0Cbh5e/MpCuP8zu3T8OeK/AxJMGwO65WL95dLi9E9PNwKgPoj8Tfz2/gOUzP4G6ib6J3jDA+cM8v2L0s8BByvQ+wqadPz+Id8Afek5Aca86QDfsTL9ZuhQ/89EMQRo4s8B79Q5BqH6yQGi1pMBpGDk/Or7KwKo7o7873yXAprUiQM7O6z9+eUVA+MlLwIyYR0BpmQZArgbmv+5/yD/rfPO/d46awBARXz2zm2dAlO+zQA==\"},\"shape\":[100],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"G8o6QOOaT8BM53JAntUnPz3HjkAj/+u/YlbaPUeoB8DQUpnA6tUFQNLSz79OdUnAAhEuQOILHr9QpUDA6t64v6oz2r6521DA49Q9wB69jsCQEVbAFRWSwKYuSUBklttAzKJYwJ/Mp7//Vzk/Q4Bsv2oqfECdH63AzXzcP8nw9z+GAwRB6c/Rv3s1c8Ctwfq/Mu18wKC4HcCVi2C+umSvwMHs478Zd4XAmhelQEa+PMCmf7JAD6YmwIriFMBgbldA5gOkQMmXGUGv8h/AEg8qQMTPpr8TxDNAFg42wLkeLcDu/kJAb7cJwEQml8BOcNY+0R4MQQAVXMDNUjXAAvQ1wK19NMB6MQNAlygVwH69ujyV/DdA/Uc+QGjSRkH10sm9CjKQv6kdSsBeJGu/DTXpv94a6r/x5APAStaPQGxABsCacqdACQQNv0IR8UAM6DXAkqpVwNh1pz9rRK9AAOd9P+Zajr+/SoQ/jmG+wMRY+T8hPxU+oxzLQG/KF0AK6WG/CGQ6QMQKsMD2YfO/vnZvPw==\"},\"shape\":[100],\"dtype\":\"float32\",\"order\":\"little\"}],[\"label\",[1,0,1,1,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,1,1,1,0,1,0,0,0,0,0,0,1,0,1,0,1,0,1,1,0,1,1,0,1,0,1,0,0,0,1,0,0,0,0,1,0,1,0,1,1,1,1,0,1,1,0,1,1,0,1,1,0,0,0,1,1,1,1,1,0,1,1,1,1,0,0,0,1,1]],[\"sentence\",[\"'' fruits , sometimes categorized with vegetables , include apple , oranges , banana , berries and lemon .\",\"history since apple release more and more macs with ssd by default , like the macbook air , it makes sense for mac users to seek ways to optimize the way they use disk space .\",\"original version the original formulation contained 106 mg of caffeine , 33 g of sugar , fruit of the a\\u00e7a\\u00ed palm , 68 % apple juice , 5 % lemon juice , carbonated water , fructose and malt extract and trace quantities of guarana and ginseng , per 250 ml can .\",\"in 2005 , north korea was ranked by the fao as an estimated 10th in the production of fresh fruit and as an estimated 19th in the production of apple .\",\"the permanent sliding calendar was invented by claussen-finks 24 years before apple presented its first iphone in 2008 , where sliding on the screen with the finger appeared in an electronic consumer product for the first time .\",\"arbitrary marks include preexisting words used in an arbitrary way , such as apple when used for computers .\",\"renderware was widely cross-platform : it ran on windows as well as apple mac os x - based applications and many video game consoles such as nintendo gamecube , wii , xbox , xbox 360 , playstation 2 , playstation 3 , and playstation portable .\",\"the town is the center of apple production in western australia .\",\"in 2001 , hcps began distributing apple ibook to every high school student .\",\"it consists of two hollow parts : a handle and a top , the so-called apple .\",\"this can be done either on a windows pc running fl studio natively , or an apple computer using either boot camp or virtualization .\",\"for example , in 1985 , digital research had to change some aspects of their graphical environment manager product and agreed to pay apple a financial compensation as part of a settlement that would keep them out of court .\",\"agricultural products include apple , banana , barley , beans , cacao , coffee , maize , oat , peach , plantain , potato , rice , sugarcane and wheat .\",\"lester patel lester patel is a member of the nerd herd who specializes in apple products .\",\"some have suggested stem could not achieve the economic goals intended , and others have added additional subjects or qualifiers : steam ( art ) -- some have discussed the need for product design in marketing new technologies , and point to the successes of apple , which combines technology and functionality with style .\",\"with regard to post-production , the university also has avid mentor status , and is the midlands ' accredited training center for apple 's final cut editing software .\",\"wheels of zeus ( or woz ) was a company founded in 2002 by apple co-founder steve wozniak .\",\"'' home '' was released to the apple itunes store on june 8 , 2010 .\",\"examples examples of file managers that use a spatial metaphor by default include : amiga 's workbench apple 's finder 5 to 9 ( versions prior to mac os x ) beos 's tracker enlightenment 17 's file manager gnome 's nautilus from version 2.6 till 2.29 the os/2 workplace shell the risc os filer the rox desktop 's rox-filer windows explorer in windows 95 was also a spatial file manager in some aspects , but became a navigational file manager in later versions of the operating system ( when using the default settings ) other objects some file managers represent other objects , such as a trash can for unwanted files , or computer or floppy disk icons to represent storage media .\",\"mobile in april 2013 , udemy was first offered as an apple ios app , allowing students to take classes directly from smartphones ; the android version was launched in january 2014 .\",\"the skateboarding bulldog featured on youtube and in an apple iphone commercial was named after tillman .\",\"altivec is a floating point and integer simd instruction set designed and owned by apple , ibm and freescale semiconductor , formerly the semiconductor products sector of motorola , ( the aim alliance ) , and implemented on versions of the powerpc including motorola 's g4 , ibm 's g5 and power6 processors , and p.a . semi 's pwrficient pa6t .\",\"the island has a mixed vegetation , including hardwood tree stands , remnant pear and apple orchards , ornamental trees and shrubs , open meadows , sumac groves , salt marsh grasses , and lawns , including a soccer field .\",\"bunbury to walpole from bunbury , the highway goes through boyanup and on to donnybrook , the heart of wa 's apple country .\",\"it used a user-friendly design that foreshadowed the advent of graphically rich applications like those that have been developed for the apple ios .\",\"his video scratching and live video manipulation performances received attention and acclaim from the likes of apple and wired magazine .\",\"cricketpaint was a second generation 1-bit ( black and white ) painting software program for the apple macintosh by cricket software .\",\"almost all modern apple in commercial use are propagated as dwarf or semi-dwarf trees for ease of picking and spraying .\",\"supper unit : canned meat , consisting of cervelat sausage , ( early version ) , either pork luncheon meat with carrot or apple ( first issue ) , beef and pork loaf ( second issue ) ; biscuits ; a 2-ounce ( 57 g ) d ration emergency chocolate bar , ( early version ) tropical bar , or ( in temperate climates ) commercial sweet chocolate bar ( late version ) , a packet of toilet paper tissues ; a four-pack of cigarettes ; chewing gum , and a bouillon packet ( cube or powder ) .\",\"it is a developer tool included in apple mac os x v10.5 and later versions of mac os x , built on top of the dtrace tracing framework from opensolaris and ported to mac os x .\",\"availability while initially available only for macs with a superdrive , it was included until 2011 with all new macs ; from idvd 6 onwards , apple supports the ability to burn projects with third-party optical drives .\",\"in hargeisa in the northwest , the preferred drinks are fiimto ( vimto ) and tufaax ( apple ) .\",\"the chairman of the board is arthur d. levinson of apple .\",\"frazeysburg is home to the world 's largest apple basket , which is located on the longaberger homestead .\",\"when apple added podcatching to its itunes software in june 2005 , it almost immediately became the most popular client .\",\"in norse mythology , i\\u00f0unn is described as providing the gods apple that grant them eternal youthfulness in the 13th century prose edda .\",\"on commercial personal computer the desktop metaphor was popularized among technical users by the original macintosh from apple in 1984 , and among the general population by windows 95 from microsoft in 1995 .\",\"pavel reportedly had considered asking manufacturers of mp3 music players for royalties , including apple ( for their popular ipod player ) .\",\"notable merchants companies like apple , target , barnes & noble , qdoba , fuddruckers , walmart , reebok , sephora , the north face , toshiba , puma , drugstore.com , adidas and pacsun offer student discounts via edhance .\",\"microsoft plans to focus the xbox music service to compete more directly with apple 's itunes store , amazon mp3 , spotify , deezer and other streaming services .\",\"john calhoun mentioned that he 'd like to create a successor to glider pro that went back to the roots of glider ( for example , by having fewer objects and all-indoor houses ) but that it would have to wait until he 's done with his work at apple .\",\"reviews initial reactions see the devices as an attempt to challenge apple 's ipad .\",\"it consists of wheat flakes , dried fruit ( sultanas , raisin , coconut , banana , and apple ) and hazelnut .\",\"swift is a multi-paradigm , compiled programming language created by apple for ios and os&nbsp;x development .\",\"this includes industries such as almond , apple , artichoke , asparagus , boysenberries , chive , corn , egg , fig , grapefruit , papaya , peach , pineapple , plum , prune , seafood , tomato and turkey .\",\"wikinodes is an app for the apple ipad which uses the spicynodes visualization method .\",\"albert etter ( 1872 -- 1950 ) was an american plant breeder best known for his work on strawberry and apple varieties .\",\"kingdom holding is an investment company founded in 1980 and 95 % owned by prince talal that has assets valued at over $ 25 billion , with interests in many major companies such as walt disney , pepsico , kodak , apple , hewlett packard , motorola , time warner , newscorp , and citigroup , as well as real estate in london through its songbird estates division .\",\"fruit trees in orchard produce cherries , apple , plum , peach and pear , while native wild grape vines , and blackberry bushes are also found in the valley .\",\"rosh hashanah rosh hashana symbols : shofar , apple and honey , pomegranates , kiddush wine erev rosh hashanah ( eve of the first day ) : 29 elul rosh hashanah : 1 -- 2 tishrei according to oral tradition , rosh hashanah ( \\u05e8\\u05d0\\u05e9 \\u05d4\\u05e9\\u05e0\\u05d4 ) ( lit. , '' head of the year '' ) is the day of memorial or remembrance ( \\u05d9\\u05d5\\u05dd \\u05d4\\u05d6\\u05db\\u05e8\\u05d5\\u05df , yom hazikaron ) , and the day of judgment ( \\u05d9\\u05d5\\u05dd \\u05d4\\u05d3\\u05d9\\u05df , yom hadin ) .\",\"some programs bundled with popular operating system which support png include microsoft 's paint and apple 's iphoto and preview , with the gimp also often being bundled with popular linux distributions .\",\"seasonal kringle flavors include rhubarb ( late may and june ) , key lime in july , mango coconut in august and apple toffee in september .\",\"however , an apple floats down from the tree and seems to hover in alice 's face .\",\"it contains branches of luxury firms such as beymen and atelier rebul , vakko , cos , moncler , bulgari , pomellato , louis vuitton , fendi , lanvin , dior , miu miu , burberry , tory burch , michael kors and valentino , and istanbul 's ( and the country 's ) first apple store .\",\"in peru it is drunk warm , and apple , guan\\u00e1bana and quince is used instead of lulo .\",\"on january 26 , 2009 , mtv and apple made all three collections available on the itunes store .\",\"the fruit is an edible oval drupe deep ; when immature it is smooth-green , with the consistency and taste of an apple , maturing brown to purplish-black and eventually wrinkled , looking like a small date .\",\"august 31 a group of hackers utilize sites like reddit and 4chan to release hundreds of private , many of them nude , photographs of around 100 individuals , most of them a-list celebrities , leading to an investigation by the fbi and criticism of apple 's icloud service .\",\"history formation dr. dre ( seen in 2012 ) , company co-founder the company was formally established in 2006 , a time when iovine perceived two key problems in the music industry : the impact of piracy on music sales and the substandard audio quality provided by apple 's plastic earbuds .\",\"esolar is honored alongside such companies as google , apple , twitter , and ibm .\",\"fruits and plants ch\\u00e8 hoa qu\\u1ea3 mixture of different fruit including pineapple , watermelon , apple , pear , mango , lychee , dried banana , cherry , and dried coconut with milk , yogurt , and syrup ch\\u00e8 nh\\u00e3n - made from longan ch\\u00e8 tr\\u00e1i c\\u00e2y - made from fruits ch\\u00e8 xo\\u00e0i - made from mango ch\\u00e8 tr\\u00e1i v\\u1ea3i - lychee and jelly ch\\u00e8 b\\u01b0\\u1edfi - made from grapefruit oil and slivered rind ch\\u00e8 chu\\u1ed1i - made from bananas and tapioca ( vietnamese : b\\u1ed9t b\\u00e1ng ) ch\\u00e8 s\\u1ea7u ri\\u00eang - made from durian ch\\u00e8 th\\u1ed1t n\\u1ed1t - made from sugar palm seeds ch\\u00e8 m\\u00edt - made from jackfruit ch\\u00e8 l\\u00f4 h\\u1ed9i - made from aloe vera h\\u1ed9t \\u00e9 - made from sterculia lychnophora extract and basil seeds one version of the ch\\u00e8 th\\u01b0ng mixed ch\\u00e8 \\u0111\\u1eadu \\u0111\\u1ecf b\\u00e1nh l\\u1ecdt - red beans and b\\u00e1nh l\\u1ecdt .\",\"the open data-link interface ( odi ) , developed by apple and novell , serves the same function as microsoft and 3com 's network driver interface specification ( ndis ) .\",\"after college , cirne held senior technical positions at apple and hummingbird communications .\",\"industry response when surface was first announced , critics noted that the device represented a significant departure for microsoft , as the company had previously relied exclusively on third-party oems to produce devices running windows , and began shifting towards a first-party hardware model with similarities to that of apple .\",\"the images had been demanded in exchange for interviews regarding an alleged leak of apple iphone and ipad user data from an fbi laptop .\",\"other uses the oil of the flower may be added to perfume to infuse an apple scent into them .\",\"the first meeting of the group had 15 people at it and within a year , the group had grown to a couple thousand apple computer fans .\",\"james grieve is an old variety of apple .\",\"as of april 2011 , enlace is one of nine channels that are featured with live real-time webcasts on the tbn mobile app , developed by trinet internet solutions , inc. , and available for free on itunes for the apple ipad , iphone and ipod touch .\",\"vigorously chewing , it is important to provide a constant and regular supply of any non-toxic materials such as non-toxic tree branches ( i.e. apple , aspen , birch , pear , pine , willow ) that has never been treated with any type of chemical products ( i.e. disinfectant , fungicide , insecticide , javel , pesticide ) the species is hardy & vigorous when acclimatized .\",\"some of the fresh produce that can be identified in this photo include : tomatoe , lettuce , spinach , banana , cherries , watermelon , melon , red and yellow apple , cucumber , garlic , apricot , grape , lemon , pear , bean , strange fruit , peach laiki agora ( \\u03bb\\u03b1\\u03ca\\u03ba\\u03ae \\u03b1\\u03b3\\u03bf\\u03c1\\u03ac , greek for people 's market ) , also common in the plural laikes agores ( \\u03bb\\u03b1\\u03ca\\u03ba\\u03ad\\u03c2 \\u03b1\\u03b3\\u03bf\\u03c1\\u03ad\\u03c2 , people 's markets ) , are street market that operate all over greece , selling foodstuffs and gardening or household equipment , as well as children toys and various '' do it yourself '' tools .\",\"'' tufts began his criminal activities at the age of 14 with thefts of '' apple , pear , cucumber , and other fruits of the earth , '' and then graduating to '' a paper money bill '' of a neighbor .\",\"in addition to oaks , archips semiferanus have been known to feed on witchhazel and apple trees occasionally .\",\"example conferences some of the busiest conferences on cix are enquire within ( general discussion ) , bikers , windows xp ( support for , and discussion of , windows xp ) , windows vista , digital tv , philology ( words and their derivations ) , cultmedia , mac ( support for , and discussion of , apple computers and mac os ) , amiga ( discussion of the 68k/ppc amiga platforms ) , carp ( the campaign for real pedantry - discussion of any fine points of detail , often concentrating on the use and abuse of the english language ) , internet , own .\",\"the grounds are decorated with apple , dogwood , maple , mimosa , and pear trees .\",\"discontinued flavors include apple and non-alcoholic champagne .\",\"based on the experiences of directing able edwards , robertson went on to write the book , desktop cinema : feature filmmaking on the home computer , a step-by-step account into how one would make their own feature film on apple 's macintosh computer .\",\"the highland area known as ba'kelalan has been experimenting with the cultivation of apple .\",\"chief crops included hay , apple , grape and cranberries .\",\"the digital media access protocol ( dmap ) is the family of proprietary protocol introduced by apple that are used by itunes , iphoto , remote and other software to share media across a local network .\",\"the collection spans a huge variety of species , such as maple , spirea , deutzia , beech , plum and rare apple family members .\",\"'' there he planted species from the mountains of asia , north america , and the caucasus , and by 1908 was growing 93 trees , including 19 apple varieties , 9 pear , 7 plum , 2 rose varieties , many shrub , and 401 herbaceous species .\",\"his commercial work includes countless tv , radio and internet commercials for a diverse clientele such as : nike for wieden kennedy , apple for tbwa/chiat/day , toyota for saatchi and saatchi , and cisco systems and yahoo! for ogilvy and mather .\",\"the currency sign was once a part of the mac os roman character set , but apple changed the symbol at that code point to the euro sign ( mac os 8.5 .\",\"in addition to windows and linux , the switch to sdl enabled pingus to be easily built on other platforms , including apple 's os x .\",\"odalengo piccolo is known for the truffle fair tufo & tartufo , which has been held during the autumn truffle season since 1994 , and for the wide variety of antique native apple cultivars which are grown locally .\",\"' ribston pippin ' is a triploid cultivar of apple , also known as ` essex pippin ' , ` beautiful pippin ' , ` formosa ' , ` glory of york ' , ` ribstone ' , ` rockhill 's russet ' and ` travers ' .\",\"it is a small plant growing up to half a meter high , bearing small , white or yellowish snapdragon-like flowers which are said to smell of apple .\",\"in the original , an apple obscures the face .\",\"a brown betty is a traditional american dessert made from fruit ( usually apple , but also berries or pear ) and sweetened crumbs .\",\"it also supported apple 's game sprockets .\",\"it also likes to feed on apple , rowan , and sweet chestnut .\",\"when they first meet , watt is disguised as an apple with a blue stalk .\",\"one major u.s. beekeeper reports moving his hives from idaho to california in january to prepare for almond pollination in february , then to apple orchard in washington in march , to north dakota two months later for honey production , and then back to idaho by november a journey of several thousands of miles .\",\"the community was founded in 1895 and it was named for pomona , the roman goddess of fruit trees ; this area was a major producer of apple .\",\"apple shake was used to digitally generate the visual effects .\",\"in contrast , the '' enter '' key is commonly labeled with its name in plain text on generic pc keyboards , or with the symbol ( u +2324 ) on many apple mac keyboards .\",\"some industrial groups allow , and often encourage , their fans to remix their music , notably nine inch nails , whose website contains a list of downloadable songs that can be remixed using apple 's garageband software .\",\"the area was spotted with apple orchard throughout much of the 1800s .\",\"other seeds , such as apple pips and plum stones , have fleshy receptacles and smaller fruits like hawthorns have seeds enclosed in edible tissue ; animals including mammals and birds eat the fruits and either discard the seeds , or swallow them so they pass through the gut to be deposited in the animal 's droppings well away from the parent tree .\"]],[\"color\",[\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\"]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1047\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1048\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1043\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":12},\"line_color\":{\"type\":\"value\",\"value\":\"DarkSlateGrey\"},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1044\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":12},\"line_color\":{\"type\":\"value\",\"value\":\"DarkSlateGrey\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1045\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":12},\"line_color\":{\"type\":\"value\",\"value\":\"DarkSlateGrey\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1013\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1026\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1027\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1028\",\"attributes\":{\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1029\",\"attributes\":{\"syncable\":false,\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"handles\":{\"type\":\"object\",\"name\":\"BoxInteractionHandles\",\"id\":\"p1035\",\"attributes\":{\"all\":{\"type\":\"object\",\"name\":\"AreaVisuals\",\"id\":\"p1034\",\"attributes\":{\"fill_color\":\"white\",\"hover_fill_color\":\"lightgray\"}}}}}}}},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1036\"},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1037\"},{\"type\":\"object\",\"name\":\"HelpTool\",\"id\":\"p1038\"},{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1039\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":[[\"Label\",\"@label\"],[\"Sentence\",\"@sentence\"]]}}]}},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1021\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1022\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1023\"},\"axis_label\":\"PCA 2\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1024\"}}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1016\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1017\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1018\"},\"axis_label\":\"PCA 1\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1019\"}}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1020\",\"attributes\":{\"axis\":{\"id\":\"p1016\"}}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1025\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1021\"}}}]}}]}};\n",
       "  const render_items = [{\"docid\":\"e98ed28a-8b96-4b03-9ead-2035a5c013bd\",\"roots\":{\"p1004\":\"cb5ef5db-3210-4283-8219-2ad822cec95d\"},\"root_ids\":[\"p1004\"]}];\n",
       "  void root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    let attempts = 0;\n",
       "    const timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "p1004"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert list of tensors to numpy array\n",
    "emb_numpy = torch.stack(emb).detach().numpy()\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "xy = pca.fit_transform(emb_numpy)\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "# Create data source for Bokeh\n",
    "source = ColumnDataSource(data=dict(\n",
    "    x=xy[:, 0],\n",
    "    y=xy[:, 1],\n",
    "    label=labels,\n",
    "    sentence=sentences\n",
    "))\n",
    "\n",
    "# Create Bokeh figure\n",
    "p = figure(title=\"Word Embeddings Visualization\", x_axis_label=\"PCA 1\", y_axis_label=\"PCA 2\",\n",
    "           width=700, height=500)\n",
    "\n",
    "# Add hover tool\n",
    "hover = HoverTool(tooltips=[\n",
    "    ('Label', '@label'),\n",
    "    ('Sentence', '@sentence')\n",
    "])\n",
    "p.add_tools(hover)\n",
    "\n",
    "# Create color map for labels\n",
    "import seaborn as sns\n",
    "\n",
    "unique_labels = list(set(labels))\n",
    "color_map = sns.color_palette().as_hex()[0:len(unique_labels)]\n",
    "source.data['color'] = [color_map[label] for label in labels]\n",
    "\n",
    "# Add scatter plot\n",
    "p.scatter('x', 'y', size=12, line_color=\"DarkSlateGrey\", line_width=2,\n",
    "         fill_color='color', source=source)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d606f3b",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "We have used the last 4 layers of BERT to generate the embeddings of the tokens. Now, let's use the last $k = 1, 2, 3$ layers of BERT to generate the embeddings of the tokens. Then plot the embeddings using PCA.\n",
    "\n",
    "## References\n",
    "\n",
    "```{footbibliography}\n",
    ":style: unsrt\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d283d5b6",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## Spectral Embedding with the Adjacency Matrix\n",
    "\n",
    "The spectral embedding with the adjacency matrix is given by the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{U}} J(\\mathbf{U}),\\quad J(\\mathbf{U}) = \\| \\mathbf{A} - \\mathbf{U}\\mathbf{U}^\\top \\|_F^2\n",
    "$$\n",
    "\n",
    "We will approach the solution step by step based on the following steps:\n",
    "\n",
    "1. We start taking a derivative of $J(\\mathbf{U})$  with respect to $\\mathbf{U}$.\n",
    "2. We then set the derivative to zero (i.e., $\\nabla J(\\mathbf{U}) = 0$) and solve for $\\mathbf{U}$.\n",
    "\n",
    "1. Expand the Frobenius norm:\n",
    "\n",
    "   The Frobenius norm for any matrix $\\mathbf{M}$ is defined as:\n",
    "\n",
    "   $\\|\\mathbf{M}\\|_F^2 = \\sum_{i,j} M_{ij}^2 = \\text{Tr}(\\mathbf{M}\\mathbf{M}^\\top)$\n",
    "\n",
    "   Applying this to our problem:\n",
    "\n",
    "   $J(\\mathbf{U}) = \\|\\mathbf{A} - \\mathbf{U}\\mathbf{U}^\\top\\|_F^2 = \\text{Tr}[(\\mathbf{A} - \\mathbf{U}\\mathbf{U}^\\top)(\\mathbf{A} - \\mathbf{U}\\mathbf{U}^\\top)^\\top]$\n",
    "\n",
    "   Expanding this:\n",
    "\n",
    "   $= \\text{Tr}(\\mathbf{A}\\mathbf{A}^\\top - 2\\mathbf{A}\\mathbf{U}\\mathbf{U}^\\top + \\mathbf{U}\\mathbf{U}^\\top\\mathbf{U}\\mathbf{U}^\\top)$\n",
    "\n",
    "2. Take the derivative with respect to $\\mathbf{U}$:\n",
    "\n",
    "   Using matrix calculus rules:\n",
    "\n",
    "   $\\frac{\\partial \\text{Tr}(\\mathbf{A}\\mathbf{A}^\\top)}{\\partial \\mathbf{U}} = 0$\n",
    "\n",
    "   $\\frac{\\partial \\text{Tr}(\\mathbf{A}\\mathbf{U}\\mathbf{U}^\\top)}{\\partial \\mathbf{U}} = 2\\mathbf{A}\\mathbf{U}$\n",
    "\n",
    "   $\\frac{\\partial \\text{Tr}(\\mathbf{U}\\mathbf{U}^\\top\\mathbf{U}\\mathbf{U}^\\top)}{\\partial \\mathbf{U}} = 4\\mathbf{U}\\mathbf{U}^\\top\\mathbf{U}$\n",
    "\n",
    "   Combining these:\n",
    "\n",
    "   $\\frac{\\partial J}{\\partial \\mathbf{U}} = -4\\mathbf{A}\\mathbf{U} + 4\\mathbf{U}\\mathbf{U}^\\top\\mathbf{U}$\n",
    "\n",
    "   Simplifying:\n",
    "\n",
    "   $\\frac{\\partial J}{\\partial \\mathbf{U}} = -2\\mathbf{A}\\mathbf{U} + 2\\mathbf{U}\\mathbf{U}^\\top\\mathbf{U}$\n",
    "\n",
    "3. Set the derivative to zero and solve:\n",
    "\n",
    "   $-2\\mathbf{A}\\mathbf{U} + 2\\mathbf{U}\\mathbf{U}^\\top\\mathbf{U} = 0$\n",
    "\n",
    "   $\\mathbf{A}\\mathbf{U} = \\mathbf{U}\\mathbf{U}^\\top\\mathbf{U}$\n",
    "\n",
    "4. This equation is satisfied when $\\mathbf{U}$ consists of eigenvectors of $\\mathbf{A}$:\n",
    "\n",
    "   Assume $\\mathbf{U}$ consists of eigenvectors of $\\mathbf{A}$:\n",
    "\n",
    "   $\\mathbf{A}\\mathbf{U} = \\mathbf{U}\\mathbf{\\Lambda}$\n",
    "\n",
    "   where $\\mathbf{\\Lambda}$ is a diagonal matrix of eigenvalues.\n",
    "\n",
    "   Since eigenvectors are orthonormal:\n",
    "\n",
    "   $\\mathbf{U}^\\top\\mathbf{U} = \\mathbf{I}$\n",
    "\n",
    "   Therefore:\n",
    "\n",
    "   $\\mathbf{U}\\mathbf{U}^\\top\\mathbf{U} = \\mathbf{U}$\n",
    "\n",
    "   This shows our equation is satisfied when $\\mathbf{U}$ consists of eigenvectors of $\\mathbf{A}$.\n",
    "\n",
    "5. To minimize $J(\\mathbf{U})$, choose the eigenvectors corresponding to the $d$ largest eigenvalues.\n",
    "\n",
    "   To understand why, consider the trace of our objective function:\n",
    "\n",
    "   $J(\\mathbf{U}) = \\text{Tr}(\\mathbf{A}\\mathbf{A}^\\top) - 2\\text{Tr}(\\mathbf{A}\\mathbf{U}\\mathbf{U}^\\top) + \\text{Tr}(\\mathbf{U}\\mathbf{U}^\\top\\mathbf{U}\\mathbf{U}^\\top)$\n",
    "\n",
    "   Since $\\mathbf{U}$ is orthogonal ($\\mathbf{U}^\\top\\mathbf{U} = \\mathbf{I}$), and trace is invariant under cyclic permutations, we can simplify:\n",
    "\n",
    "   $J(\\mathbf{U}) = \\text{Tr}(\\mathbf{A}\\mathbf{A}^\\top) - \\text{Tr}(\\mathbf{U}^\\top\\mathbf{A}\\mathbf{U})$\n",
    "\n",
    "   Let $\\mathbf{U} = [\\mathbf{u}_1, ..., \\mathbf{u}_d]$ be the eigenvectors of $\\mathbf{A}$ with corresponding eigenvalues $\\lambda_1 \\geq ... \\geq \\lambda_d$. Then:\n",
    "\n",
    "   $\\text{Tr}(\\mathbf{U}^\\top\\mathbf{A}\\mathbf{U}) = \\sum_{i=1}^d \\lambda_i$\n",
    "\n",
    "   To minimize $J(\\mathbf{U})$, maximize $\\sum_{i=1}^d \\lambda_i$ by selecting the eigenvectors corresponding to the $d$ largest eigenvalues.\n",
    "\n",
    "The result is the collection of the $d$ eigenvectors corresponding to the $d$ largest eigenvalues, and it is one form of the spectral embedding.\n",
    "\n",
    "\n",
    "## The proof of the Laplacian Eigenmap\n",
    "\n",
    "The Laplacian Eigenmap is given by the following optimization problem:\n",
    "\n",
    "$$\n",
    "J_{LE}(\\mathbf{U}) = \\text{Tr}(\\mathbf{U}^\\top \\mathbf{L} \\mathbf{U})\n",
    "$$\n",
    "\n",
    "The step where we rewrite $J_{LE}(\\mathbf{U})$ as $\\text{Tr}(\\mathbf{U}^\\top \\mathbf{L} \\mathbf{U})$ is crucial for leveraging matrix derivatives. Let's break down this transformation step by step:\n",
    "\n",
    "1. First, we rewrite $\\mathbf{U}$ by column vectors:\n",
    "\n",
    "   $$\n",
    "   \\mathbf{U} =\n",
    "   \\begin{bmatrix}\n",
    "   \\vert & \\vert & & \\vert \\\\\n",
    "   \\mathbf{x}_1 & \\mathbf{x}_2 & \\cdots & \\mathbf{x}_d \\\\\n",
    "   \\vert & \\vert & & \\vert\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   where $\\mathbf{x}_i$ is the $i$-th column of $\\mathbf{U}$.\n",
    "\n",
    "2. We can expand the loss function $J_{LE}(\\mathbf{U})$:\n",
    "\n",
    "   $$\n",
    "   J_{LE}(\\mathbf{U}) = \\sum_{i} \\sum_{j} L_{ij} u_i^\\top u_j = \\sum_{i} \\sum_{j} \\sum_{d'} L_{ij} u_{i,d'} u_{j,d'}\n",
    "   $$\n",
    "\n",
    "3. Rearranging the order of summation:\n",
    "\n",
    "   $$\n",
    "   J_{LE}(\\mathbf{U}) = \\sum_{d'} \\sum_{i} \\sum_{j} L_{ij} u_{i,d'} u_{j,d'}\n",
    "   $$\n",
    "\n",
    "4. We can rewrite this as a matrix multiplication for each $d'$:\n",
    "\n",
    "   $$\n",
    "   J_{LE}(\\mathbf{U}) = \\sum_{d'} \\mathbf{x}_{d'}^\\top \\mathbf{L} \\mathbf{x}_{d'}\n",
    "   $$\n",
    "\n",
    "   where $\\mathbf{x}_{d'}$ is the $d'$-th column of $\\mathbf{U}$.\n",
    "\n",
    "5. Finally, we can express this as a trace:\n",
    "\n",
    "   $$\n",
    "   J_{LE}(\\mathbf{U}) = \\text{Tr}(\\mathbf{U}^\\top \\mathbf{L} \\mathbf{U})\n",
    "   $$\n",
    "\n",
    "This final form expresses our objective function in terms of matrix operations, which allows us to use matrix calculus to find the optimal solution. The trace representation is a useful technique to leverage matrix calculus."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
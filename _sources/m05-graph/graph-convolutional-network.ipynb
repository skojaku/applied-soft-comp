{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "422807d5",
   "metadata": {},
   "source": [
    "# Graph Convolutional Networks\n",
    "We have seen that spectral filters give us a principled way to think about \"convolution\" on irregular graph structures, and controlling the frequency components brings out different aspects of the data. We now go one step further: instead of designing filters by hand, we can learn them from data for specific tasks.\n",
    "\n",
    "\n",
    "## Spectral Graph Convolutional Networks\n",
    "\n",
    "A simplest form of learnable spectral filter is given by\n",
    "\n",
    "$$\n",
    "{\\bf L}_{\\text{learn}} = \\sum_{k=1}^K \\theta_k {\\mathbf u}_k {\\mathbf u}_k^\\top,\n",
    "$$\n",
    "\n",
    "where ${\\mathbf u}_k$ are the eigenvectors and $\\theta_k$ are the learnable parameters. The variable $K$ is the number of eigenvectors used (i.e., the rank of the filter). The weight $\\theta_k$ is learned to maximize the performance of the task at hand.\n",
    "\n",
    "Building on this idea, {footcite}`bruna2014spectral` added a nonlinearity to the filter and proposed a spectral convolutional neural network (GCN) by\n",
    "\n",
    "$$\n",
    "{\\bf x}^{(\\ell+1)} = h\\left( L_{\\text{learn}} {\\bf x}^{(\\ell)}\\right),\n",
    "$$\n",
    "\n",
    "where $h$ is an activation function, and ${\\bf x}^{(\\ell)}$ is the feature vector of the $\\ell$-th convolution. They further extend this idea to convolve on multidimensional feature vectors, ${\\bf X} \\in \\mathbb{R}^{N \\times f_{\\text{in}}}$ to produce new feature vectors of different dimensionality, ${\\bf X}' \\in \\mathbb{R}^{N \\times f_{\\text{out}}}$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\bf X}^{(\\ell+1)}_i &= h\\left( \\sum_j L_{\\text{learn}}^{(i,j)} {\\bf X}^{(\\ell)}_j\\right),\\quad \\text{where} \\quad L^{(i,j)}_{\\text{learn}} = \\sum_{k=1}^K \\theta_{k, (i,j)} {\\mathbf u}_k {\\mathbf u}_k^\\top,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Notice that the learnable filter $L_{\\text{learn}}^{(i,j)}$ is defined for each pair of input $i$ and output $j$ dimensions.\n",
    "\n",
    "\n",
    "```{note}\n",
    "Many GCNs simple when it comes to implementation despite the complicated formula. And this is one of my ways to learn GNNs. Check out the [Appendix for the Python implementation](appendix.md).\n",
    "\n",
    "```\n",
    "\n",
    "## From Spectral to Spatial\n",
    "\n",
    "Spectral GCNs are mathematically elegant but have two main limitations:\n",
    "1. **Computational Limitation**: Computing the spectra of the Laplacian is expensive ${\\cal O}(N^3)$ and prohibitive for large graphs\n",
    "2. **Spatial Locality**: The learned filters are not spatially localized. A node can be influenced by all other nodes in the graph.\n",
    "\n",
    "These two limitations motivate the development of spatial GCNs.\n",
    "\n",
    "### ChebNet\n",
    "\n",
    "ChebNet {footcite}`defferrard2016convolutional` is one of the earliest spatial GCNs that bridges the gap between spectral and spatial domains.\n",
    "The key idea is to leverage Chebyshev polynomials to approximate ${\\bf L}_{\\text{learn}}$ by\n",
    "\n",
    "$$\n",
    "{\\bf L}_{\\text{learn}} \\approx \\sum_{k=0}^{K-1} \\theta_k T_k(\\tilde{{\\bf L}}), \\quad \\text{where} \\quad \\tilde{{\\bf L}} = \\frac{2}{\\lambda_{\\text{max}}}{\\bf L} - {\\bf I},\n",
    "$$\n",
    "\n",
    "where $\\tilde{{\\bf L}}$ is the scaled and normalized Laplacian matrix in order to have eigenvalues in the range of $[-1,1]$. The Chebyshev polynomials $T_k(\\tilde{{\\bf L}})$ transforms the eigenvalues $\\tilde{{\\bf L}}$ to the following recursively:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "T_0(\\tilde{{\\bf L}}) &= {\\bf I} \\\\\n",
    "T_1(\\tilde{{\\bf L}}) &= \\tilde{{\\bf L}} \\\\\n",
    "T_k(\\tilde{{\\bf L}}) &= 2\\tilde{{\\bf L}} T_{k-1}(\\tilde{{\\bf L}}) - T_{k-2}(\\tilde{{\\bf L}})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We then replace ${\\bf L}_{\\text{learn}}$ in the original spectral GCN with the Chebyshev polynomial approximation:\n",
    "\n",
    "$$\n",
    "{\\bf x}^{(\\ell+1)} = h\\left( \\sum_{k=0}^{K-1} \\theta_k T_k(\\tilde{{\\bf L}}){\\bf x}^{(\\ell)}\\right),\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $T_k(\\tilde{{\\bf L}})$ applies the k-th Chebyshev polynomial to the scaled Laplacian matrix\n",
    "- $\\theta_k$ are the learnable parameters\n",
    "- K is the order of the polynomial (typically small, e.g., K=3)\n",
    "\n",
    "### Graph Convolutional Networks by Kipf and Welling\n",
    "\n",
    "While ChebNet offers a principled way to approximate spectral convolutions, Kipf and Welling (2017) {footcite}`kipf2017semi` proposed an even simpler and highly effective variant called **Graph Convolutional Networks (GCN)**.\n",
    "\n",
    "\n",
    "#### First-order Approximation\n",
    "\n",
    "The key departure is to use the first-order approximation of the Chebyshev polynomials.\n",
    "\n",
    "$$\n",
    "g_{\\theta'} * x \\approx \\theta'_0x + \\theta'_1(L - I_N)x = \\theta'_0x - \\theta'_1D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}x\n",
    "$$\n",
    "\n",
    "This is crude approximation but it leads to a much simpler form, leaving only two learnable parameters, instead of $K$ parameters in the original ChebNet.\n",
    "\n",
    "Additionally, they further simplify the formula by using the same $\\theta$ for both remaining parameters (i.e., $\\theta_0 = \\theta$ and $\\theta_1 = -\\theta$). The result is the following convolutional filter:\n",
    "\n",
    "$$\n",
    "g_{\\theta} * x \\approx \\theta(I_N + D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}})x\n",
    "$$\n",
    "\n",
    "While this is a very simple filter, one can stack multiple layers of convolutions to perform high-order graph convolutions.\n",
    "\n",
    "#### Deep GCNs can suffer from over-smoothing\n",
    "\n",
    "GCN models can be deep, and when they are too deep, they start suffering from an ill-posed problem called *gradient vanishing/exploding*, where the gradients of the loss function becomes too small or too large to update the model parameters. It is a common problem in deep learning.\n",
    "\n",
    "To facilitate the training of deep GCNs, the authors introduce a very simple trick called *renormalization*. The idea is to add self-connections to the graph:\n",
    "\n",
    "$$\n",
    "\\tilde{A} = A + I_N, \\quad \\text{and} \\quad \\tilde{D}_{ii} = \\sum_j \\tilde{A}_{ij}\n",
    "$$\n",
    "\n",
    "And use $\\tilde{A}$ and $\\tilde{D}$ to form the convolutional filter.\n",
    "\n",
    "Altogether, this leads to the following layer-wise propagation rule:\n",
    "\n",
    "$$X^{(\\ell+1)} = \\sigma(\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}X^{(\\ell)}W^{(\\ell)})$$\n",
    "\n",
    "where:\n",
    "- $X^{(\\ell)}$ is the matrix of node features at layer $\\ell$\n",
    "- $W^{(\\ell)}$ is the layer's trainable weight matrix\n",
    "- $\\sigma$ is a nonlinear activation function (e.g., ReLU)\n",
    "\n",
    "These simplifications offer several advantages:\n",
    "- **Efficiency**: Linear complexity in number of edges\n",
    "- **Localization**: Each layer only aggregates information from immediate neighbors\n",
    "- **Depth**: Fewer parameters allow building deeper models\n",
    "- **Performance**: Despite (or perhaps due to) its simplicity, it often outperforms more complex models\n",
    "\n",
    "```{admonition} Exercise\n",
    ":class: note\n",
    "\n",
    "Let's implement a simple GCN model for node classification.\n",
    "[Coding Exercise](../../../notebooks/exercise-m09-graph-neural-net.ipynb)\n",
    "```\n",
    "\n",
    "\n",
    "```{footbibliography}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
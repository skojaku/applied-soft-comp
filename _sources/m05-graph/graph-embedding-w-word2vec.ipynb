{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdc1f21b",
   "metadata": {},
   "source": [
    "# Graph embedding with word2vec\n",
    "\n",
    "How can we apply word2vec to graph data? There is a critical challenge: word2vec takes sequence of words as input, while graph data are discrete and unordered. A solution to fill this gap is *random walk*, which transforms graph data into a sequence of nodes. Once we have a sequence of nodes, we can treat it as a sequence of words and apply word2vec.\n",
    "\n",
    "\n",
    "## DeepWalk\n",
    "\n",
    "![](https://dt5vp8kor0orz.cloudfront.net/7c56c256b9fbf06693da47737ac57fae803a5a4f/1-Figure1-1.png)\n",
    "\n",
    "DeepWalk is one of the pioneering works to apply word2vec to graph data {footcite}`perozzi2014deepwalk`. It views the nodes as words and the nodes random walks on the graph as sentences, and applies word2vec to learn the node embeddings.\n",
    "\n",
    "More specifically, the method contains the following steps:\n",
    "\n",
    "1. Sample multiple random walks from the graph.\n",
    "2. Treat the random walks as sentences and feed them to word2vev to learn the node embeddings.\n",
    "\n",
    "\n",
    "There are some technical details that we need to be aware of, which we will learn by implementing DeepWalk in the following exercise.\n",
    "\n",
    "### Exercise 01: Implement DeepWalk\n",
    "\n",
    "In this exercise, we implement DeepWalk step by step.\n",
    "\n",
    "#### Step 1: Data preparation\n",
    "\n",
    "We will use the karate club network as an example.\n",
    "\n",
    "**Load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12ca5423",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'igraph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01migraph\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnx\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'igraph'"
     ]
    }
   ],
   "source": [
    "import igraph\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "g = igraph.Graph.Famous(\"Zachary\")\n",
    "A = g.get_adjacency_sparse()\n",
    "\n",
    "# Add the community labels to the nodes for visualization\n",
    "g.vs[\"label\"] = np.unique([d[1]['club'] for d in nx.karate_club_graph().nodes(data=True)], return_inverse=True)[1]\n",
    "\n",
    "palette = sns.color_palette().as_hex()\n",
    "igraph.plot(g, vertex_color=[palette[label] for label in g.vs[\"label\"]], bbox=(300, 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dadeee1",
   "metadata": {},
   "source": [
    "#### Step 2: Generate random walks\n",
    "\n",
    "Next, we generate the training data for the word2vec model by generating multiple random walks starting from each node in the network.\n",
    "Let us first implement a function to sample random walks from a given network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e03f406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk(net, start_node, walk_length):\n",
    "    # Initialize the walk with the starting node\n",
    "    walk = [start_node]\n",
    "\n",
    "    # Continue the walk until the desired length is reached\n",
    "    while len(walk) < walk_length:\n",
    "        # Get the current node (the last node in the walk)\n",
    "        cur = walk[-1]\n",
    "\n",
    "        # Get the neighbors of the current node\n",
    "        cur_nbrs = list(net[cur].indices)\n",
    "\n",
    "        # If the current node has neighbors, randomly choose one and add it to the walk\n",
    "        if len(cur_nbrs) > 0:\n",
    "            walk.append(np.random.choice(cur_nbrs))\n",
    "        else:\n",
    "            # If the current node has no neighbors, terminate the walk\n",
    "            break\n",
    "\n",
    "    # Return the generated walk\n",
    "    return walk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec3c279",
   "metadata": {},
   "source": [
    "Generate 10 random walks of length 50 starting from each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb38e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = g.vcount()\n",
    "n_walkers_per_node = 10\n",
    "walk_length = 50\n",
    "walks = []\n",
    "for i in range(n_nodes):\n",
    "    for _ in range(n_walkers_per_node):\n",
    "        walks.append(random_walk(A, i, walk_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca94d5c",
   "metadata": {},
   "source": [
    "#### Step 3: Train the word2vec model\n",
    "Then, we feed the random walks to the word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5f99a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(walks, vector_size=32, window=3, min_count=1, sg=1, hs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1982da42",
   "metadata": {},
   "source": [
    "Here,\n",
    "\n",
    "- `vector_size` is the dimension of the embedding vectors.\n",
    "- `window` indicates the maximum distance between a word and its context words. For example, in the random walk `[0, 1, 2, 3, 4, 5, 6, 7]`, the context words of node 2 are `[0, 1, 3, 4, 5]` when `window=3`.\n",
    "- `min_count` is the minimum number of times a word must appear in the training data to be included in the vocabulary.\n",
    "\n",
    "Two parameters `sg=1` and `hs=1` indicate that we are using the skip-gram model with negative sampling. Let us understand what they mean in detail as follows.\n",
    "\n",
    "- **Skip-gram model**: it trains word2vec by predicting context words given a target word. For example, given the sentence \"The quick brown fox jumps over the lazy dog\", in the skip-gram model, given the target word \"fox\", the model will try to predict the context words \"quick\", \"brown\", \"jumps\", and \"over\". If `sg=0`, the input and output are swapped: the model will predict the target word from the context words, e.g., given the context words \"quick\", \"brown\", \"jumps\", and \"over\", the model will predict the target word \"fox\".\n",
    "\n",
    "- **Hierarchical softmax**: To understand hierarchical softmax better, let's break down how the word2vec model works. The goal of word2vec is to predict context words given a target word. For example, if our target word is $w_t$ and our context word is $w_c$, we want to find the probability of $w_c$ given $w_t$. This probability is calculated using the softmax function:\n",
    "\n",
    "    $$\n",
    "    P(w_c | w_t) = \\frac{\\exp(\\mathbf{v}_{w_c} \\cdot \\mathbf{v}_{w_t})}{\\sum_{w \\in V} \\exp(\\mathbf{v}_w \\cdot \\mathbf{u}_{w_t})}\n",
    "   $$\n",
    "\n",
    "    Here, $\\mathbf{v}_w$ and $\\mathbf{u}_w$ represent the vector for word $w$ as context and target respectively, and $V$ is the entire vocabulary. The tricky part is the denominator, which requires summing over all words in the vocabulary. If we have a large vocabulary, this can be very computationally expensive. Imagine having to compute 100,000 exponentials and their sum for each training example if our vocabulary size is 100,000!\n",
    "\n",
    "    Hierarchical softmax helps us solve this problem. Instead of calculating the probability directly, it organizes the vocabulary into a binary tree, where each word is a leaf node. To find the probability of a word, we calculate the product of probabilities along the path from the root to the leaf node. This method significantly reduces the computational complexity. Instead of being proportional to the vocabulary size, it becomes proportional to the logarithm of the vocabulary size. This makes it much more efficient, especially for large vocabularies.\n",
    "\n",
    "    ![](https://lh5.googleusercontent.com/proxy/_omrC8G6quTl2SGarwFe57qzbIs-PtGkEA5yODFE5I0Ny2IHGiJwsUhMrcuUqg5o-R2nD9hkgMuZsQJKoCggP29zXtj-Vz-X8BE)\n",
    "\n",
    "\n",
    "By using the skip-gram model with hierarchical softmax, we can efficiently learn high-quality word embeddings even when dealing with large vocabularies.\n",
    "\n",
    "Now, we extract the node embeddings from the word2vec model. In the word2vec model, the embeddings are stored in the `wv` attribute. The embedding of node $i$ is given by `model.wv[i]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bccd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = []\n",
    "for i in range(n_nodes):\n",
    "    embedding.append(model.wv[i])\n",
    "embedding = np.array(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3441ea62",
   "metadata": {},
   "source": [
    "`embedding` is the matrix of node embeddings. It has the same number of rows as the number of nodes in the network, and the number of columns is the embedding dimension.\n",
    "\n",
    "**Print the first 3 nodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98ea0da",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "embedding[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92019223",
   "metadata": {},
   "source": [
    "Let's visualize the node embeddings using UMAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b5c695",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import umap\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "\n",
    "\n",
    "reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, metric=\"cosine\")\n",
    "xy = reducer.fit_transform(embedding)\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "# Calculate the degree of each node\n",
    "degrees = A.sum(axis=1).A1\n",
    "\n",
    "source = ColumnDataSource(data=dict(\n",
    "    x=xy[:, 0],\n",
    "    y=xy[:, 1],\n",
    "    size=np.sqrt(degrees / np.max(degrees)) * 30,\n",
    "    community=[palette[label] for label in g.vs[\"label\"]]\n",
    "))\n",
    "\n",
    "p = figure(title=\"Node Embeddings from Word2Vec\", x_axis_label=\"X\", y_axis_label=\"Y\")\n",
    "\n",
    "p.scatter('x', 'y', size='size', source=source, line_color=\"black\", color=\"community\")\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d2148f",
   "metadata": {},
   "source": [
    "#### Step 4: Clustering\n",
    "\n",
    "One of the interesting applications with node embeddings is clustering. While we have good community detection methods, like the modularity maximization and stochastic block model, we can use clustering methods from machine learning, such as $K$-means and Gaussian mixture model. Let's see what we can get from the node embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9dd603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Determine the optimal number of clusters using the silhouette score\n",
    "def Kmeans_with_silhouette(embedding, n_clusters_range=(2, 10)):\n",
    "    silhouette_scores = []\n",
    "\n",
    "    # Iterate over a range of cluster numbers from 2 to 9\n",
    "    for n_clusters in range(*n_clusters_range):\n",
    "        # Create a KMeans object with the current number of clusters\n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "\n",
    "        # Fit the KMeans model to the embedding data\n",
    "        kmeans.fit(embedding)\n",
    "\n",
    "        # Calculate the silhouette score for the current clustering\n",
    "        score = silhouette_score(embedding, kmeans.labels_)\n",
    "\n",
    "        # Append the number of clusters and its corresponding silhouette score to the list\n",
    "        silhouette_scores.append((n_clusters, score))\n",
    "\n",
    "    # Find the number of clusters that has the highest silhouette score\n",
    "    optimal_n_clusters = max(silhouette_scores, key=lambda x: x[1])[0]\n",
    "\n",
    "    # Create a KMeans object with the optimal number of clusters\n",
    "    kmeans = KMeans(n_clusters=optimal_n_clusters)\n",
    "\n",
    "    # Fit the KMeans model to the embedding data with the optimal number of clusters\n",
    "    kmeans.fit(embedding)\n",
    "\n",
    "    # Return the labels (cluster assignments) for each data point\n",
    "    return kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed8d17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "labels = Kmeans_with_silhouette(embedding)\n",
    "cmap = sns.color_palette().as_hex()\n",
    "igraph.plot(g, vertex_color=[cmap[label] for label in labels], bbox=(500, 500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dc7b1f",
   "metadata": {},
   "source": [
    "## node2vec\n",
    "\n",
    "node2vec is a sibling of DeepWalk proposed by {footcite}`grover2016node2vec`. Both use word2vec trained on random walks on networks. So, it appears that they are very similar. However, the following two components make them very different.\n",
    "\n",
    "- **Biased random walk**: node2vec uses biased random walks that can move in different directions. The bias walk is parameterized by two parameters, $p$ and $q$:\n",
    "\n",
    "    $$\n",
    "    P(v_{t+1} = x | v_t = v, v_{t-1} = t) \\propto\n",
    "    \\begin{cases}\n",
    "    \\frac{1}{p} & \\text{if } d(v,t) = 0 \\\\\n",
    "    1 & \\text{if } d(v,t) = 1 \\\\\n",
    "    \\frac{1}{q} & \\text{if } d(v,t) = 2 \\\\\n",
    "    \\end{cases}\n",
    "    $$\n",
    "\n",
    "    where $d(v,x)$ is the shortest path distance between node $v$ and $x$. A smaller $p$ leads to more biased towards the previous node, $v_{t-1} = t$. A smaller $q$ leads to more biased towards the nodes that are further away from the previous node, $v_{t-1} = t$.\n",
    "\n",
    "    By adjusting the parameters $p$ and $q$, we can influence the random walk to behave more like either breadth-first sampling (BFS) or depth-first sampling (DFS).\n",
    "\n",
    "    - **Breadth-First Sampling (BFS)**: This type of sampling explores all the neighbors of a node before moving on to the next level of neighbors. It is useful for capturing community structures within the graph. When we set the parameters to favor BFS, the resulting embeddings will reflect these community structures.\n",
    "\n",
    "    - **Depth-First Sampling (DFS)**: This type of sampling goes deep into the graph, exploring as far as possible along each branch before backtracking. It is useful for capturing structural equivalence, where nodes that have similar roles in the graph (even if they are not directly connected) are represented similarly. When we set the parameters to favor DFS, the resulting embeddings will reflect these structural equivalences.\n",
    "\n",
    "    ![](https://www.researchgate.net/publication/354654762/figure/fig3/AS:1069013035655173@1631883977008/A-biased-random-walk-procedure-of-node2vec-B-BFS-and-DFS-search-strategies-from-node-u.png)\n",
    "\n",
    "    The embeddings generated by node2vec can capture different aspects of the graph depending on the sampling strategy used. With BFS, we capture community structures, and with DFS, we capture structural equivalence.\n",
    "\n",
    "    ![](https://miro.medium.com/v2/resize:fit:1138/format:webp/1*nCyF5jFSU5uJVdAPdf-0HA.png)\n",
    "\n",
    "- **Negative sampling**: node2vec uses negative sampling, instead of hierarchical softmax. This difference appears to be minor, but it has significant consequences on the characteristics of the embeddings. This is beyond the scope of this lecture, but you can refer to {footcite}`kojaku2021neurips` and {footcite}`dyer2014notes` for more details.\n",
    "\n",
    "\n",
    "### Exercise 02: Implement node2vec\n",
    "\n",
    "Let's implement the biased random walk for node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8390497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node2vec_random_walk(net, start_node, walk_length, p, q):\n",
    "    \"\"\"\n",
    "    Sample a random walk starting from start_node.\n",
    "    \"\"\"\n",
    "    # Initialize the walk with the start_node\n",
    "    walk = [start_node]\n",
    "\n",
    "    # Continue the walk until it reaches the desired length\n",
    "    while len(walk) < walk_length:\n",
    "        # Get the current node in the walk\n",
    "        cur = walk[-1]\n",
    "        # Get the neighbors of the current node\n",
    "        cur_nbrs = list(net[cur].indices)\n",
    "        # Check if the current node has any neighbors\n",
    "        if len(cur_nbrs) > 0:\n",
    "            # If the walk has just started, randomly choose the next node from the neighbors\n",
    "            if len(walk) == 1:\n",
    "                walk.append(np.random.choice(cur_nbrs))\n",
    "            else:\n",
    "                # Get the previous node in the walk\n",
    "                prev = walk[-2]\n",
    "                # Use the alias sampling method to choose the next node based on the bias parameters p and q\n",
    "                next_node = alias_sample(net, cur_nbrs, prev, p, q)\n",
    "                # Append the chosen next node to the walk\n",
    "                walk.append(next_node)\n",
    "        else:\n",
    "            # If the current node has no neighbors, terminate the walk\n",
    "            break\n",
    "\n",
    "    return walk\n",
    "\n",
    "def alias_sample(net, neighbors, prev, p, q):\n",
    "    \"\"\"\n",
    "    Helper function to sample the next node in the walk.\n",
    "    \"\"\"\n",
    "    # Implement the logic to sample the next node based on the bias parameters p and q\n",
    "    # You can use the formula provided in the instructions to calculate the probabilities\n",
    "    # and then sample the next node accordingly.\n",
    "    # Initialize an empty list to store the unnormalized probabilities for each neighbor\n",
    "    unnormalized_probs = []\n",
    "\n",
    "    # Iterate over each neighbor of the current node\n",
    "    for neighbor in neighbors:\n",
    "        # If the neighbor is the same as the previous node in the walk\n",
    "        if neighbor == prev:\n",
    "            # Append the probability 1/p to the unnormalized probabilities list\n",
    "            unnormalized_probs.append(1 / p)\n",
    "        # If the neighbor is connected to the previous node in the walk\n",
    "        elif neighbor in net[prev].indices:\n",
    "            # Append the probability 1 to the unnormalized probabilities list\n",
    "            unnormalized_probs.append(1)\n",
    "        # If the neighbor is not connected to the previous node in the walk\n",
    "        else:\n",
    "            # Append the probability 1/q to the unnormalized probabilities list\n",
    "            unnormalized_probs.append(1 / q)\n",
    "\n",
    "    # Calculate the normalization constant by summing all unnormalized probabilities\n",
    "    norm_const = sum(unnormalized_probs)\n",
    "\n",
    "    # Normalize the probabilities by dividing each unnormalized probability by the normalization constant\n",
    "    normalized_probs = [float(prob) / norm_const for prob in unnormalized_probs]\n",
    "\n",
    "    # Randomly choose the next node from the neighbors based on the normalized probabilities\n",
    "    next_node = np.random.choice(neighbors, size=1, p=normalized_probs)[0]\n",
    "\n",
    "    # Return the chosen next node\n",
    "    return next_node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb293b62",
   "metadata": {},
   "source": [
    "Now, let's set up the word2vec model for node2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab2b11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "walks = []\n",
    "p = 1\n",
    "q = 0.1\n",
    "for i in range(n_nodes):\n",
    "    for _ in range(n_walkers_per_node):\n",
    "        walks.append(node2vec_random_walk(A, i, walk_length, p, q))\n",
    "model = Word2Vec(walks, vector_size=32, window=3, min_count=1, sg=1, hs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a8cd65",
   "metadata": {},
   "source": [
    "where `hs=0` indicates that we are using negative sampling.\n",
    "Notice that we set `sg=1` and `hs=1` instead of `sg=1` and `hs=0` in DeepWalk. This is because node2vec uses the skip-gram model with negative sampling.\n",
    "\n",
    "Now, we extract the node embeddings from the word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da550d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = []\n",
    "for i in range(n_nodes):\n",
    "    embedding.append(model.wv[i])\n",
    "embedding = np.array(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d507d7",
   "metadata": {},
   "source": [
    "Let's visualize the node embeddings from node2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b371ec",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, metric=\"cosine\")\n",
    "xy = reducer.fit_transform(embedding)\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "# Calculate the degree of each node\n",
    "degrees = A.sum(axis=1).A1\n",
    "\n",
    "source = ColumnDataSource(data=dict(\n",
    "    x=xy[:, 0],\n",
    "    y=xy[:, 1],\n",
    "    size=np.sqrt(degrees / np.max(degrees)) * 30,\n",
    "    community=[palette[label] for label in g.vs[\"label\"]],\n",
    "    name = [str(i) for i in range(n_nodes)]\n",
    "))\n",
    "\n",
    "p = figure(title=\"Node Embeddings from Word2Vec\", x_axis_label=\"X\", y_axis_label=\"Y\")\n",
    "\n",
    "p.scatter('x', 'y', size='size', source=source, line_color=\"black\", color=\"community\")\n",
    "\n",
    "hover = HoverTool()\n",
    "hover.tooltips = [\n",
    "    (\"Name\", \"@name\"),\n",
    "    (\"Community\", \"@community\")\n",
    "]\n",
    "p.add_tools(hover)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db81fa3f",
   "metadata": {},
   "source": [
    "The results for clustering are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9b640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "labels = Kmeans_with_silhouette(embedding)\n",
    "\n",
    "\n",
    "cmap = sns.color_palette().as_hex()\n",
    "igraph.plot(g, vertex_color=[cmap[label] for label in labels], bbox=(500, 500), vertex_label=[\"%d\" %  d for d in  np.arange(n_nodes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e4078e",
   "metadata": {},
   "source": [
    "## LINE\n",
    "\n",
    "LINE {footcite}`tang2015line` is another pioneering work to learn node embeddings by directly optimizing the graph structure.\n",
    "It is equivalent to node2vec with $p=1$, $q=1$, and window size 1.\n",
    "\n",
    "\n",
    "```{footbibliography}\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d53fe943",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## Bruna's Spectral GCN\n",
    "\n",
    "\n",
    "Let's first implement Bruna's spectral GCN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf5476c3",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Users/skojaku-admin/miniforge3/envs/applsoftcomp/lib/python3.10/site-packages/torch/_C.cpython-310-darwin.so, 0x0002): Symbol not found: __ZN2at3cpu20is_arm_sve_supportedEv\n  Referenced from: <D93D20A5-7F4C-32F1-874B-FE7B416FFD91> /Users/skojaku-admin/miniforge3/envs/applsoftcomp/lib/python3.10/site-packages/torch/lib/libtorch_python.dylib\n  Expected in:     <616791F0-29C3-3F88-8A88-D072E7E40979> /Users/skojaku-admin/miniforge3/envs/applsoftcomp/lib/libtorch_cpu.dylib",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mslinalg\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.10/site-packages/torch/__init__.py:367\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    366\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSymInt\u001b[39;00m:\n\u001b[1;32m    371\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m    Like an int (including magic methods), but redirects all operations on the\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    wrapped node. This is used in particular to symbolically record operations\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    in the symbolic shape workflow.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Users/skojaku-admin/miniforge3/envs/applsoftcomp/lib/python3.10/site-packages/torch/_C.cpython-310-darwin.so, 0x0002): Symbol not found: __ZN2at3cpu20is_arm_sve_supportedEv\n  Referenced from: <D93D20A5-7F4C-32F1-874B-FE7B416FFD91> /Users/skojaku-admin/miniforge3/envs/applsoftcomp/lib/python3.10/site-packages/torch/lib/libtorch_python.dylib\n  Expected in:     <616791F0-29C3-3F88-8A88-D072E7E40979> /Users/skojaku-admin/miniforge3/envs/applsoftcomp/lib/libtorch_cpu.dylib"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy.sparse.linalg as slinalg\n",
    "\n",
    "class BrunaGraphConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Bruna's Spectral Graph Convolution Layer\n",
    "\n",
    "    This implementation follows the original formulation by Joan Bruna et al.,\n",
    "    using the eigendecomposition of the graph Laplacian for spectral convolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, n_nodes):\n",
    "        \"\"\"\n",
    "        Initialize the Bruna Graph Convolution layer\n",
    "\n",
    "        Args:\n",
    "            in_features (int): Number of input features\n",
    "            out_features (int): Number of output features\n",
    "        \"\"\"\n",
    "        super(BrunaGraphConv, self).__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # Learnable spectral filter parameters\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.FloatTensor(in_features, out_features, n_nodes-1)\n",
    "        )\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Initialize weights using Glorot initialization\"\"\"\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_laplacian_eigenvectors(adj):\n",
    "        \"\"\"\n",
    "        Compute eigendecomposition of the normalized graph Laplacian\n",
    "\n",
    "        Args:\n",
    "            adj: Adjacency matrix\n",
    "\n",
    "        Returns:\n",
    "            eigenvalues, eigenvectors of the normalized Laplacian\n",
    "        \"\"\"\n",
    "        # Compute normalized Laplacian\n",
    "        # Add self-loops\n",
    "        adj = adj + sp.eye(adj.shape[0])\n",
    "\n",
    "        # Compute degree matrix\n",
    "        deg = np.array(adj.sum(axis=1))\n",
    "        Dsqrt_inv = sp.diags(1.0 / np.sqrt(deg).flatten())\n",
    "\n",
    "        # Compute normalized Laplacian: D^(-1/2) A D^(-1/2)\n",
    "        laplacian = sp.eye(adj.shape[0]) - Dsqrt_inv @ adj @ Dsqrt_inv\n",
    "\n",
    "        # Compute eigendecomposition\n",
    "        # Using k=adj.shape[0]-1 to get all non-zero eigenvalues\n",
    "        eigenvals, eigenvecs = slinalg.eigsh(laplacian.tocsc(), k=adj.shape[0]-1,which='SM', tol=1e-6)\n",
    "\n",
    "        return torch.FloatTensor(eigenvals), torch.FloatTensor(eigenvecs)\n",
    "\n",
    "    def forward(self, x, eigenvecs):\n",
    "        \"\"\"\n",
    "        Forward pass implementing Bruna's spectral convolution\n",
    "\n",
    "        Args:\n",
    "            x: Input features [num_nodes, in_features]\n",
    "            eigenvecs: Eigenvectors of the graph Laplacian [num_nodes, num_nodes-1]\n",
    "\n",
    "        Returns:\n",
    "            Output features [num_nodes, out_features]\n",
    "        \"\"\"\n",
    "        # Transform to spectral domain\n",
    "        x_spectral = torch.matmul(eigenvecs.t(), x)  # [num_nodes-1, in_features]\n",
    "\n",
    "        # Initialize output tensor\n",
    "        out = torch.zeros(x.size(0), self.out_features, device=x.device)\n",
    "\n",
    "        # For each input-output feature pair\n",
    "        for i in range(self.in_features):\n",
    "            for j in range(self.out_features):\n",
    "                # Element-wise multiplication in spectral domain\n",
    "                # This is the actual spectral filtering operation\n",
    "                filtered = x_spectral[:, i] * self.weight[i, j, :]  # [num_spectrum]\n",
    "\n",
    "                # Transform back to spatial domain and accumulate\n",
    "                out[:, j] += torch.matmul(eigenvecs, filtered)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993232cd",
   "metadata": {},
   "source": [
    "Next, we will train the model on the karate club network to predict the given node labels indicating nodes' community memberships. We load the data by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6fd72b",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load karate club network\n",
    "G = nx.karate_club_graph()\n",
    "adj = nx.to_scipy_sparse_array(G)\n",
    "features = torch.eye(G.number_of_nodes())\n",
    "labels = torch.tensor([G.nodes[i]['club'] == 'Officer' for i in G.nodes()], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57e9c6d",
   "metadata": {},
   "source": [
    "We apply the convolution twice with ReLu activation in between. This can be implemented by preparing two independent `BrunaGraphConv` layers, applying them consecutively, and adding a ReLu activation in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d43a45",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Define a simple GCN model\n",
    "class SimpleGCN(nn.Module):\n",
    "    def __init__(self, in_features, out_features, hidden_features, n_nodes):\n",
    "        super(SimpleGCN, self).__init__()\n",
    "        self.conv1 = BrunaGraphConv(in_features, hidden_features, n_nodes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = BrunaGraphConv(hidden_features, out_features, n_nodes)\n",
    "\n",
    "    def forward(self, x, eigenvecs):\n",
    "        x = self.conv1(x, eigenvecs)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x, eigenvecs)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3948a0c",
   "metadata": {},
   "source": [
    "We then train the model by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24c1343",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get eigenvectors of the Laplacian\n",
    "eigenvals, eigenvecs = BrunaGraphConv.get_laplacian_eigenvectors(adj)\n",
    "\n",
    "# Initialize the model\n",
    "hidden_features = 10\n",
    "input_features = features.shape[1]\n",
    "output_features = 2\n",
    "n_nodes = G.number_of_nodes()\n",
    "model = SimpleGCN(input_features, output_features, hidden_features, n_nodes)\n",
    "\n",
    "# Train the model\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_idx, test_idx = train_test_split(np.arange(G.number_of_nodes()), test_size=0.2, random_state=42)\n",
    "train_features = features[train_idx]\n",
    "train_labels = labels[train_idx]\n",
    "test_features = features[test_idx]\n",
    "test_labels = labels[test_idx]\n",
    "\n",
    "\n",
    "n_train = 100\n",
    "for epoch in range(n_train):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_features, eigenvecs[train_idx, :])\n",
    "    loss = criterion(output, train_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate the model\n",
    "    if epoch == 0 or (epoch+1) % 25 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(test_features, eigenvecs[test_idx, :])\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            accuracy = (predicted == test_labels).float().mean()\n",
    "            print(f'Epoch {epoch+1}/{n_train}, Loss: {loss.item():.4f}, Accuracy: {accuracy.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f0b4e9",
   "metadata": {},
   "source": [
    "Observe that the accuracy increases as the training progresses. We can use the model to predict the labels.\n",
    "The model has a hidden layer, and let's visualize the data in the hidden space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afe9ec3",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Visualize the learned embeddings\n",
    "embeddings = model.conv1(features, eigenvecs).detach().numpy()\n",
    "\n",
    "xy = TSNE(n_components=2).fit_transform(embeddings)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.scatterplot(x = xy[:, 0].reshape(-1), y = xy[:, 1].reshape(-1), hue=labels.numpy(), palette='tab10', ax = ax)\n",
    "ax.set_title(\"Learned Node Embeddings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1560c198",
   "metadata": {},
   "source": [
    "## ChebNet\n",
    "\n",
    "Let's implement the ChebNet layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122d1ba2",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy.sparse as sp\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse(sparse_mx):\n",
    "    \"\"\"Convert scipy sparse matrix to torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo()\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64)\n",
    "    )\n",
    "    values = torch.from_numpy(sparse_mx.data.astype(np.float32))\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse_coo_tensor(indices, values, shape)\n",
    "\n",
    "\n",
    "class ChebConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Chebyshev Spectral Graph Convolutional Layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, K: int, bias: bool = True):\n",
    "        super(ChebConv, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.K = K\n",
    "\n",
    "        # Trainable parameters\n",
    "        self.weight = nn.Parameter(torch.Tensor(K, in_channels, out_channels))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Initialize parameters.\"\"\"\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)\n",
    "\n",
    "    def _normalize_laplacian(self, adj_matrix):\n",
    "        \"\"\"\n",
    "        Compute normalized Laplacian L = I - D^(-1/2)AD^(-1/2)\n",
    "        \"\"\"\n",
    "        # Convert to scipy if it's not already\n",
    "        if not sp.isspmatrix(adj_matrix):\n",
    "            adj_matrix = sp.csr_matrix(adj_matrix)\n",
    "\n",
    "        adj_matrix = adj_matrix.astype(float)\n",
    "\n",
    "        # Compute degree matrix D\n",
    "        rowsum = np.array(adj_matrix.sum(1)).flatten()\n",
    "        d_inv_sqrt = np.power(rowsum, -0.5)\n",
    "        d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.0\n",
    "        d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "\n",
    "        # Compute L = I - D^(-1/2)AD^(-1/2)\n",
    "        n = adj_matrix.shape[0]\n",
    "        L = sp.eye(n) - d_mat_inv_sqrt @ adj_matrix @ d_mat_inv_sqrt\n",
    "        return L\n",
    "\n",
    "    def _scale_laplacian(self, L):\n",
    "        \"\"\"\n",
    "        Scale Laplacian eigenvalues to [-1, 1] interval\n",
    "        L_scaled = 2L/lambda_max - I\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Compute largest eigenvalue\n",
    "            eigenval, _ = sp.linalg.eigsh(L, k=1, which=\"LM\", return_eigenvectors=False)\n",
    "            lambda_max = eigenval[0]\n",
    "        except:\n",
    "            # Approximate lambda_max = 2 if eigenvalue computation fails\n",
    "            lambda_max = 2.0\n",
    "\n",
    "        n = L.shape[0]\n",
    "        L_scaled = (2.0 / lambda_max) * L - sp.eye(n)\n",
    "        return L_scaled\n",
    "\n",
    "    def chebyshev_basis(self, L_sparse: torch.sparse.Tensor, X: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute Chebyshev polynomials basis up to order K.\n",
    "        \"\"\"\n",
    "        # List to store Chebyshev polynomials\n",
    "        cheb_polynomials = []\n",
    "\n",
    "        # T_0(L) = I\n",
    "        cheb_polynomials.append(X)\n",
    "\n",
    "        if self.K > 1:\n",
    "            # T_1(L) = L\n",
    "            X_1 = torch.sparse.mm(L_sparse, X)\n",
    "            cheb_polynomials.append(X_1)\n",
    "\n",
    "        # Recurrence T_k(L) = 2LÂ·T_{k-1}(L) - T_{k-2}(L)\n",
    "        for k in range(2, self.K):\n",
    "            X_k = (\n",
    "                2 * torch.sparse.mm(L_sparse, cheb_polynomials[k - 1])\n",
    "                - cheb_polynomials[k - 2]\n",
    "            )\n",
    "            cheb_polynomials.append(X_k)\n",
    "\n",
    "        return torch.stack(cheb_polynomials, dim=0)  # [K, num_nodes, in_channels]\n",
    "\n",
    "    def forward(self, X: torch.Tensor, adj_matrix: sp.spmatrix):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            X: Node features tensor of shape [num_nodes, in_channels]\n",
    "            adj_matrix: Adjacency matrix in scipy sparse format\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape [num_nodes, out_channels]\n",
    "        \"\"\"\n",
    "        # Compute normalized and scaled Laplacian\n",
    "        L_norm = self._normalize_laplacian(adj_matrix)\n",
    "        L_scaled = self._scale_laplacian(L_norm)\n",
    "\n",
    "        # Convert to torch sparse tensor\n",
    "        L_scaled = sparse_mx_to_torch_sparse(L_scaled).to(X.device)\n",
    "\n",
    "        # Compute Chebyshev polynomials basis\n",
    "        Tx = self.chebyshev_basis(L_scaled, X)  # [K, num_nodes, in_channels]\n",
    "\n",
    "        # Perform convolution using learned weights\n",
    "        out = torch.einsum(\"kni,kio->no\", Tx, self.weight)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4a9d59",
   "metadata": {},
   "source": [
    "We stack the layers to form a simple GCN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068bd97b",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class ChebNet(nn.Module):\n",
    "    \"\"\"\n",
    "    ChebNet model for node classification\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        hidden_channels: int,\n",
    "        out_channels: int,\n",
    "        K: int,\n",
    "        num_layers: int,\n",
    "        dropout: float = 0.5,\n",
    "    ):\n",
    "        super(ChebNet, self).__init__()\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "\n",
    "        # First layer\n",
    "        self.convs.append(ChebConv(in_channels, hidden_channels, K))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(ChebConv(hidden_channels, hidden_channels, K))\n",
    "\n",
    "        # Output layer\n",
    "        self.convs.append(ChebConv(hidden_channels, out_channels, K))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, X: torch.Tensor, adj_matrix: sp.spmatrix):\n",
    "        \"\"\"\n",
    "        Forward pass through all layers\n",
    "        \"\"\"\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            X = conv(X, adj_matrix)\n",
    "            X = self.activation(X)\n",
    "            X = self.dropout(X)\n",
    "\n",
    "        # Output layer\n",
    "        X = self.convs[-1](X, adj_matrix)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86959ab5",
   "metadata": {},
   "source": [
    "Let's train the model on the karate club network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6198bd52",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import networkx as nx\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load karate club network\n",
    "G = nx.karate_club_graph()\n",
    "adj = nx.to_scipy_sparse_array(G)\n",
    "features = torch.eye(G.number_of_nodes())\n",
    "labels = torch.tensor(\n",
    "    [G.nodes[i][\"club\"] == \"Officer\" for i in G.nodes()], dtype=torch.long\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "hidden_features = 10\n",
    "input_features = features.shape[1]\n",
    "output_features = 2\n",
    "n_nodes = G.number_of_nodes()\n",
    "K = 3\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "model = ChebNet(\n",
    "    input_features, hidden_features, output_features, K, num_layers, dropout\n",
    ")\n",
    "\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train the model\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_idx, test_idx = train_test_split(\n",
    "    np.arange(G.number_of_nodes()), test_size=0.2, random_state=42\n",
    ")\n",
    "train_features = features[train_idx]\n",
    "train_labels = labels[train_idx]\n",
    "test_features = features[test_idx]\n",
    "test_labels = labels[test_idx]\n",
    "\n",
    "\n",
    "n_train = 100\n",
    "for epoch in range(n_train):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss = criterion(output[train_idx], train_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate the model\n",
    "    if epoch == 0 or (epoch + 1) % 25 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(features, adj)\n",
    "            _, predicted = torch.max(output[test_idx], 1)\n",
    "            accuracy = (predicted == test_labels).float().mean()\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{n_train}, Loss: {loss.item():.4f}, Accuracy: {accuracy.item():.4f}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c936decb",
   "metadata": {},
   "source": [
    "Let's visualize the learned embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fc2dfd",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get embeddings from the last hidden layer\n",
    "    X_hidden = features\n",
    "    for conv in model.convs[:-1]:\n",
    "        X_hidden = conv(X_hidden, adj)\n",
    "        X_hidden = model.activation(X_hidden)\n",
    "\n",
    "# Reduce dimensionality for visualization\n",
    "xy = TSNE(n_components=2).fit_transform(X_hidden.numpy())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.scatterplot(\n",
    "    x=xy[:, 0].reshape(-1),\n",
    "    y=xy[:, 1].reshape(-1),\n",
    "    hue=labels.numpy(),\n",
    "    palette=\"tab10\",\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title(\"Learned Node Embeddings\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Transformers &#8212; Applied Soft Computing</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'm03-transformers/transformers';</script>
    <script src="../_static/js/custom.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Bidirectional Encoder Representations from Transformers (BERT)" href="bert.html" />
    <link rel="prev" title="Module 3: Transformers" href="what-to-learn.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../home.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.jpg" class="logo__image only-light" alt="Applied Soft Computing - Home"/>
    <script>document.write(`<img src="../_static/logo.jpg" class="logo__image only-dark" alt="Applied Soft Computing - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../home.html">
                    Welcome to SSIE 541/441 Applied Soft Computing
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Intro</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../intro/why-applied-soft-computing.html">Why applied soft computing?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/setup.html">Set up</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/deliverables.html">Deliverables</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Word and Document Embeddings</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/what-to-learn.html">Module 1: Word Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/pen-and-paper.html">Pen and Paper Exercise</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/tf-idf.html">Teaching computers how to understand words</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/word2vec.html">word2vec: a small model with a big idea</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/word2vec_plus.html">GloVe and FastText: Building on Word2Vec’s Foundation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/sem-axis.html">Understanding SemAxis: Semantic Axes in Word Vector Spaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/bias-in-embedding.html">Bias in Word Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/doc2vec.html">Doc2Vec: From Words to Documents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/summary.html">Summary</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Recurrent Neural Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/what-to-learn.html">Module 2: Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/rnn-interactive.html">🧠 Learn RNNs Through Physics!</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/reccurrent-neural-net.html">Recurrent Neural Network (RNN)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/lstm.html">Long Short-Term Memory (LSTM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/elmo.html">Embedding from Language Models (ELMo)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/seq2seq.html">Sequence-to-Sequence Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/summary.html">Summary</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Transformers</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="what-to-learn.html">Module 3: Transformers</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="bert.html">Bidirectional Encoder Representations from Transformers (BERT)</a></li>
<li class="toctree-l1"><a class="reference internal" href="sentence-bert.html">Sentence-BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpt.html">Generative Pre-trained Transformer (GPT)</a></li>
<li class="toctree-l1"><a class="reference internal" href="from-language-model-to-instruction-following.html">From Language Model to Instruction-Following</a></li>
<li class="toctree-l1"><a class="reference internal" href="prompt-tuning.html">Prompt Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="prompt-tuning-exercise.html">Prompt Tuning Exercise</a></li>
<li class="toctree-l1"><a class="reference internal" href="summary.html">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix-t5.html">Appendix: Text-to-Text Transfer Transformer (T5)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Teaching Machine How to Understand Images</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m04-image-processing/what-to-learn.html">Module 4: Image Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m04-image-processing/pen-and-paper.html">Pen and paper exercises</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m04-image-processing/image-processing.html">Preliminaries: Image Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m04-image-processing/cnn.html">From Traditional Image Processing to Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m04-image-processing/lenet.html">LeNet and LeNet-5: Pioneering CNN Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m04-image-processing/alexnet.html">AlexNet: A Breakthrough in Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m04-image-processing/vgg.html">VGGNet - A Deep Convolutional Neural Network for Image Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m04-image-processing/inception.html">GoogleNet and the Inception Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m04-image-processing/resnet.html">ResNet (Residual Neural Networks)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Teaching Machine How to Understand Graphs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/what-to-learn.html">Module 4: Graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/pen-and-paper.html">Pen and paper exercises</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/spectral-embedding.html">Spectral Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/graph-embedding-w-word2vec.html">Graph embedding with word2vec</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/spectral-vs-neural-embedding.html">Spectral vs Neural Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/from-image-to-graph.html">From Image to Graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/graph-convolutional-network.html">Graph Convolutional Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/popular-gnn.html">Popular Graph Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/appendix.html">Appendix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/software.html">Software for Network Embedding</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/skojaku/applied-soft-comp/gh-pages?urlpath=tree/docs/lecture-note/m03-transformers/transformers.md" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/skojaku/applied-soft-comp" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/skojaku/applied-soft-comp/issues/new?title=Issue%20on%20page%20%2Fm03-transformers/transformers.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/m03-transformers/transformers.ipynb" target="_blank"
   class="btn btn-sm btn-download-notebook-button dropdown-item"
   title="Download notebook file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li><a href="../_sources/m03-transformers/transformers.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Transformers</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-building-block-of-llms">A building block of LLMs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-mechanism">Attention Mechanism</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention">Self-Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention">Multi-Head Attention</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization">Layer Normalization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#wiring-it-all-together">Wiring it all together</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder-transformer-block">Encoder Transformer Block</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-connection">Residual Connection</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder-transformer-block">Decoder Transformer Block</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#masked-multi-head-attention">Masked Multi-Head Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-attention">Cross-Attention</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-miscellaneous-components">Other miscellaneous components</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#position-embedding">Position embedding</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="transformers">
<h1>Transformers<a class="headerlink" href="#transformers" title="Link to this heading">#</a></h1>
<p>Transformers are the cornerstone of modern NLP that gives rise to the recent success of LLMs. We will learn how transformers work and how they are used to build LLMs.</p>
<section id="a-building-block-of-llms">
<h2>A building block of LLMs<a class="headerlink" href="#a-building-block-of-llms" title="Link to this heading">#</a></h2>
<p>Many large language models (LLMs) including GPT-3, GPT-4, and Claude are built based on a stack of <em>transformer</em> blocks <a class="footnote-reference brackets" href="#footcite-vaswani2017attention" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>.
Each transformer block takes a sequence of token vectors as input and outputs a sequence of token vectors (sequence-to-sequence!).</p>
<figure class="align-center" id="transformer-overview">
<a class="reference internal image-reference" href="../_images/transformer-overview.jpg"><img alt="Transformer Overview" src="../_images/transformer-overview.jpg" style="width: 50%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 18 </span><span class="caption-text">The basic architecture of the transformer-based LLMs.</span><a class="headerlink" href="#transformer-overview" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>These transformer blocks can be further divided into encoder and decoder components.
The encoder is used for encoding the input sequence, while the decoder is used for generating the output sequence. Like seq2se models with attention, the decoder can also see the encoder outputs for invidiual tokens, along with the previous output tokens. The output of the decoder is then passed through a linear layer to produce the probability distribution over the next token.</p>
<figure class="align-center" id="transformer-encoder-decoder">
<a class="reference internal image-reference" href="../_images/transformer-encoder-decoder.jpg"><img alt="Transformer Encoder-Decoder" src="../_images/transformer-encoder-decoder.jpg" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 19 </span><span class="caption-text">The basic architecture of the transformer encoder-decoder. The encoder is used for encoding the input sequence, while the decoder is used for generating the output sequence. The encoder takes the input sequence as input and outputs a sequence of token vectors, which are then passed to the decoder. The decoder takes the encoder outputs, along with the previous output tokens, and outputs the probability distribution over the next token.</span><a class="headerlink" href="#transformer-encoder-decoder" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Inside the encoder and decoder transformer blocks are essentially three components, i.e., <em>multi-head attention</em>, <em>layer normalization</em>, and <em>feed-forward networks</em>. We will learn individual components in the following sections.</p>
<figure class="align-center" id="transformer-wired-components">
<a class="reference internal image-reference" href="../_images/transformer-component.jpg"><img alt="Transformer Wired Components" src="../_images/transformer-component.jpg" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 20 </span><span class="caption-text">The encoder-decoder architecture of the transformer.</span><a class="headerlink" href="#transformer-wired-components" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="attention-mechanism">
<h2>Attention Mechanism<a class="headerlink" href="#attention-mechanism" title="Link to this heading">#</a></h2>
<p>Perhaps the most crucial component of the transformer is the <em>attention mechanism</em>, which allows the model to pay attention to particular parts of the input sequence.</p>
<section id="self-attention">
<h3>Self-Attention<a class="headerlink" href="#self-attention" title="Link to this heading">#</a></h3>
<p>In transformers, the attention is called <em>self-attention</em>, since the attention is paid within the same sentence, unlike the sequence-to-sequence models that pays attention from one sentence to another. At its core, self-attention is about relationships. When you read the sentence “The cat sat on the mat because it was tired”, how do you know what “it” refers to? You naturally look back at the previous words and determine that “it” refers to “the cat”. Self-attention works similarly, but does this for every word in relation to every other word, simultaneously.</p>
<figure class="align-center" id="transformer-attention">
<a class="reference internal image-reference" href="../_images/transformer-attention.jpg"><img alt="Attention Mechanism" src="../_images/transformer-attention.jpg" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 21 </span><span class="caption-text">The attention mechanism in transformers.</span><a class="headerlink" href="#transformer-attention" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>To compute the attention between words, the attention head creates three types of vectors—<strong>query, key, and value</strong>—for each word. Each of these vectors are created by a neural network (w/ single linear layer) that takes the input word as input, and outputs another vector.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Think of this like a library system: The Query is what you’re looking for, the Keys are like book titles, and the Values are the actual content of the books. When you search (Q) for a specific topic, you match it against book titles (K) to find the relevant content (V).</p>
</div>
<p>The query and key vectors are used to compute the attention score, which represents how much attention the model pays to each key word for the query word, with a larger score indicating a stronger attention. For example, in the sentence “The cat sat on the mat because it was tired”, a good model should pay more attention to “cat” than “mat” for the word “it”. The atttention score computed by the dot product of the query and key vectors.  The score is then normalized by the softmax function, with rescaling by <span class="math notranslate nohighlight">\(\sqrt{d_k}\)</span> to prevent the score from becoming too large. More formally, the attention score is computed as:</p>
<div class="math notranslate nohighlight">
\[
\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right),
\]</div>
<p>where <span class="math notranslate nohighlight">\(Q \in \mathbb{R}^{n \times d_k}\)</span> is the query matrix containing <span class="math notranslate nohighlight">\(n\)</span> query vectors of dimension <span class="math notranslate nohighlight">\(d_k\)</span>, <span class="math notranslate nohighlight">\(K \in \mathbb{R}^{n \times d_k}\)</span> is the key matrix containing <span class="math notranslate nohighlight">\(n\)</span> key vectors of dimension <span class="math notranslate nohighlight">\(d_k\)</span>, and <span class="math notranslate nohighlight">\(V \in \mathbb{R}^{n \times d_v}\)</span> is the value matrix containing <span class="math notranslate nohighlight">\(n\)</span> value vectors of dimension <span class="math notranslate nohighlight">\(d_v\)</span>.</p>
<p>The normalized attention score is used as a weight for the weghted sum of the value vectors, which results in <em>the contextualized vector of the query word</em>. Putting all the pieces together, the attention mechanism is computed as:</p>
<div class="math notranslate nohighlight">
\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V,
\]</div>
<p>where <span class="math notranslate nohighlight">\(V \in \mathbb{R}^{n \times d_v}\)</span> is the value matrix containing <span class="math notranslate nohighlight">\(n\)</span> value vectors of dimension <span class="math notranslate nohighlight">\(d_v\)</span>. In the original paper on transformers <a class="footnote-reference brackets" href="#footcite-vaswani2017attention" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>, the dimension of the query, key, and value vectors are all set to be the same, i.e., <span class="math notranslate nohighlight">\(d_k = d_v = d_q = d / h\)</span>, where <span class="math notranslate nohighlight">\(h\)</span> is the number of attention heads and <span class="math notranslate nohighlight">\(d\)</span> is the dimension of the input vector, though this is not a strict requirement.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The output of the attention mechanism is the <em>contextualized vector</em>, meaning that the vector for a word can vary depending on other words input to the attention module. This is ideal for language modeling, since the meaning of a word can vary depending on the context, e.g., “bank” can mean “river bank” or “financial institution” depending on the words surrounding it.</p>
</div>
</section>
<section id="multi-head-attention">
<h3>Multi-Head Attention<a class="headerlink" href="#multi-head-attention" title="Link to this heading">#</a></h3>
<p>Multi-head attention consists of multiple attention heads to enable a model to pay attentions to multiple aspects of the input sequence. Each attention head can have different parameters and thus produces different “contextualized vectors.” These different vector are then concatenated and fed into a feed-forward network to produce the final output.</p>
<figure class="align-center" id="transformer-multihead-attention">
<a class="reference internal image-reference" href="../_images/transformer-multihead-attention.jpg"><img alt="Multi-Head Attention" src="../_images/transformer-multihead-attention.jpg" style="width: 50%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 22 </span><span class="caption-text">Multi-head attention mechanism.</span><a class="headerlink" href="#transformer-multihead-attention" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="layer-normalization">
<h2>Layer Normalization<a class="headerlink" href="#layer-normalization" title="Link to this heading">#</a></h2>
<figure class="align-center" id="transformer-layer-normalization">
<a class="reference internal image-reference" href="https://miro.medium.com/v2/resize:fit:1400/0*Agdt1zYwfUxXMJGJ"><img alt="Layer Normalization" src="https://miro.medium.com/v2/resize:fit:1400/0*Agdt1zYwfUxXMJGJ" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 23 </span><span class="caption-text">Layer normalization works by normalizing each individual sample across its features. For each sample, it calculates the mean and standard deviation across all feature dimensions, then uses these statistics to normalize that sample’s values.</span><a class="headerlink" href="#transformer-layer-normalization" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><em>Layer normalization</em> is a technique used to stabilize the training of deep neural networks. It mitigates the problem of too large or too small input values, which can cause the network to become unstable. This normalization shifts and scales the input values to prevent this issue. More specifically, the layer normalization is computed as:</p>
<div class="math notranslate nohighlight">
\[
\text{LayerNorm}(x) = \gamma \frac{x - \mu}{\sigma} + \beta,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> are the mean and standard deviation of the input, <span class="math notranslate nohighlight">\(\gamma\)</span> is the scaling factor, and <span class="math notranslate nohighlight">\(\beta\)</span> is the shifting factor. The variables <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable parameters that are initialized to 1 and 0, respectively, and are updated during training.</p>
</section>
<section id="wiring-it-all-together">
<h2>Wiring it all together<a class="headerlink" href="#wiring-it-all-together" title="Link to this heading">#</a></h2>
<section id="encoder-transformer-block">
<h3>Encoder Transformer Block<a class="headerlink" href="#encoder-transformer-block" title="Link to this heading">#</a></h3>
<p>Now, we have all the components to build a transformer block. Let’s wire them together.</p>
<figure class="align-center" id="transformer-block">
<a class="reference internal image-reference" href="../_images/transformer-encoder.jpg"><img alt="Transformer Block" src="../_images/transformer-encoder.jpg" style="width: 50%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 24 </span><span class="caption-text">Input flows through multi-head attention, layer normalization, feed-forward networks, and another normalization step.</span><a class="headerlink" href="#transformer-block" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Let us ignore the residual connection for now. The input is first passed through multi-head attention, followed by layer normalization. Then, the output of the normalization is passed through feed-forward networks and another layer normalization step.</p>
<section id="residual-connection">
<h4>Residual Connection<a class="headerlink" href="#residual-connection" title="Link to this heading">#</a></h4>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="https://i.sstatic.net/UcJSa.png"><img alt="Residual Connection" src="https://i.sstatic.net/UcJSa.png" style="width: 30%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 25 </span><span class="caption-text">Residual connection.</span><a class="headerlink" href="#id3" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Now, let us consider the <em>residual connection</em>.
A residual connection, also known as a <em>skip connection</em>, is a technique used to stabilize the training of deep neural networks. More specifically, let us denote by <span class="math notranslate nohighlight">\(f\)</span> the neural network that we want to train, which is the multi-head attention or feed-forward networks in the transformer block. The residual connection is defined as:</p>
<div class="math notranslate nohighlight">
\[
\underbrace{x_{\text{out}}}_{\text{output}} = \underbrace{x_{\text{in}}}_{\text{input}} + \underbrace{f(x_{\text{in}})}_{\text{component}}.
\]</div>
<p>Note that rather than learning the complete mapping from input to output, the network <span class="math notranslate nohighlight">\(f\)</span> learns to model the residual (difference) between them. This is particularly advantageous when the desired transformation approximates an identity mapping, as the network can simply learn to output values near zero.</p>
<p>Residual connections help prevent the vanishing gradient problem.
Deep learning models like LLMs consist of many layers, which are trained to minimize the loss function <span class="math notranslate nohighlight">\({\cal L}_{\text{loss}}\)</span> with respect to the parameters <span class="math notranslate nohighlight">\(\theta\)</span>.
To this end, the gradient of the loss function is computed using the chain rule as</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial {\cal L}_{\text{loss}}}{\partial \theta} = \frac{\partial {\cal L}_{\text{loss}}}{\partial f_L} \cdot \frac{\partial f_L}{\partial f_{L-1}} \cdot \frac{\partial f_{L-1}}{\partial f_{L-2}} \cdot ... \cdot \frac{\partial f_{l+1}}{\partial f_l} \cdot \frac{\partial f_l}{\partial \theta}
\]</div>
<p>where <span class="math notranslate nohighlight">\(f_i\)</span> is the output of the <span class="math notranslate nohighlight">\(i\)</span>-th layer. The gradient vanishing problem occurs when the individual terms <span class="math notranslate nohighlight">\(\frac{\partial f_{i+1}}{\partial f_i}\)</span> are less than 1. As a result, the gradient becomes smaller and smaller as the gradient flows backward through earlier layers.
By adding the residual connection, the gradient for the individual term becomes:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial x_{i+1}}{\partial x_i} = 1 + \frac{\partial f_i(x_i)}{\partial x_i}
\]</div>
<p>Notice the “+1” term, which is the direct path from the input to the output. The chain rule is thus modified as:</p>
<div class="math notranslate nohighlight">
\[\left(1 + \frac{\partial f_{L-1}}{\partial x_{L-1}}\right)\left(1 + \frac{\partial f_{L-2}}{\partial x_{L-2}}\right)\left(1 + \frac{\partial f_{L-3}}{\partial x_{L-3}}\right)...\]</div>
<p>When we expand this, we can group terms by their order (how many <span class="math notranslate nohighlight">\(\partial f_i\)</span> terms are multiplied together):
We can write this more concisely using <span class="math notranslate nohighlight">\(O_n\)</span> to represent terms of nth order:</p>
<div class="math notranslate nohighlight">
\[1 + O_1 + O_2 + O_3 + ...\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(O_1 = \frac{\partial f_{L-1}}{\partial x_{L-1}} + \frac{\partial f_{L-2}}{\partial x_{L-2}} + \frac{\partial f_{L-3}}{\partial x_{L-3}} + ...\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(O_2 = \frac{\partial f_{L-1}}{\partial x_{L-1}}\frac{\partial f_{L-2}}{\partial x_{L-2}} + \frac{\partial f_{L-2}}{\partial x_{L-2}}\frac{\partial f_{L-3}}{\partial x_{L-3}} + \frac{\partial f_{L-1}}{\partial x_{L-1}}\frac{\partial f_{L-3}}{\partial x_{L-3}} + ...\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(O_3 = \frac{\partial f_{L-1}}{\partial x_{L-1}}\frac{\partial f_{L-2}}{\partial x_{L-2}}\frac{\partial f_{L-3}}{\partial x_{L-3}} + ...\)</span></p></li>
</ul>
<p>Without the residual connection, we only have the <span class="math notranslate nohighlight">\(O_L\)</span> terms for the network with <span class="math notranslate nohighlight">\(L\)</span> layers, which is subject to the gradient vanishing problem. Whereas with the residual connection, we have the lower-order terms like <span class="math notranslate nohighlight">\(O_1, O_2, O_3, ...\)</span> for the network with <span class="math notranslate nohighlight">\(L\)</span> layers, which is less susceptible to the gradient vanishing problem.</p>
<div class="tip admonition">
<p class="admonition-title">Residual Connection</p>
<p>Residual connections are a architectural innovation that allows neural networks to be much deeper without degrading performance. It was proposed by He et al.  for image processing from Microsoft Research.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Residual connection mitigates gradient explosion</p>
<p>Residual connections also help prevent gradient explosion, even though this may not be obvious from the chain rule perspective. As shown in <a class="footnote-reference brackets" href="#footcite-philipp2017exploding" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>, the residual connection provides an alternative path for gradients to flow through. By distributing gradients between the residual path and the learning component path, the gradient is less likely to explode.</p>
</div>
</section>
</section>
</section>
<section id="decoder-transformer-block">
<h2>Decoder Transformer Block<a class="headerlink" href="#decoder-transformer-block" title="Link to this heading">#</a></h2>
<p>The decoder transformer block is similar to the encoder transformer block, but it also includes the <em>masked multi-head attention</em> and <em>cross-attention</em> components.</p>
<figure class="align-center" id="transformer-decoder">
<a class="reference internal image-reference" href="../_images/transformer-decoder.jpg"><img alt="Transformer Decoder" src="../_images/transformer-decoder.jpg" style="width: 50%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 26 </span><span class="caption-text">The decoder transformer block.</span><a class="headerlink" href="#transformer-decoder" title="Link to this image">#</a></p>
</figcaption>
</figure>
<section id="masked-multi-head-attention">
<h3>Masked Multi-Head Attention<a class="headerlink" href="#masked-multi-head-attention" title="Link to this heading">#</a></h3>
<p>The masked multi-head attention is used during training to prevent the decoder from seeing the future tokens. During inference, the masked mult-head attention acts as a regular attention module.</p>
<p>The masked multi-head attention is crucial for enabling parallel training of the decoder. During training, we know the entire expected output sequence, but we need to ensure the model learns to generate tokens sequentially without “peeking” at future tokens.</p>
<p>Let’s understand this with an example. Suppose we’re training a model to translate “I love you” to French “Je t’aime”. The encoder processes the input sequence in parallel, producing vector representations (say 11, 12, 13 for simplicity). For the decoder training, we have two options:</p>
<ol class="arabic">
<li><p><strong>Sequential Training (without masking)</strong>: Process one token at a time</p>
<ul class="simple">
<li><p>Step 1: Input (11,12,13) → Predict “Je”</p></li>
<li><p>Step 2: Input (11,12,13) + predicted “Je” → Predict “t’aime”</p></li>
<li><p>Step 3: Input (11,12,13) + predicted “t’aime” → Predict final token</p></li>
</ul>
<p>This is slow and errors accumulate across steps.</p>
</li>
<li><p><strong>Parallel Training (with masking)</strong>: Process all tokens simultaneously</p>
<ul class="simple">
<li><p>Operation A: Input (11,12,13) → Predict “Je” (mask out “t’aime”)</p></li>
<li><p>Operation B: Input (11,12,13) + “Je” → Predict “t’aime” (mask out final token)</p></li>
<li><p>Operation C: Input (11,12,13) + “Je” + “t’aime” → Predict final token</p></li>
</ul>
</li>
</ol>
<p>The parallel training is much faster and more efficient, since the model can process all tokens simultaneously. Additionally, the model does not suffer from the error accumulation problem, where the prediction error from one step is carried over to the next step.</p>
<p>To implement the masking, we set the attention scores to negative infinity for future tokens before the softmax operation, effectively zeroing out their contribution:</p>
<div class="math notranslate nohighlight">
\[
\text{Mask}(Q, K, V) = \text{softmax}\left(\frac{QK^T + M}{\sqrt{d_k}}\right)V
\]</div>
<p>where <span class="math notranslate nohighlight">\(M\)</span> is a matrix with <span class="math notranslate nohighlight">\(-\infty\)</span> for positions corresponding to future tokens. The result is the attention scores, where the tokens attend only to the previous tokens.</p>
<figure class="align-center" id="transformer-masked-attention">
<a class="reference internal image-reference" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png"><img alt="Masked Attention" src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 27 </span><span class="caption-text">The masked attention mechanism.</span><a class="headerlink" href="#transformer-masked-attention" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="cross-attention">
<h3>Cross-Attention<a class="headerlink" href="#cross-attention" title="Link to this heading">#</a></h3>
<p>Cross-attention is the second multi-head attention component in the decoder transformer block. It creates a connection between the decoder and encoder by allowing the decoder to access information from the encoder’s output.</p>
<p>The mechanism works by using queries (Q) from the decoder’s previous layer and keys (K) and values (V) from the encoder’s output. This enables each position in the decoder to attend to the full encoder sequence without any masking, since encoding is already complete.</p>
<p>For instance, in translating “I love you” to “Je t’aime”, cross-attention helps each French word focus on relevant English words - “Je” attending to “I”, and “t’aime” to “love”. This maintains semantic relationships between input and output.</p>
<p>The cross-attention formula is:</p>
<div class="math notranslate nohighlight">
\[
\text{CrossAttention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]</div>
<p>where Q comes from the decoder and K,V come from the encoder. This effectively bridges the encoding and decoding processes.</p>
<figure class="align-center" id="transformer-cross-attention">
<a class="reference internal image-reference" href="../_images/transformer-cross-attention.jpg"><img alt="Cross-Attention" src="../_images/transformer-cross-attention.jpg" style="width: 60%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 28 </span><span class="caption-text">The cross-attention mechanism.</span><a class="headerlink" href="#transformer-cross-attention" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="other-miscellaneous-components">
<h2>Other miscellaneous components<a class="headerlink" href="#other-miscellaneous-components" title="Link to this heading">#</a></h2>
<section id="position-embedding">
<h3>Position embedding<a class="headerlink" href="#position-embedding" title="Link to this heading">#</a></h3>
<p>Position embedding is also an interesting component that is used to encode the position of the tokens in the sequence.
A key limitation of the attention mechanism is that it is <em>permutation invariant</em>.
This means that the order of the input tokens does not matter, e.g., “The cat sat on the mat” and “The mat sat on the cat” are the same.
To better capture the position information, transformers add to the input token embedding <em>a position embedding</em>.</p>
<p>To understand how this works, let us approach from a naive approach.
Suppose that we have a sequence of <span class="math notranslate nohighlight">\(T\)</span> token embeddings, denoted by <span class="math notranslate nohighlight">\(x_1, x_2, ..., x_T\)</span>, each of which is a <span class="math notranslate nohighlight">\(d\)</span>-dimensional vector.
A simple way to encode the position information is to add a position index to each token embedding, i.e.,</p>
<div class="math notranslate nohighlight">
\[
x_t := x_t + \beta t,
\]</div>
<p>where <span class="math notranslate nohighlight">\(t = 1, 2, ..., T\)</span> is the position index of the token in the sequence, and <span class="math notranslate nohighlight">\(\beta\)</span> is the step size. This appears to be simple but has a critical problem.</p>
<ol class="arabic simple">
<li><p><strong>Unbounded</strong>: The position index can be arbitrarily large. When the models see a sequence longer than those in training data, it may suffer since the model will be exposed to a new position index that the model has never seen before.</p></li>
<li><p><strong>Discrete</strong>: The position index is discrete, which means that the model cannot capture the position information in a smooth manner.</p></li>
</ol>
<p>Because this naive approach has the problems, let us consider another approach. Let us represent the position index using a binary vector of length <span class="math notranslate nohighlight">\(d\)</span>. For example, in case of <span class="math notranslate nohighlight">\(d=4\)</span>, we have the following binary vectors:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
  0: \ \ \ \ \color{orange}{\texttt{0}} \ \ \color{green}{\texttt{0}} \ \ \color{blue}{\texttt{0}} \ \ \color{red}{\texttt{0}} &amp; &amp;
  8: \ \ \ \ \color{orange}{\texttt{1}} \ \ \color{green}{\texttt{0}} \ \ \color{blue}{\texttt{0}} \ \ \color{red}{\texttt{0}} \\
  1: \ \ \ \ \color{orange}{\texttt{0}} \ \ \color{green}{\texttt{0}} \ \ \color{blue}{\texttt{0}} \ \ \color{red}{\texttt{1}} &amp; &amp;
  9: \ \ \ \ \color{orange}{\texttt{1}} \ \ \color{green}{\texttt{0}} \ \ \color{blue}{\texttt{0}} \ \ \color{red}{\texttt{1}} \\
  2: \ \ \ \ \color{orange}{\texttt{0}} \ \ \color{green}{\texttt{0}} \ \ \color{blue}{\texttt{1}} \ \ \color{red}{\texttt{0}} &amp; &amp;
  10: \ \ \ \ \color{orange}{\texttt{1}} \ \ \color{green}{\texttt{0}} \ \ \color{blue}{\texttt{1}} \ \ \color{red}{\texttt{0}} \\
  3: \ \ \ \ \color{orange}{\texttt{0}} \ \ \color{green}{\texttt{0}} \ \ \color{blue}{\texttt{1}} \ \ \color{red}{\texttt{1}} &amp; &amp;
  11: \ \ \ \ \color{orange}{\texttt{1}} \ \ \color{green}{\texttt{0}} \ \ \color{blue}{\texttt{1}} \ \ \color{red}{\texttt{1}} \\
  4: \ \ \ \ \color{orange}{\texttt{0}} \ \ \color{green}{\texttt{1}} \ \ \color{blue}{\texttt{0}} \ \ \color{red}{\texttt{0}} &amp; &amp;
  12: \ \ \ \ \color{orange}{\texttt{1}} \ \ \color{green}{\texttt{1}} \ \ \color{blue}{\texttt{0}} \ \ \color{red}{\texttt{0}} \\
  5: \ \ \ \ \color{orange}{\texttt{0}} \ \ \color{green}{\texttt{1}} \ \ \color{blue}{\texttt{0}} \ \ \color{red}{\texttt{1}} &amp; &amp;
  13: \ \ \ \ \color{orange}{\texttt{1}} \ \ \color{green}{\texttt{1}} \ \ \color{blue}{\texttt{0}} \ \ \color{red}{\texttt{1}} \\
  6: \ \ \ \ \color{orange}{\texttt{0}} \ \ \color{green}{\texttt{1}} \ \ \color{blue}{\texttt{1}} \ \ \color{red}{\texttt{0}} &amp; &amp;
  14: \ \ \ \ \color{orange}{\texttt{1}} \ \ \color{green}{\texttt{1}} \ \ \color{blue}{\texttt{1}} \ \ \color{red}{\texttt{0}} \\
  7: \ \ \ \ \color{orange}{\texttt{0}} \ \ \color{green}{\texttt{1}} \ \ \color{blue}{\texttt{1}} \ \ \color{red}{\texttt{1}} &amp; &amp;
  15: \ \ \ \ \color{orange}{\texttt{1}} \ \ \color{green}{\texttt{1}} \ \ \color{blue}{\texttt{1}} \ \ \color{red}{\texttt{1}} \\
\end{align*}
\end{split}\]</div>
<p>Then, one may use the binary vector as the position embedding as follows:</p>
<div class="math notranslate nohighlight">
\[
x_{t,i} := x_{t,i} + \text{Pos}(t, i),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\text{Pos}(t, i)\)</span> is the position embedding vector of the position index <span class="math notranslate nohighlight">\(t\)</span> and the dimension index <span class="math notranslate nohighlight">\(i\)</span>.
This representation is good in the sense that it is unbounded. Yet, it is still discrete.</p>
<p>An elegant position embedding, which is used in transformers, is the <em>sinusoidal position embedding</em> <a class="footnote-reference brackets" href="#footcite-vaswani2017attention" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>. It appears to be complicated but stay with me for a moment.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\text{Pos}(t, i) =
\begin{cases}
\sin\left(\dfrac{t}{10000^{2i/d}}\right), &amp; \text{if } i \text{ is even} \\
\cos\left(\dfrac{t}{10000^{2i/d}}\right), &amp; \text{if } i \text{ is odd}
\end{cases},
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(i\)</span> is the dimension index of the position embedding vector. This position embedding is added to the input token embedding as:</p>
<div class="math notranslate nohighlight">
\[
x_{t,i} := x_{t,i} + \text{Pos}(t, i),
\]</div>
<p>It appears to be complicated but it can be seen as a continuous version of the binary position embedding above. To see this, let us plot the position embedding for the first 100 positions.</p>
<figure class="align-center" id="transformer-position-embedding">
<a class="reference internal image-reference" href="https://kazemnejad.com/img/transformer_architecture_positional_encoding/positional_encoding.png"><img alt="Transformer Position Embedding" src="https://kazemnejad.com/img/transformer_architecture_positional_encoding/positional_encoding.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 29 </span><span class="caption-text">The position embedding. The image is taken from <a class="reference external" href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/">https://kazemnejad.com/blog/transformer_architecture_positional_encoding/</a></span><a class="headerlink" href="#transformer-position-embedding" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>We note that, just like the binary position embedding, the sinusoidal position embedding also exhibits the alternating pattern (vertically) with frequency increasing as the dimension index increases (horizontal axis). Additionally, the sinusoidal position embedding is continuous, which means that the model can capture the position information in a smooth manner.</p>
<p>Another key property of the sinusoidal position embedding is that the dot similarity between the two position embedding vectors represent the similarity between the two positions, regardless of the position index.</p>
<figure class="align-default" id="id9">
<img alt="https://kazemnejad.com/img/transformer_architecture_positional_encoding/time-steps_dot_product.png" src="https://kazemnejad.com/img/transformer_architecture_positional_encoding/time-steps_dot_product.png" />
<figcaption>
<p><span class="caption-number">Fig. 30 </span><span class="caption-text">:name: transformer-position-embedding-similarity
:alt: Transformer Position Embedding Similarity
:width: 80%
:align: center</span><a class="headerlink" href="#id9" title="Link to this image">#</a></p>
<div class="legend">
<p>The dot similarity between the two position embedding vectors represent the distance between the two positions, regardless of the position index. The image is taken from <a class="reference external" href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/">https://kazemnejad.com/blog/transformer_architecture_positional_encoding/</a></p>
</div>
</figcaption>
</figure>
<div class="tip admonition">
<p class="admonition-title">Why additive position embedding?</p>
<p>The sinusoidal position embedding is additive, which alter the token embedding. Alternatively, one may concatenate, instead of adding, the position embedding to the token embedding, i.e., <span class="math notranslate nohighlight">\(x_{t,i} := [x_{t,i}; \text{Pos}(t, i)]\)</span>. This makes it easier for a model to distinguish the position information from the token information. So why not use the concatenation?</p>
<p>One reason is that the concatenation requires a larger embedding dimension, which increases the number of parameters in the model.
Instead, adding the position embedding creates an interesting effect in the attention mechanism.
Interested readers can check out <a class="reference external" href="https://www.reddit.com/r/MachineLearning/comments/cttefo/comment/exs7d08/?utm_source=reddit&amp;amp;utm_medium=web2x&amp;amp;context=3">this Reddit post</a>.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Absolute vs Relative Position Embedding</p>
<p>Absolute position embedding is the one we discussed above, where each position is represented by a unique vector.
On the other hand, relative position embedding represents the position difference between two positions, rather than the absolute position <a class="footnote-reference brackets" href="#footcite-shaw2018self" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>.
The relative position embedding can be implemented by adding a learnable scalar to the unnormalized attention scores before softmax operation <a class="footnote-reference brackets" href="#footcite-raffel2020exploring" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a>, i.e.,</p>
<div class="math notranslate nohighlight">
\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T + B}{\sqrt{d_k}}\right)V
\]</div>
<p>where <span class="math notranslate nohighlight">\(B\)</span> is a learnable offset matrix that is added to the unnormalized attention scores. The matrix <span class="math notranslate nohighlight">\(B\)</span> is a function of the position difference between the query and key, i.e., <span class="math notranslate nohighlight">\(B = f(i-j)\)</span>, where <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> are the position indices of the query and key, respectively. Such a formulation is useful when the model needs to capture the relative position between two tokens.</p>
</div>
<div class="docutils container" id="id8">
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="footcite-vaswani2017attention" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id2">2</a>,<a role="doc-backlink" href="#id5">3</a>)</span>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, <em>Advances in Neural Information Processing Systems</em>, volume 30. Curran Associates, Inc., 2017. URL: <a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-philipp2017exploding" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">2</a><span class="fn-bracket">]</span></span>
<p>George Philipp, Dawn Song, and Jaime G Carbonell. The exploding gradient problem demystified-definition, prevalence, impact, origin, tradeoffs, and solutions. <em>arXiv preprint arXiv:1712.05577</em>, 2017.</p>
</aside>
<aside class="footnote brackets" id="footcite-shaw2018self" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">3</a><span class="fn-bracket">]</span></span>
<p>Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. <em>arXiv preprint arXiv:1803.02155</em>, 2018.</p>
</aside>
<aside class="footnote brackets" id="footcite-raffel2020exploring" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">4</a><span class="fn-bracket">]</span></span>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. <em>Journal of machine learning research</em>, 21(140):1–67, 2020.</p>
</aside>
</aside>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "skojaku/applied-soft-comp",
            ref: "gh-pages",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./m03-transformers"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="what-to-learn.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Module 3: Transformers</p>
      </div>
    </a>
    <a class="right-next"
       href="bert.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Bidirectional Encoder Representations from Transformers (BERT)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-building-block-of-llms">A building block of LLMs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-mechanism">Attention Mechanism</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention">Self-Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention">Multi-Head Attention</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization">Layer Normalization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#wiring-it-all-together">Wiring it all together</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder-transformer-block">Encoder Transformer Block</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-connection">Residual Connection</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder-transformer-block">Decoder Transformer Block</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#masked-multi-head-attention">Masked Multi-Head Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-attention">Cross-Attention</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-miscellaneous-components">Other miscellaneous components</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#position-embedding">Position embedding</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sadamori Kojaku
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
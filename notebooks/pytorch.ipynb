{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b539a62a",
   "metadata": {},
   "source": [
    "\n",
    "# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/skojaku/applied-soft-comp/blob/main/notebooks/pytorch.ipynb)\n",
    "\n",
    "\n",
    "# Introduction to PyTorch\n",
    "\n",
    "This notebook introduces the fundamentals of PyTorch, focusing on tensor operations and linear algebra concepts. We'll start from the basics and gradually build up to more complex operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "031af134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's import our required libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930dd4a2",
   "metadata": {},
   "source": [
    "## Introduction to PyTorch Tensors\n",
    "\n",
    "### What is a Tensor?\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:880/1*WbLIc4-xIOfHiO2oWzimyA.png)\n",
    "\n",
    "A tensor is simply a container for numbers. It can be:\n",
    "- A single number (scalar) - 0-dimensional tensor\n",
    "- A list of numbers (vector) - 1-dimensional tensor\n",
    "- A table of numbers (matrix) - 2-dimensional tensor\n",
    "- A cube of numbers (and beyond!) - 3+ dimensional tensor\n",
    "\n",
    "Let's create some tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adc2e6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar shape: torch.Size([])\n",
      "Vector shape: torch.Size([5])\n",
      "Matrix shape: torch.Size([3, 3])\n",
      "Cube shape: torch.Size([2, 2, 2])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating tensors of different dimensions\n",
    "scalar = torch.tensor(3.14)\n",
    "vector = torch.tensor([1, 2, 3, 4, 5])\n",
    "matrix = torch.tensor([[1, 2, 3],\n",
    "                      [4, 5, 6],\n",
    "                      [7, 8, 9]])\n",
    "cube = torch.tensor([[[1, 2], [3, 4]],\n",
    "                    [[5, 6], [7, 8]]])\n",
    "\n",
    "# Let's print their shapes\n",
    "print(f\"Scalar shape: {scalar.shape}\")\n",
    "print(f\"Vector shape: {vector.shape}\")\n",
    "print(f\"Matrix shape: {matrix.shape}\")\n",
    "print(f\"Cube shape: {cube.shape}\")\n",
    "\n",
    "# Visualize the matrix using matplotlib\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(matrix.numpy(), cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('Visualization of our 3x3 Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba796e28",
   "metadata": {},
   "source": [
    "### Creating Tensors\n",
    "PyTorch provides several ways to create tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d52d73e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeros tensor:\n",
      " tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "\n",
      "Ones tensor:\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "Random tensor:\n",
      " tensor([[0.1568, 0.1792, 0.6185],\n",
      "        [0.5723, 0.8979, 0.1207],\n",
      "        [0.4178, 0.5432, 0.7317]])\n",
      "\n",
      "Range tensor: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\n",
      "Linspace tensor: tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "# Create tensors with specific values\n",
    "zeros = torch.zeros(3, 4)  # 3x4 tensor of zeros\n",
    "ones = torch.ones(2, 3)    # 2x3 tensor of ones\n",
    "random = torch.rand(3, 3)  # 3x3 tensor of random numbers between 0 and 1\n",
    "\n",
    "# Create tensors with specific ranges\n",
    "range_tensor = torch.arange(0, 10, step=1)  # Creates [0, 1, 2, ..., 9]\n",
    "linspace = torch.linspace(0, 1, steps=5)    # Creates 5 evenly spaced points between 0 and 1\n",
    "\n",
    "print(\"Zeros tensor:\\n\", zeros)\n",
    "print(\"\\nOnes tensor:\\n\", ones)\n",
    "print(\"\\nRandom tensor:\\n\", random)\n",
    "print(\"\\nRange tensor:\", range_tensor)\n",
    "print(\"\\nLinspace tensor:\", linspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e22ae6",
   "metadata": {},
   "source": [
    "### Working with GPU (if available)\n",
    "PyTorch makes it easy to move computations to GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "299e9060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Tensor on cpu:\n",
      " tensor([[0.8234, 0.0439, 0.2012],\n",
      "        [0.6818, 0.7916, 0.7941],\n",
      "        [0.7189, 0.0458, 0.9823]])\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create a tensor and move it to the available device\n",
    "tensor_on_device = torch.rand(3, 3).to(device)\n",
    "print(f\"Tensor on {device}:\\n\", tensor_on_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6317d3a4",
   "metadata": {},
   "source": [
    "### Basic Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4213cf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition:\n",
      " tensor([[ 6,  8],\n",
      "        [10, 12]])\n",
      "\n",
      "Element-wise multiplication:\n",
      " tensor([[ 5, 12],\n",
      "        [21, 32]])\n",
      "\n",
      "Matrix multiplication:\n",
      " tensor([[19, 22],\n",
      "        [43, 50]])\n",
      "\n",
      "Matrix multiplication (using @):\n",
      " tensor([[19, 22],\n",
      "        [43, 50]])\n"
     ]
    }
   ],
   "source": [
    "# Create two tensors\n",
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "b = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "# Addition\n",
    "print(\"Addition:\\n\", a + b)\n",
    "\n",
    "# Multiplication (element-wise)\n",
    "print(\"\\nElement-wise multiplication:\\n\", a * b)\n",
    "\n",
    "# Matrix multiplication\n",
    "print(\"\\nMatrix multiplication:\\n\", torch.matmul(a, b))\n",
    "\n",
    "# or using the @ operator\n",
    "print(\"\\nMatrix multiplication (using @):\\n\", a @ b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8211567",
   "metadata": {},
   "source": [
    "### Common Tensor Operations and Troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "234fb766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Torch not compiled with CUDA enabled\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Create example tensors\n",
    "    a = torch.tensor([[1, 2], [3, 4]]).to(\"cpu\")\n",
    "    b = torch.tensor([[5, 6], [7, 8]]).to(\"cuda\")\n",
    "\n",
    "    # TROUBLESHOOTING TIP: Shape mismatches\n",
    "    # If you get a RuntimeError about shape mismatch, check tensor shapes:\n",
    "    print(f\"Shape of a: {a.shape}\")\n",
    "    print(f\"Shape of b: {b.shape}\")\n",
    "    # Fix by reshaping: tensor.reshape(new_shape) or tensor.view(new_shape)\n",
    "\n",
    "    # TROUBLESHOOTING TIP: Wrong device\n",
    "    # If you get a \"tensors must be on the same device\" error:\n",
    "    print(f\"Device of a: {a.device}\")\n",
    "    print(f\"Device of b: {b.device}\")\n",
    "    # Fix by moving tensors to same device: tensor.to(device)\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(f\"Runtime Error occurred: {e}\")\n",
    "    print(\"This may be due to CUDA not being available or tensors being on different devices\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19383f07",
   "metadata": {},
   "source": [
    "### Understanding Broadcasting in Detail\n",
    "Broadcasting follows these rules:\n",
    "1. Arrays must have the same number of dimensions, or\n",
    "2. One array can have fewer dimensions if they match from right to left\n",
    "3. Each dimension must be equal, or one must be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a1220d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 1\n",
      "Fix: Ensure shapes are compatible for broadcasting\n"
     ]
    }
   ],
   "source": [
    "# Examples of valid broadcasting\n",
    "a = torch.ones(3, 4)\n",
    "b = torch.ones(4)  # Will be broadcast to (3, 4)\n",
    "c = a + b  # Works!\n",
    "\n",
    "# TROUBLESHOOTING TIP: Invalid broadcasting\n",
    "try:\n",
    "    d = torch.ones(4, 3)\n",
    "    result = a + d  # Will fail!\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Fix: Ensure shapes are compatible for broadcasting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4746dafa",
   "metadata": {},
   "source": [
    "### Memory Management Best Practices\n",
    "\n",
    "In-place operations modify tensors directly instead of creating new ones.\n",
    "They are denoted by a trailing underscore (_) in PyTorch.\n",
    "Examples:\n",
    "- add_() instead of add()\n",
    "- mul_() instead of mul()\n",
    "- sub_() instead of sub()\n",
    "\n",
    "These operations are memory efficient since they don't create new tensors, making them valuable for large models and datasets where memory is constrained. However, they can't be used with tensors requiring gradients and may cause issues with shared tensors, so they should be implemented carefully, especially in training loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41866587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TROUBLESHOOTING TIP: Memory leaks\n",
    "# Use in-place operations when possible to reduce memory usage\n",
    "x = torch.ones(1000, 1000)\n",
    "# Instead of: x = x + 1\n",
    "x.add_(1)  # In-place addition\n",
    "# x+=1\n",
    "# Clear GPU cache if needed\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a34d58",
   "metadata": {},
   "source": [
    "When working with CUDA tensors, memory isn't automatically released back to the GPU.\n",
    "torch.cuda.empty_cache() forces unused memory to be released back to the GPU.\n",
    "This is useful when:\n",
    "- You get \"out of memory\" errors\n",
    "- You've deleted large tensors but memory usage remains high\n",
    "- Between training runs to ensure a clean GPU state\n",
    "Note: This only frees unused memory - tensors still in use won't be affected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f0c41d",
   "metadata": {},
   "source": [
    "### Understanding Tensor Memory Layout and Contiguity\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e92qaBl4Kly5CKzGRNRZIQ.png)\n",
    "\n",
    "When we create a tensor in PyTorch, it arranges the data sequentially in memory, like books lined up on a shelf. This is called a *contiguous tensor*. We can check if a tensor is contiguous using `is_contiguous()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a07dbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12]])\n"
     ]
    }
   ],
   "source": [
    "# Creating a contiguous tensor - data is stored sequentially in memory\n",
    "x = torch.arange(1, 13)  # [1, 2, 3, ..., 12]\n",
    "print(x.is_contiguous())  # True\n",
    "print(x)\n",
    "# We can reshape it while keeping data contiguous\n",
    "y = x.view(4, 3)\n",
    "print(y)  # Shows a 4x3 matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a33436",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xm3ZfWtIn7TUD9lcwhpacg.png)\n",
    "\n",
    "A `view` is like looking at the same data from a different angle; it doesn't create a new copy but provides a new way to access the same data. Think of it as rearranging books on a shelf without actually moving them. When you change data in a view, it changes the original data because they share the same memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34e8c860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 13)  # Original data\n",
    "y = x.view(4, 3)        # Viewed as 4x3 matrix\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597deb47",
   "metadata": {},
   "source": [
    "Changes in view affect original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40a9cd6f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([100,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12])\n"
     ]
    }
   ],
   "source": [
    "y[0, 0] = 100\n",
    "print(x)  # First element changes in both x and y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0a41c6",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AnQ2KLhfYmt17ldTKpKvfQ.png)\n",
    "\n",
    "Strides tell us how many steps to jump to move in each dimension of the tensor. `strides` give us directions: how many elements in the tensor to skip to get to the next row or the next column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49e726b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12]])\n",
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 13).view(4, 3)\n",
    "print(x)\n",
    "print(x.stride())  # (3, 1)\n",
    "# 3 means: skip 3 elements to move down one row\n",
    "# 1 means: skip 1 element to move right one column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc243a53",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HRWWBxD3H0rkO4r5J64dVg.png)\n",
    "\n",
    "Sometimes operations like transpose() create non-contiguous tensors, where data isn't stored sequentially anymore. While `view()` only works with contiguous data, `reshape()` can work with both types by creating a new copy when necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d61604be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 13).view(4, 3)\n",
    "y = x.transpose(0, 1)   # Creates non-contiguous tensor\n",
    "print(y.is_contiguous())  # False\n",
    "\n",
    "z = x.reshape(2, 6)     # reshape works fine with non-contiguous data\n",
    "print(z.is_contiguous())  # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d6a661",
   "metadata": {},
   "source": [
    "The key takeaway is that understanding how data is organized in memory helps us choose the right operations and optimize our code's performance. Using `view()` is faster but more restrictive, while `reshape()` is more flexible but might use more memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762ad80f",
   "metadata": {},
   "source": [
    "### Understanding Squeeze and Unsqueeze\n",
    "\n",
    "![](https://i.sstatic.net/9AJJA.png)\n",
    "\n",
    "It is often necessary to change the dimensions of a tensor to match the expected shape for a particular operation. For example, a tensor with shape (32, 10) might need to be reshaped to (32, 1, 10) for a specific layer. `unsqueeze()` and `squeeze()` are two operations that can help with this.\n",
    "\n",
    "`unsqueeze()` adds a dimension of size 1 (like turning a 2D sheet into a 3D book with one page). This is useful when you need to match shapes for broadcasting or add batch dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb2f7c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10]) torch.Size([32, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "sequence = torch.randn(32, 10)  # (batch_size, sequence_length)\n",
    "sequence_expanded = sequence.unsqueeze(1)  # Adds dimension at position 1\n",
    "# Shape changes from (32, 10) to (32, 1, 10)\n",
    "\n",
    "print(sequence.shape, sequence_expanded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566c780a",
   "metadata": {},
   "source": [
    "`squeeze()` removes all dimensions of size 1 (like flattening a book with one page back into a sheet). This helps clean up tensor shapes after operations that add extra dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "664afe08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4]), torch.Size([1, 1, 3, 1, 1, 4]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nested = torch.randn(1, 1, 3, 1, 1, 4)  # Tensor with many size-1 dimensions\n",
    "squeezed = nested.squeeze()  # Removes all size-1 dimensions\n",
    "# Shape changes from (1, 1, 3, 1, 1, 4) to (3, 4)\n",
    "squeezed.shape, nested.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f56e1a",
   "metadata": {},
   "source": [
    "You can specify which dimensions to squeeze or unsqueeze:\n",
    "- Use positive indices to count from the front\n",
    "- Use negative indices to count from the back\n",
    "- Use `dim` parameter to squeeze specific dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7343018",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.randn(3, 4)\n",
    "# Both add dimension at the start:\n",
    "front_expanded = tensor.unsqueeze(0)     # (1, 3, 4)\n",
    "back_expanded = tensor.unsqueeze(-3)     # (1, 3, 4)\n",
    "\n",
    "# Squeeze only specific dimensions:\n",
    "partial = torch.randn(1, 3, 1, 4, 1)\n",
    "result = partial.squeeze(dim=0)  # Only removes first dimension\n",
    "# Shape changes from (1, 3, 1, 4, 1) to (3, 1, 4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05e912a",
   "metadata": {},
   "source": [
    "The key is to understand that these operations help match tensor shapes for various operations in neural networks, especially when dealing with batches, channels, or preparing data for specific layer requirements.\n",
    "\n",
    "### Reduction Operations\n",
    "\n",
    "Reduction operations can collapse all dimensions into a single value, giving you an overall summary of your tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47eaac3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)  # 2x3 tensor\n",
    "\n",
    "total = x.sum()     # Adds all numbers: 21\n",
    "average = x.mean()  # Averages all numbers: 3.5\n",
    "total, average\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98db6b0",
   "metadata": {},
   "source": [
    "You can reduce specific dimensions while keeping others, which is useful for batch operations or feature aggregation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e828d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5., 7., 9.]), torch.Size([2, 3]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum along rows (dim=0): combines values vertically\n",
    "row_sum = x.sum(dim=0)  # Result: [5, 7, 9]\n",
    "# Each position adds numbers in that column\n",
    "\n",
    "# Sum along columns (dim=1): combines values horizontally\n",
    "col_sum = x.sum(dim=1)  # Result: [6, 15]\n",
    "# Each position adds numbers in that row\n",
    "\n",
    "row_sum, x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8155f9",
   "metadata": {},
   "source": [
    "PyTorch provides methods to find maximum and minimum values, optionally returning their positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "521fd98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3.],\n",
       "         [4., 5., 6.]]),\n",
       " tensor([3., 6.]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple max/min across all elements\n",
    "max_val = x.max()  # Returns 6\n",
    "min_val = x.min()  # Returns 1\n",
    "\n",
    "# Getting both values and positions along a dimension\n",
    "max_vals, max_idx = x.max(dim=1)\n",
    "# max_vals: highest number in each row [3, 6]\n",
    "# max_idx: position of highest number [2, 2]\n",
    "x, max_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f43547",
   "metadata": {},
   "source": [
    "These operations are fundamental for many tasks:\n",
    "- Computing loss functions (like mean squared error)\n",
    "- Calculating accuracy metrics\n",
    "- Finding the strongest predictions in classification\n",
    "- Pooling operations in neural networks\n",
    "\n",
    "Remember that when using `dim`, think about which dimension you want to \"collapse.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefd3890",
   "metadata": {},
   "source": [
    "### Indexing and Slicing\n",
    "\n",
    "Just like accessing items in a list, you can pick specific elements from a tensor using indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "afaf0c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2, 3, 4],\n",
    "                 [5, 6, 7, 8],\n",
    "                 [9, 10, 11, 12]])\n",
    "\n",
    "first_row = x[0]           # Gets [1, 2, 3, 4]\n",
    "specific_element = x[1,2]  # Gets 7 (row 1, column 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c5c6a7",
   "metadata": {},
   "source": [
    "Slicing lets you select ranges of elements using the `start:end` syntax:\n",
    "Slicing lets you select ranges of elements using the `start:end` syntax:\n",
    "- `:` means \"take everything\"\n",
    "- `0:2` means \"take elements from index 0 up to (but not including) 2\"\n",
    "- `-2:` means \"take the last two elements\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "224a9e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_rows = x[:2]        # Takes first two rows\n",
    "last_columns = x[:,-2:] # Takes last two columns from all rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35351d04",
   "metadata": {},
   "source": [
    "You can select elements that meet certain conditions by creating a mask of True/False values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0883cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6,  7,  8,  9, 10, 11, 12])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = x > 5            # Creates a boolean mask\n",
    "big_numbers = x[mask]   # Selects only elements greater than 5\n",
    "big_numbers\n",
    "# Results in: [6, 7, 8, 9, 10, 11, 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fed922a",
   "metadata": {},
   "source": [
    "Using arrays of indices to select multiple elements at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84bc2663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2, 12])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = torch.tensor([0, 2])      # Select first and last rows\n",
    "cols = torch.tensor([1, 3])      # Select second and last columns\n",
    "selected = x[(rows, cols)] # Gets specific elements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36220351",
   "metadata": {},
   "source": [
    "These indexing methods are important for:\n",
    "- Selecting specific batches or features in deep learning\n",
    "- Applying operations to parts of your data\n",
    "- Filtering data based on conditions\n",
    "- Efficiently accessing and modifying specific elements\n",
    "\n",
    "Remember: Unlike NumPy, PyTorch indexing returns views when possible, which means modifications to the slice will affect the original tensor.\n",
    "\n",
    "## Autograd Mechanics\n",
    "\n",
    "### Gradient Tracking\n",
    "\n",
    "A key feature of PyTorch is *autograd*, which is a system for automatic differentiation. When we create a tensor with `requires_grad=True`, PyTorch starts tracking all operations performed on it for automatic differentiation. This is essential for training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0b7e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x ** 2  # This operation is tracked\n",
    "z = y.sum()\n",
    "z.backward() # This computes the gradient of z with respect to x\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a518ca38",
   "metadata": {},
   "source": [
    "By default, PyTorch accumulates gradients across multiple backward passes. This means if you call `backward()` multiple times, the gradients add up instead of being overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3533e21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x ** 2  # This operation is tracked\n",
    "\n",
    "for i in range(2):\n",
    "    z = y.sum()\n",
    "    z.backward(retain_graph=True)  # First pass\n",
    "    print(f\"Pass {i+1} gradient:\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683f10b6",
   "metadata": {},
   "source": [
    "To reset gradients to zero, use `zero_()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4002f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad.zero_()  # Resets gradients to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb607c5",
   "metadata": {},
   "source": [
    "### Not Tracking Gradients\n",
    "\n",
    "Sometimes we want to perform operations without tracking gradients (like during evaluation). The `torch.no_grad()` context manager temporarily disables gradient tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabfde07",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    intermediate = x * 2  # No gradients tracked here\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc21b348",
   "metadata": {},
   "source": [
    "This is useful for:\n",
    "- Saving memory during inference\n",
    "- Preventing gradients from flowing through certain parts of your model\n",
    "- Performing operations that don't need gradient tracking\n",
    "\n",
    "Understanding these concepts is crucial for efficient training of neural networks, especially when dealing with memory constraints or implementing custom training loops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6be04ed",
   "metadata": {},
   "source": [
    "### Computation Graph\n",
    "\n",
    "We can take the gradient of a function with respect to multiple variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22d296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensors with gradient tracking\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Building a computation graph\n",
    "z = x**2 + y**3\n",
    "\n",
    "# Compute gradients\n",
    "z.backward()\n",
    "\n",
    "print(f\"dz/dx: {x.grad}\")  # Should be 2 * x = 4\n",
    "print(f\"dz/dy: {y.grad}\")  # Should be 3 * y^2 = 27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416f3506",
   "metadata": {},
   "source": [
    "Internally, PyTorch constructs so-called a *computation graph* that represents the operations performed on the tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6247a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0abb49a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the computation graph\n",
    "try:\n",
    "    from torchviz import make_dot\n",
    "    import shutil\n",
    "    if shutil.which('dot') is not None:  # Check if graphviz is installed\n",
    "        make_dot(z, {'x': x, 'y': y, \"z\":z}).render(\"computation_graph\", format=\"png\")\n",
    "    else:\n",
    "        print(\"graphviz not installed - skipping visualization\")\n",
    "except ImportError:\n",
    "    print(\"torchviz not installed - skipping visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87081c2d",
   "metadata": {},
   "source": [
    "### Higher-Order Derivatives\n",
    "\n",
    "PyTorch can compute higher-Order derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1c3dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# First derivative of x^3\n",
    "y = x**3\n",
    "first_derivative = torch.autograd.grad(y, x, create_graph=True)[0]\n",
    "print(f\"First derivative at x=2: {first_derivative}\")\n",
    "\n",
    "# Second derivative\n",
    "second_derivative = torch.autograd.grad(first_derivative, x)[0]\n",
    "print(f\"Second derivative at x=2: {second_derivative}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e839459c",
   "metadata": {},
   "source": [
    "### Common Autograd Pitfalls and Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5410b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. In-place operations with autograd\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "y = torch.tensor([3.0, 4.0], requires_grad=True)\n",
    "\n",
    "# This will raise an error\n",
    "try:\n",
    "    x.add_(y)  # In-place addition\n",
    "    # This is essentially the same as x = x + y, creating a loop in the computation graph\n",
    "except RuntimeError as e:\n",
    "    print(\"Error with in-place operation:\", e)\n",
    "\n",
    "# Correct way\n",
    "z = x + y  # Create new tensor instead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cef1c9",
   "metadata": {},
   "source": [
    "## Example of Linear Regression\n",
    "\n",
    "Let us demonstrate all the concepts we have learned so far by implementing a simple linear regression model. Our goal is to find the parameters of a line (y = wx + b) that best fits our data. We generate synthetic data following this pattern with added noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0c396a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.linspace(-5, 5, 100).reshape(-1, 1)\n",
    "y = 2 * X + 1 + torch.randn_like(X) * 0.5  # y = 2x + 1 + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2fd53d",
   "metadata": {},
   "source": [
    "We will use gradient descent to find the best parameters for our model, which updates the parameter in the direction of the negative gradient. Namely,\n",
    "\n",
    "$$\n",
    "\\theta_{new} = \\theta_{old} - \\eta \\nabla_{\\theta} L(\\theta)\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the learning rate, and $\\nabla_{\\theta} L(\\theta)$ is the gradient of the loss function with respect to the parameters.\n",
    "\n",
    "We will start with random parameters and iteratively update them to minimize the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2949bddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters with gradient tracking\n",
    "w = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "# Training loop showing tensor operations\n",
    "learning_rate = 0.01\n",
    "for epoch in range(100):\n",
    "    y_pred = w * X + b\n",
    "    loss = ((y_pred - y) ** 2).mean()\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b8c376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X.detach().numpy(), y.detach().numpy(), label='Data')\n",
    "plt.plot(X.detach().numpy(), (w * X + b).detach().numpy(), 'r', label=f'Fitted line: y = {w.item():.2f}x + {b.item():.2f}')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression Results')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376f0480",
   "metadata": {},
   "source": [
    "This code showcases key PyTorch concepts for efficient tensor operations and gradient handling. Tensor operations are vectorized for speed, while `requires_grad=True` enables automatic differentiation. The `with torch.no_grad()` context prevents unnecessary gradient tracking during updates, in-place operations minimize memory usage, and explicit gradient zeroing ensures correct parameter updates across training iterations.\n",
    "\n",
    "# Neural Network Basics\n",
    "\n",
    "PyTorch offers a high-level interface for building neural networks, which is built on top of the tensor operations we have learned so far. We will learn how to build neural networks using this interface.\n",
    "\n",
    "## Linear Layer\n",
    "\n",
    "A linear layer (also called a fully connected or dense layer) performs the operation: y = Wx + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39372bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Create a linear layer\n",
    "linear = nn.Linear(in_features=2, out_features=3)\n",
    "\n",
    "# Input tensor\n",
    "x = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "\n",
    "# Forward pass\n",
    "y = linear(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", y.shape)\n",
    "print(\"\\nLayer weights shape:\", linear.weight.shape)\n",
    "print(\"Layer bias shape:\", linear.bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d462b24",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity into our networks. Let's visualize common activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e2971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input values\n",
    "x = torch.linspace(-5, 5, 100)\n",
    "\n",
    "# Common activation functions\n",
    "relu = nn.ReLU()\n",
    "sigmoid = nn.Sigmoid()\n",
    "tanh = nn.Tanh()\n",
    "\n",
    "# Plot activation functions\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(x.numpy(), relu(x).numpy())\n",
    "plt.title('ReLU')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(x.numpy(), sigmoid(x).numpy())\n",
    "plt.title('Sigmoid')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(x.numpy(), tanh(x).numpy())\n",
    "plt.title('Tanh')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c3e4ec",
   "metadata": {},
   "source": [
    "## `nn.Module`\n",
    "\n",
    "`nn.Module` is the foundational building block in PyTorch's neural network architecture. It automatically manages parameters, enables modular design, and handles training mechanics. When creating a neural network class that inherits from `nn.Module`, you must initialize the parent class, define layers as attributes, and implement a `forward` method that specifies how data flows through your network.\n",
    "\n",
    "Here's a simple neural network implementation that performs binary classification on points in a 2D space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "93b7992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(2, 8)    # Input -> Hidden\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(8, 1)    # Hidden -> Output\n",
    "        self.activation2 = nn.Sigmoid()   # For binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.activation2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f95a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_nn = SimpleNN()\n",
    "simple_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e6f467",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_nn(torch.tensor([[1.0, 2.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ed00ec",
   "metadata": {},
   "source": [
    "The network processes input through two linear layers with activations. The first layer transforms 2D input into 8 features using ReLU activation, while the second layer produces a single output transformed by Sigmoid for binary classification. This structure can learn to classify points based on their position in 2D space.\n",
    "\n",
    "Let us train this neural network by using the following data. We will generate a circular pattern dataset and train the network to classify the points as inside or outside the circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1276e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate circular pattern dataset\n",
    "X = torch.randn(1000, 2)\n",
    "y = ((X[:, 0]**2 + X[:, 1]**2) < 2).float().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a092b8",
   "metadata": {},
   "source": [
    "Using the `SimpleNN` class we defined earlier, we can now train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f060ee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training\n",
    "model = SimpleNN()\n",
    "criterion = nn.BCELoss() # We will learn more about loss functions later\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # We will learn more about optimizers later\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6124cac",
   "metadata": {},
   "source": [
    "Let us visualize the decision boundary of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f677f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mesh grid of points\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                    np.linspace(y_min, y_max, 100))\n",
    "\n",
    "# Convert grid points to PyTorch tensor\n",
    "grid_points = torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Get predictions\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    Z = model(grid_points)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.contourf(xx, yy, Z.numpy(), alpha=0.4, cmap='RdYlBu')\n",
    "\n",
    "# Plot training points\n",
    "plt.scatter(X[:, 0][y.squeeze() == 1], X[:, 1][y.squeeze() == 1],\n",
    "           c='blue', label='Class 1', alpha=0.7)\n",
    "plt.scatter(X[:, 0][y.squeeze() == 0], X[:, 1][y.squeeze() == 0],\n",
    "           c='red', label='Class 0', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Neural Network Decision Boundary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49464139",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "When training a neural network, we need to measure how well the network is performing. We do this by using a loss function. The loss function takes the predicted output of the network and the true output, and returns a single number that represents the error of the network. PyTorch provides many loss functions. Here are some common ones:\n",
    "\n",
    "- `nn.MSELoss`: [Mean Squared Error](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html)\n",
    "- `nn.CrossEntropyLoss`: [Cross Entropy](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "- `nn.BCELoss`: [Binary Cross Entropy](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)\n",
    "\n",
    "The choice of loss function depends on the type of problem you are trying to solve. For example, if you are doing classification, you will use a loss function that is appropriate for classification. If you are doing regression, you will use a loss function that is appropriate for regression.\n",
    "\n",
    "## Optimizers\n",
    "\n",
    "Optimizers are used to update the parameters of the network. PyTorch provides many optimizers. Here are some common ones:\n",
    "\n",
    "- `torch.optim.SGD`: [Stochastic Gradient Descent](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)\n",
    "- `torch.optim.Adam`: [Adaptive Moment Estimation](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)\n",
    "- `torch.optim.AdamW`: [Adam with Weight Decay](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)\n",
    "\n",
    "An optimizer takes the parameters of the network and updates them. For example, the Adam optimizer updates the parameters by adding the gradient of the loss function with respect to the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fde69d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # lr is the learning rate\n",
    "optimizer.step() # This updates the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4b09a8",
   "metadata": {},
   "source": [
    "# Data pipeline\n",
    "\n",
    "Data pipeline is the process of loading, preprocessing, and splitting data into training, validation, and test sets.\n",
    "The efficiency of the data pipeline is crucial for the performance of the network.\n",
    "PyTorch provides a high-level interface for creating a data pipeline using the `Dataset` and `DataLoader` classes.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "A `Dataset` is an abstract class that represents a dataset. It is the base class for all datasets in PyTorch. A convenient way to create a dataset is to use the `TensorDataset` class, which is a subclass of `Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "27ef17de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Example usage\n",
    "features = torch.randn(1000, 3)  # 1000 houses with 3 features each\n",
    "prices = torch.randn(1000, 1)    # corresponding prices\n",
    "\n",
    "dataset = TensorDataset(features, prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f77a3e8",
   "metadata": {},
   "source": [
    "where `features` and `prices` are PyTorch tensors. The data can be retrieved by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ee3887",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e262c8cb",
   "metadata": {},
   "source": [
    "You can also get the length of the dataset by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a588033",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91464f5",
   "metadata": {},
   "source": [
    "When working with complex datasets, you might want to create a custom dataset.\n",
    "\n",
    "This can be done by subclassing the `Dataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d3cc5a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class HousingDataset(Dataset):\n",
    "    def __init__(self, features, prices):\n",
    "        self.features = features\n",
    "        self.prices = prices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.prices[idx]\n",
    "\n",
    "dataset = HousingDataset(features, prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153cbfc3",
   "metadata": {},
   "source": [
    "- The `__init__` function initializes the dataset by storing the input features and prices. Think of this as setting up your data container when you first create it.\n",
    "- The `__len__` function returns the total number of samples in your dataset - PyTorch needs this to know how many samples are available.\n",
    "- The `__getitem__` function is called when you request a specific data point using an index. It returns a tuple of (feature, price) for the house at that index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5815b32",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "\n",
    "The DataLoader class comes with several important parameters that control how data is loaded during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28284a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create data loader with detailed parameters\n",
    "batch_size = 300\n",
    "train_loader = DataLoader(\n",
    "    dataset=dataset,          # your dataset instance\n",
    "    batch_size=batch_size,    # how many samples per batch\n",
    "    shuffle=True,             # randomly shuffle data\n",
    "    drop_last=False,          # keep incomplete final batch\n",
    "    pin_memory=True          # faster data transfer to GPU\n",
    ")\n",
    "\n",
    "# Example of accessing batches\n",
    "for batch_idx, (batch_features, batch_prices) in enumerate(train_loader):\n",
    "    # batch_idx gives you the batch number\n",
    "    # batch_features and batch_prices are your data\n",
    "    print(f\"Batch {batch_idx}: Features shape {batch_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3092d6",
   "metadata": {},
   "source": [
    "The DataLoader's parameters serve specific purposes: `batch_size` determines how many samples are processed at once, `shuffle` randomizes data order each epoch to prevent learning order-dependent patterns, `num_workers` enables parallel data loading for better performance, `drop_last` decides whether to keep partial batches at the end, and `pin_memory` optimizes data transfer to GPU if you're using one. When you iterate over the DataLoader, it automatically handles creating these batches and loading them efficiently.\n",
    "\n",
    "## Example of Moon Dataset\n",
    "\n",
    "First, let's create a proper data pipeline using PyTorch's dataset and dataloader classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f1a22851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.datasets import make_moons\n",
    "import numpy as np\n",
    "\n",
    "# Create a custom dataset\n",
    "class MoonDataset(Dataset):\n",
    "    def __init__(self, n_samples=1000):\n",
    "        # Generate the moon dataset\n",
    "        X, y = make_moons(n_samples=n_samples, noise=0.1, random_state=42)\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y).reshape(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create train and validation datasets\n",
    "train_dataset = MoonDataset(n_samples=1000)\n",
    "val_dataset = MoonDataset(n_samples=200)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54daf0ca",
   "metadata": {},
   "source": [
    "We will now implement a simple neural network to classify the moon dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f645362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin_1 = nn.Linear(2, 16)\n",
    "        self.relu_1 = nn.ReLU()\n",
    "        self.lin_2 = nn.Linear(16, 16)\n",
    "        self.relu_2 = nn.ReLU()\n",
    "        self.lin_3 = nn.Linear(16, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin_1(x)\n",
    "        x = self.relu_1(x)\n",
    "        x = self.lin_2(x)\n",
    "        x = self.relu_2(x)\n",
    "        x = self.lin_3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = BinaryClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2eb87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer,\n",
    "                n_epochs=100, device='cpu'):\n",
    "    # Training history\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Transfer model to device (CPU/GPU)\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Training phase\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # Move batch to device\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():  # No need to track gradients\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Record losses\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Train the model\n",
    "train_losses, val_losses = train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer,\n",
    "    n_epochs=100, device=device\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training History')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431455db",
   "metadata": {},
   "source": [
    "Evaluate the model by plotting the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fc83b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y):\n",
    "    model.eval()\n",
    "\n",
    "    # Create a mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                        np.linspace(y_min, y_max, 100))\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        Z = model(torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()]))\n",
    "        Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z.numpy(), alpha=0.4)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "    plt.title('Decision Boundary')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the decision boundary\n",
    "X, y = train_dataset.X, train_dataset.y\n",
    "plot_decision_boundary(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3471d1",
   "metadata": {},
   "source": [
    "# PyTorch Lightning\n",
    "\n",
    "PyTorch Lightning is a high-level library for PyTorch that provides a more user-friendly interface for training and evaluating models. It provides a number of features that make it easier to train and evaluate models, such as automatic mixed precision, gradient accumulation, and early stopping.\n",
    "\n",
    "To use PyTorch Lightning, you need to install it. (Uncomment the following line to install it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c4b53fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad82de06",
   "metadata": {},
   "source": [
    "Now, let's re-implement the previous example using PyTorch Lightning.\n",
    "With PyTorch Lightning, we will define a model by subclassing the `LightningModule` class, instead of `nn.Module`. This module will contain the model definition (`__init__` and `forward`), the training loop (`training_step`), the validation loop (`validation_step`), and the optimizer (`configure_optimizers`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5767be1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "\n",
    "class BinaryClassifier(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin_1 = nn.Linear(2, 16)\n",
    "        self.relu_1 = nn.ReLU()\n",
    "        self.lin_2 = nn.Linear(16, 16)\n",
    "        self.relu_2 = nn.ReLU()\n",
    "        self.lin_3 = nn.Linear(16, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin_1(x)\n",
    "        x = self.relu_1(x)\n",
    "        x = self.lin_2(x)\n",
    "        x = self.relu_2(x)\n",
    "        x = self.lin_3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self(X)\n",
    "        loss = nn.BCELoss()(y_pred, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self(X)\n",
    "        loss = nn.BCELoss()(y_pred, y)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1494d8",
   "metadata": {},
   "source": [
    "A key feature of PyTorch Lightning is that it provides a `Trainer` class that handles the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d072b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryClassifier()\n",
    "trainer = Trainer(max_epochs=50)\n",
    "trainer.fit(model, train_loader, val_loader) # This will automatically handle the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932f4be5",
   "metadata": {},
   "source": [
    "Now, let's evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f2dddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision boundary\n",
    "X, y = train_dataset.X, train_dataset.y\n",
    "plot_decision_boundary(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f67b39",
   "metadata": {},
   "source": [
    "A cool feature of PyTorch Lightning is that it provides a `TensorBoard` callback that allows you to visualize the training process.\n",
    "\n",
    "To use this, you need to install the `tensorboard` package. (Uncomment the following line to install it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bfb83488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc44f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard  --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7912f4c",
   "metadata": {},
   "source": [
    "\n",
    "Then, you can run the following command to start the TensorBoard server.\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b84d31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "model = BinaryClassifier()\n",
    "logger = TensorBoardLogger(save_dir='logs')\n",
    "trainer = Trainer(logger=logger, max_epochs=100, log_every_n_steps=1)\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cd3291",
   "metadata": {},
   "source": [
    "# References\n",
    "- [\\[Pytorch\\] Contiguous vs Non-Contiguous Tensor / View  Understanding view(), reshape(), transpose() | by Kathryn | Analytics Vidhya | Medium](https://medium.com/analytics-vidhya/pytorch-contiguous-vs-non-contiguous-tensor-view-understanding-view-reshape-73e10cdfa0dd)\n",
    "- [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/index.html)\n",
    "- [TensorBoard](https://www.tensorflow.org/tensorboard)\n",
    "- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6343c7b4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advnetsci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

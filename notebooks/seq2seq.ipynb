{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/sk-classroom/asc-transformers/blob/main/exercise/exercise_01.ipynb)\n",
    "\n",
    "![](https://cdn.britannica.com/03/134503-050-060DD73F/Bombe-American-version-messages-cipher-machines-Britain.jpg)\n",
    "\n",
    "In this notebook, we will be creating a seq2seq model for deciphering a simple cipher. \n",
    "References: \n",
    "- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
    "- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
    "\n",
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using Google Colab or local environments, install the following packages:\n",
    "#!pip install spacy\n",
    "#!pip install torchtext\n",
    "#!pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the necessary packages\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy import linalg, sparse\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pyl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq model\n",
    "\n",
    "Let us implement a seq2seq model with attention mechanism. We will first implement its building blocks, namely `Encoder`, `Decoder`, and `Attention`, and then put them together to form a seq2seq model.\n",
    "\n",
    "\n",
    "## Encoder \n",
    "\n",
    "Let's implement `Encoder`. \n",
    "While [the original paper uses four-layer LSTM](https://arxiv.org/abs/1409.3215), we will cut down it to simpler encoder, namely two-layer [Gated Recurrent Unit (GRU) by Cho et al.](https://arxiv.org/pdf/1406.1078v3.pdf). GRU simplifies LTCM by omitting the cell state and produces only the hidden state. Namely, \n",
    "\n",
    "$$\n",
    "h_{t} = \\text{GRU}(x_{t}, h_{t-1})\n",
    "$$\n",
    "\n",
    "*Multi-layered* GRU means that GRU units are stacked on top of each other, where $\\ell$th ($\\ell \\geq 2$) GRU will take $\\ell-1$th GRU's hidden state as the input. For example, two-layer GRU is given by \n",
    "\n",
    "$$\n",
    "h^{(1)}_{t} = \\text{GRU}(x_{t}, h^{(1)}_{t-1}) \\\\\n",
    "h^{(2)}_{t} = \\text{GRU}(h^{(1)}_{t}, h^{(2)}_{t-1})\n",
    "$$\n",
    "\n",
    "where $h^{(\\ell)}_t$ represents the hidden state for the $\\ell$ th layer at the time $t$. We will then use all layer's hidden states at the end of the sequence as the inputs to the decoder. \n",
    "With PyTorch, we can easily implement the multi-layer GRU. See [the documentation](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html).\n",
    "\n",
    "Here, let's implement `Encoder` class as follows. \n",
    "\n",
    "**Step 1**: `Encoder` will take sequences of integer tokens, represented as a tensor of size <batch_size x max_length>, where `batch_size` is the number of sentences in a batch, and `max_length` is the maximum length of the sentences in the batch. \n",
    "\n",
    "**Step 2**: The integer tokens are mapped to the vectors of size `embedding_size` by using `torch.nn.Embedding`, namely\n",
    "$$\n",
    "z_t = \\text{Embedding}(x_t)\n",
    "$$\n",
    "where $z_t$ is the vector representation of the token $x_t$. \n",
    "\n",
    "**Step 3**: A dropout is performed on $z_t$:\n",
    "\n",
    "$$\n",
    "z_t = \\text{Dropout}(z_t )\n",
    "$$\n",
    "\n",
    "Dropout is a technique to prevent overfitting by randomly dropping out some neurons during training. For example, if we set the dropout rate to 0.5, then 50% of the neurons will output zeros randomly during the training.  Neural networks with dropout tend to avoid relying on specific neurons, and instead learn robust mapping between the input and output. \n",
    "\n",
    "**Step 4**: Embedding $z_t$ will be fed into the two-layer GRUs:\n",
    "$$\n",
    "h^{(1)}_t = \\text{GRU}(z_t, h^{(1)}_{t-1}) \\\\\n",
    "h^{(2)}_t = \\text{GRU}(h^{(1)}_t, h^{(2)}_{t-1})\n",
    "$$\n",
    "\n",
    "**Step 5**: Output the hidden states at the last sequence time $T$, namely \n",
    "\n",
    "$$\n",
    "(h^{(1)}_T, h^{(2)}_T)\n",
    "$$\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    input[/\"Input Sequence\n",
    "    (batch_size × max_length)\"/]\n",
    "    embed[\"Embedding Layer\n",
    "    zt = Embedding(xt)\"]\n",
    "    dropout[\"Dropout Layer\"]\n",
    "    gru[\"Two-Layer GRU\n",
    "    h1(t), h2(t) = GRU(zt)\"]\n",
    "    output[/\"Final Hidden States\n",
    "    (h1(T), h2(T)) and Outputs\"/]\n",
    "    input --> embed\n",
    "    embed --> dropout\n",
    "    dropout --> gru\n",
    "    gru --> output\n",
    "\n",
    "    style input fill:#D4E6F1,stroke:#000,color:#000\n",
    "    style embed fill:#FAE5D3,stroke:#000,color:#000\n",
    "    style dropout fill:#D5F5E3,stroke:#000,color:#000\n",
    "    style gru fill:#E8DAEF,stroke:#000,color:#000\n",
    "    style output fill:#FADBD8,stroke:#000,color:#000\n",
    "\n",
    "    linkStyle default stroke:#000,stroke-width:2px\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        embedding_size,\n",
    "        hidden_size,\n",
    "        n_layers=2,\n",
    "        dropout=0.1,\n",
    "        bidirectional=False,\n",
    "    ):\n",
    "        \"\"\"Encoder class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size: int\n",
    "            The number of unique tokens in the input sequence\n",
    "        embedding_size: int\n",
    "            The dimension of the embedding vectors\n",
    "        hidden_size: int\n",
    "            The dimension of the hidden states\n",
    "        n_layers: int\n",
    "            The number of layers in the GRU\n",
    "        dropout: float\n",
    "            The dropout rate\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # TODO:\n",
    "        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n",
    "        self.gru = torch.nn.GRU(\n",
    "            embedding_size,\n",
    "            hidden_size,\n",
    "            n_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # Initialize the embedding\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tokens: Tensor of shape <batch_size x max_length>\n",
    "            The input sequence\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        hidden: Tensor of shape <batch_size x hidden_size>\n",
    "            The hidden states of the last layer\n",
    "        \"\"\"\n",
    "        Z = self.embedding(X)\n",
    "        Z = self.dropout(Z)\n",
    "        outputs, hidden = self.gru(Z)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention \n",
    "\n",
    "Let's implement attention module. This module takes two inputs: \n",
    "- the hidden states of the encoder, $h_{1}, \\ldots, h_{n}$\n",
    "- the hidden states of the decoder, $s_{t-1}$\n",
    "\n",
    "These states are concatenated and fed into an MLP to generate the attention scores, $e_{1t}, e_{2t}, \\ldots, e_{nt}$, i.e., \n",
    "\n",
    "$$\n",
    "e_{it} = \\text{MLP}([h_i, s_{t-1}])\n",
    "$$\n",
    "\n",
    "where $i$ is the index of the encoder hidden states.  These scores are then normalized by the softmax function to generate the attention weights, $\\alpha_{1t}, \\alpha_{2t}, \\ldots, \\alpha_{nt}$, i.e., \n",
    "\n",
    "$$\n",
    "\\alpha_{it} = \\frac{\\exp(e_{it})}{\\sum_{j=1}^{n} \\exp(e_{jt})}\n",
    "$$\n",
    "\n",
    "Finally, a new context vector is generated by taking the  weighted average: \n",
    "\n",
    "$$\n",
    "c_t = \\sum_{i=1}^{n} \\alpha_{it} h_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_decoder_hidden_size,\n",
    "        attention_hidden_size,\n",
    "        n_layers_hidden,\n",
    "        bidirectional=False,\n",
    "    ):\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_decoder_hidden_size = encoder_decoder_hidden_size\n",
    "        self.attention_hidden_size = attention_hidden_size\n",
    "        self.n_layers_hidden = n_layers_hidden\n",
    "        self.bidirectional = 2 if bidirectional else 1\n",
    "        self.enc2hidden = torch.nn.Linear(\n",
    "            encoder_decoder_hidden_size * self.bidirectional, attention_hidden_size\n",
    "        )\n",
    "        self.dec2hidden = torch.nn.Linear(\n",
    "            encoder_decoder_hidden_size * self.bidirectional * self.n_layers_hidden,\n",
    "            attention_hidden_size,\n",
    "        )\n",
    "        self.activation = torch.nn.Tanh()\n",
    "        self.hidden2score = torch.nn.Linear(attention_hidden_size, 1)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, encoder_outputs, decoder_hidden):\n",
    "        \"\"\"\n",
    "        encoder_outputs: Tensor of shape <batch_size x seq_len x output_size>\n",
    "        decoder_hidden: Tensor of shape <batch_size x 1 x (n_layers*hidden_size)>\n",
    "        \"\"\"\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "\n",
    "        # Project encoder hidden states\n",
    "        enc_proj = self.enc2hidden(encoder_outputs)  # [batch x seq_len x hidden]\n",
    "\n",
    "        # Project decoder hidden state\n",
    "        # Reshape decoder hidden from (x, batch_size, hidden_dim) to (batch_size, x*hidden_dim)\n",
    "        concat_hidden = decoder_hidden.permute(1, 0, 2)  # (batch_size, x, hidden_dim)\n",
    "        concat_hidden = concat_hidden.reshape(\n",
    "            batch_size, -1\n",
    "        )  # (batch_size, x*hidden_dim)\n",
    "        concat_hidden = concat_hidden.unsqueeze(1)  # (batch_size, 1, x*hidden_dim)\n",
    "        dec_proj = self.dec2hidden(concat_hidden)  # [batch x 1 x hidden]\n",
    "        # dec_proj = dec_proj.expand(-1, seq_len, -1)  # Expand to match encoder sequence length\n",
    "\n",
    "        # Combine and get attention scores\n",
    "        hidden = enc_proj + dec_proj\n",
    "        hidden = self.activation(hidden)\n",
    "        scores = self.hidden2score(hidden)  # [batch x seq_len x 1]\n",
    "        scores = self.softmax(scores)\n",
    "\n",
    "        # Get context vector via weighted sum\n",
    "        context_vector = torch.bmm(\n",
    "            scores.transpose(1, 2), encoder_outputs\n",
    "        )  # [batch x 1 x hidden]\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder \n",
    "\n",
    "Let's implement `Decoder`. Following the `Encoder`, we will simplify the original implementation by using two-layer GRUs. \n",
    "\n",
    "The input to the decoder are\n",
    "- The hidden states of the encoder, $h_{1}, \\ldots, h_{n}$\n",
    "- The hidden state of the decoder at the previous time step, $s_{t-1}$\n",
    "- The previous token, $x_{t-1}$\n",
    "\n",
    "The decoder first computes the context vector, $c_t$, by using the attention mechanism. \n",
    "\n",
    "$$\n",
    "c_t = \\text{Attention}(h_1, \\ldots, h_n, s_{t-1})\n",
    "$$\n",
    "\n",
    "Apply dropout to $c_t$ and concatenate it with the embedding of the previous token, $x_{t-1}$. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x_t &= [\\text{Dropout}(c_t), \\text{Embedding}(x_{t-1})]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We then concatenate the context vector and the embedding of the previous token, and feeds them into the GRU:\n",
    "\n",
    "$$\n",
    "s_t = \\text{GRU}\\left(z_t, s_t\\right)\n",
    "$$\n",
    "\n",
    "where $\\text{Embedding}$ is the embedding layer that maps the previous token to the embedding vector. Finally, the decoder outputs the probability distribution of the next token, $P(x_t \\vert x_0, \\ldots, x_{t-1})$ using a linear layer based on the hidden state $s_t$.  \n",
    "\n",
    "$$\n",
    "P(x_t \\vert x_0, \\ldots, x_{t-1}) = \\text{Linear}(s_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        embedding_size,\n",
    "        readout_hidden_size,\n",
    "        encoder_decoder_hidden_size,\n",
    "        attention_hidden_size,\n",
    "        output_size,\n",
    "        n_layers=2,\n",
    "        dropout=0.1,\n",
    "        bidirectional=False,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.encoder_decoder_hidden_size = encoder_decoder_hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.attention_hidden_size = attention_hidden_size\n",
    "        self.readout_hidden_size = readout_hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Embedding layer to convert input tokens to vectors\n",
    "        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # GRU that takes concatenated context vector and embedded input\n",
    "        self.gru = torch.nn.GRU(\n",
    "            embedding_size\n",
    "            + encoder_decoder_hidden_size,  # Input size is embedding + context vector\n",
    "            encoder_decoder_hidden_size,\n",
    "            n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention = Attention(\n",
    "            encoder_decoder_hidden_size=encoder_decoder_hidden_size,\n",
    "            attention_hidden_size=attention_hidden_size,\n",
    "            n_layers_hidden=n_layers,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "\n",
    "        # Output layer to predict next token\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(encoder_decoder_hidden_size, readout_hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(readout_hidden_size, output_size),\n",
    "        )\n",
    "\n",
    "        # Softmax for converting outputs to probabilities\n",
    "        self.softmax = torch.nn.Softmax(dim=2)\n",
    "\n",
    "        # Initialize embeddings\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
    "\n",
    "    def forward(self, input_tokens, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tokens: Tensor of shape <batch_size x 1>\n",
    "            The input sequence\n",
    "        hidden: Tensor of shape <num_layers x batch_size x hidden_size>\n",
    "            The hidden states of the decoder from previous timestep\n",
    "        encoder_outputs: Tensor of shape <batch_size x seq_len x hidden_size>\n",
    "            The hidden states from the encoder\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output: Tensor of shape <batch_size x 1 x output_size>\n",
    "            Probability distribution over next token\n",
    "        hidden: Tensor of shape <num_layers x batch_size x hidden_size>\n",
    "            Updated decoder hidden states\n",
    "        \"\"\"\n",
    "\n",
    "        # Get context vector using attention\n",
    "        context_vector = self.attention(encoder_outputs, hidden)\n",
    "        context_vector = self.dropout(context_vector)\n",
    "\n",
    "        # Embed input tokens and ensure it has batch dimension\n",
    "        input_vector = (\n",
    "            self.embedding(input_tokens).unsqueeze(1)\n",
    "            if input_tokens.dim() == 1\n",
    "            else self.embedding(input_tokens)\n",
    "        )\n",
    "\n",
    "        # Concatenate context vector and embedded input\n",
    "        z = torch.cat([context_vector, input_vector], dim=2)\n",
    "\n",
    "        # Pass through GRU\n",
    "        output, hidden = self.gru(z, hidden)\n",
    "\n",
    "        # Generate output probabilities\n",
    "        output = self.fc(output)\n",
    "        output = self.softmax(output)\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq \n",
    "\n",
    "Now, let's put them together to build a seq2seq model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningModule, Trainer\n",
    "\n",
    "\n",
    "class Seq2Seq(LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder,\n",
    "        decoder,\n",
    "        sos_token_id,\n",
    "        eos_token_id,\n",
    "        vocab_size,\n",
    "        teacher_forcing_ratio=0.5,\n",
    "    ):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.sos_token_id = sos_token_id\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, input_tokens, max_output_len, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Forward pass of the seq2seq model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tokens: Tensor of shape <batch_size x max_length>\n",
    "            The input sequence\n",
    "        max_output_len: int\n",
    "            The maximum length of the output sequence\n",
    "        \"\"\"\n",
    "        batch_size = input_tokens.size(0)\n",
    "        vocab_size = self.vocab_size\n",
    "\n",
    "        # Get encoder outputs and hidden states\n",
    "        encoder_outputs, encoder_hiddens = self.encoder(input_tokens)\n",
    "        decoder_hidden = encoder_hiddens\n",
    "\n",
    "        # First input to decoder is SOS token\n",
    "        decoder_input = (\n",
    "            torch.ones((batch_size, 1), dtype=torch.long, device=self.device)\n",
    "            * self.sos_token_id\n",
    "        )\n",
    "\n",
    "        # Initialize outputs tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, max_output_len, device=self.device)\n",
    "\n",
    "        # Generate sequence\n",
    "        for t in range(max_output_len):\n",
    "            output, decoder_hidden = self.decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            outputs[:, t] = torch.argmax(output, dim=2)\n",
    "\n",
    "            # Sample next token using temperature\n",
    "            if temperature > 0:\n",
    "                probs = torch.nn.functional.softmax(\n",
    "                    output.squeeze(1) / temperature, dim=-1\n",
    "                )\n",
    "                decoder_input = torch.multinomial(probs, 1)\n",
    "            else:\n",
    "                decoder_input = torch.argmax(output, dim=2)\n",
    "\n",
    "            # Stop if all sequences in batch hit EOS\n",
    "            if (decoder_input == self.eos_token_id).all():\n",
    "                break\n",
    "        return outputs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        src, trg = batch\n",
    "\n",
    "        # Teacher forcing: use the actual target tokens as input to decoder\n",
    "        batch_size = src.size(0)\n",
    "        target_length = trg.size(1)\n",
    "        vocab_size = self.decoder.fc[-1].out_features\n",
    "\n",
    "        # Initialize outputs tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, target_length, vocab_size)\n",
    "\n",
    "        # Get initial decoder hidden state from encoder\n",
    "        encoder_outputs, encoder_hiddens = self.encoder(src)\n",
    "        decoder_hidden = encoder_hiddens\n",
    "\n",
    "        # First input to decoder is SOS token\n",
    "        decoder_input = trg[:, 0].unsqueeze(1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        loss = 0\n",
    "        for t in range(1, target_length):\n",
    "            output, decoder_hidden = self.decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            if np.random.rand() < self.teacher_forcing_ratio:\n",
    "                decoder_input = trg[:, t].unsqueeze(1)  # Use target token as next input\n",
    "            else:\n",
    "                decoder_input = torch.argmax(output, dim=2)\n",
    "\n",
    "            loss += self.loss_fn(output.squeeze(1), trg[:, t])\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            src, trg = batch\n",
    "\n",
    "            # Teacher forcing: use the actual target tokens as input to decoder\n",
    "            batch_size = src.size(0)\n",
    "            target_length = trg.size(1)\n",
    "            vocab_size = self.vocab_size\n",
    "\n",
    "            # Initialize outputs tensor to store decoder outputs\n",
    "            outputs = torch.zeros(batch_size, target_length, vocab_size)\n",
    "\n",
    "            # Get initial decoder hidden state from encoder\n",
    "            encoder_outputs, encoder_hiddens = self.encoder(src)\n",
    "            decoder_hidden = encoder_hiddens  # Add layer dimension\n",
    "\n",
    "            # First input to decoder is SOS token\n",
    "            decoder_input = trg[:, 0].unsqueeze(1)\n",
    "\n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            loss = 0\n",
    "            for t in range(1, target_length):\n",
    "                output, decoder_hidden = self.decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs\n",
    "                )\n",
    "                if np.random.rand() < self.teacher_forcing_ratio:\n",
    "                    decoder_input = trg[:, t].unsqueeze(\n",
    "                        1\n",
    "                    )  # Use target token as next input\n",
    "                else:\n",
    "                    decoder_input = torch.argmax(output, dim=2)\n",
    "\n",
    "                loss += self.loss_fn(output.squeeze(1), trg[:, t])\n",
    "\n",
    "            self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['oswzhhdfia', 'tebbhnmdwo', 'vzkvcyqshg']\n",
      "Ciphered: ['rwbfopmptm', 'wighovvnha', 'ydpbjgzcss']\n"
     ]
    }
   ],
   "source": [
    "from secretpy import Caesar, CaesarProgressive, alphabets as al\n",
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "def generate_random_sequences(cipher_key, cipher, n_seqs, seq_len):\n",
    "\n",
    "    sents = []\n",
    "    ciphered_sents = []\n",
    "    for _ in range(n_seqs):\n",
    "        sequence = \"\".join(random.choices(string.ascii_lowercase, k=seq_len))\n",
    "        ciphered_sequence = cipher.encrypt(sequence, cipher_key, al.ENGLISH)\n",
    "        # assert len(sequence) == len(ciphered_sequence)\n",
    "        assert sequence == cipher.decrypt(ciphered_sequence, cipher_key)\n",
    "        sents.append(sequence)\n",
    "        ciphered_sents.append(ciphered_sequence)\n",
    "    return ciphered_sents, sents\n",
    "\n",
    "\n",
    "key = 3\n",
    "cipher = CaesarProgressive()\n",
    "ciphered_sents, sents = generate_random_sequences(\n",
    "    cipher_key=key, cipher=cipher, n_seqs=100000, seq_len=10\n",
    ")\n",
    "\n",
    "print(\"Original:\", sents[:3])\n",
    "print(\"Ciphered:\", ciphered_sents[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26, 22, 8, 6, 7, 14, 21, 21, 13, 7, 0, 27]\n",
      "[26, 14, 13, 13, 10, 21, 25, 0, 17, 0, 23, 27] [26, 19, 4, 1, 1, 7, 13, 12, 3, 22, 14, 27]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def build_tokenizer(sents):\n",
    "    vocab = sorted(list(set(\"\".join(sents))))\n",
    "\n",
    "    vocab.append(\"<sos>\")  # <sos> token\n",
    "    vocab.append(\"<eos>\")  # <eos> token\n",
    "    vocab.append(\"<unk>\")  # <unk> token used to represent the unknown token\n",
    "\n",
    "    vocab_stoi = {token: i for i, token in enumerate(vocab)}\n",
    "    vocab_itos = {i: token for i, token in enumerate(vocab)}\n",
    "\n",
    "    sos_token_id = vocab_stoi[\"<sos>\"]\n",
    "    eos_token_id = vocab_stoi[\"<eos>\"]\n",
    "    unk_token_id = vocab_stoi[\"<unk>\"]\n",
    "\n",
    "    # If the token is not in the vocabulary, then return the unk_token_id\n",
    "    # vocab_stoi = defaultdict(lambda: unk_token_id, vocab_stoi)\n",
    "    # vocab_itos = defaultdict(lambda: unk_token_id, vocab_itos)\n",
    "\n",
    "    return {\n",
    "        \"stoi\": vocab_stoi,\n",
    "        \"itos\": vocab_itos,\n",
    "        \"sos_token_id\": sos_token_id,\n",
    "        \"eos_token_id\": eos_token_id,\n",
    "        \"unk_token_id\": unk_token_id,\n",
    "    }\n",
    "\n",
    "\n",
    "def tokenize(sents, vocab):\n",
    "    retval = []\n",
    "    for sent in sents:\n",
    "        _retval = [vocab[\"sos_token_id\"]]\n",
    "        for letter in sent:\n",
    "            _retval.append(vocab[\"stoi\"][letter])\n",
    "        _retval.append(vocab[\"eos_token_id\"])\n",
    "        retval.append(_retval)\n",
    "\n",
    "    return retval\n",
    "\n",
    "\n",
    "src_vocab = build_tokenizer(ciphered_sents)\n",
    "trg_vocab = build_tokenizer(sents)\n",
    "\n",
    "src_tokenized = tokenize(ciphered_sents, src_vocab)\n",
    "trg_tokenized = tokenize(sents, trg_vocab)\n",
    "\n",
    "print(src_tokenized[1])\n",
    "print(trg_tokenized[3], trg_tokenized[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pipeloine\n",
    "batch_size = 1024\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(src_tokenized, dtype=torch.long),\n",
    "    torch.tensor(trg_tokenized, dtype=torch.long),\n",
    ")\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [0.8, 0.2])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_src_vocab = len(src_vocab[\"stoi\"]) + 3\n",
    "n_trg_vocab = len(trg_vocab[\"stoi\"]) + 3\n",
    "\n",
    "n_layers = 2  # number of layers in the GRU\n",
    "bidirectional = False  # whether to use bidirectional GRU\n",
    "embedding_size = 32  # embedding size\n",
    "hidden_size = 16  # hidden size\n",
    "dropout = 0.1  # dropout rate\n",
    "readout_hidden_size = 16  # hidden size of the readout layer\n",
    "attention_hidden_size = 16  # hidden size of the attention layer\n",
    "\n",
    "encoder = Encoder(\n",
    "    input_size=n_src_vocab,\n",
    "    embedding_size=embedding_size,\n",
    "    hidden_size=hidden_size,\n",
    "    n_layers=n_layers,\n",
    "    dropout=dropout,\n",
    "    bidirectional=bidirectional,\n",
    ")\n",
    "decoder = Decoder(\n",
    "    input_size=n_trg_vocab,\n",
    "    embedding_size=embedding_size,\n",
    "    readout_hidden_size=readout_hidden_size,\n",
    "    n_layers=n_layers,\n",
    "    output_size=n_trg_vocab,\n",
    "    dropout=dropout,\n",
    "    encoder_decoder_hidden_size=hidden_size,\n",
    "    attention_hidden_size=attention_hidden_size,\n",
    "    bidirectional=bidirectional,\n",
    ")\n",
    "sos_token_id = trg_vocab[\"sos_token_id\"]\n",
    "eos_token_id = trg_vocab[\"eos_token_id\"]\n",
    "model = Seq2Seq(encoder, decoder, sos_token_id, eos_token_id, vocab_size=n_trg_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training \n",
    "\n",
    "Let us implement a trainer for the seq2seq model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | encoder | Encoder          | 5.1 K  | train\n",
      "1 | decoder | Decoder          | 7.5 K  | train\n",
      "2 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "12.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 K    Total params\n",
      "0.050     Total estimated model params size (MB)\n",
      "21        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skojaku/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/skojaku/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skojaku/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199:   3%|▎         | 2/78 [00:00<00:02, 27.52it/s, v_num=4] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    571\u001b[0m     ckpt_path,\n\u001b[1;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:982\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 982\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1026\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:216\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:455\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:150\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:320\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m     batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:192\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:270\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:171\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 171\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/pytorch_lightning/core/module.py:1302\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;124;03mthe optimizer.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1300\u001b[0m \n\u001b[1;32m   1301\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1302\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/pytorch_lightning/core/optimizer.py:154\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:239\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/precision.py:123\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/torch/optim/adam.py:205\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 205\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/precision.py:109\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03mhook is called.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m \n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:146\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:140\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:241\u001b[0m, in \u001b[0;36m_AutomaticOptimization._make_backward_fn.<locals>.backward_fn\u001b[0;34m(loss)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward_fn\u001b[39m(loss: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:323\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 323\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:213\u001b[0m, in \u001b[0;36mStrategy.backward\u001b[0;34m(self, closure_loss, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m closure_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mpre_backward(closure_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module)\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m closure_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mpost_backward(closure_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module)\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/precision.py:73\u001b[0m, in \u001b[0;36mPrecision.backward\u001b[0;34m(self, tensor, model, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Performs the actual backpropagation.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/pytorch_lightning/core/module.py:1097\u001b[0m, in \u001b[0;36mLightningModule.backward\u001b[0;34m(self, loss, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1097\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m logger \u001b[38;5;241m=\u001b[39m TensorBoardLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq2seq\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, logger\u001b[38;5;241m=\u001b[39mlogger, devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/applsoftcomp/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "torch.set_float32_matmul_precision(\n",
    "    \"medium\"\n",
    ")  # set the precision of the matmul operation to medium for computational purpose\n",
    "\n",
    "logger = TensorBoardLogger(\"logs\", name=\"seq2seq\")\n",
    "\n",
    "trainer = Trainer(max_epochs=1000, logger=logger, devices=1)\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "\n",
    "Let's validate the seq2seq model with [Caesar cipher](https://en.wikipedia.org/wiki/Caesar_cipher). \n",
    "We will generate ciphered texts to train seq2seq and see if the trained seq2seq decipher the text correctly.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you decipher?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original-Deciphered: hxoyegijga  <-->  <sos>ttyyyyyqni<eos>\n",
      "Original-Deciphered: yspbhsipjg  <-->  <sos>ttyyyyqqni<eos>\n",
      "Original-Deciphered: xifncmxvyd  <-->  <sos>ttyyyyyqni<eos>\n",
      "Original-Deciphered: hfjvgoerej  <-->  <sos>ttyyyyyqni<eos>\n",
      "Original-Deciphered: bimuawqxbm  <-->  <sos>wtyyyyyqny<eos>\n",
      "Original-Deciphered: utujnpwwhw  <-->  <sos>ttyyyyyqni<eos>\n",
      "Original-Deciphered: cogifgpvsu  <-->  <sos>ctyyyyqqnu<eos>\n",
      "Original-Deciphered: ojelrbryaq  <-->  <sos>ttyyyqqqnq<eos>\n",
      "Original-Deciphered: sfykmegzqn  <-->  <sos>wtyyyqqqnn<eos>\n",
      "Original-Deciphered: eggrfcqkjj  <-->  <sos>ttyyyyyqni<eos>\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# text = \"iamastudent\"\n",
    "for i in range(10):\n",
    "    text = sents[i]\n",
    "    ciphered_text = cipher.encrypt(text, key)\n",
    "    ciphered_text_tokenized = torch.tensor(tokenize([ciphered_text], src_vocab))\n",
    "    seqs = model(ciphered_text_tokenized, max_output_len=12, temperature=1e-3)\n",
    "    deciphered_text = \"\"\n",
    "    seqs = seqs.detach().cpu().numpy().astype(int)\n",
    "    for i in range(len(seqs[0])):\n",
    "        deciphered_text += trg_vocab[\"itos\"][seqs[0][i].item()]\n",
    "    print(\"Original-Deciphered:\", text, \" <--> \", deciphered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[26., 19., 19., 19., 19., 19., 19., 19., 19., 19., 27.,  0.]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forward(self, input_tokens, max_output_len, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Forward pass of the seq2seq model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_tokens: Tensor of shape <batch_size x max_length>\n",
    "        The input sequence\n",
    "    max_output_len: int\n",
    "        The maximum length of the output sequence\n",
    "    \"\"\"\n",
    "    batch_size = input_tokens.size(0)\n",
    "    vocab_size = self.vocab_size\n",
    "\n",
    "    # Get encoder outputs and hidden states\n",
    "    print(input_tokens.shape)\n",
    "    encoder_outputs, encoder_hiddens = self.encoder(input_tokens)\n",
    "    decoder_hidden = encoder_hiddens\n",
    "\n",
    "    # First input to decoder is SOS token\n",
    "    decoder_input = (\n",
    "        torch.ones((batch_size, 1), dtype=torch.long, device=self.device)\n",
    "        * self.sos_token_id\n",
    "    )\n",
    "\n",
    "    # Initialize outputs tensor to store decoder outputs\n",
    "    outputs = torch.zeros(batch_size, max_output_len, device=self.device)\n",
    "\n",
    "    # Generate sequence\n",
    "    for t in range(max_output_len):\n",
    "        output, decoder_hidden = self.decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "        outputs[:, t] = torch.argmax(output, dim=2)\n",
    "\n",
    "        # Sample next token using temperature\n",
    "        if temperature > 0:\n",
    "            probs = torch.nn.functional.softmax(output.squeeze(1) / temperature, dim=-1)\n",
    "            decoder_input = torch.multinomial(probs, 1)\n",
    "        else:\n",
    "            decoder_input = torch.argmax(output, dim=2)\n",
    "\n",
    "        # Stop if all sequences in batch hit EOS\n",
    "        if (decoder_input == self.eos_token_id).all():\n",
    "            break\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "forward(model, ciphered_text_tokenized, max_output_len=12, temperature=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advnetsci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

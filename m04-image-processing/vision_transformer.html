
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Vision Transformers &#8212; Applied Soft Computing</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'm04-image-processing/vision_transformer';</script>
    <script src="../_static/js/custom.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../home.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.jpg" class="logo__image only-light" alt="Applied Soft Computing - Home"/>
    <script>document.write(`<img src="../_static/logo.jpg" class="logo__image only-dark" alt="Applied Soft Computing - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../home.html">
                    Welcome to SSIE 541/441 Applied Soft Computing
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Intro</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../intro/why-applied-soft-computing.html">Why applied soft computing?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/setup.html">Set up</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/deliverables.html">Deliverables</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Word and Document Embeddings</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/what-to-learn.html">Module 1: Word Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/pen-and-paper.html">Pen and Paper Exercise</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/tf-idf.html">Teaching computers how to understand words</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/word2vec.html">word2vec: a small model with a big idea</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/word2vec_plus.html">GloVe and FastText: Building on Word2Vec‚Äôs Foundation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/sem-axis.html">Understanding SemAxis: Semantic Axes in Word Vector Spaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/bias-in-embedding.html">Bias in Word Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/doc2vec.html">Doc2Vec: From Words to Documents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/summary.html">Summary</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Recurrent Neural Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/what-to-learn.html">Module 2: Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/rnn-interactive.html">üß† Learn RNNs Through Physics!</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/reccurrent-neural-net.html">Recurrent Neural Network (RNN)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/lstm.html">Long Short-Term Memory (LSTM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/elmo.html">Embedding from Language Models (ELMo)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/seq2seq.html">Sequence-to-Sequence Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/summary.html">Summary</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Transformers</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/what-to-learn.html">Module 3: Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/transformers.html">Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/bert.html">Bidirectional Encoder Representations from Transformers (BERT)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/sentence-bert.html">Sentence-BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/gpt.html">Generative Pre-trained Transformer (GPT)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/from-language-model-to-instruction-following.html">From Language Model to Instruction-Following</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/prompt-tuning.html">Prompt Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/prompt-tuning-exercise.html">Prompt Tuning Exercise</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/summary.html">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/appendix-t5.html">Appendix: Text-to-Text Transfer Transformer (T5)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Teaching Machine How to Understand Images</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="what-to-learn.html">Module 4: Image Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="pen-and-paper.html">Pen and paper exercises</a></li>
<li class="toctree-l1"><a class="reference internal" href="image-processing.html">Preliminaries: Image Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="cnn.html">From Traditional Image Processing to Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="lenet.html">LeNet and LeNet-5: Pioneering CNN Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="alexnet.html">AlexNet: A Breakthrough in Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="vgg.html">VGGNet - A Deep Convolutional Neural Network for Image Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="inception.html">GoogleNet and the Inception Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="resnet.html">ResNet (Residual Neural Networks)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Teaching Machine How to Understand Graphs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/what-to-learn.html">Module 4: Graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/pen-and-paper.html">Pen and paper exercises</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/spectral-embedding.html">Spectral Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/graph-embedding-w-word2vec.html">Graph embedding with word2vec</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/spectral-vs-neural-embedding.html">Spectral vs Neural Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/from-image-to-graph.html">From Image to Graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/graph-convolutional-network.html">Graph Convolutional Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/popular-gnn.html">Popular Graph Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/appendix.html">Appendix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/software.html">Software for Network Embedding</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/skojaku/applied-soft-comp" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/skojaku/applied-soft-comp/issues/new?title=Issue%20on%20page%20%2Fm04-image-processing/vision_transformer.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/m04-image-processing/vision_transformer.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Vision Transformers</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-genesis-of-vision-transformers">The Genesis of Vision Transformers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conceptual-foundation">Conceptual Foundation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-vision-transformer-vit-architecture">The Vision Transformer (ViT) Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-encodings">2.2 Positional Encodings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-encoder">2.3 Transformer Encoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-head">2.4 Classification Head</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-attention-mechanisms">3. Types of Attention Mechanisms</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-example">4. Implementation Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advancements-in-vision-transformer-architectures">5. Advancements in Vision Transformer Architectures</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#improved-training-and-architectures">5.1 Improved Training and Architectures</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hierarchical-and-hybrid-approaches">5.2 Hierarchical and Hybrid Approaches</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#strengths-and-weaknesses">6. Strengths and Weaknesses</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-advantages">Key Advantages</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#main-limitations">Main Limitations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#real-world-applications">7. Real-World Applications</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#future-directions">8. Future Directions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reflection-and-exercises">9. Reflection and Exercises</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="vision-transformers">
<h1>Vision Transformers<a class="headerlink" href="#vision-transformers" title="Link to this heading">#</a></h1>
<p><img alt="" src="https://miro.medium.com/v2/resize:fit:700/0*Rtb7Jt6378xfe6Z1.png" /></p>
<p>What if we could capture not just the local features in images (like corners, edges, or textures) but the entire global context all at once? Could that help a model better understand complex scenes and relationships between objects? Vision Transformers (ViT) attempt exactly that by leveraging <strong>self-attention</strong>, a mechanism originally popularized in Natural Language Processing.</p>
<section id="the-genesis-of-vision-transformers">
<h2>The Genesis of Vision Transformers<a class="headerlink" href="#the-genesis-of-vision-transformers" title="Link to this heading">#</a></h2>
<section id="conceptual-foundation">
<h3>Conceptual Foundation<a class="headerlink" href="#conceptual-foundation" title="Link to this heading">#</a></h3>
<p><em>Why were Vision Transformers developed, given that Convolutional Neural Networks (CNNs) already excel in computer vision tasks?</em></p>
<p>CNNs have been the cornerstone of computer vision for years, particularly good at capturing local patterns through convolutional filters. However, they can struggle to efficiently capture <strong>global context</strong> and <strong>long-range dependencies</strong>. In scenarios where relationships between objects spread across an entire image become crucial (e.g., understanding crowd scenes or satellite imagery), this limitation can be significant.</p>
<p>Meanwhile, the <strong>Transformer architecture</strong> (from the paper ‚ÄúAttention Is All You Need‚Äù) revolutionized NLP by modeling long-range dependencies in sequential data. This success inspired researchers to ask: <em>Could the same self-attention mechanism help models ‚Äòsee‚Äô the entire image at once, instead of focusing on small, local regions?</em></p>
<figure class="align-center" id="fig-cnn-vit">
<a class="reference internal image-reference" href="https://www.researchgate.net/publication/361733806/figure/fig3/AS%3A1173979050057729%401656909825527/Operation-of-CNN-and-ViT.ppm"><img alt="https://www.researchgate.net/publication/361733806/figure/fig3/AS%3A1173979050057729%401656909825527/Operation-of-CNN-and-ViT.ppm" src="https://www.researchgate.net/publication/361733806/figure/fig3/AS%3A1173979050057729%401656909825527/Operation-of-CNN-and-ViT.ppm" style="width: 500px;" />
</a>
<figcaption>
<p><span class="caption-text">Comparison of the receptive field of CNNs and Vision Transformers. CNN has a local receptive field constrained by the convolutional filters, while ViT has a global receptive field, allowing it to capture long-range dependencies between different parts of the image.</span><a class="headerlink" href="#fig-cnn-vit" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Historical Context</strong></p>
<ul class="simple">
<li><p><strong>CNN Dominance (2010s)</strong>: CNNs (e.g., AlexNet, VGG, ResNet) drove huge leaps in image classification and object detection.</p></li>
<li><p><strong>Transformer Breakthrough (2017)</strong>: In NLP, Transformers replaced recurrent architectures (LSTMs, GRUs) for tasks like machine translation.</p></li>
<li><p><strong>ViT Emerges (2020)</strong>: Google researchers introduced the idea of applying pure Transformers to image patches, showing excellent results on large-scale image datasets.</p></li>
</ul>
</div>
</section>
</section>
<section id="the-vision-transformer-vit-architecture">
<h2>The Vision Transformer (ViT) Architecture<a class="headerlink" href="#the-vision-transformer-vit-architecture" title="Link to this heading">#</a></h2>
<p><em>How do we adapt an NLP-centric Transformer to handle 2D image data?</em></p>
<p>In <strong>Vision Transformers</strong>, an image is first split into a grid of small, equally sized patches‚Äîcommonly <span class="math notranslate nohighlight">\(16 \times 16\)</span> pixels each. Each patch is <strong>flattened</strong> and fed into a linear layer that creates a higher-dimensional embedding. You can think of each patch embedding as analogous to a ‚Äúword embedding‚Äù in NLP.</p>
<figure class="align-center" id="fig-vit-patch">
<a class="reference internal image-reference" href="https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F7a096efc8f3cc40849ee17a546dc0e685da2dc73-4237x1515.png&amp;w=3840&amp;q=75"><img alt="https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F7a096efc8f3cc40849ee17a546dc0e685da2dc73-4237x1515.png&amp;w=3840&amp;q=75" src="https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F7a096efc8f3cc40849ee17a546dc0e685da2dc73-4237x1515.png&amp;w=3840&amp;q=75" style="width: 500px;" />
</a>
<figcaption>
<p><span class="caption-text">The process of splitting an image into patches and feeding them into a Vision Transformer. Image taken from <a class="reference external" href="https://www.pinecone.io/learn/series/image-search/vision-transformers/">Pinecone</a>.</span><a class="headerlink" href="#fig-vit-patch" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Why Patches Instead of Pixels?</strong></p>
<ul class="simple">
<li><p>Handling each pixel independently would create a massive sequence (e.g., a 224x224 image has 50176 pixels!).</p></li>
<li><p>Using patches reduces sequence length substantially and preserves local spatial structure.</p></li>
</ul>
</div>
<section id="positional-encodings">
<h3>2.2 Positional Encodings<a class="headerlink" href="#positional-encodings" title="Link to this heading">#</a></h3>
<p>Because Transformers are order-agnostic, we add <strong>positional encodings</strong> to each patch embedding. These encodings help the model understand the position of each patch in the original image grid.</p>
</section>
<section id="transformer-encoder">
<h3>2.3 Transformer Encoder<a class="headerlink" href="#transformer-encoder" title="Link to this heading">#</a></h3>
<p>The sequence of patch embeddings (plus positional encodings) goes through a <strong>Transformer encoder</strong>, consisting of:</p>
<ul class="simple">
<li><p><strong>Multi-Head Self-Attention</strong>: Allows each patch to attend to others, learning both local and global image features.</p></li>
<li><p><strong>Feed-Forward Layers (MLP blocks)</strong>: Expands and contracts the hidden dimension to add non-linear transformations.</p></li>
</ul>
</section>
<section id="classification-head">
<h3>2.4 Classification Head<a class="headerlink" href="#classification-head" title="Link to this heading">#</a></h3>
<p>Typically, the <strong>[CLS] token</strong> (a special token prepended to the sequence) serves as the global representation. After passing through all encoder layers, it goes to a lightweight classification head (a small MLP) to predict the output class.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Mathematical Foundation (Simplified)</strong></p>
<p>Self-attention for a single head can be described as:
$<span class="math notranslate nohighlight">\(
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
\)</span>$</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Q, K, V\)</span> are linear projections of the input (patch embeddings).</p></li>
<li><p><span class="math notranslate nohighlight">\(d_k\)</span> is the dimension of <span class="math notranslate nohighlight">\(K\)</span>.</p></li>
<li><p>Multi-head attention runs this process in parallel with different learnable projections.</p></li>
</ul>
</div>
</section>
</section>
<hr class="docutils" />
<section id="types-of-attention-mechanisms">
<h2>3. Types of Attention Mechanisms<a class="headerlink" href="#types-of-attention-mechanisms" title="Link to this heading">#</a></h2>
<p>Even though Vision Transformers generally use <strong>multi-head self-attention</strong>, research has explored variations:</p>
<ol class="arabic simple">
<li><p><strong>Stochastic ‚ÄúHard‚Äù Attention</strong>: Focuses on a subset of patches while ignoring others.</p></li>
<li><p><strong>Deterministic ‚ÄúSoft‚Äù Attention</strong>: Assigns weights to all patches.</p></li>
<li><p><strong>Multi-Head Attention</strong>: Employs multiple attention heads to learn different aspects (textures, edges, shapes) simultaneously.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Implementation Insight</strong></p>
<p>You can vary the attention mechanism to strike different balances between computational cost and representational capacity. Hard attention can be more efficient but trickier to train.</p>
</div>
</section>
<hr class="docutils" />
<section id="implementation-example">
<h2>4. Implementation Example<a class="headerlink" href="#implementation-example" title="Link to this heading">#</a></h2>
<p>Let‚Äôs walk through a simplified code snippet using <strong>Hugging Face Transformers</strong> to classify images with a Vision Transformer. This gives a concrete look at how to build upon these theoretical concepts in practice.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul class="simple">
<li><p><strong>Data Augmentation</strong>: When training from scratch on smaller datasets, apply heavy data augmentation (random crops, flips, color jitter) to avoid overfitting.</p></li>
<li><p><strong>Transfer Learning</strong>: Leverage pre-trained weights from large datasets and then fine-tune on your target task for improved performance.</p></li>
<li><p><strong>Batch Size</strong>: ViTs can be memory-heavy; consider smaller batch sizes and gradient accumulation if GPU memory is limited.</p></li>
</ul>
</div>
<p><img alt="" src="https://via.placeholder.com/600x300" />
[Figure: A schematic visualization of patches being extracted from an image and fed into a Vision Transformer pipeline.]</p>
</section>
<hr class="docutils" />
<section id="advancements-in-vision-transformer-architectures">
<h2>5. Advancements in Vision Transformer Architectures<a class="headerlink" href="#advancements-in-vision-transformer-architectures" title="Link to this heading">#</a></h2>
<p>Could we make ViTs more data-efficient, faster, or better at capturing hierarchical features?</p>
<section id="improved-training-and-architectures">
<h3>5.1 Improved Training and Architectures<a class="headerlink" href="#improved-training-and-architectures" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>DeiT (Data-efficient Image Transformers)</strong>: Introduces a distillation step to improve data efficiency, making ViTs competitive with CNNs on smaller datasets.</p></li>
<li><p><strong>Model Soups</strong>: Averages predictions from multiple ViT models to harness their individual strengths for higher accuracy.</p></li>
</ul>
</section>
<section id="hierarchical-and-hybrid-approaches">
<h3>5.2 Hierarchical and Hybrid Approaches<a class="headerlink" href="#hierarchical-and-hybrid-approaches" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Swin Transformer</strong>: Processes images in a hierarchical manner using non-overlapping patches at different resolutions, improving scalability to arbitrary image sizes.</p></li>
<li><p><strong>CaiT (Cross-Attention Image Transformer)</strong>: Uses cross-attention between different patch groups to capture more complex relationships.</p></li>
<li><p><strong>CSWin Transformer</strong>: Adopts a cross-shaped window self-attention pattern to optimize the balance between spatial coverage and computational cost.</p></li>
<li><p><strong>FDViT</strong>: Employs flexible downsampling layers for smoother feature map reductions, improving efficiency and classification accuracy.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Common Misconception</strong></p>
<p>It‚Äôs tempting to think ViTs automatically solve all the limitations of CNNs. However, they still require careful tuning, large datasets (or pre-training), and thoughtful architecture decisions to perform at their best.</p>
</div>
</section>
</section>
<hr class="docutils" />
<section id="strengths-and-weaknesses">
<h2>6. Strengths and Weaknesses<a class="headerlink" href="#strengths-and-weaknesses" title="Link to this heading">#</a></h2>
<p>When do Vision Transformers shine, and where do they falter?</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Convolutional Neural Networks (CNNs)</p></th>
<th class="head"><p>Vision Transformers (ViTs)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Architecture</strong></p></td>
<td><p>Convolutional + pooling + MLP</p></td>
<td><p>Pure Transformer with self-attention</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Input Processing</strong></p></td>
<td><p>Processes entire image as is</p></td>
<td><p>Splits image into patches (tokens)</p></td>
</tr>
<tr class="row-even"><td><p><strong>Global Context</strong></p></td>
<td><p>Emerges in deeper layers</p></td>
<td><p>Captured from the start across all patches</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Data Requirements</strong></p></td>
<td><p>Perform well with moderate data</p></td>
<td><p>Often require very large datasets or pre-training</p></td>
</tr>
<tr class="row-even"><td><p><strong>Compute Cost</strong></p></td>
<td><p>Usually lower, localized ops</p></td>
<td><p>Higher due to self-attention on all patches</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Performance</strong></p></td>
<td><p>Excellent with well-tuned architectures</p></td>
<td><p>Excels on large-scale data, state-of-the-art SOTA</p></td>
</tr>
</tbody>
</table>
</div>
<section id="key-advantages">
<h3>Key Advantages<a class="headerlink" href="#key-advantages" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Global Context</strong>: The self-attention mechanism can integrate information from all patches simultaneously.</p></li>
<li><p><strong>Scalability</strong>: ViTs shine on large datasets, often surpassing CNNs.</p></li>
<li><p><strong>Reduced Inductive Bias</strong>: They learn more general representations since they are not hard-coded to look for local spatial features like CNNs.</p></li>
</ul>
</section>
<section id="main-limitations">
<h3>Main Limitations<a class="headerlink" href="#main-limitations" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Data-Hungry</strong>: Tend to overfit on small datasets; methods like DeiT and heavy augmentation help.</p></li>
<li><p><strong>High Computational Cost</strong>: Each patch attends to all others, which can be expensive for high-resolution images.</p></li>
<li><p><strong>Interpretability</strong>: Visualizing attention maps is possible, but can still be less intuitive than CNN feature maps.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="real-world-applications">
<h2>7. Real-World Applications<a class="headerlink" href="#real-world-applications" title="Link to this heading">#</a></h2>
<p>How are Vision Transformers being used beyond simple image classification?</p>
<ul class="simple">
<li><p><strong>Object Detection &amp; Image Segmentation</strong>: Self-attention helps capture relationships among objects scattered across the scene.</p></li>
<li><p><strong>Medical Imaging</strong>: Identifying tumors in X-rays or segmenting organ boundaries in MRI scans.</p></li>
<li><p><strong>Remote Sensing</strong>: Analyzing satellite imagery for deforestation tracking or disaster management.</p></li>
<li><p><strong>Action Recognition in Videos</strong>: Extended to video frames, ViTs can learn complex spatiotemporal patterns.</p></li>
<li><p><strong>Multi-Modal Tasks</strong>: Works well with textual data (e.g., image captioning, visual question answering).</p></li>
<li><p><strong>Autonomous Driving</strong>: Understanding global context on the road is critical for safe navigation.</p></li>
<li><p><strong>Anomaly Detection</strong>: Identifying unusual patterns or defects in manufacturing lines.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Real-World Use Case</strong></p>
<p>In <strong>medical imaging</strong>, ViTs can better spot anomalies by focusing on subtle global context differences in scans. This can help radiologists detect diseases in early stages and potentially save lives.</p>
</div>
</section>
<hr class="docutils" />
<section id="future-directions">
<h2>8. Future Directions<a class="headerlink" href="#future-directions" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Enhanced Efficiency</strong>: Model compression, pruning, and improved patch strategies aim to reduce computational overhead.</p></li>
<li><p><strong>Smaller Dataset Training</strong>: More advanced self-supervision, distillation, and data-augmentation techniques are being developed to tackle data limitations.</p></li>
<li><p><strong>Interpretability</strong>: Research on attention visualization tools and explanations is growing, aiming to make ViTs more transparent.</p></li>
<li><p><strong>New Domains</strong>: From multi-modal reasoning to video analysis, ViTs are expanding across countless tasks in AI.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Performance Optimization</strong></p>
<ul class="simple">
<li><p>Distillation from large teacher ViTs or even CNNs can help small ViTs converge faster with less data.</p></li>
<li><p>Layer-wise learning rate decay and progressive resizing of patches are common training tricks.</p></li>
</ul>
</div>
</section>
<hr class="docutils" />
<section id="reflection-and-exercises">
<h2>9. Reflection and Exercises<a class="headerlink" href="#reflection-and-exercises" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><strong>Reflection</strong>: Why do you think ViTs require large datasets to perform optimally, and how might transfer learning mitigate this requirement?</p></li>
<li><p><strong>Exercise</strong>: Implement a fine-tuning script for a ViT on a smaller dataset (e.g., CIFAR-10). Try various data augmentation strategies. Compare results with a CNN baseline.</p></li>
<li><p><strong>Advanced Exploration</strong>: Experiment with Swin Transformer or CSWin Transformer. Observe how hierarchical patching or specialized window attention changes performance and training speed.</p></li>
</ol>
</section>
<hr class="docutils" />
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Vision Transformers - The Future of Computer Vision! [ResearchGate]</p></li>
<li><p>Introduction to Vision Transformers | Original ViT Paper Explained [<a class="reference external" href="http://aipapersacademy.com">aipapersacademy.com</a>]</p></li>
<li><p>Deploying Attention-Based Vision Transformers to Apple Neural Engine [<a class="reference external" href="http://machinelearning.apple.com">machinelearning.apple.com</a>]</p></li>
<li><p>Vision Transformers (ViT) in Image Recognition: Full Guide [<a class="reference external" href="http://viso.ai">viso.ai</a>]</p></li>
<li><p>Vision Transformers, Explained. A Full Walk-Through of Vision‚Ä¶ [Towards Data Science]</p></li>
<li><p>From Transformers to Vision Transformers (ViT): Applying NLP Models to Computer Vision [Medium]</p></li>
<li><p>A Comprehensive Study of Vision Transformers in Image Classification Tasks [arXiv]</p></li>
<li><p>Introductory guide to Vision Transformers [Encord]</p></li>
<li><p>Efficient Training of Visual Transformers with Small Datasets [NeurIPS Proceedings]</p></li>
<li><p>FDViT: Improve the Hierarchical Architecture of Vision Transformer [ICCV 2023]</p></li>
<li><p>Vision Transformers vs. Convolutional Neural Networks (CNNs) [GeeksforGeeks]</p></li>
<li><p>Vision Transformers vs CNNs at the Edge [Edge AI Vision]</p></li>
<li><p>What is a Vision Transformer (ViT)? Real-World Applications [SJ Innovation]</p></li>
<li><p>Vision Transformer: An Introduction [Built In]</p></li>
<li><p>Mastering Vision Transformers with Hugging Face [Rapid Innovation]</p></li>
<li><p>Vision Transformer (ViT) - Hugging Face [<a class="reference external" href="http://huggingface.co">huggingface.co</a>]</p></li>
<li><p>Top 10 Open Source Computer Vision Repositories [Encord]</p></li>
<li><p>yhlleo/VTs-Drloc: Efficient Training of Visual Transformers [GitHub]</p></li>
<li><p>Vision Transformers for Image Classification: A Comparative Survey [MDPI]</p></li>
<li><p>BMVC 2022: How to Train Vision Transformer on Small-scale Datasets? [GitHub]</p></li>
<li><p>Vision Transformer: What It Is &amp; How It Works [V7 Labs]</p></li>
</ol>
<hr class="docutils" />
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Key Takeaway</strong></p>
<p>Vision Transformers offer a fresh approach to image understanding by modeling global relationships among patches from the get-go. As architectures and training strategies evolve, they stand poised to become foundational building blocks in next-generation computer vision systems.</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "skojaku/applied-soft-comp",
            ref: "gh-pages",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./m04-image-processing"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-genesis-of-vision-transformers">The Genesis of Vision Transformers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conceptual-foundation">Conceptual Foundation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-vision-transformer-vit-architecture">The Vision Transformer (ViT) Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-encodings">2.2 Positional Encodings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-encoder">2.3 Transformer Encoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-head">2.4 Classification Head</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-attention-mechanisms">3. Types of Attention Mechanisms</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-example">4. Implementation Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advancements-in-vision-transformer-architectures">5. Advancements in Vision Transformer Architectures</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#improved-training-and-architectures">5.1 Improved Training and Architectures</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hierarchical-and-hybrid-approaches">5.2 Hierarchical and Hybrid Approaches</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#strengths-and-weaknesses">6. Strengths and Weaknesses</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-advantages">Key Advantages</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#main-limitations">Main Limitations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#real-world-applications">7. Real-World Applications</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#future-directions">8. Future Directions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reflection-and-exercises">9. Reflection and Exercises</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sadamori Kojaku
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
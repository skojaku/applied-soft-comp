
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>AlexNet: A Breakthrough in Deep Learning &#8212; Applied Soft Computing</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'm04-image-processing/alexnet';</script>
    <script src="../_static/js/custom.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="VGGNet - A Deep Convolutional Neural Network for Image Recognition" href="vgg.html" />
    <link rel="prev" title="LeNet and LeNet-5: Pioneering CNN Architectures" href="lenet.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../home.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.jpg" class="logo__image only-light" alt="Applied Soft Computing - Home"/>
    <script>document.write(`<img src="../_static/logo.jpg" class="logo__image only-dark" alt="Applied Soft Computing - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../home.html">
                    Welcome to SSIE 541/441 Applied Soft Computing
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Intro</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../intro/why-applied-soft-computing.html">Why applied soft computing?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/setup.html">Set up</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/deliverables.html">Deliverables</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Word and Document Embeddings</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/what-to-learn.html">Module 1: Word Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/pen-and-paper.html">Pen and Paper Exercise</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/tf-idf.html">Teaching computers how to understand words</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/word2vec.html">word2vec: a small model with a big idea</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/word2vec_plus.html">GloVe and FastText: Building on Word2Vec’s Foundation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/sem-axis.html">Understanding SemAxis: Semantic Axes in Word Vector Spaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/bias-in-embedding.html">Bias in Word Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/doc2vec.html">Doc2Vec: From Words to Documents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-word-embedding/summary.html">Summary</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Recurrent Neural Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/what-to-learn.html">Module 2: Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/pen-and-paper.html">Pen and Paper Exercise</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/rnn-interactive.html">Interactive RNN Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/reccurrent-neural-net.html">Recurrent Neural Network (RNN)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/memory-passing-game.html">Memory Passing Game</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/lstm.html">Long Short-Term Memory (LSTM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/elmo.html">Embedding from Language Models (ELMo)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/seq2seq.html">Sequence-to-Sequence Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-recurrent-neural-network/summary.html">Summary</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Transformers</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/what-to-learn.html">Module 3: Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/transformers.html">Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/bert.html">Bidirectional Encoder Representations from Transformers (BERT)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/sentence-bert.html">Sentence-BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/gpt.html">Generative Pre-trained Transformer (GPT)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/t5.html">Text-to-Text Transfer Transformer (T5)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-transformers/summary.html">Summary</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Teaching Machine How to Understand Images</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="what-to-learn.html">Module 3: Image Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="pen-and-paper.html">Pen and paper exercises</a></li>
<li class="toctree-l1"><a class="reference internal" href="image-processing.html">Preliminaries: Image Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="cnn.html">From Traditional Image Processing to Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="lenet.html">LeNet and LeNet-5: Pioneering CNN Architectures</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">AlexNet: A Breakthrough in Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="vgg.html">VGGNet - A Deep Convolutional Neural Network for Image Recognition</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Teaching Machine How to Understand Graphs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/what-to-learn.html">Module 4: Graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/pen-and-paper.html">Pen and paper exercises</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/spectral-embedding.html">Spectral Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/graph-embedding-w-word2vec.html">Graph embedding with word2vec</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/spectral-vs-neural-embedding.html">Spectral vs Neural Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/from-image-to-graph.html">From Image to Graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/graph-convolutional-network.html">Graph Convolutional Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/popular-gnn.html">Popular Graph Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/appendix.html">Appendix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-graph/software.html">Software for Network Embedding</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/skojaku/applied-soft-comp/gh-pages?urlpath=tree/docs/lecture-note/m04-image-processing/alexnet.md" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/skojaku/applied-soft-comp" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/skojaku/applied-soft-comp/issues/new?title=Issue%20on%20page%20%2Fm04-image-processing/alexnet.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/m04-image-processing/alexnet.ipynb" target="_blank"
   class="btn btn-sm btn-download-notebook-button dropdown-item"
   title="Download notebook file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li><a href="../_sources/m04-image-processing/alexnet.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>AlexNet: A Breakthrough in Deep Learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-imagenet-challenge-and-the-deep-learning-revolution">The ImageNet Challenge and the Deep Learning Revolution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-innovations-of-alexnet">Key Innovations of AlexNet</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relu-activation-function">ReLU Activation Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dropout-regularization">Dropout Regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#local-response-normalization-lrn">Local Response Normalization (LRN)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture-and-implementation">Architecture and Implementation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">Implementation</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="alexnet-a-breakthrough-in-deep-learning">
<h1>AlexNet: A Breakthrough in Deep Learning<a class="headerlink" href="#alexnet-a-breakthrough-in-deep-learning" title="Link to this heading">#</a></h1>
<section id="the-imagenet-challenge-and-the-deep-learning-revolution">
<h2>The ImageNet Challenge and the Deep Learning Revolution<a class="headerlink" href="#the-imagenet-challenge-and-the-deep-learning-revolution" title="Link to this heading">#</a></h2>
<p>The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) represented one of the most ambitious and challenging tasks in computer vision: classifying images across 1000 different categories. Before 2012, the best performing systems struggled with error rates above 25%, relying primarily on hand-crafted features and traditional machine learning approaches. The sheer scale of the dataset—–with over 1.2 million training images—–posed significant computational challenges that many believed would make deep learning approaches impractical.</p>
<figure class="align-default" id="id5">
<img alt="https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11263-015-0816-y/MediaObjects/11263_2015_816_Fig2_HTML.gif" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11263-015-0816-y/MediaObjects/11263_2015_816_Fig2_HTML.gif" />
<figcaption>
<p><span class="caption-number">Fig. 60 </span><span class="caption-text">The ImageNet Large Scale Visual Recognition Challenge (ILSVRC)</span><a class="headerlink" href="#id5" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Prior to AlexNet, the dominant approaches in computer vision relied heavily on hand-engineered features like SIFT (Scale-Invariant Feature Transform) and HOG (Histogram of Oriented Gradients). These methods required significant domain expertise and often failed to generalize well across different types of images.</p>
</div>
<p>In 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton submitted a deep convolutional neural network that achieved a top-5 error rate of 16.4%—an unprecedented improvement of over 10 percentage points compared to the second-best entry <a class="footnote-reference brackets" href="#footcite-krizhevsky2012alexnet" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>.
This represents a paradigm shift in computer vision and machine learning towards deep learning.</p>
<figure class="align-center" id="id6">
<a class="reference internal image-reference" href="https://viso.ai/wp-content/uploads/2024/02/imagenet-winners-by-year.jpg"><img alt="https://viso.ai/wp-content/uploads/2024/02/imagenet-winners-by-year.jpg" src="https://viso.ai/wp-content/uploads/2024/02/imagenet-winners-by-year.jpg" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 61 </span><span class="caption-text">Top 5 error rates of the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) from 2010 to 2017. AlexNet reduced the error rate by over 10 percentage points compared to the best performing method based on human-crafted features in 2011.</span><a class="headerlink" href="#id6" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="key-innovations-of-alexnet">
<h2>Key Innovations of AlexNet<a class="headerlink" href="#key-innovations-of-alexnet" title="Link to this heading">#</a></h2>
<p>The idea of using multiple layers of neurons has been around for a long time. However, deep neural networks have cricial issues in training. AlexNet introduced several crucial innovations to address this challenge, which have since become standard practices in deep learning:</p>
<section id="relu-activation-function">
<h3>ReLU Activation Function<a class="headerlink" href="#relu-activation-function" title="Link to this heading">#</a></h3>
<figure class="align-center" id="id7">
<a class="reference internal image-reference" href="https://miro.medium.com/v2/resize:fit:474/1*HGctgaVdv9rEHIVvLYONdQ.jpeg"><img alt="https://miro.medium.com/v2/resize:fit:474/1*HGctgaVdv9rEHIVvLYONdQ.jpeg" src="https://miro.medium.com/v2/resize:fit:474/1*HGctgaVdv9rEHIVvLYONdQ.jpeg" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 62 </span><span class="caption-text">Sigmoid and ReLU activation functions. Sigmoid is prone to the vanishing gradient problem due to the plateau for input <span class="math notranslate nohighlight">\(x\)</span> far from zero. ReLU, on the other hand, does not suffer from this problem as long as <span class="math notranslate nohighlight">\(x&gt;0\)</span>. The image is taken from <a class="reference external" href="https://medium.com/&#64;jarajan123/a-comparative-analysis-relu-vs-sigmoid-activation-functions-fa1dbe481d80">https://medium.com/&#64;jarajan123/a-comparative-analysis-relu-vs-sigmoid-activation-functions-fa1dbe481d80</a></span><a class="headerlink" href="#id7" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Traditional neural networks with multiple layers often suffer from <em>the vanishing gradient problem</em>. It is a phenomenon where the gradient of parameters in early layers approaches zero, hindering the training process. This is attributed to the activation functions, the most common one being the sigmoid function.</p>
<p>The sigmoid function is defined as:</p>
<div class="math notranslate nohighlight">
\[
f(x) = \frac{1}{1 + e^{-x}}
\]</div>
<p>The sigmoid function has a gradient of:</p>
<div class="math notranslate nohighlight">
\[
f'(x) = f(x) (1 - f(x))
\]</div>
<p>Input <span class="math notranslate nohighlight">\(x\)</span> is random and effectively distributed around zero when the neural network is initialized. The gradient is thus large. However, as the neural network learns the data, the input signal <span class="math notranslate nohighlight">\(x\)</span> tends to be far from zero, making the gradient of the sigmoid function approach zero.</p>
<p>The Rectified Linear Unit (ReLU) was proposed in the previous year of AlexNet by one of the authors of AlexNet, Hinton <a class="footnote-reference brackets" href="#footcite-nair2010rectified" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>.</p>
<p>The Rectified Linear Unit (ReLU) is defined by a simple operation:</p>
<div class="math notranslate nohighlight">
\[
f(x) = \max(0, x)
\]</div>
<p>The gradient of the ReLU function is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
f'(x) = \begin{cases}
0 &amp; \text{if } x \leq 0 \\
1 &amp; \text{if } x &gt; 0
\end{cases}
\end{split}\]</div>
<p>The ReLU function solves the vanishing gradient problem since its gradient is always either 0 or 1 when the input is positive. Additionally, ReLU is computationally efficient because it only needs to check if the input is greater than zero, making it faster than more complex activation functions like sigmoid.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>ReLU suffers from so-called “dying neurons” problem, where neurons can get stuck in the dead state (i.e., <span class="math notranslate nohighlight">\(x \leq 0\)</span>) and never activate, since the gradient is zero. This issue leads to various activation functions that add a small positive value for <span class="math notranslate nohighlight">\(x \leq 0\)</span> to the ReLU function. For example, Leaky ReLU and Parametric ReLU (PReLU) are two such activation functions defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
f(x) = \begin{cases}
x &amp; \text{if } x &gt; 0 \\
\alpha x &amp; \text{if } x \leq 0
\end{cases}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> is a small positive constant.</p>
<figure class="align-center" id="id8">
<a class="reference internal image-reference" href="https://media.licdn.com/dms/image/v2/D4D12AQH2F3GJ9wen_Q/article-cover_image-shrink_720_1280/article-cover_image-shrink_720_1280/0/1688885174323?e=2147483647&amp;v=beta&amp;t=dY_S6xeNsRCIvpIrjrPFzq8qgHPgmP4e_HLaA15ufPM"><img alt="https://media.licdn.com/dms/image/v2/D4D12AQH2F3GJ9wen_Q/article-cover_image-shrink_720_1280/article-cover_image-shrink_720_1280/0/1688885174323?e=2147483647&amp;v=beta&amp;t=dY_S6xeNsRCIvpIrjrPFzq8qgHPgmP4e_HLaA15ufPM" src="https://media.licdn.com/dms/image/v2/D4D12AQH2F3GJ9wen_Q/article-cover_image-shrink_720_1280/article-cover_image-shrink_720_1280/0/1688885174323?e=2147483647&amp;v=beta&amp;t=dY_S6xeNsRCIvpIrjrPFzq8qgHPgmP4e_HLaA15ufPM" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 63 </span><span class="caption-text">Comparison of various activation functions. The image is taken from <a class="reference external" href="https://www.linkedin.com/pulse/activation-functions-heba-al-haddad">https://www.linkedin.com/pulse/activation-functions-heba-al-haddad</a></span><a class="headerlink" href="#id8" title="Link to this image">#</a></p>
</figcaption>
</figure>
</div>
</section>
<section id="dropout-regularization">
<h3>Dropout Regularization<a class="headerlink" href="#dropout-regularization" title="Link to this heading">#</a></h3>
<p>Another key limitation of deep neural networks is overfitting. Overfitting occurs when the neural network learns the training data too well, resulting in poor generalization to unseen data. It is thus crucial to prevent overfitting by “regularizing” the neural network.</p>
<p>A traditional way to prevent overfitting is to use L2 regularization, which adds a penalty term to the loss function to prevent the weights from becoming too large. While this method is effective, an easier alternative is <em>dropout</em> <a class="footnote-reference brackets" href="#footcite-srivastava2014dropout" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>.</p>
<figure class="align-center" id="id9">
<a class="reference internal image-reference" href="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/1IrdJ5PghD9YoOyVAQ73MJw.gif"><img alt="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/1IrdJ5PghD9YoOyVAQ73MJw.gif" src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/1IrdJ5PghD9YoOyVAQ73MJw.gif" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 64 </span><span class="caption-text">Dropout in action. The image is taken from <a class="reference external" href="https://primo.ai/index.php?title=Dropout">https://primo.ai/index.php?title=Dropout</a></span><a class="headerlink" href="#id9" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Dropout is a regularization technique that randomly drops out neurons during training to prevent overfitting. It works in two modes:</p>
<ol class="arabic simple">
<li><p>During training, each neuron has a probability <span class="math notranslate nohighlight">\(p\)</span> (typically 0.5) of being temporarily “dropped out” of the network. Specifically, for a given input <span class="math notranslate nohighlight">\(x\)</span>, the output <span class="math notranslate nohighlight">\(y\)</span> of the neuron is given by
$<span class="math notranslate nohighlight">\(
y = r \cdot x, \quad \text{where } r \sim \text{Bernoulli}(p)
\)</span>$</p></li>
<li><p>During inference, all neurons are used without dropping out. However, because all neurons can attend to the same input in the subsequent layers, their outputs might be changed. To compensate for this, the outputs of the neurons are scaled by the “keep probability” <span class="math notranslate nohighlight">\(1-p\)</span> to maintain the distribution of the outputs.
$<span class="math notranslate nohighlight">\(
y = (1-p) x
\)</span>$</p></li>
</ol>
<p>This technique effectively forces the network to learn redundant representations, as it cannot rely on any single neuron being present. During inference, all neurons are used, but their outputs are scaled by the dropout probability to maintain consistent expected values.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>An alternative way to implement dropout is so-called <em>inverse dropout</em>, where, instead of scaling the input <span class="math notranslate nohighlight">\(x\)</span>, scale the weights of the neurons by <span class="math notranslate nohighlight">\(1/(1-p)\)</span> <strong>during training</strong>. During inference, no scaling is applied to both the input and the weights. This is what is used in TensorFlow.</p>
</div>
</section>
<section id="local-response-normalization-lrn">
<h3>Local Response Normalization (LRN)<a class="headerlink" href="#local-response-normalization-lrn" title="Link to this heading">#</a></h3>
<p>Similar to how our eyes adjust to see details in both bright and dark areas of an image (like being able to see both the details of a person standing in the shadows and the bright sky behind them), Local Response Normalization (LRN) helps neural networks balance and normalize feature responses across channels. LRN is a normalization technique for CNNs originally proposed in AlexNet <a class="footnote-reference brackets" href="#footcite-krizhevsky2012alexnet" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> that normalizes feature map responses across adjacent channels to enhance local contrast.</p>
<p>The LRN operation for a given activity <span class="math notranslate nohighlight">\(a^i_{x,y}\)</span> at position <span class="math notranslate nohighlight">\((x,y)\)</span> in the <span class="math notranslate nohighlight">\(i\)</span>-th channel of the feature map was defined as:</p>
<div class="math notranslate nohighlight">
\[
b^i_{x,y} = a^i_{x,y} / \left(k + \alpha \sum_{j=\max(0,i-n/2)}^{\min(N-1,i+n/2)} (a^j_{x,y})^2\right)^\beta
\]</div>
<p>where the constants <span class="math notranslate nohighlight">\(k\)</span>, <span class="math notranslate nohighlight">\(n\)</span>, <span class="math notranslate nohighlight">\(\alpha\)</span>, and <span class="math notranslate nohighlight">\(\beta\)</span> were determined through validation. Note that this is the channel-wise normalization. See the original paper for more details.</p>
<p>LRN normalizes each neuron’s response based on the activity of its neighboring neurons across the channels. It may improve CNN generalization by making features locally contrast-invariant and potentially stabilizing training. However, LRN has limitations, one of which is that it can only be applied to multi-channel visual features and might potentially reduce discriminative power by altering the overall representation.
A remedy for this is Local Contrast Normalization (LCN) , which normalizes within a local window of a single channel.</p>
</section>
</section>
<section id="architecture-and-implementation">
<h2>Architecture and Implementation<a class="headerlink" href="#architecture-and-implementation" title="Link to this heading">#</a></h2>
<p>AlexNet’s architecture consisted of five convolutional layers followed by three fully connected layers.</p>
<figure class="align-center" id="id10">
<a class="reference internal image-reference" href="../_images/alexnet-architecture.jpg"><img alt="../_images/alexnet-architecture.jpg" src="../_images/alexnet-architecture.jpg" style="width: 50%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 65 </span><span class="caption-text">Detailed architecture diagram showing the parallel GPU implementation</span><a class="headerlink" href="#id10" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The network was split across two GPUs due to memory constraints of the hardware available at the time:</p>
<ul class="simple">
<li><p><strong>Input image</strong>: [224 x 224] normalized, 3-channel color image (with color whitening, section 3.2.1)</p></li>
<li><p><strong>Conv1</strong>: Convolutional layer - [11 x 11] kernel x 96 channels, stride = 4, padding = 2</p></li>
<li><p><strong>Activation function</strong>: ReLU + Local Response Normalization (section 3.2.2)</p></li>
<li><p><strong>P1</strong>: Pooling layer - Overlapping max pooling, [3 x 3] kernel, stride = 2</p></li>
<li><p><strong>Conv2</strong>: Convolutional layer - [5 x 5] kernel x 256 channels, stride = 1, padding = 2</p></li>
<li><p><strong>Activation function</strong>: ReLU + Local Response Normalization (section 3.2.2)</p></li>
<li><p><strong>P2</strong>: Pooling layer - Overlapping max pooling, [3 x 3] kernel, stride = 2</p></li>
<li><p><strong>Conv3</strong>: Convolutional layer - [3 x 3] kernel x 384 channels, stride = 1, padding = 1</p></li>
<li><p><strong>Conv4</strong>: Convolutional layer - [3 x 3] kernel x 384 channels, stride = 1, padding = 1</p></li>
<li><p><strong>Conv5</strong>: Convolutional layer - [3 x 3] kernel x 256 channels, stride = 1, padding = 1</p></li>
<li><p><strong>P3</strong>: Pooling layer - Overlapping max pooling, [3 x 3] kernel, stride = 2</p></li>
<li><p>(During training only: <strong>Dropout</strong>)</p></li>
<li><p><strong>FC6</strong>: Fully connected layer - 9216 (= 256 x 6 x 6) → 4096</p></li>
<li><p><strong>Activation function</strong>: ReLU</p></li>
<li><p>(During training only: <strong>Dropout</strong>)</p></li>
<li><p><strong>FC7</strong>: Fully connected layer - 4096 → 4096</p></li>
<li><p><strong>Activation function</strong>: ReLU</p></li>
<li><p><strong>FC8</strong>: Fully connected layer - 4096 → 1000</p></li>
<li><p><strong>Output</strong>: 1000-dimensional vector probability distribution (output probability for each dimension) using softmax function</p></li>
</ul>
<p>It appears to be complicated. However, the architecture follows a clear pattern common to many convolutional neural networks.</p>
<ol class="arabic simple">
<li><p>It starts with convolutional layers that extract features from the input image, followed by pooling layers that reduce spatial dimensions while preserving important information.</p></li>
<li><p>The network begins with larger kernels (11x11, 5x5) to capture broad features and transitions to smaller ones (3x3) for more detailed features.</p></li>
<li><p>The three consecutive 3x3 convolutions (Conv3, Conv4, Conv5) effectively create a larger receptive field while using fewer parameters than a single large kernel.</p></li>
<li><p>The network then flattens the spatial features and passes them through fully connected layers, progressively reducing dimensions from 9216 to 4096 and finally to 1000 classes.</p></li>
<li><p>The use of ReLU activation functions helps prevent vanishing gradients, while Local Response Normalization and Dropout serve as regularization techniques to prevent overfitting.</p></li>
</ol>
<p>AlexNet consists of two paths that are parallelly executed on two GPUs. This is to reduce the memory footprint of the network, since at that time, the memory of a single GPU was insufficient for the network (i.e., 3GB of memory for a single GPU).</p>
<ul class="simple">
<li><p>The first Convolutional layer (Conv1) produces a feature map of 96 channels, which are split into two halves, where 48 channels for each GPU. The pooling and normalization are applied to each half.</p></li>
<li><p>The second Convolutional layer (Conv2) independently processes the feature maps of the two GPUs, producing two 128-channel feature maps.</p></li>
<li><p>In the third Convolutional layer (Conv3), the feature maps of the two GPUs interact with each other, producing two 192-channel feature maps, each of which lives on a different GPU.</p></li>
<li><p>The fourth and fifth Convolutional layer (Conv4 and Conv5) independently processes the feature maps of the two GPUs.</p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The use of GPUs for training neural networks was not new, but AlexNet demonstrated their practical necessity for training large-scale networks on real-world datasets. This helped establish GPU computing as a cornerstone of modern deep learning.</p>
</div>
</section>
<section id="implementation">
<h2>Implementation<a class="headerlink" href="#implementation" title="Link to this heading">#</a></h2>
<p>Interested in implementing AlexNet? You can find the implementation in
<a class="reference external" href="https://www.digitalocean.com/community/tutorials/alexnet-pytorch">Writing AlexNet from Scratch in PyTorch | DigitalOcean</a></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "skojaku/applied-soft-comp",
            ref: "gh-pages",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./m04-image-processing"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="lenet.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">LeNet and LeNet-5: Pioneering CNN Architectures</p>
      </div>
    </a>
    <a class="right-next"
       href="vgg.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">VGGNet - A Deep Convolutional Neural Network for Image Recognition</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-imagenet-challenge-and-the-deep-learning-revolution">The ImageNet Challenge and the Deep Learning Revolution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-innovations-of-alexnet">Key Innovations of AlexNet</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relu-activation-function">ReLU Activation Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dropout-regularization">Dropout Regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#local-response-normalization-lrn">Local Response Normalization (LRN)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture-and-implementation">Architecture and Implementation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">Implementation</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sadamori Kojaku
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>